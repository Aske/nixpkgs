#
# Copyright (c) 2012, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains features to parse an audit log file, including
searching and displaying the results.
"""

import re


class AuditLogParser(AuditLogReader):
    """The AuditLogParser class is used to parse the audit log file, applying
    search criterion and filtering the logged data.
    """

    def __init__(self, options):
        """Constructor

        options[in]       dictionary of options (e.g. log_name and verbosity)
        """
        self.options = options
        AuditLogReader.__init__(self, options)
        self.header_rows = []
        self.connects = []
        self.rows = []
        self.connection_ids = []

        # Compile regexp pattern
        self.regexp_pattern = None
        if self.options['pattern']:
            try:
                self.regexp_pattern = re.compile(self.options['pattern'])
            except:
                raise UtilError("Invalid Pattern: " + self.options['pattern'])

        # Add a space after the query type to reduce false positives.
        # Note: Although not perfect, this simple trick considerably reduce
        # false positives, avoiding the use of complex regex (with lower
        # performance).
        self.match_qtypes = []  # list of matching SQL statement/command types.
        self.regexp_comment = None
        self.regexp_quoted = None
        self.regexp_backtick = None
        if self.options['query_type']:
            # Generate strings to match query types
            for qt in self.options['query_type']:
                if qt == "commit":
                    # COMMIT is an exception (can appear alone without spaces)
                    self.match_qtypes.append(qt)
                else:
                    self.match_qtypes.append("{0} ".format(qt))
            # Compile regexp to match comments (/*...*/) to be ignored/removed.
            self.regexp_comment = re.compile(r'/\*.*?\*/', re.DOTALL)
            # Compile regexp to match single quoted text ('...') to be ignored.
            self.regexp_quoted = re.compile(r"'.*?'", re.DOTALL)
            # Compile regexp to match text between backticks (`) to be ignored.
            self.regexp_backtick = re.compile(r'`.*?`', re.DOTALL)

    def parse_log(self):
        """Parse audit log records, apply search criteria and store results.
        """
        # Find and store records matching search criteria
        for record, line in self.get_next_record():
            name = record.get("NAME")
            name_case = name.upper()
            # The variable matching_record is used to avoid unnecessary
            # executions the match_* function of the remaining search criteria
            # to check, as it suffice that one match fails to not store the
            # records in the results. This implementation technique was applied
            # to avoid the use of too deep nested if-else statements that will
            # make the code more complex and difficult to read and understand,
            # trying to optimize the execution performance.
            matching_record = True
            if name_case == 'AUDIT':
                # Store audit start record
                self.header_rows.append(record)

            # Apply filters and search criteria
            if self.options['users']:
                self._track_new_users_connection_id(record, name_case)
                # Check if record matches users search criteria
                if not self.match_users(record):
                    matching_record = False

            # Check if record matches event type criteria
            if (matching_record and self.options['event_type'] and
                    not self.match_event_type(record,
                                              self.options['event_type'])):
                matching_record = False

            # Check if record matches status criteria
            if (matching_record and self.options['status'] and
                    not self.match_status(record, self.options['status'])):
                matching_record = False

            # Check if record matches datetime range criteria
            if (matching_record and
                    not self.match_datetime_range(record,
                                                  self.options['start_date'],
                                                  self.options['end_date'])):
                matching_record = False

            # Check if record matches query type criteria
            if (matching_record and self.options['query_type'] and
                    not self.match_query_type(record)):
                matching_record = False

            # Search attributes values for matching pattern
            if (matching_record and self.regexp_pattern and
                    not self.match_pattern(record)):
                matching_record = False

            # Store record into resulting rows (i.e., survived defined filters)
            if matching_record:
                if self.options['format'] == 'raw':
                    self.rows.append(line)
                else:
                    self.rows.append(record)

    def retrieve_rows(self):
        """Retrieve the resulting entries from the log parsing process
        """
        return self.rows if self.rows != [] else None

    def _track_new_users_connection_id(self, record, name_upper):
        """Track CONNECT records and store information of users and associated
        connection IDs.
        """
        user = record.get("USER", None)
        priv_user = record.get("PRIV_USER", None)

        # Register new connection_id (and corresponding user)
        if (name_upper.upper() == "CONNECT" and
                (user and (user in self.options['users'])) or
                (priv_user and (priv_user in self.options['users']))):
            self.connection_ids.append((user, priv_user,
                                        record.get("CONNECTION_ID")))

    def match_users(self, record):
        """Match users.

        Check if the given record match the user search criteria.
        Returns True if the record matches one of the specified users.

        record[in] audit log record to check
        """
        for con_id in self.connection_ids:
            if record.get('CONNECTION_ID', None) == con_id[2]:
                # Add user columns
                record['USER'] = con_id[0]
                record['PRIV_USER'] = con_id[1]
                # Add server_id column
                if self.header_rows:
                    record['SERVER_ID'] = self.header_rows[0]['SERVER_ID']
                return True
        return False

    @staticmethod
    def match_datetime_range(record, start_date, end_date):
        """Match date/time range.

        Check if the given record match the datetime range criteria.
        Returns True if the record matches the specified date range.

        record[in] audit log record to check;
        start_date[in] start date/time of the record (inclusive);
        end_date[in] end date/time of the record (inclusive);
        """
        if (start_date and (record.get('TIMESTAMP', None) < start_date)) or \
           (end_date and (end_date < record.get('TIMESTAMP', None))):
            # Not within datetime range
            return False
        else:
            return True

    def match_pattern(self, record):
        """Match REGEXP pattern.

        Check if the given record matches the defined pattern.
        Returns True if one of the record values matches the pattern.

        record[in] audit log record to check;
        """
        for val in record.values():
            if val and self.regexp_pattern.match(val):
                return True
        return False

    def match_query_type(self, record):
        """Match query types.

        Check if the given record matches one of the given query types.
        Returns True if the record possesses a SQL statement/command that
        matches one of the query types from the given list of query types.

        record[in]          audit log record to check;
        """
        sqltext = record.get('SQLTEXT', None)
        if sqltext:
            # Ignore (i.e., remove) comments in query.
            if self.regexp_comment:
                sqltext = re.sub(self.regexp_comment, '', sqltext)
            # Ignore (i.e., remove) quoted text in query.
            if self.regexp_quoted:
                sqltext = re.sub(self.regexp_quoted, '', sqltext)
            # Ignore (i.e., remove) names quoted with backticks in query.
            if self.regexp_backtick:
                sqltext = re.sub(self.regexp_backtick, '', sqltext)
            # Search query types strings inside text.
            sqltext = sqltext.lower()
            for qtype in self.match_qtypes:
                # Handle specific query-types to avoid false positives.
                if (qtype.startswith('set') and
                        ('insert ' in sqltext or 'update ' in sqltext)):
                    # Do not match SET in INSERT or UPDATE queries.
                    continue
                if (qtype.startswith('prepare') and
                        ('drop ' in sqltext or 'deallocate ' in sqltext)):
                    # Do not match PREPARE in DROP or DEALLOCATE queries.
                    continue
                # Check if query type is found.
                if qtype in sqltext:
                    return True
        return False

    @staticmethod
    def match_event_type(record, event_types):
        """Match audit log event/record type.

        Check if the given record matches one of the given event types.
        Returns True if the record type (i.e., logged event) matches one of the
        types from the given list of event types.

        record[in] audit log record to check;
        event_types[in] list of matching record/event types;
        """
        name = record.get('NAME').lower()
        return(name in event_types)

    @staticmethod
    def match_status(record, status_list):
        """Match the record status.

        Check if the given record match the specified status criteria.

        record[in]          audit log record to check;
        status_list[in]     list of status values or intervals (representing
                            MySQL error codes) to match;

        Returns True if the record status matches one of the specified values
        or intervals in the list.
        """
        rec_status = record.get('STATUS', None)
        if rec_status:
            rec_status = int(rec_status)
            for status_val in status_list:
                # Check if the status value is an interval (tuple) or int
                if isinstance(status_val, tuple):
                    # It is an interval; Check if it contains the record
                    # status.
                    if status_val[0] <= rec_status <= status_val[1]:
                        return True
                else:
                    # Directly check if the status match (is equal).
                    if rec_status == status_val:
                        return True
        return False
#
# Copyright (c) 2012, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains the methods for reading the audit log.
"""

import os
import xml.etree.ElementTree as xml


# Import appropriate XML exception to be compatible with python 2.6.
try:
    # Exception only available from python 2.7 (i.e., ElementTree 1.3)
    # pylint: disable=E0611,C0411
    from xml.etree.ElementTree import ParseError
except ImportError:
    # Instead use ExpatError for earlier python versions.
    # pylint: disable=C0411
    from xml.parsers.expat import ExpatError as ParseError


# Fields for the old format.
_MANDATORY_FIELDS = ['NAME', 'TIMESTAMP']
_OPTIONAL_FIELDS = ['CONNECTION_ID', 'DB', 'HOST', 'IP', 'MYSQL_VERSION',
                    'OS_LOGIN', 'OS_VERSION', 'PRIV_USER', 'PROXY_USER',
                    'SERVER_ID', 'SQLTEXT', 'STARTUP_OPTIONS', 'STATUS',
                    'USER', 'VERSION']

# Fields for the new format.
_NEW_MANDATORY_FIELDS = _MANDATORY_FIELDS + ['RECORD_ID']
_NEW_OPTIONAL_FIELDS = _OPTIONAL_FIELDS + ['COMMAND_CLASS', 'STATUS_CODE']


class AuditLogReader(object):
    """The AuditLogReader class is used to read the data stored in the audit
    log file. This class provide methods to open the audit log, get the next
    record, and close the file.
    """

    def __init__(self, options=None):
        """Constructor

        options[in]       dictionary of options (e.g. log_name and verbosity)
        """
        if options is None:
            options = {}
        self.verbosity = options.get('verbosity', 0)
        self.log_name = options.get('log_name', None)
        self.log = None
        self.tree = None
        self.root = None
        self.remote_file = False

    def __del__(self):
        """Destructor
        """
        if self.remote_file:
            os.unlink(self.log_name)

    def open_log(self):
        """Open the audit log file.
        """
        # Get the log from a remote server
        # TODO : check to see if the log is local. If not, attempt
        #        to log into the server via rsh and copy the file locally.
        self.remote_file = False
        if not self.log_name or not os.path.exists(self.log_name):
            raise UtilError("Cannot read log file '%s'." % self.log_name)
        self.log = open(self.log_name)

    def close_log(self):
        """Close the previously opened audit log.
        """
        self.log.close()

    @staticmethod
    def _validXML(line):
        """Check if line is a valid XML element, apart from audit records.
        """
        return (('<?xml ' in line) or
                ('<AUDIT>' in line) or ('</AUDIT>' in line))

    def get_next_record(self):
        """Get the next audit log record.

        Generator function that return the next audit log record.
        More precisely, it returns a tuple with a formatted record dict and
        the original record.
        """
        next_line = ""
        new_format = False
        multiline = False
        for line in self.log:
            if line.lstrip().startswith('<AUDIT_RECORD>'):
                # Found first record line in the new format.
                new_format = True
                multiline = True
                next_line = line
                continue
            elif (line.lstrip().startswith('<AUDIT_RECORD') and
                  not line.endswith('/>\n')):
                # Found (first) record line in the old format.
                next_line = "{0} ".format(line.strip('\n'))
                if not line.endswith('/>\n'):
                    multiline = True
                    continue
            elif multiline:
                if ((new_format and
                     line.strip().endswith('</AUDIT_RECORD>')) or
                        (not new_format and line.endswith('/>\n'))):
                    # Detect end of record in the old and new format and
                    # append last record line.
                    next_line += line
                else:
                    if not line.strip().startswith('<'):
                        # Handle SQL queries broke into multiple lines,
                        # removing newline characters.
                        next_line = '{0}{1}'.format(next_line.strip('\n'),
                                                    line.strip('\n'))
                    else:
                        next_line += line
                    continue
            else:
                next_line += line
            log_entry = next_line
            next_line = ""
            try:
                yield (
                    self._make_record(xml.fromstring(log_entry), new_format),
                    log_entry
                )
            except (ParseError, SyntaxError):
                # SyntaxError is also caught for compatibility reasons with
                # python 2.6. In case an ExpatError which does not inherits
                # from SyntaxError is used as a ParseError.
                if not self._validXML(log_entry):
                    raise UtilError("Malformed XML - Cannot parse log file: "
                                    "'{0}'\nInvalid XML element: "
                                    "{1!r}".format(self.log_name, log_entry))

    @staticmethod
    def _do_replacements(old_str):
        """Replace special masked characters.
        """
        new_str = old_str.replace("&lt;", "<")
        new_str = new_str.replace("&gt;", ">")
        new_str = new_str.replace("&quot;", '"')
        new_str = new_str.replace("&amp;", "&")
        return new_str

    def _make_record(self, node, new_format=False):
        """Make a dictionary record from the node element.

        The given node is converted to a dictionary record, reformatting
        as needed for the special characters.

        node[in]        XML node holding a single audit log record.
        new_format[in]  Flag indicating if the new XML format is used for the
                        audit log record. By default False (old format used).

        Return a dictionary with the data in the given audit log record.
        """
        if new_format:
            # Handle audit record in the new format.
            # Do mandatory fields.
            # Note: Use dict constructor for compatibility with Python 2.6.
            record = dict((field, node.find(field).text)
                          for field in _NEW_MANDATORY_FIELDS)
            # Do optional fields.
            for field in _NEW_OPTIONAL_FIELDS:
                field_node = node.find(field)
                if field_node is not None and field_node.text:
                    record[field] = self._do_replacements(field_node.text)
        else:
            # Handle audit record in the old format.
            # Do mandatory fields.
            # Note: Use dict constructor for compatibility with Python 2.6.
            record = dict((field, node.get(field))
                          for field in _MANDATORY_FIELDS)
            # Do optional fields.
            for field in _OPTIONAL_FIELDS:
                if node.get(field, None):
                    record[field] = self._do_replacements(node.get(field))
        return record
#
# Copyright (c) 2014, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains common features to manage and handle binary log files.
"""
import io
import errno
import os
import shutil
import time

from datetime import datetime


LOG_TYPES = ['bin', 'relay', 'all']
LOG_TYPE_BIN = LOG_TYPES[0]
LOG_TYPE_RELAY = LOG_TYPES[1]
LOG_TYPE_ALL = LOG_TYPES[2]

_DAY_IN_SECONDS = 86400


def is_binary_log_filename(filename, log_type=LOG_TYPE_ALL, basename=None):
    """Check if the filename matches the name format for binary log files.

    This function checks if the given filename corresponds to the filename
    format of known binary log files, according to the specified log_type and
    optional basename. The file extension is a sequence number (.nnnnnn). If
    a basename is given then the filename for the binary log file must have
    the format 'basename.nnnnnn'. Otherwise the default filename is assumed,
    depending on the log_type: '*-bin.nnnnnn' for the 'bin' log type,
    '*-relay-bin.nnnnnn' for the 'relay' log type, and both for the 'all' type.

    filename[in]    Filename to check.
    log_type[in]    Type of the binary log, must be one of the following
                    values: 'bin' for binlog files, 'relay' for relay log
                    files, 'all' for both binary log files. By default = 'all'.
    basename[in]    Basename defined for the binary log file. None by default,
                    meaning that the default server name formats are assumed
                    (according to the given log type).
    """
    # Split file basename and extension.
    f_base, f_ext = os.path.splitext(filename)
    f_ext = f_ext[1:]  # remove starting dot '.'

    # Check file basename.
    if basename:
        if f_base != basename:
            # Defined basename does not match.
            return False
    else:
        # Check default serve basename for the given log_type.
        if log_type == LOG_TYPE_BIN:
            # *-bin.nnnnnn (excluding *-relay-bin.nnnnnn)
            if not f_base.endswith('-bin') or f_base.endswith('-relay-bin'):
                return False
        elif log_type == LOG_TYPE_RELAY:
            # *-relay-bin.nnnnnn
            if not f_base.endswith('-relay-bin'):
                return False
        elif log_type == LOG_TYPE_ALL:
            # *-bin.nnnnnn (including *-relay-bin.nnnnnn)
            if not f_base.endswith('-bin'):
                return False
        else:
            raise UtilError("Unsupported log-type: {0}".format(log_type))

    # Check file extension.
    try:
        int(f_ext)
    except ValueError:
        # Extension is not a sequence number (error converting to integer).
        return False

    # Return true if basename and extension checks passed.
    return True


def get_index_file(source, binary_log_file):
    """ Find the binary log index file.

    Search the index file in the specified source directory for the given
    binary log file and retrieve its location (i.e., full path).

    source[in]              Source directory to search for the index file.
    binary_log_file[in]     Binary log file associated to the index file.

    Return the location (full path) of the binary log index file.
    """
    f_base, _ = os.path.splitext(binary_log_file)
    index_filename = '{0}.index'.format(f_base)
    index_file = os.path.join(source, index_filename)
    if os.path.isfile(index_file):
        return index_file
    else:
        raise UtilError("Unable to find the index file associated to file "
                        "'{0}'.".format(binary_log_file))


def filter_binary_logs_by_sequence(filenames, seq_list):
    """Filter filenames according to the given sequence number list.

    This function filters the given list of filenames according to the given
    sequence number list, excluding the filenames that do not match.

    Note: It is assumed that given filenames are valid binary log files.
    Use is_binary_log_filename() to check each filenames.

    filenames[in]   List of binary log filenames to check.
    seq_list[in]    List of allowed sequence numbers or intervals.
                    For example: 3,5-12,16,21.

    Returns a list of the filenames matching the given sequence number filter.
    """
    res_list = []
    for filename in filenames:
        # Split file basename and extension.
        _, f_ext = os.path.splitext(filename)
        f_ext = int(f_ext[1:])  # remove starting dot '.' and convert to int
        for seq_value in seq_list:
            # Check if the sequence value is an interval (tuple) or int.
            if isinstance(seq_value, tuple):
                # It is an interval; Check if it contains the file sequence
                # number.
                if seq_value[0] <= f_ext <= seq_value[1]:
                    res_list.append(filename)
                    break
            else:
                # Directly check if the sequence numbers match (are equal).
                if f_ext == seq_value:
                    res_list.append(filename)
                    break

    # Retrieve the resulting filename list (filtered by sequence number).
    return res_list


def filter_binary_logs_by_date(filenames, source, max_date):
    """Filter filenames according their last modification date.

    This function filters the given list of files according to their last
    modification date, excluding those with the last change before the given
    max_date.

    Note: It is assumed that given filenames are valid binary log files.
    Use is_binary_log_filename() to check each filename.

    filenames[in]   List of binary log filenames to check.
    source[in]      Source directory where the files are located.
    max_date[in]    Maximum modification date, in the format 'yyyy-mm-dd' or
                    'yyyy-mm-ddThh:mm:ss', or number of days since the last
                    modification.

    Returns a list of the filenames not changed within the given elapsed days
    (i.e., recently changed files will be excluded).
    """
    res_list = []
    # Compute maximum modified date/time, according to supported formats.
    try:
        elapsed_days = int(max_date)
    except ValueError:
        # Max date is not a valid integer (i.e., number of days).
        elapsed_days = None
    if elapsed_days:  # Process the specified number fo days
        if elapsed_days < 1:
            raise UtilError(
                "Invalid number of days (must be an integer greater than "
                "zero): {0}".format(max_date)
            )
        # Get current local time.
        ct_tuple = time.localtime()
        # Set time to 00:00:00.
        ct_list = list(ct_tuple)
        ct_list[3] = 0  # hours
        ct_list[4] = 0  # minutes
        ct_list[5] = 0  # seconds
        ct_tuple_0000 = tuple(ct_list)
        # Get seconds since epoch for the current day at 00:00.
        day_start_time = time.mktime(ct_tuple_0000)
        # Compute max modified date based on elapsed days ignoring time, i.e.,
        # 00:00 is used as reference to count days. Current day count as one.
        max_time = day_start_time - (_DAY_IN_SECONDS * (elapsed_days - 1))
        max_date = time.strftime('%Y-%m-%dT%H:%M:%S', time.localtime(max_time))
    else:  # Process the specified date
        # Check the date format.
        _, _, time_val = max_date.partition('T')
        if time_val:
            try:
                dt_max_date = datetime.strptime(max_date, '%Y-%m-%dT%H:%M:%S')
            except ValueError:
                raise UtilError(
                    "Invalid date/time format (yyyy-mm-ddThh:mm:ss): "
                    "{0}".format(max_date)
                )
        else:
            try:
                dt_max_date = datetime.strptime(max_date, '%Y-%m-%d')
            except ValueError:
                raise UtilError(
                    "Invalid date format (yyyy-mm-dd): {0}".format(max_date)
                )
        max_date = dt_max_date.strftime('%Y-%m-%dT%H:%M:%S')

    # Check modified date for each file.
    for filename in filenames:
        source_file = os.path.join(source, filename)
        modified_time = os.path.getmtime(source_file)
        modified_date = time.strftime('%Y-%m-%dT%H:%M:%S',
                                      time.localtime(modified_time))
        if modified_date < max_date:
            res_list.append(filename)

    # Retrieve the resulting filename list (filtered by modified date).
    return res_list


def move_binary_log(source, destination, filename, log_index,
                    undo_on_error=True):
    """Move a binary log file to a specific destination.

    This method move the given binary log file (filename), located in the
    source directory, to the specified destination directory and updates the
    respective index file accordingly.

    Note: An error is raised if any issue occurs during the process.
    Additionally, if the undo_on_error=True (default) then the file is moved
    back to the source directory if an error occurred while updating the index
    file (keeping the file in the original location and the index file
    unchanged). Otherwise the file might be moved and the index file not
    correctly updated. In either cases an error is issued.

    source[in]          Source directory where the binary log file is located.
    destination[in]     Destination directory to move the binary log.
    filename[in]        Name of the binary log file to move.
    log_index[in]       Location (full path) of the binary log index file.
    undo_on_error[in]   Flag to undo the file move if an error occurs (when
                        updating the index file) or not. By default = True,
                        meaning that the move operation is reverted ().
    """
    def _move_file_back():
        """Try to move the file back to its original source directory.
        Returns a warning message indicating if the file was moved back
        successfully or not.
        """
        try:
            # Move file back to source directory.
            destination_file = os.path.join(destination, filename)
            shutil.move(destination_file, source)
        except (IOError, shutil.Error) as move_err:
            # Warn the user that an error occurred while trying to
            # move the file back.
            return ("\nWARNING: Failed to move file back to source directory: "
                    "{0}").format(move_err)
        else:
            # Notify user that the file was successfully moved back.
            return "\nWARNING: File move aborted."

    # Move file to destination directory.
    source_file = os.path.join(source, filename)
    if os.path.isdir(destination):
        shutil.move(source_file, destination)
    else:
        # Raise an error if the destination dir does not exist.
        # Note: To be consistent with the IOError raised by shutil.move() if
        # the source file does not exist.
        raise IOError(errno.ENOENT, "No such destination directory",
                      destination)

    # Update index file.
    found_pos = None
    try:
        with io.open(log_index, 'r') as index_file:
            # Read all data from index file.
            data = index_file.readlines()
            # Search for the binary log file entry.
            for pos, line in enumerate(data):
                if line.strip().endswith(filename):
                    found_pos = pos
                    break
            if found_pos is not None:
                # Replace binary file entry with absolute destination path.
                data[found_pos] = u'{0}\n'.format(
                    os.path.join(destination, filename)
                )
            else:
                warning = ""  # No warning if undo_on_error = False.
                if undo_on_error:
                    warning = _move_file_back()
                # Raise error (including cause).
                raise UtilError("Entry for file '{0}' not found in index "
                                "file: {1}{2}".format(filename, log_index,
                                                      warning))
            # Create a new temporary index_file with the update entry.
            # Note: original file is safe is something goes wrong during write.
            tmp_file = '{0}.tmp'.format(log_index)
            try:
                with io.open(tmp_file, 'w', newline='\n') as tmp_index_file:
                    tmp_index_file.writelines(data)
            except IOError as err:
                warning = ""  # No warning if undo_on_error = False.
                if undo_on_error:
                    warning = _move_file_back()
                # Raise error (including cause).
                raise UtilError('Unable to write temporary index file: '
                                '{0}{1}'.format(err, warning))
    except IOError as err:
        warning = ""  # No warning if undo_on_error = False.
        if undo_on_error:
            warning = _move_file_back()
        # Raise error (including cause).
        raise UtilError('Failed to update index file: '
                        '{0}{1}'.format(err, warning))
    # Replace the original index file with the new one.
    if os.name == 'posix':
        os.rename(tmp_file, log_index)
    else:
        # On windows, rename does not work if the target file already exists.
        shutil.move(tmp_file, log_index)
#
# Copyright (c) 2014, 2016 Oracle and/or its affiliates. All rights
# reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains the binary log administrative operations purge and rotate
operations.
"""

import os



def get_binlog_info(server, reporter=None, server_name="server", verbosity=0):
    """Get binlog information from the server.

    This method queries the server for binary log information as the binlog
    base name, binlog file name and the active binlog file index.
    Note: An error is raised in case the binlog information can not be retried.

    server[in]       Source instance server to obtain information from.
    reporter[in]     Method to invoke to report messages.
    server_name[in]  Name of server to use when reporting. Default "server".
    verbosity[in]    Level of verbosity for report purposes.

    Returns a tuple with the active binlog base name, file name and index.
    """

    res = server.show_server_variable('log_bin_basename')
    binlog_b_name = None
    if res and res[0][1]:
        binlog_basename_path = res[0][1]
        if reporter is not None and verbosity >= 3:
            reporter("# Binary log basename path: {0}"
                     "".format(binlog_basename_path))
        binlog_b_name = os.path.basename(binlog_basename_path)
        if reporter is not None and verbosity >= 3:
            reporter("# Binary log basename: {0}"
                     "".format(binlog_b_name))

    res = server.exec_query("SHOW MASTER STATUS")

    if not res:
        raise UtilError("Unable to get binlog information from {0} at {1}:{2}"
                        "".format(server_name, server.host, server.port))
    else:
        master_active_binlog_file = res[0][0]

        master_active_binlog_index = int(res[0][0].split('.')[1])

        if binlog_b_name is None:
            binlog_b_name = res[0][0].split('.')[0]
            if reporter is not None and verbosity >= 3:
                reporter("# Binary log basename: {0}"
                         "".format(binlog_b_name))

        if reporter is not None and verbosity > 0:
            reporter("# {server_name} active binlog file: {act_log}"
                     "".format(server_name=server_name.capitalize(),
                               act_log=master_active_binlog_file))

    return (binlog_b_name, master_active_binlog_file,
            master_active_binlog_index)


def determine_purgeable_binlogs(active_binlog_index, slaves, reporter,
                                verbosity=0):
    """Determine the purgeable binary logs.

    This method will look at each slave given and will determinate the lowest
    binary log file that is being in use.

    active_binlog_index[in]    Index of binlog currently in use by the
                               master server or the higher binlog index value
                               it wants to be purged.
    slaves[in]                 Slaves list.
    reporter[in]               Method to call to report.
    verbosity[in]              The verbosity level for reporting information.

    Returns the last index in use by the slaves, that is the newest binlog
    index that has between read by all the slave servers.
    """
    # Determine old no needed binlogs
    master_log_file_in_use = []
    index_last_in_use = active_binlog_index
    # pylint: disable=R0101
    if slaves:
        for slave in slaves:
            if reporter is not None and verbosity >= 1:
                reporter("# Checking slave: {0}@{1}"
                         "".format(slave['host'], slave['port']))

            res = slave['instance'].get_status()

            if res:
                master_log_file = res[0][5]

                if reporter is not None and verbosity >= 1:
                    reporter("# I/O thread is currently reading: {0}"
                             "".format(master_log_file))
                master_log_file_in_use.append(master_log_file)
                reading_index_file = int(master_log_file.split('.')[1])

                if index_last_in_use > reading_index_file:
                    index_last_in_use = reading_index_file

                if reporter is not None and verbosity >= 2:
                    reporter("# File position of the I/O thread: {0}"
                             "".format(res[0][6]))
                    reporter("# Master binlog file with last event executed "
                             "by the SQL thread: {0}".format(res[0][9]))
                    reporter("# I/O thread running: {0}".format(res[0][10]))
                    reporter("# SQL thread running: {0}".format(res[0][11]))
                    if len(res[0]) > 52:
                        if res[0][51]:
                            reporter("# Retrieved GTid_Set: {0}"
                                     "".format(res[0][51]))
                        if res[0][52]:
                            reporter("# Executed GTid_Set: {0}"
                                     "".format(res[0][52]))
        return index_last_in_use
    else:
        raise UtilError("None Slave is connected to master")


def purge(server, purge_to_binlog, server_binlogs_list=None,
          reporter=None, dryrun=False, verbosity=0):
    """Purge the binary log for the given server.

    This method purges all the binary logs from the given server that are older
    than the given binlog file name specified by purge_to_binlog. The method
    can receive a list of the binary logs listed on the server to avoid
    querying the server again for this list. If The given purge_to_binlog is
    not listed on the server_binlogs_list the purge will not occur. For
    reporting capabilities if given the method report will be invoked to
    report messages and the server name that appears on the messages can be
    change with server_name.

    server[in]                server instance where to purge binlogs on
    purge_to_binlog[in]       purge binlog files older than this binlog file
                              name.
    server_binlogs_list[in]   A list of binlog files available on the given
                              server, if not given, the list will be retrieved
                              from the given server (default None).
    server_name[in]           This name will appear when reporting (default
                              'Server').
    reporter[in]              A method to invoke with messages and warnings
                              (default None).
    dryrun[in]                boolean value that indicates if the purge query
                              should be run on the server or reported only
                              (default False).
    verbosity[in]             The verbosity level for report messages.
    """
    if server_binlogs_list is None:
        server_binlogs_list = server.get_server_binlogs_list()

    # The PURGE BINARY LOGS statement deletes all the binary log files listed
    # in the log index file, prior to the specified log file name.
    # Verify purge_to_binlog is listed on server binlog list and if not is the
    # first in the list continue the purge, else there is no binlogs to purge
    if (purge_to_binlog in server_binlogs_list and
            purge_to_binlog != server_binlogs_list[0]):
        purge_query = (
            "PURGE BINARY LOGS TO '{0}'"
        ).format(purge_to_binlog)
        if dryrun:
            reporter("# To manually purge purge the binary logs Execute the "
                     "following query:")
            reporter(purge_query)
        else:
            if verbosity > 1:
                reporter("# Executing query {0}".format(purge_query))
            else:
                reporter("# Purging binary logs prior to '{0}'"
                         "".format(purge_to_binlog))
            try:
                server.exec_query(purge_query)
            except UtilDBError as err:
                raise UtilError("Unable to purge binary log, reason: {0}"
                                "".format(err.errmsg))

    else:
        reporter("# No binlog files can be purged.")


def get_active_binlog_and_size(server):
    """Retrieves the current active binlog file name and his size

    server[in]    server instance to query for the required info.

    Returns a tuple with two values, active binlog file name and his size
    """
    binlogs_list = server.get_server_binlogs_list(include_size=True)
    if binlogs_list:
        active_binlog_and_size = binlogs_list[-1]
        active_binlog = active_binlog_and_size[0]
        binlog_size = int(active_binlog_and_size[1])
        return active_binlog, binlog_size
    return None, None


def rotate(server, min_size=-1, reporter=None):
    """Rotates the binary log on the given server.

    This method rotates the active binary log from the given server, if
    min_size is given the size of the active binlog will be compared with this
    value, and rotation will only occur if the binlog size is greater than the
    given value. This method will execute the FLUSH BINARY LOGS on MySQL
    servers version 5.5.3 and greater and in older ones the FLUSH LOGS command
    to rotate the active binary log.

    server[in]      The source server instance where log rotation will occur
    min_size[in]    An integer value representing the minimum file size that
                    the active binlog must reach before rotate it. (default -1)
    reporter[in]    A method to invoke with messages and warnings.

    Returns True if the rotation command has been executed on the given server.
    """
    # Retrieve current active binlog and his file size.
    active_binlog, binlog_size = get_active_binlog_and_size(server)

    # Compare the active binlog size with the min_size
    # if the active binlog file size is greater than the min_size totate it
    # else show a Warning.
    if binlog_size >= min_size:
        if server.check_version_compat(5, 5, 3):
            type_log = "BINARY"
        else:
            type_log = ""
        server.exec_query("FLUSH {type_log} LOGS".format(type_log=type_log))
        return True
    else:
        if reporter:
            reporter("WARNING: The active binlog file '{0}' was not rotated "
                     "because it's size {1} is lower than the minimum "
                     "specified size: {2}".format(active_binlog, binlog_size,
                                                  min_size))
        return False
#
# Copyright (c) 2013, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains the charset_info class designed to read character set
and collation information from /share/charsets/index.xml.
"""

import sys


_CHARSET_INDEXES = ID, CHARACTER_SET_NAME, COLLATION_NAME, MAXLEN, IS_DEFAULT \
    = range(0, 5)

_CHARSET_QUERY = """
SELECT CL.ID,CL.CHARACTER_SET_NAME,CL.COLLATION_NAME,CS.MAXLEN, CL.IS_DEFAULT
FROM INFORMATION_SCHEMA.CHARACTER_SETS CS, INFORMATION_SCHEMA.COLLATIONS CL
WHERE CS.CHARACTER_SET_NAME=CL.CHARACTER_SET_NAME ORDER BY CHARACTER_SET_NAME
"""


class CharsetInfo(object):
    """
    Read character set information for lookup. Methods include:

      - get_charset_name(id) : get the name for a characterset id
      - get_default_collation(name) : get default collation name
      - get_name_by_collation(name) : given collation, find charset name
      - print_charsets() : print the character set map

    """

    def __init__(self, options=None):
        """Constructor

        options[in]        array of general options
        """
        if options is None:
            options = {}
        self.verbosity = options.get("verbosity", 0)
        self.format = options.get("format", "grid")
        self.server = options.get("server", None)

        self.charset_map = None

        if self.server:
            self.charset_map = self.server.exec_query(_CHARSET_QUERY)

    def print_charsets(self):
        """Print the character set list
        """
        print_list(sys.stdout, self.format,
                   ["id", "character_set_name", "collation_name",
                    "maxlen", "is_default"],
                   self.charset_map)
        print len(self.charset_map), "rows in set."

    def get_name(self, chr_id):
        """Get the character set name for the given id

        chr_id[in]     id for character set (as read from .frm file)

        Returns string - character set name or None if not found.
        """
        for cs in self.charset_map:
            if int(chr_id) == int(cs[ID]):
                return cs[CHARACTER_SET_NAME]
        return None

    def get_collation(self, col_id):
        """Get the collation name for the given id

        col_id[in]     id for collation (as read from .frm file)

        Returns string - collation name or None if not found.
        """
        for cs in self.charset_map:
            if int(col_id) == int(cs[ID]):
                return cs[COLLATION_NAME]
        return None

    def get_name_by_collation(self, colname):
        """Get the character set name for the given collation

        colname[in]    collation name

        Returns string - character set name or None if not found.
        """
        for cs in self.charset_map:
            if cs[COLLATION_NAME] == colname:
                return cs[CHARACTER_SET_NAME]
        return None

    def get_default_collation(self, col_id):
        """Get the default collation for the character set

        col_id[in]     id for collation (as read from .frm file)

        Returns tuple - (default collation id, name) or None if not found.
        """
        # Exception for utf8
        if col_id == 83:
            return "utf8_bin"
        for cs in self.charset_map:
            if int(cs[ID]) == int(col_id) and cs[IS_DEFAULT].upper() == "YES":
                return cs[COLLATION_NAME]
        return None

    def get_maxlen(self, col_id):
        """Get the maximum length for the character set

        col_id[in]     id for collation (as read from .frm file)

        Returns int - max length or 1 if not found.
        """
        for cs in self.charset_map:
            if int(cs[ID]) == int(col_id):
                return int(cs[MAXLEN])
        return int(1)
#
# Copyright (c) 2011, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains classes and functions used to manage a console utility.
"""

import os
import sys
import shlex


_COMMAND_COMPLETE = 0
_OPTION_COMPLETE = 1
_VARIABLE_COMPLETE = 2

# TODO remove this pylint disable regarding duplicate keys
# pylint: disable=W0109
_COMMAND_KEY = {
    '\x7f': 'DELETE_POSIX',
    '\x1b[3~': 'DELETE_MAC',
    '\x0a': 'ENTER_POSIX',
    '\r': 'ENTER_WIN',
    '\x1b': 'ESCAPE',
    '\x1b[A': 'ARROW_UP',
    '\x1b[B': 'ARROW_DN',
    '\x1b[C': 'ARROW_RT',
    '\x1b[D': 'ARROW_LT',
    '\t': 'TAB',
    '\x7f': 'BACKSPACE_POSIX',
    '\xe0': 'SPECIAL_WIN',
    '\x08': 'BACKSPACE_WIN',
    '\x1bOH': 'HOME',
    '\x1bOF': 'END'
}

# Some windows keys are different and require reading two keys.
# The following are the second characters.
_WIN_COMMAND_KEY = {
    'S': 'DELETE_WIN',
    'H': 'ARROW_UP',
    'P': 'ARROW_DN',
    'M': 'ARROW_RT',
    'K': 'ARROW_LT',
    'G': 'HOME',
    'O': 'END'
}

_COMMAND_COMPLETE = 0
_OPTION_COMPLETE = 1
_VARIABLE_COMPLETE = 2

# Base commands for all consoles.
#
# The list includes a tuple for each command that contains the name of the
# command, an alias (if defined) and its help text.
_BASE_COMMANDS = [
    {'name': 'help',
     'alias': 'help commands',
     'text': 'Show this list.'},
    {'name': 'exit',
     'alias': 'quit',
     'text': 'Exit the console.'},
    {'name': 'set <variable>=<value>',
     'alias': '',
     'text': 'Store a variable for recall in commands.'},
    {'name': 'show options',
     'alias': '',
     'text': 'Display list of options specified by the user on launch.'},
    {'name': 'show variables',
     'alias': '',
     'text': 'Display list of variables.'},
    {'name': '<ENTER>',
     'alias': '',
     'text': 'Press ENTER to execute command.'},
    {'name': '<ESCAPE>',
     'alias': '',
     'text': 'Press ESCAPE to clear the command entry.'},
    {'name': '<DOWN>',
     'alias': '',
     'text': 'Press DOWN to retrieve the previous command.'},
    {'name': '<UP>',
     'alias': '',
     'text': 'Press UP to retrieve the next command in history.'},
    {'name': '<TAB>',
     'alias': '',
     'text': 'Press TAB for type completion of utility, '
             'option, or variable names.'},
    {'name': '<TAB><TAB>',
     'alias': '',
     'text': 'Press TAB twice for list of matching type '
             'completion (context sensitive).'}
]


# Try to import the windows getch() if it fails, we're on Posix so define
# a custom getch() method to return keys.
try:
    # Win32
    # pylint: disable=C0413
    from msvcrt import getch  # pylint: disable=F0401
except ImportError:
    # UNIX/Posix
    # pylint: disable=C0411,C0413
    import termios

    def getch():
        """getch function
        """
        fd = sys.stdin.fileno()
        old = termios.tcgetattr(fd)
        new = termios.tcgetattr(fd)
        new[3] = new[3] & ~termios.ICANON & ~termios.ECHO
        new[6][termios.VMIN] = 1
        new[6][termios.VTIME] = 0
        termios.tcsetattr(fd, termios.TCSANOW, new)
        key = None
        try:
            key = os.read(fd, 80)
        finally:
            termios.tcsetattr(fd, termios.TCSAFLUSH, old)
        return key


class _CommandHistory(object):
    """
    The _CommandHistory class encapsulates a list of commands that can be
    retrieved either via the previous or next command in the list. The
    list grows to a circular list of max size as specified at initialization.
    """

    def __init__(self, options=None):
        """Constructor

        options[in]        Options for the class member variables
        """
        if options is None:
            options = {}
        self.position = 0
        self.commands = []
        self.max_size = options.get('max_size', 40)

    def add(self, command):
        """Add a command to the history list

        This method appends the command list if the max size is not met or
        replaces the last entry if the max size has been met.
        """
        if len(self.commands) < self.max_size:
            self.commands.append(command)
            self.position = 0
        else:
            if self.position == 0:
                self.commands[self.max_size - 1] = command
            else:
                self.commands[self.position - 1] = command

    def next(self):
        """Get next command in list.

        Returns string next command
        """
        if len(self.commands) == 0:
            return ''
        if self.position == len(self.commands) - 1:
            self.position = 0
        else:
            self.position += 1
        return self.commands[self.position]

    def previous(self):
        """Get previous command in list.

        Returns string prev command
        """
        if len(self.commands) == 0:
            return ''
        if self.position == 0:
            self.position = len(self.commands) - 1
        else:
            self.position -= 1
        return self.commands[self.position]


class _Command(object):
    """
    The _Command class encapsulates the operations of a console command line.
    """

    def __init__(self, prompt):
        """Constructor

        prompt[in]         The prompt written to the screen after each command
        """
        self.prompt = prompt
        self.position = 0
        self.command = ""
        self.length = 0

    @staticmethod
    def _erase_portion(num):
        """Erase a portion of the command line using backspace and spaces.

        num[in]            Number of spaces to erase starting from cursor left
        """
        i = 0
        while i < num:
            sys.stdout.write('\b')
            sys.stdout.write(' ')
            sys.stdout.write('\b')
            i += 1

    def get_command(self):
        """Return the current command.

        Returns string - the current command
        """
        return self.command

    def get_nearest_option(self):
        """Get the option for tab completion that is closest to the cursor

        This method returns the portion of the command line nearest the cursor
        and to the left until a space is encountered. For example, if the
        cursor was one space to the right of 'b' in some_command --verb --some
        it would return '--verb' or if the cursor is at the end of the command
        it will return the last portion of the command. In the previous
        example it would return '--some'.

        This portion is used for tab completion of options.

        Returns string - most local portion of the command.
        """
        parts = self.command.split(' ')
        # if not at the end of the command line, return the phrase where
        # the cursor is located indicated by self.position
        if self.position < self.length:
            for i in range(self.position - 1, len(parts[0]) - 1, -1):
                if self.command[i] == ' ':
                    return self.command[i + 1:self.position].strip(' ')
            return 'ERROR'
        else:
            return parts[len(parts) - 1]

    def erase_command(self):
        """Erase the command and reprint the prompt.
        """
        sys.stdout.write(' ' * (self.length - self.position))
        self._erase_portion(self.length)
        self.command = ''

    def _erase_inline(self, backspace=True):
        """Adjust command line by removing current char

        backspace[in]      If True, erase to the left (backspace)
                           If False, erase to the right
        """
        if self.position < self.length:
            num_erase = 1 + self.length - self.position
            if backspace:
                sys.stdout.write('\b')
            i = 0
            while i < num_erase:
                sys.stdout.write(' ')
                i += 1
            i = 0
            while i < num_erase:
                sys.stdout.write('\b')
                i += 1
        elif backspace:
            self._erase_portion(1)

    def home_keypress(self):
        """Executes the 'HOME' key press.

        This moves the cursor to the beginning of the command.
        """
        tmp = self.position
        self.position = 0
        sys.stdout.write('\b' * tmp)

    def end_keypress(self):
        """Executes the 'END' key press.

        This moves the cursor to the end of the command.
        """
        sys.stdout.write(self.command[self.position:self.length])
        self.position = self.length

    def delete_keypress(self):
        """Execute the 'DELETE' key press.

        This deletes one character from the right of the cursor.
        """
        if self.position < self.length:
            if self.length == 1:
                self.command = ''
                sys.stdout.write(' ')
                sys.stdout.write('\b')
                self.length = 0
            elif self.length > 0:
                self._erase_inline(False)
                old_command = self.command
                self.command = old_command[0:self.position]
                if self.position < self.length:
                    self.command += old_command[self.position + 1:]
                sys.stdout.write(self.command[self.position:])
                self.length = len(self.command)
                spaces = len(self.command[self.position:])
                # pylint: disable=W0612
                for i in range(0, spaces):
                    sys.stdout.write('\b')
                sys.stdout.flush()

    def backspace_keypress(self):
        """Execute the 'BACKSPACE' key press.

        This deletes one character to the left of the cursor.
        """
        # Here we need to move back one character calculating for in-string
        # edits (self.position < self.length)
        # if position less than length, we're inserting values
        if self.position <= 0:
            return
        if self.position < self.length:
            self._erase_inline(True)
            # build new command
            self.command = self.command[0:self.position - 1] + \
                self.command[self.position:]
            sys.stdout.write(self.command[self.position - 1:])
            i = 0
            while i < (self.length - self.position):
                sys.stdout.write('\b')
                i += 1
        else:
            self._erase_portion(1)
            self.command = self.command[0:self.length - 1]
        self.length -= 1
        self.position -= 1

    def left_arrow_keypress(self):
        """Execute the 'LEFT ARROW' keypress

        This moves the cursor position one place to the left until the
        beginning of the command.
        """
        if self.position > 0:
            self.position -= 1
            sys.stdout.write('\b')

    def right_arrow_keypress(self):
        """Execute the 'RIGHT ARROW' keypress

        This moves the cursor to the right one space until the end of the
        command.
        """
        # Here we need to move to the right one space but we don't have a
        # forward space print character. So we reprint the one character where
        # the position indicator is.
        if self.position < self.length:
            sys.stdout.write(self.command[self.position:self.position + 1])
            self.position += 1

    def replace_command(self, new_cmd):
        """Replace the command with a new command.

        This replaces the command and redisplays the prompt and new command.
        """
        if new_cmd != '':
            self._erase_portion(self.length)
            self.command = new_cmd
            sys.stdout.write(self.command)
            self.position = len(self.command)
            self.length = len(self.command)

    def add(self, key):
        """Add one or more characters to the command

        This method adds the characters specified in key to the command based
        on the location of the cursor. If in-string, the characters will be
        inserted accordingly or if at the end of the command (if cursor at
        the end).
        """
        if key is None:
            return
        # if position less than length, we're inserting values
        if self.position < self.length:
            # erase position forward.
            num_erase = self.length - self.position
            i = 0
            while i < num_erase:
                sys.stdout.write(' ')
                i += 1
            i = 0
            while i < num_erase:
                sys.stdout.write('\b')
                i += 1
            # build new command
            self.command = self.command[0:self.position] + key + \
                self.command[self.position:]
            sys.stdout.write(self.command[self.position:])
            self.position += len(key)
            # move cursor back to location at end of new key
            i = 0
            while i < (self.length - self.position + len(key)):
                sys.stdout.write('\b')
                i += 1
            self.length += len(key)
        else:
            self.command += key
            sys.stdout.write(key)
            self.position += len(key)
            self.length += len(key)

    def display_command(self):
        """Redisplay the command
        """
        sys.stdout.write(self.prompt + self.command)
        sys.stdout.flush()

    def clear(self):
        """Clear the command line - user must get the command first.
        """
        self.command = ''
        self.position = 0
        self.length = 0


class Console(object):
    """Console class
    """
    def __init__(self, new_base_commands, options):
        """Constructor

        new_base_commands  Additions to the base commands
        options[in]        Options for the class member variables
        """
        self.options = options
        self.tab_count = 0
        self.base_commands = []
        self.base_commands.extend(new_base_commands)
        self.base_commands.extend(_BASE_COMMANDS)
        self.type_complete_mode = _COMMAND_COMPLETE
        self.cmd_line = _Command(self.options.get('prompt', '> '))
        self.width = self.options.get('width', 80)
        self.commands = self.options.get("commands", None)
        self.custom_commands = self.options.get("custom", False)
        self.quiet = self.options.get("quiet", False)
        self.variables = Variables(options)
        self.history = _CommandHistory({'max_size': 20})
        self.position = 0
        self.errors = []
        var_list = self.options.get('variables', [])
        for var in var_list:
            self.variables.add_variable(var['name'], var['value'])

    def show_errors(self):
        """Show errors

        Displays the errors captured when executing an utility.
        """
        if self.quiet:
            return
        if not self.errors:
            print
            print("No errors to display.\n")
        for error in self.errors:
            print
            print("{0}\n".format(error))

    def clear_errors(self):
        """Clear errors

        Clears captured errors occurring while executing an utility.
        """
        if self.quiet:
            return
        self.errors = []
        print

    def show_last_error(self):
        """Show errors

        Displays the last error occurred when executing an utility.
        """
        if self.quiet:
            return
        if not self.errors:
            print
            print("No error to display.\n")
        else:
            print
            print("{0}\n".format(self.errors[-1]))

    def show_custom_command_help(self, arg):
        """Display the help for a custom command.

        Note: Override this method for help on custom commands.

        arg[in]            Help command argument
        """
        if self.quiet:
            return
        print "\nNo commands like '%s' exist.\n" % arg

    def do_custom_tab(self, prefix):
        """Do custom tab key processing

        Note: Override this method for tab completion for custom commands.

        prefix[in]        Prefix of the custom command
        """
        pass

    def do_custom_option_tab(self, prefix):
        """Do custom command option tab key processing

        Note: Override this method for tab completion for options for custom
        commands.

        prefix[in]        Prefix of the custom command
        """
        if self.quiet:
            return
        print "\n\nNo custom commands found.\n"

    @staticmethod
    def is_valid_custom_command(command_text):
        """Is command a valid custom command?

        This method evaluates the custom command for validity.

        Note: Override this command to determine if command_text is a valid
        custom command.

        command_text[in]   The complete command as entered by user

        Returns bool - True if valid, False if not recognized
        """
        return False  # return False by default if method not overridden

    def execute_custom_command(self, command, parameters):
        """Execute a custom command.
        Note: Override this method to execute a custom command.
        """
        pass

    def show_custom_options(self):
        """Show custom options

        Note: Override this for 'show options' functionality.
        """
        if self.quiet:
            return

    def do_option_tab(self, prefix):
        """Do tab completion for options

        This method will search for an option using the prefix passed. It
        first searches the console commands defined at instantiation (the
        general commands for the shell) and if not found, checks the
        options for a custom command.

        prefix[in]        Prefix of the option
        """
        full_command = self.cmd_line.get_command()
        matches = self.get_commands(full_command.strip(' '))
        if len(matches) > 0:
            self.do_base_command_tab(full_command, matches)
        elif self.custom_commands:
            # if prefix is 'help', try command complete for custom commands
            if full_command[0:4].lower() == 'help':
                self.do_custom_tab(prefix)
            else:
                self.do_custom_option_tab(prefix)

    def _set_complete_mode(self):
        """Set the tab completion mode

        If the command buffer is only 1 part (command and no options),
        we are in _COMMAND_COMPLETE mode.

        Else if the nearest option contains a $ at the start, we are in
        _VARIABLE_COMPLETE mode.

        Else we are in _OPTION_COMPLETE mode.

        _COMMAND_COMPLETE = tab complete for base and custom commands
        _VARIABLE_COMPLETE = tab complete for user-defined variables
        _OPTION_COMPLETE = tab complete for base or custom command options
        """
        buf = self.cmd_line.get_command()
        parts = buf.split(' ')
        segment = ''
        if (len(buf) > 0 and len(parts) == 1):
            self.type_complete_mode = _COMMAND_COMPLETE
        else:
            segment = self.cmd_line.get_nearest_option()
            if segment.find('$') > 0:
                self.type_complete_mode = _VARIABLE_COMPLETE
            else:
                self.type_complete_mode = _OPTION_COMPLETE
        return segment

    def show_help(self, parameter):
        """Display the help for either all commands or the help for a
        custom command.

        parameter[in]      Any parameter for the help command.
                           For example, 'help commands'
        """
        if self.quiet:
            return
        if not parameter or (parameter and parameter.lower() == 'commands'):
            print
            print_dictionary_list(['Command', 'Description'],
                                  ['name', 'text', 'alias'],
                                  self.base_commands, self.width, True)
            print
        else:
            matches = self.get_commands(parameter)
            if len(matches) > 0:
                self.show_command_help(matches)
            elif self.custom_commands:
                self.show_custom_command_help(parameter)

    def do_variable_tab(self, segment):
        """Do the tab completion for a variable

        This method will attempt to find a variable in the list of user-
        defined variables and complete the name of variable. If the user
        types 'TAB' twice, it will display a list of all possible matches.
        """
        # find the last $
        variable = ''
        start_var = 0
        new_var = ''
        stop = len(segment)
        for i in range(stop - 1, 0, -1):
            if segment[i] == ' ':
                break
            elif segment[i] == '$':
                variable = segment[i + 1:]
                start_var = i

        if start_var == stop:
            # show all of the variables
            matches = self.variables.get_matches({})
        else:
            matches = self.variables.get_matches(variable)

        if self.tab_count == 2:
            if len(matches) > 0:
                self.variables.show_variables(matches)
            else:
                self.variables.show_variables({})
            self.cmd_line.display_command()
            self.tab_count = 0
        else:
            # Do command completion here
            if len(matches) == 1:
                new_var = matches[0].items()[0][0] + ' '
                self.cmd_line.add(new_var[len(variable):])
                self.tab_count = 0

    def do_command_tab(self, command_text):
        """Do the tab completion for a command

        If the command is in the base commands, complete it there. If not,
        attempt to perform tab completion for custom commands (if defined).
        """
        # See if command is in the base command list first
        matches = self.get_commands(command_text)
        if len(matches) > 0:
            self.do_base_command_tab(command_text, matches)
        # Ok, not in command list, now check custom commands
        elif self.custom_commands:
            self.do_custom_tab(command_text)

    def do_base_command_tab(self, command_text, matches):
        """Do the tab completion for a base command.

        This method prints the list of base commands that match the
        command. If the user pressed TAB twice, it displays the list of all
        matches. If a single match is found, it returns the balance of the
        command.

        Note: this method gets its matches from do_command_tab.

        command_text[in]   Command
        matches[in]        Known matches (from do_command_tab)
        """
        if self.tab_count == 2:
            print "\n"
            print_dictionary_list(['Command', 'Description'],
                                  ['name', 'text', 'alias'],
                                  matches, self.width, True)
            print
            self.cmd_line.display_command()
            self.tab_count = 0
        else:
            if len(matches) == 1:
                if matches[0]['name'][:len(command_text)] == command_text:
                    new_cmd = matches[0]['name'] + ' '
                else:
                    new_cmd = matches[0]['alias'] + ' '
                self.tab_count = 0
                self.cmd_line.add(new_cmd[len(command_text):])

    def get_commands(self, cmd_prefix):
        """Get list of commands that match a prefix

        cmd_prefix[in]  prefix for name of command

        Returns dictionary entry for command based on matching first n chars
        """
        matches = []
        stop = len(cmd_prefix)
        start = 0
        for cmd in self.base_commands:
            if cmd['name'][start:stop] == cmd_prefix or \
               cmd['alias'][start:stop] == cmd_prefix:
                matches.append(cmd)

        return matches

    def show_command_help(self, commands):
        """Show the help for a list of commands.

        commands[in]       List of commands
        """
        if self.quiet:
            return
        print
        print_dictionary_list(['Command', 'Description'],
                              ['name', 'text', 'alias'],
                              commands, self.width, True)
        print

    def _do_command(self, command):
        """Execute a command

        This method routes the command to the appropriate methods for
        execution.

        command[in]        Command to execute

        Returns bool True - exit utility, False - do not exit
        """
        # do variable replacement
        command = self._replace_variables(command.strip(' '))
        if self.options.get('verbosity', False):
            print "\nExecuting command:", command
        # process simple commands
        if command.lower().startswith('set '):
            self._add_variable(command[4:])
            if not self.quiet:
                print
        elif command[0:11].lower() == 'show errors':
            self.show_errors()
        elif command[0:12].lower() == 'clear errors':
            self.clear_errors()
        elif command[0:15].lower() == 'show last error':
            self.show_last_error()
        elif command[0:14].lower() == 'show variables':
            self.variables.show_variables()
        elif self.custom_commands and command[0:12].lower() == 'show options':
            self.show_custom_options()
        else:
            cmd, parameters = self._get_util_parameters(command)
            if cmd is None:
                return False
            else:
                if cmd.lower() == 'help':
                    token = parameters[0] if parameters else ''
                    self.show_help(token)
                    self.cmd_line.clear()
                    self.tab_count = 0
                elif cmd == '':
                    print
                elif cmd.lower() in ['exit', 'quit']:
                    print
                    return True
                elif self.custom_commands:
                    if not self.is_valid_custom_command(cmd):
                        print("\n\nUnknown command: {0} {1}\n"
                              "".format(cmd, ' '.join(parameters)))
                    else:
                        try:
                            self.execute_custom_command(cmd, parameters)
                            print
                        except UtilError as err:
                            print err.errmsg

        self.cmd_line.clear()
        self.tab_count = 0
        return False

    def _process_command_keys(self, cmd_key):
        """Do the action associated with a command key.

        This method will act on the recognized command keys and execute the
        effect for each.

        cmd_key[in]        Key pressed
        """
        if cmd_key in ['ESCAPE']:
            self.cmd_line.erase_command()
        elif cmd_key in ['DELETE_POSIX', 'DELETE_WIN', 'DELETE_MAC']:
            self.cmd_line.delete_keypress()
            self.tab_count = 0
        elif cmd_key == 'ARROW_UP':
            self.cmd_line.replace_command(self.history.previous())
        elif cmd_key == 'ARROW_DN':
            self.cmd_line.replace_command(self.history.next())
        elif cmd_key == 'ARROW_LT':
            self.cmd_line.left_arrow_keypress()
        elif cmd_key == 'ARROW_RT':
            self.cmd_line.right_arrow_keypress()
        elif cmd_key in ['BACKSPACE_POSIX', 'BACKSPACE_WIN']:
            self.cmd_line.backspace_keypress()
        elif cmd_key == 'HOME':
            self.cmd_line.home_keypress()
        elif cmd_key == 'END':
            self.cmd_line.end_keypress()
        else:  # 'TAB'
            segment = self._set_complete_mode()
            self.tab_count += 1
            if self.type_complete_mode == _COMMAND_COMPLETE:
                self.do_command_tab(self.cmd_line.get_command())
            elif self.type_complete_mode == _OPTION_COMPLETE:
                self.do_option_tab(segment)
            else:  # _VARIABLE_COMPLETE
                self.do_variable_tab(segment)
        cmd_key = ''

    def _add_variable(self, set_command):
        """Add a variable to the list of variables.

        This method adds the user-defined variable to the internal list.

        set_command[in]    Set command from the user
        """
        if set_command.find('=') <= 0:
            print "\n\nSET command invalid. Syntax: SET <NAME> = <value>"
            return

        # get name and value
        name, value = set_command.split('=')
        name = name.strip().strip('$')
        value = value.strip()
        self.variables.add_variable(name, value)

    def _replace_variables(self, cmd_string):
        """Replace user-defined variables with values from the internal list.

        This method replaces $VARNAME with the value stored when the set
        command was issued.

        cmd_string[in]     Command from the user

        Returns string - command string with replacements
        """
        i = 1
        new_cmd = cmd_string
        while i > 0:
            i = new_cmd.find('$', i)
            if i > 0:
                j = new_cmd.find(' ', i)
                if j == -1:
                    j = len(new_cmd)
                if j > i:
                    var_name = new_cmd[i + 1:j]
                    var = self.variables.find_variable(var_name)
                    if var is not None:
                        new_cmd = new_cmd[0:i] + var[var_name] + new_cmd[j:]
                    else:
                        i = j

        return new_cmd

    @staticmethod
    def _get_util_parameters(cmd_string):
        """Split the command name from the command and return balance as
        parameters.

        cmd_string[in]     Command

        Returns tuple - command, list of parameters
        """
        try:
            tokens = shlex.split(cmd_string)
        except ValueError as err:
            print
            print("WARNING: Unable to execute command, reason: {0}"
                  "".format(str(err)))
            return None, None
        else:
            if len(tokens) > 1:
                return tokens[0], tokens[1:]
        return cmd_string.strip(' '), []

    def get_user_command(self):
        """Get a command from the user.

        This method displays a prompt to the user and returns when one of
        the command keys is pressed.
        """
        self.cmd_line.display_command()
        cmd_string = ''
        cmd_key = None
        self.tab_count = 0
        while cmd_key not in ['ENTER_POSIX', 'ENTER_WIN']:
            key = getch()
            # If a special key, act on it
            if key in _COMMAND_KEY:
                cmd_key = _COMMAND_KEY[key]
                # Windows does things oddly for some keys
                if os.name != 'posix' and cmd_key == 'SPECIAL_WIN':
                    key = getch()
                    cmd_key = _WIN_COMMAND_KEY.get(key)
                    if cmd_key is None:
                        continue
                self._process_command_keys(cmd_key)
                cmd_string = self.cmd_line.get_command()
            # else add key to command buffer
            else:
                cmd_string = self.cmd_line.get_command()
                self.cmd_line.add(key)
                cmd_string = self.cmd_line.get_command()
            sys.stdout.flush()

        self.position = 0
        return cmd_string

    def run_console(self, lines=None):
        """Run the console.

        This method is the main loop for executing commands. For all subclassed
        classes, the user need only call this method to execute an interactive
        shell or execute commands and exit. It can be used in three modes:

        1) it can process commands passed via lines list
        2) it can process commands passed via a pipe to the python exec
        3) it can prompt for commands and execute them as entered

        Modes (1) and (2) execute all commands then exit.

        lines[in]          If not empty, execute the list of commands.
        """
        if not lines:
            lines = []
        # If we have commands issued by the command line, execute and exit.
        if self.commands is not None:
            command_list = self.commands.split(';')
            for command in command_list:
                command = command.strip('\n').strip(' ')
                if os.name == 'nt':
                    command = command.strip('"')
                if self._do_command(command.strip('"')):
                    break

        # If we have piped input, read the input by line and execute
        elif not os.isatty(sys.stdin.fileno()) or len(lines) > 0:
            for command in sys.stdin.readlines():
                command_list = command.split(';')
                for cmd in command_list:
                    cmd = cmd.strip('\n').strip(' ')
                    if os.name == 'nt':
                        cmd = cmd.strip('"')
                    if self._do_command(cmd.strip('"')):
                        break

        # Otherwise, we are in an interactive mode where we get a command
        # from the user and execute
        else:
            cmd = ''
            if not self.quiet:
                print self.options.get('welcome', 'Welcome to the console!\n')
            while cmd.lower() not in ['exit', 'quit']:
                command = self.get_user_command()
                self.history.add(command)
                if self._do_command(command):
                    break
            if not self.quiet:
                print self.options.get('goodbye',
                                       'Thanks for using the console.\n')
#
# Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains a base class that implements a POSIX daemon.
"""

import os
import sys
import time
import atexit
import signal
import logging



class Daemon(object):
    """Posix Daemon.

    This is a base class for implementing a POSIX daemon.
    """
    def __init__(self, pidfile, umask=0o27, chdir="/", stdin=None, stdout=None,
                 stderr=None):
        """Constructor

        pidfile[in]  pid filename.
        umask[in]    posix umask.
        chdir[in]    working directory.
        stdin[in]    standard input object.
        stdout[in]   standard output object.
        stderr[in]   standard error object.
        """
        self.pid = None
        self.pidfile = os.path.realpath(os.path.normpath(pidfile))
        self.umask = umask
        self.chdir = chdir
        self.stdin = stdin
        self.stdout = stdout
        self.stderr = stderr

    def _report(self, message, level=logging.INFO, print_msg=True):
        """Log message if logging is on.

        This method will log the message presented if the log is turned on.
        Specifically, if options['log_file'] is not None. It will also
        print the message to stdout.

        This method should be overridden when subclassing Daemon.

        message[in]    message to be printed.
        level[in]      level of message to log. Default = INFO.
        print_msg[in]  if True, print the message to stdout. Default = True.
        """
        raise NotImplementedError("_report() method is not implemented.")

    def run(self, *args, **kwargs):
        """It will be called after the process has been daemonized by start()
        or restart.

        This method should be overridden when subclassing Daemon.
        """
        raise NotImplementedError("run() method is not implemented.")

    def start(self, detach_process=True):
        """Starts the daemon.

        It will start the daemon if detach_process is True.
        """
        if detach_process:
            # Check for a pidfile presence
            try:
                with open(self.pidfile, "rb") as f:
                    self.pid = int(f.read().strip())
            except IOError:
                self.pid = None
            except SystemExit:
                self.pid = None
            except ValueError:
                self.pid = None

            if self.pid:
                # Daemon already runs
                msg = ("pidfile {0} already exists. The daemon is already "
                       "running?".format(self.pidfile))
                self._report(msg, logging.CRITICAL)
                raise UtilDaemonError(msg)

            # Start the daemon
            self.daemonize()

        # Run automatic failover
        return self.run()

    def cleanup(self):
        """It will be called during the process to stop the daemon.

        This method should be overridden when subclassing Daemon.
        """
        raise NotImplementedError("cleanup() method is not implemented.")

    def stop(self):
        """Stops the daemon.

        It will stop the daemon by sending a signal.SIGTERM to the process.
        """
        # Get the pid from the pidfile
        try:
            with open(self.pidfile, "rb") as f:
                self.pid = int(f.read().strip())
        except IOError:
            self._report("pidfile {0} does not exist.".format(self.pidfile),
                         logging.ERROR)
            return False
        except ValueError:
            self._report("Invalid pid in pidfile {0}.".format(self.pidfile),
                         logging.ERROR)
            return False

        # Kill the daemon process
        try:
            while 1:
                os.kill(self.pid, signal.SIGTERM)
                time.sleep(0.1)
        except OSError as err:
            strerror = err.strerror
            if err.errno == 3:  # No such process
                if os.path.exists(self.pidfile):
                    self.delete_pidfile()
            else:
                msg = "Unable to delete pidfile: {0}".format(strerror)
                self._report(msg, logging.ERROR)
                raise UtilDaemonError(msg)

        return True

    def restart(self):
        """Restarts the daemon.

        It will execute a stop and start on the daemon.
        """
        self.stop()
        return self.start()

    def daemonize(self):
        """Creates the daemon.

        It will fork a child process and then exit parent. By performing a
        double fork, set the current process's user id, change the current
        working directory, set the current numeric umask, redirect standard
        streams and write the pid to a file.
        """
        def redirect_stream(system_stream, target_stream):
            """Redirect a system stream to a specified file.
            """
            if target_stream is None:
                target_f = os.open(os.devnull, os.O_RDWR)
            else:
                target_f = target_stream.fileno()
            os.dup2(target_f, system_stream.fileno())

        def fork_then_exit_parent(error_message):
            """Fork a child process, then exit the parent process.
            """
            try:
                pid = os.fork()
                if pid > 0:
                    os._exit(0)  # pylint: disable=W0212
            except OSError as err:
                msg = "{0}: [{1}] {2}".format(error_message, err.errno,
                                              err.strerror)
                self._report(msg, logging.CRITICAL)
                raise UtilDaemonError(msg)

        # Fork
        fork_then_exit_parent("Failed first fork.")

        try:
            os.setsid()
            os.chdir(self.chdir)
            os.umask(self.umask)
        except Exception as err:
            msg = "Unable to change directory ({0})".format(err)
            self._report(msg, logging.CRITICAL)
            raise UtilDaemonError(msg)

        # Double fork
        fork_then_exit_parent("Failed second fork.")

        # Redirect streams
        redirect_stream(sys.stdin, self.stdin)
        redirect_stream(sys.stdout, self.stdout)
        redirect_stream(sys.stderr, self.stderr)

        # Call a cleanup task to unregister the master.
        atexit.register(self.cleanup)
        # write pidfile
        atexit.register(self.delete_pidfile)
        pid = str(os.getpid())
        try:
            with open(self.pidfile, "w") as f:
                f.write("{0}\n".format(pid))
        except IOError as err:
            msg = "Unable to write pidfile: {0}".format(err.strerror)
            self._report(msg, logging.CRITICAL)
            raise UtilDaemonError(msg)

    def delete_pidfile(self):
        """Deletes pidfile.
        """
        try:
            os.remove(self.pidfile)
        except (OSError, IOError) as err:
            msg = "Unable to delete pidfile: {0}".format(err.strerror)
            self._report(msg, logging.ERROR)
            raise UtilDaemonError(msg)
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains abstractions of a MySQL Database object used by
multiple utilities.
"""

import multiprocessing
import os
import re
import sys

from collections import deque


# List of database objects for enumeration
_DATABASE, _TABLE, _VIEW, _TRIG, _PROC, _FUNC, _EVENT, _GRANT = "DATABASE", \
    "TABLE", "VIEW", "TRIGGER", "PROCEDURE", "FUNCTION", "EVENT", "GRANT"

_OBJTYPE_QUERY = """
    (
       SELECT TABLE_TYPE as object_type
       FROM INFORMATION_SCHEMA.TABLES
       WHERE TABLES.TABLE_SCHEMA = '%(db_name)s' AND
         TABLES.TABLE_NAME = '%(obj_name)s'
    )
    UNION
    (
        SELECT 'TRIGGER' as object_type
        FROM INFORMATION_SCHEMA.TRIGGERS
        WHERE TRIGGER_SCHEMA = '%(db_name)s' AND
          TRIGGER_NAME = '%(obj_name)s'
    )
    UNION
    (
        SELECT TYPE as object_type
        FROM mysql.proc
        WHERE DB = '%(db_name)s' AND NAME = '%(obj_name)s'
    )
    UNION
    (
        SELECT 'EVENT' as object_type
        FROM mysql.event
        WHERE DB = '%(db_name)s' AND NAME = '%(obj_name)s'
    )
"""

_DEFINITION_QUERY = """
  SELECT %(columns)s
  FROM INFORMATION_SCHEMA.%(table_name)s WHERE %(conditions)s
"""

_PARTITION_QUERY = """
  SELECT PARTITION_NAME, SUBPARTITION_NAME, PARTITION_ORDINAL_POSITION,
         SUBPARTITION_ORDINAL_POSITION, PARTITION_METHOD, SUBPARTITION_METHOD,
         PARTITION_EXPRESSION, SUBPARTITION_EXPRESSION, PARTITION_DESCRIPTION
  FROM INFORMATION_SCHEMA.PARTITIONS
  WHERE TABLE_SCHEMA = '%(db)s' AND TABLE_NAME = '%(name)s'
"""

_COLUMN_QUERY = """
  SELECT ORDINAL_POSITION, COLUMN_NAME, COLUMN_TYPE, IS_NULLABLE,
         COLUMN_DEFAULT, EXTRA, COLUMN_COMMENT, COLUMN_KEY
  FROM INFORMATION_SCHEMA.COLUMNS
  WHERE TABLE_SCHEMA = '%(db)s' AND TABLE_NAME = '%(name)s'
"""

_FK_CONSTRAINT_QUERY = """
SELECT TABLE_NAME, CONSTRAINT_NAME, COLUMN_NAME, REFERENCED_TABLE_SCHEMA,
REFERENCED_TABLE_NAME, REFERENCED_COLUMN_NAME, UPDATE_RULE, DELETE_RULE
FROM INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS
JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE
USING (CONSTRAINT_SCHEMA, CONSTRAINT_NAME, TABLE_NAME, REFERENCED_TABLE_NAME)
WHERE CONSTRAINT_SCHEMA = '{DATABASE!s}'
AND TABLE_NAME = '{TABLE!s}'
"""

_ALTER_TABLE_ADD_FK_CONSTRAINT = """
ALTER TABLE {DATABASE!s}.{TABLE!s} add CONSTRAINT `{CONSTRAINT_NAME!s}`
FOREIGN KEY (`{COLUMN_NAMES}`)
REFERENCES `{REFERENCED_DATABASE}`.`{REFERENCED_TABLE!s}`
(`{REFERENCED_COLUMNS!s}`)
ON UPDATE {UPDATE_RULE}
ON DELETE {DELETE_RULE}
"""


def _multiprocess_tbl_copy_task(copy_tbl_task):
    """Multiprocess copy table data method.

    This method wraps the copy of the table's data to allow its concurrent
    execution by a pool of processes.

    copy_tbl_task[in]   dictionary of values required by a process to
                        perform the table copy task, namely:
                        'source_srv': <dict with source connections values>,
                        'dest_srv': <dict with destination connections values>,
                        'source_db': <source database name>,
                        'destination_db': <destination database name>,
                        'table': <table to copy>,
                        'options': <dict of options>,
                        'cloning': <cloning flag>,
                        'connections': <number of concurrent connections>,
                        'q_source_db': <quoted source database name>.
    """
    # Get input to execute task.
    source_srv = copy_tbl_task.get('source_srv')
    dest_srv = copy_tbl_task.get('dest_srv')
    source_db = copy_tbl_task.get('source_db')
    target_db = copy_tbl_task.get('target_db')
    table = copy_tbl_task.get('table')
    options = copy_tbl_task.get('options')
    cloning = copy_tbl_task.get('cloning')
    # Execute copy table task.
    # NOTE: Must handle any exception here, because worker processes will not
    # propagate them to the main process.
    try:
        _copy_table_data(source_srv, dest_srv, source_db, target_db, table,
                         options, cloning)
    except UtilError:
        _, err, _ = sys.exc_info()
        print("ERROR copying data for table '{0}': {1}".format(table,
                                                               err.errmsg))


def _copy_table_data(source_srv, destination_srv, db_name, new_db_name,
                     tbl_name, tbl_options, cloning, connections=1):
    """Copy the data of the specified table.

    This method copies/clones all the data from a table to another (new)
    database.

    source_srv[in]      Source server (Server instance or dict. with the
                        connection values).
    destination_srv[in] Destination server (Server instance or dict. with the
                        connection values).
    db_name[in]         Name of the database with the table to copy.
    new_db_name[in]     Name of the destination database to copy the table.
    tbl_name[in]        Name of the table to copy.
    tbl_options[in]     Table options.
    cloning[in]         Cloning flag, in order to use a different method to
                        copy data on the same server
    connections[in]     Specify the use of multiple connections/processes to
                        copy the table data (rows). By default, only 1 used.
                        Note: Multiprocessing option should be preferred.
    """
    # Import table needed here to avoid circular import issues.
    from mysql.utilities.common.table import Table
    # Handle source and destination server instances or connection values.
    # Note: For multiprocessing the use of connection values instead of a
    # server instance is required to avoid internal errors.
    if isinstance(source_srv, Server):
        source = source_srv
    else:
        # Get source server instance from connection values.
        conn_options = {
            'quiet': True,  # Avoid repeating output for multiprocessing.
            'version': "5.1.30",
        }
        servers = connect_servers(source_srv, None, conn_options)
        source = servers[0]
    if isinstance(destination_srv, Server):
        destination = destination_srv
    else:
        # Get source server instance from connection values.
        conn_options = {
            'quiet': True,  # Avoid repeating output for multiprocessing.
            'version': "5.1.30",
        }
        servers = connect_servers(destination_srv, None, conn_options)
        destination = servers[0]

    # Copy table data.
    if not tbl_options.get("quiet", False):
        print("# Copying data for TABLE {0}.{1}".format(db_name,
                                                        tbl_name))
    source_sql_mode = source.select_variable("SQL_MODE")
    q_tbl_name = "{0}.{1}".format(quote_with_backticks(db_name,
                                                       source_sql_mode),
                                  quote_with_backticks(tbl_name,
                                                       source_sql_mode))
    tbl = Table(source, q_tbl_name, tbl_options)
    if tbl is None:
        raise UtilDBError("Cannot create table object before copy.", -1,
                          db_name)
    tbl.copy_data(destination, cloning, new_db_name, connections)


class Database(object):
    """
    The Database class encapsulates a database. The class has the following
    capabilities:

        - Check to see if the database exists
        - Drop the database
        - Create the database
        - Clone the database
        - Print CREATE statements for all objects
    """
    obj_type = _DATABASE

    def __init__(self, source, name, options=None):
        """Constructor

        source[in]         A Server object
        name[in]           Name of database
        verbose[in]        print extra data during operations (optional)
                           default value = False
        options[in]        Array of options for controlling what is included
                           and how operations perform (e.g., verbose)
        """
        if options is None:
            options = {}
        self.source = source
        # Get the SQL_MODE set on the source
        self.sql_mode = self.source.select_variable("SQL_MODE")
        # Keep database identifier considering backtick quotes
        if is_quoted_with_backticks(name, self.sql_mode):
            self.q_db_name = name
            self.db_name = remove_backtick_quoting(self.q_db_name,
                                                   self.sql_mode)
        else:
            self.db_name = name
            self.q_db_name = quote_with_backticks(self.db_name,
                                                  self.sql_mode)
        self.verbose = options.get("verbose", False)
        self.skip_tables = options.get("skip_tables", False)
        self.skip_views = options.get("skip_views", False)
        self.skip_triggers = options.get("skip_triggers", False)
        self.skip_procs = options.get("skip_procs", False)
        self.skip_funcs = options.get("skip_funcs", False)
        self.skip_events = options.get("skip_events", False)
        self.skip_grants = options.get("skip_grants", False)
        self.skip_create = options.get("skip_create", False)
        self.skip_data = options.get("skip_data", False)
        self.exclude_patterns = options.get("exclude_patterns", None)
        self.use_regexp = options.get("use_regexp", False)
        self.skip_table_opts = options.get("skip_table_opts", False)
        self.new_db = None
        self.q_new_db = None
        self.init_called = False
        self.destination = None  # Used for copy mode
        self.cloning = False    # Used for clone mode
        self.query_options = {  # Used for skipping buffered fetch of rows
            'fetch': False,
            'commit': False,  # No COMMIT needed for DDL operations (default).
        }
        # Used to store constraints to execute
        # after table creation, deque is
        # thread-safe
        self.constraints = deque()

        self.objects = []
        self.new_objects = []

    def exists(self, server=None, db_name=None):
        """Check to see if the database exists

        server[in]         A Server object
                           (optional) If omitted, operation is performed
                           using the source server connection.
        db_name[in]        database name
                           (optional) If omitted, operation is performed
                           on the class instance table name.

        return True = database exists, False = database does not exist
        """

        if not server:
            server = self.source
        db = None
        if db_name:
            db = db_name
        else:
            db = self.db_name

        _QUERY = """
            SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA
            WHERE SCHEMA_NAME = '%s'
        """
        res = server.exec_query(_QUERY % db)
        return (res is not None and len(res) >= 1)

    def drop(self, server, quiet, db_name=None):
        """Drop the database

        server[in]         A Server object
        quiet[in]          ignore error on drop
        db_name[in]        database name
                           (optional) If omitted, operation is performed
                           on the class instance table name.

        return True = database successfully dropped, False = error
        """

        db = None
        # Get the SQL_MODE set on the server
        sql_mode = server.select_variable("SQL_MODE")
        if db_name:
            db = db_name if is_quoted_with_backticks(db_name, sql_mode) \
                else quote_with_backticks(db_name, sql_mode)
        else:
            db = self.q_db_name
        op_ok = False
        if quiet:
            try:
                server.exec_query("DROP DATABASE %s" % (db),
                                  self.query_options)
                op_ok = True
            except:
                pass
        else:
            server.exec_query("DROP DATABASE %s" % (db),
                              self.query_options)
            op_ok = True
        return op_ok

    def create(self, server, db_name=None, charset_name=None,
               collation_name=None):
        """Create the database

        server[in]         A Server object
        db_name[in]        database name
                           (optional) If omitted, operation is performed
                           on the class instance table name.

        return True = database successfully created, False = error
        """
        # Get the SQL_MODE set on the server
        sql_mode = server.select_variable("SQL_MODE")
        if db_name:
            db = db_name if is_quoted_with_backticks(db_name, sql_mode) \
                else quote_with_backticks(db_name, sql_mode)
        else:
            db = self.q_db_name

        specification = ""
        if charset_name:
            specification = " DEFAULT CHARACTER SET {0}".format(charset_name)
        if collation_name:
            specification = "{0} DEFAULT COLLATE {1}".format(specification,
                                                             collation_name)
        query_create_db = "CREATE DATABASE {0} {1}".format(db, specification)
        server.exec_query(query_create_db, self.query_options)

        return True

    def __make_create_statement(self, obj_type, obj):
        """Construct a CREATE statement for a database object.

        This method will get the CREATE statement from the method
        get_create_statement() and also replace all occurrances of the
        old database name with the new.

        obj_type[in]       Object type (string) e.g. DATABASE
        obj[in]            A row from the get_db_objects() method
                           that contains the elements of the object

        Note: This does not work for tables.

        Returns the CREATE string
        """

        if not self.new_db:
            self.new_db = self.db_name
            self.q_new_db = self.q_db_name
        create_str = None
        # Tables are not supported
        if obj_type == _TABLE and self.cloning:
            return None
        # Grants are a different animal!
        if obj_type == _GRANT:
            if obj[3]:
                create_str = "GRANT %s ON %s.%s TO %s" % \
                             (obj[1], self.q_new_db, obj[3], obj[0])
            else:
                create_str = "GRANT %s ON %s.* TO %s" % \
                             (obj[1], self.q_new_db, obj[0])
        else:
            create_str = self.get_create_statement(self.db_name,
                                                   obj[0], obj_type)
            if self.new_db != self.db_name:
                # Replace the occurrences of the old database name (quoted with
                # backticks) with the new one when preceded by: a whitespace
                # character, comma or optionally a left parentheses.
                create_str = re.sub(
                    r"(\s|,)(\(?){0}\.".format(self.q_db_name),
                    r"\1\2{0}.".format(self.q_new_db),
                    create_str
                )
                # Replace the occurrences of the old database name (without
                # backticks) with the new one when preceded by: a whitespace
                # character, comma or optionally a left parentheses and
                # surrounded by single or double quotes.
                create_str = re.sub(
                    r"(\s|,)(\(?)(\"|\'?){0}(\"|\'?)\.".format(self.db_name),
                    r"\1\2\3{0}\4.".format(self.new_db),
                    create_str
                )
        return create_str

    def _get_views_sorted_by_dependencies(self, views, columns,
                                          need_backtick=True):
        """Get a list of views sorted by their dependencies.

        views[in]          List of views objects
        columns[in]        Column mode - names (default), brief, or full
        need_backtick[in]  True if view need backticks in the name

        Returns the list of view sorted by their dependencies
        """
        if columns == "names":
            name_idx = 0
        elif columns == "full":
            name_idx = 2
        else:
            name_idx = 1

        def _get_dependent_views(view, v_name_dict):
            """Get a list with all the dependent views for a given view
            view          [in]  current view being analyzed
            v_name_dict   [in]  mapping from short view names to used view_stm
            """
            # Get view name and use backticks if necessary
            v_name = view[name_idx]
            if need_backtick:
                v_name = quote_with_backticks(v_name, self.sql_mode)

            # Get view create statement and for each view in views_to_check
            # see if it is mentioned in the statement
            stmt = self.get_create_statement(self.db_name, v_name, _VIEW)
            base_views = []
            for v in v_name_dict:
                # No looking for itself
                if v != v_name:
                    # split off the from clause
                    # strip WHERE, ORDER BY, and GROUP BY
                    try:
                        from_clause = stmt.rsplit('from', 1)[1]
                        from_clause = from_clause.split('WHERE', 1)[0]
                    except:
                        from_clause = None
                    if from_clause:
                        index = from_clause.find(v)
                    else:
                        index = stmt.find(v)
                    if index >= 0:
                        base_views.append(v_name_dict[v])
            return base_views

        def build_view_deps(view_lst):
            """Get a list of views sorted by their dependencies.

            view_lst   [in]   list with views yet to to be ordered

            Returns the list of view sorted by their dependencies
            """
            # Mapping from view_names to views(brief, name or full)
            v_name_dict = {}
            for view in view_lst:
                key = quote_with_backticks(view[name_idx], self.sql_mode) if \
                    need_backtick else view[name_idx]
                v_name_dict[key] = view

            # Initialize sorted_tpl
            sorted_views = []
            # set with view whose dependencies were/are being analyzed.key
            visited_views = set()

            # set with views that have already been processed
            # (subset of processed_views). Contains the same elements as
            # sorted_views.
            processed_views = set()

            # Init stack
            view_stack = view_lst[:]
            while view_stack:
                curr_view = view_stack[-1]  # look at top of the stack
                if curr_view in visited_views:
                    view_stack.pop()
                    if curr_view not in processed_views:
                        sorted_views.append(curr_view)
                        processed_views.add(curr_view)
                else:
                    visited_views.add(curr_view)
                    children_views = _get_dependent_views(curr_view,
                                                          v_name_dict)
                    if children_views:
                        for child in children_views:
                            # store not yet processed base views the temp stack
                            if child not in processed_views:
                                view_stack.append(child)
            # No more views on the stack, return list of sorted views
            return sorted_views
        # Returns without columns names
        if isinstance(views[0], tuple):
            return build_view_deps(views)

        # Returns the tuple reconstructed with views sorted
        return (views[0], build_view_deps(views[1]),)

    def __add_db_objects(self, obj_type):
        """Get a list of objects from a database based on type.

        This method retrieves the list of objects for a specific object
        type and adds it to the class' master object list.

        obj_type[in]       Object type (string) e.g. DATABASE
        """

        rows = self.get_db_objects(obj_type)
        if rows:
            for row in rows:
                tup = (obj_type, row)
                self.objects.append(tup)

    def init(self):
        """Get all objects for the database based on options set.

        This method initializes the database object with a list of all
        objects except those object types that are excluded. It calls
        the helper method self.__add_db_objects() for each type of
        object.

        NOTE: This method must be called before the copy method. A
              guard is in place to ensure this.
        """
        self.init_called = True
        # Get tables
        if not self.skip_tables:
            self.__add_db_objects(_TABLE)
        # Get functions
        if not self.skip_funcs:
            self.__add_db_objects(_FUNC)
        # Get stored procedures
        if not self.skip_procs:
            self.__add_db_objects(_PROC)
        # Get views
        if not self.skip_views:
            self.__add_db_objects(_VIEW)
        # Get triggers
        if not self.skip_triggers:
            self.__add_db_objects(_TRIG)
        # Get events
        if not self.skip_events:
            self.__add_db_objects(_EVENT)
        # Get grants
        if not self.skip_grants:
            self.__add_db_objects(_GRANT)

    def __drop_object(self, obj_type, name):
        """Drop a database object.

        Attempts a quiet drop of a database object (no errors are
        printed).

        obj_type[in]       Object type (string) e.g. DATABASE
        name[in]           Name of the object
        """

        if self.verbose:
            print "# Dropping new object %s %s.%s" % \
                  (obj_type, self.new_db, name)
        drop_str = "DROP %s %s.%s" % \
                   (obj_type, self.q_new_db, name)
        # Suppress the error on drop
        if self.cloning:
            try:
                self.source.exec_query(drop_str, self.query_options)
            except UtilError:
                if self.verbose:
                    print("# WARNING: Unable to drop {0} from {1} database "
                          "(object may not exist): {2}".format(name,
                                                               "source",
                                                               drop_str))
        else:
            try:
                self.destination.exec_query(drop_str, self.query_options)
            except UtilError:
                if self.verbose:
                    print("# WARNING: Unable to drop {0} from {1} database "
                          "(object may not exist): {2}".format(name,
                                                               "destination",
                                                               drop_str))

    def __create_object(self, obj_type, obj, show_grant_msg,
                        quiet=True, new_engine=None, def_engine=None):
        """Create a database object.

        obj_type[in]       Object type (string) e.g. DATABASE
        obj[in]            A row from the get_db_object_names() method
                           that contains the elements of the object
        show_grant_msg[in] If true, display diagnostic information
        quiet[in]          do not print informational messages
        new_engine[in]     Use this engine if not None for object
        def_engine[in]     If target storage engine doesn't exist, use
                           this engine.

        Note: will handle exception and print error if query fails
        """
        # Use the sql_mode set on destination server
        dest_sql_mode = self.destination.select_variable("SQL_MODE")
        q_new_db = quote_with_backticks(self.new_db, dest_sql_mode)
        q_db_name = quote_with_backticks(self.db_name, dest_sql_mode)
        if obj_type == _TABLE and self.cloning:
            obj_name = quote_with_backticks(obj[0], dest_sql_mode)
            create_list = [
                "CREATE TABLE {0!s}.{1!s} LIKE {2!s}.{1!s}"
                "".format(q_new_db, obj_name, q_db_name)
            ]
        else:
            create_list = [self.__make_create_statement(obj_type, obj)]
        if obj_type == _TABLE:
            may_skip_fk = False  # Check possible issues with FK Constraints
            obj_name = quote_with_backticks(obj[0], dest_sql_mode)
            tbl_name = "%s.%s" % (self.q_new_db, obj_name)
            create_list = self.destination.substitute_engine(tbl_name,
                                                             create_list[0],
                                                             new_engine,
                                                             def_engine,
                                                             quiet)

            # Get storage engines from the source table and destination table
            # If the source table's engine is INNODB and the destination is
            # not we will loose any FK constraints that may exist
            src_eng = self.get_object_definition(self.q_db_name,
                                                 obj[0], obj_type)[0][0][2]
            dest_eng = None

            # Information about the engine is always in the last statement of
            # the list, be it a regular create table statement or a create
            # table; alter table statement.
            i = create_list[-1].find("ENGINE=")
            if i > 0:
                j = create_list[-1].find(" ", i)
                dest_eng = create_list[-1][i + 7:j]
            dest_eng = dest_eng or src_eng

            if src_eng.upper() == 'INNODB' and dest_eng.upper() != 'INNODB':
                may_skip_fk = True

        string = "# Copying"
        if not quiet:
            if obj_type == _GRANT:
                if show_grant_msg:
                    print "%s GRANTS from %s" % (string, self.db_name)
            else:
                print "%s %s %s.%s" % \
                      (string, obj_type, self.db_name, obj[0])
            if self.verbose:
                print("; ".join(create_list))

        try:
            self.destination.exec_query("USE %s" % self.q_new_db,
                                        self.query_options)
        except:
            pass
        for stm in create_list:
            try:
                if obj_type == _GRANT:
                    user = User(self.destination, obj[0])
                    if not user.exists():
                        user.create()
                self.destination.exec_query(stm, self.query_options)
            except UtilDBError as e:
                raise UtilDBError("Cannot operate on {0} object."
                                  " Error: {1}".format(obj_type, e.errmsg),
                                  -1, self.db_name)

        # Look for foreign key constraints
        if obj_type == _TABLE:
            params = {
                'DATABASE': self.db_name,
                'TABLE': obj[0],
            }
            try:
                query = _FK_CONSTRAINT_QUERY.format(**params)
                fkey_constr = self.source.exec_query(query)
            except UtilDBError as e:
                raise UtilDBError("Unable to obtain Foreign Key constraint "
                                  "information for table {0}.{1}. "
                                  "Error: {2}".format(self.db_name, obj[0],
                                                      e.errmsg), -1,
                                  self.db_name)

            # Get information about the foreign keys of the table being
            # copied/cloned.
            if fkey_constr and not may_skip_fk:

                # Create a constraint dictionary with the constraint
                # name as key
                constr_dict = {}

                # This list is used to ensure the same constraints are applied
                # in the same order, because iterating the dictionary doesn't
                # offer any guarantees regarding order, and Python 2.6 has
                # no ordered_dict
                constr_lst = []

                for fkey in fkey_constr:
                    params = constr_dict.get(fkey[1])
                    # in case the constraint entry already exists, it means it
                    # is composite, just update the columns names and
                    # referenced column fields
                    if params:
                        params['COLUMN_NAMES'].append(fkey[2])
                        params['REFERENCED_COLUMNS'].append(fkey[5])
                    else:  # else create a new entry
                        constr_lst.append(fkey[1])
                        constr_dict[fkey[1]] = {
                            'DATABASE': self.new_db,
                            'TABLE': fkey[0],
                            'CONSTRAINT_NAME': fkey[1],
                            'COLUMN_NAMES': [fkey[2]],
                            'REFERENCED_DATABASE': fkey[3],
                            'REFERENCED_TABLE': fkey[4],
                            'REFERENCED_COLUMNS': [fkey[5]],
                            'UPDATE_RULE': fkey[6],
                            'DELETE_RULE': fkey[7],
                        }
                # Iterate all the constraints and get the necessary parameters
                # to create the query
                for constr in constr_lst:
                    params = constr_dict[constr]
                    if self.cloning:  # if it is a cloning table operation

                        # In case the foreign key is composite we need to join
                        # the columns to use in in alter table query. Only
                        # useful when cloning
                        params['COLUMN_NAMES'] = '`,`'.join(
                            params['COLUMN_NAMES'])
                        params['REFERENCED_COLUMNS'] = '`,`'.join(
                            params['REFERENCED_COLUMNS'])

                        # If the foreign key points to a table under the
                        # database being cloned, change the referenced database
                        #  name to the new cloned database
                        if params['REFERENCED_DATABASE'] == self.db_name:
                            params['REFERENCED_DATABASE'] = self.new_db
                        else:
                            print("# WARNING: The database being cloned has "
                                  "external Foreign Key constraint "
                                  "dependencies, {0}.{1} depends on {2}."
                                  "{3}".format(params['DATABASE'],
                                               params['TABLE'],
                                               params['REFERENCED_DATABASE'],
                                               params['REFERENCED_TABLE']))
                        query = _ALTER_TABLE_ADD_FK_CONSTRAINT.format(**params)

                        # Store constraint query for later execution
                        self.constraints.append(query)
                        if self.verbose:
                            print(query)
                    else:  # if we are copying
                        if params['REFERENCED_DATABASE'] != self.db_name:
                            # if the table being copied has dependencies
                            # to external databases
                            print("# WARNING: The database being copied has "
                                  "external Foreign Key constraint "
                                  "dependencies, {0}.{1} depends on {2}."
                                  "{3}".format(params['DATABASE'],
                                               params['TABLE'],
                                               params['REFERENCED_DATABASE'],
                                               params['REFERENCED_TABLE']))
            elif fkey_constr and may_skip_fk:
                print("# WARNING: FOREIGN KEY constraints for table {0}.{1} "
                      "are missing because the new storage engine for "
                      "the table is not InnoDB".format(self.new_db, obj[0]))

    def __apply_constraints(self):
        """This method applies to the database the constraints stored in the
        self.constraints instance variable
        """

        # Enable Foreign Key Checks to prevent the swapping of
        # RESTRICT referential actions with NO ACTION
        query_opts = {'fetch': False, 'commit': False}
        self.destination.exec_query("SET FOREIGN_KEY_CHECKS=1", query_opts)

        # while constraint queue is not empty
        while self.constraints:
            try:
                query = self.constraints.pop()
            except IndexError:
                # queue is empty, exit while statement
                break
            if self.verbose:
                print(query)
            try:
                self.destination.exec_query(query, query_opts)
            except UtilDBError as err:
                raise UtilDBError("Unable to execute constraint query "
                                  "{0}. Error: {1}".format(query, err.errmsg),
                                  -1, self.new_db)

        # Turn Foreign Key Checks off again
        self.destination.exec_query("SET FOREIGN_KEY_CHECKS=0", query_opts)

    def copy_objects(self, new_db, options, new_server=None,
                     connections=1, check_exists=True):
        """Copy the database objects.

        This method will copy a database and all of its objects and data
        to another, new database. Options set at instantiation will determine
        if there are objects that are excluded from the copy. Likewise,
        the method will also skip data if that option was set and process
        an input file with INSERT statements if that option was set.

        The method can also be used to copy a database to another server
        by providing the new server object (new_server). Copy to the same
        name by setting new_db = old_db or as a new database.

        new_db[in]         Name of the new database
        options[in]        Options for copy e.g. do_drop, etc.
        new_server[in]     Connection to another server for copying the db
                           Default is None (copy to same server - clone)
        connections[in]    Number of threads(connections) to use for insert
        check_exists[in]   If True, check for database existence before copy
                           Default is True
        """

        # Must call init() first!
        # Guard for init() prerequisite
        assert self.init_called, "You must call db.init() before " + \
                                 "db.copy_objects()."

        grant_msg_displayed = False

        # Get sql_mode in new_server
        sql_mode = new_server.select_variable("SQL_MODE")

        if new_db:
            # Assign new database identifier considering backtick quotes.
            if is_quoted_with_backticks(new_db, sql_mode):
                self.q_new_db = new_db
                self.new_db = remove_backtick_quoting(new_db, sql_mode)
            else:
                self.new_db = new_db
                self.q_new_db = quote_with_backticks(new_db, sql_mode)
        else:
            # If new_db is not defined use the same as source database.
            self.new_db = self.db_name
            self.q_new_db = self.q_db_name

        self.destination = new_server

        # We know we're cloning if there is no new connection.
        self.cloning = (new_server == self.source)

        if self.cloning:
            self.destination = self.source

        # Check to see if database exists
        if check_exists:
            if self.cloning:
                exists = self.exists(self.source, new_db)
                drop_server = self.source
            else:
                exists = self.exists(self.destination, new_db)
                drop_server = self.destination
            if exists:
                if options.get("do_drop", False):
                    self.drop(drop_server, True, new_db)
                elif not self.skip_create:
                    raise UtilDBError("destination database exists. Use "
                                      "--drop-first to overwrite existing "
                                      "database.", -1, new_db)

        db_name = self.db_name
        definition = self.get_object_definition(db_name, db_name, _DATABASE)
        _, character_set, collation, _ = definition[0]
        # Create new database first
        if not self.skip_create:
            if self.cloning:
                self.create(self.source, new_db, character_set,
                            collation)
            else:
                self.create(self.destination, new_db, character_set,
                            collation)

        # Get sql_mode set on destination server
        dest_sql_mode = self.destination.select_variable("SQL_MODE")

        # Create the objects in the new database
        # Save any views that fail due to dependencies
        dependent_views = []
        for obj in self.objects:
            # Drop object if --drop-first specified and database not dropped
            # Grants do not need to be dropped for overwriting
            if options.get("do_drop", False) and obj[0] != _GRANT:
                obj_name = quote_with_backticks(obj[1][0], dest_sql_mode)
                self.__drop_object(obj[0], obj_name)

            # Attempt to create the object.
            try:
                # Create the object
                self.__create_object(obj[0], obj[1], not grant_msg_displayed,
                                     options.get("quiet", False),
                                     options.get("new_engine", None),
                                     options.get("def_engine", None))
            except UtilDBError as err:
                # If this is a view and it fails dependency checking, save
                # it and retry the view later, but only if we're not skipping
                # tables.
                if (obj[0] == _VIEW and "doesn't exist" in err.errmsg and
                        not self.skip_tables):
                    dependent_views.append(obj)
                else:
                    raise err

            if obj[0] == _GRANT and not grant_msg_displayed:
                grant_msg_displayed = True

        # Now retry the views
        if self.verbose and len(dependent_views) > 0:
            print("# Attempting to create views that failed dependency "
                  "checks on first pass.")
        for obj in dependent_views:
            # Drop object if --drop-first specified and database not dropped
            if self.verbose:
                print("#  Retrying view {0}".format(obj[1]))
            if options.get("do_drop", False):
                obj_name = quote_with_backticks(obj[1][0], dest_sql_mode)
                self.__drop_object(obj[0], obj_name)

            # Create the object
            self.__create_object(obj[0], obj[1], not grant_msg_displayed,
                                 options.get("quiet", False),
                                 options.get("new_engine", None),
                                 options.get("def_engine", None))

        # After object creation, add the constraints
        if self.constraints:
            self.__apply_constraints()

    def copy_data(self, new_db, options, new_server=None, connections=1,
                  src_con_val=None, dest_con_val=None):
        """Copy the data for the tables.

        This method will copy the data for all of the tables to another, new
        database. The method will process an input file with INSERT statements
        if the option was selected by the caller.

        new_db[in]          Name of the new database
        options[in]         Options for copy e.g. do_drop, etc.
        new_server[in]      Connection to another server for copying the db
                            Default is None (copy to same server - clone)
        connections[in]     Number of threads(connections) to use for insert
        src_con_val[in]     Dict. with the connection values of the source
                            server (required for multiprocessing).
        dest_con_val[in]    Dict. with the connection values of the
                            destination server (required for multiprocessing).
        """

        # Must call init() first!
        # Guard for init() prerequisite
        assert self.init_called, "You must call db.init() before " + \
                                 "db.copy_data()."

        if self.skip_data:
            return

        self.destination = new_server

        # We know we're cloning if there is no new connection.
        self.cloning = (new_server == self.source)

        if self.cloning:
            self.destination = self.source

        quiet = options.get("quiet", False)

        tbl_options = {
            'verbose': self.verbose,
            'get_cols': True,
            'quiet': quiet
        }

        copy_tbl_tasks = []
        table_names = [obj[0] for obj in self.get_db_objects(_TABLE)]
        for tblname in table_names:
            # Check multiprocess table copy (only on POSIX systems).
            if options['multiprocess'] > 1 and os.name == 'posix':
                # Create copy task.
                copy_task = {
                    'source_srv': src_con_val,
                    'dest_srv': dest_con_val,
                    'source_db': self.db_name,
                    'target_db': new_db,
                    'table': tblname,
                    'options': tbl_options,
                    'cloning': self.cloning,
                }
                copy_tbl_tasks.append(copy_task)
            else:
                # Copy data from a table (no multiprocessing).
                _copy_table_data(self.source, self.destination, self.db_name,
                                 new_db, tblname, tbl_options, self.cloning)

        # Copy tables concurrently.
        if copy_tbl_tasks:
            # Create process pool.
            workers_pool = multiprocessing.Pool(
                processes=options['multiprocess']
            )
            # Concurrently export tables.
            workers_pool.map_async(_multiprocess_tbl_copy_task, copy_tbl_tasks)
            workers_pool.close()
            # Wait for all task to be completed by workers.
            workers_pool.join()

    def get_create_statement(self, db, name, obj_type):
        """Return the create statement for the object

        db[in]             Database name
        name[in]           Name of the object
        obj_type[in]       Object type (string) e.g. DATABASE
                           Note: this is used to form the correct SHOW command

        Returns create statement
        """
        # Save current sql_mode and switch it to '' momentarily as this
        # prevents issues when copying blobs and destination server is
        # set with SQL_MODE='NO_BACKSLASH_ESCAPES'
        prev_sql_mode = ''
        if (self.destination is not None and 'ANSI_QUOTES' in self.sql_mode and
                'ANSI_QUOTES' not in
                self.destination.select_variable("SQL_MODE")):
            prev_sql_mode = self.source.select_variable("SQL_MODE")
            self.source.exec_query("SET @@SESSION.SQL_MODE=''")
            self.sql_mode = ""
            # Quote with current sql_mode
            name = (name if not is_quoted_with_backticks(name, prev_sql_mode)
                    else remove_backtick_quoting(name, prev_sql_mode))
            db = (db if not is_quoted_with_backticks(db, prev_sql_mode)
                  else remove_backtick_quoting(db, prev_sql_mode))
        # Quote database and object name with backticks.
        q_name = (name if is_quoted_with_backticks(name, self.sql_mode)
                  else quote_with_backticks(name, self.sql_mode))
        if obj_type == _DATABASE:
            name_str = q_name
        else:
            q_db = (db if is_quoted_with_backticks(db, self.sql_mode)
                    else quote_with_backticks(db, self.sql_mode))

            # Switch the default database to execute the
            # SHOW CREATE statement without needing to specify the database
            # This is for 5.1 compatibility reasons:
            try:
                self.source.exec_query("USE {0}".format(q_db),
                                       self.query_options)
            except UtilError as err:
                raise UtilDBError("ERROR: Couldn't change "
                                  "default database: {0}".format(err.errmsg))
        name_str = q_name

        # Retrieve the CREATE statement.
        row = self.source.exec_query(
            "SHOW CREATE {0} {1}".format(obj_type, name_str)
        )

        # Restore previews sql_mode
        if prev_sql_mode:
            self.source.exec_query("SET @@SESSION.SQL_MODE={0}"
                                   "".format(prev_sql_mode))
            self.sql_mode = prev_sql_mode

        create_statement = None
        if row:
            if obj_type == _TABLE or obj_type == _VIEW or \
               obj_type == _DATABASE:
                create_statement = row[0][1]
            elif obj_type == _EVENT:
                create_statement = row[0][3]
            else:
                create_statement = row[0][2]

        # Remove all table options from the CREATE statement (if requested).
        if self.skip_table_opts and obj_type == _TABLE:
            # First, get partition options.
            create_tbl, sep, part_opts = create_statement.rpartition('\n/*')
            # Handle situation where no partition options are found.
            if not create_tbl:
                create_tbl = part_opts
                part_opts = ''
            else:
                part_opts = "{0}{1}".format(sep, part_opts)
            # Then, separate table definitions from table options.
            create_tbl, sep, _ = create_tbl.rpartition(') ')
            # Reconstruct CREATE statement without table options.
            create_statement = "{0}{1}{2}".format(create_tbl, sep, part_opts)

        return create_statement

    def get_create_table(self, db, table):
        """Return the create table statement for the given table.

        This method returns the CREATE TABLE statement for the given table with
        or without the table options, according to the Database object
        property 'skip_table_opts'.

        db[in]             Database name.
        table[in]          Table name.

        Returns a tuple with the CREATE TABLE statement and table options
        (or None). If skip_table_opts=True the CREATE statement does not
        include the table options that are returned separately, otherwise the
        table options are included in the CREATE statement and None is returned
        as the second tuple element.
        """
        # Quote database and table name with backticks.
        q_table = (table if is_quoted_with_backticks(table, self.sql_mode)
                   else quote_with_backticks(table, self.sql_mode))
        q_db = db if is_quoted_with_backticks(db, self.sql_mode) else \
            quote_with_backticks(db, self.sql_mode)

        # Retrieve CREATE TABLE.
        try:
            row = self.source.exec_query(
                "SHOW CREATE TABLE {0}.{1}".format(q_db, q_table)
            )
            create_tbl = row[0][1]
        except UtilError as err:
            raise UtilDBError("Error retrieving CREATE TABLE for {0}.{1}: "
                              "{2}".format(q_db, q_table, err.errmsg))

        # Separate table options from table definition.
        tbl_opts = None
        if self.skip_table_opts:
            # First, get partition options.
            create_tbl, sep, part_opts = create_tbl.rpartition('\n/*')
            # Handle situation where no partition options are found.
            if not create_tbl:
                create_tbl = part_opts
                part_opts = ''
            else:
                part_opts = "{0}{1}".format(sep, part_opts)
            # Then, separate table definitions from table options.
            create_tbl, sep, tbl_opts = create_tbl.rpartition(') ')
            # Reconstruct CREATE TABLE without table options.
            create_tbl = "{0}{1}{2}".format(create_tbl, sep, part_opts)

        return create_tbl, tbl_opts

    def get_table_options(self, db, table):
        """Return the table options.

        This method returns the list of used table options (from the CREATE
        TABLE statement).

        db[in]             Database name.
        table[in]          Table name.

        Returns a list of table options.
        For example: ['AUTO_INCREMENT=5','ENGINE=InnoDB']
        """
        # Quote database and table name with backticks.
        q_table = (table if is_quoted_with_backticks(table, self.sql_mode)
                   else quote_with_backticks(table, self.sql_mode))
        q_db = db if is_quoted_with_backticks(db, self.sql_mode) else \
            quote_with_backticks(db, self.sql_mode)

        # Retrieve CREATE TABLE statement.
        try:
            row = self.source.exec_query(
                "SHOW CREATE TABLE {0}.{1}".format(q_db, q_table)
            )
            create_tbl = row[0][1]
        except UtilError as err:
            raise UtilDBError("Error retrieving CREATE TABLE for {0}.{1}: "
                              "{2}".format(q_db, q_table, err.errmsg))

        # First, separate partition options.
        create_tbl, _, part_opts = create_tbl.rpartition('\n/*')
        # Handle situation where no partition options are found.
        create_tbl = part_opts if not create_tbl else create_tbl
        # Then, separate table options from table definition.
        create_tbl, _, tbl_opts = create_tbl.rpartition(') ')
        table_options = tbl_opts.split()

        return table_options

    def get_object_definition(self, db, name, obj_type):
        """Return a list of the object's creation metadata.

        This method queries the INFORMATION_SCHEMA or MYSQL database for the
        row-based (list) description of the object. This is similar to the
        output EXPLAIN <object>.

        db[in]             Database name
        name[in]           Name of the object
        obj_type[in]       Object type (string) e.g. DATABASE
                           Note: this is used to form the correct SHOW command

        Returns list - object definition, None if db.object does not exist
        """
        definition = []
        from_name = None
        condition = None

        # Remove objects backticks if needed
        db = remove_backtick_quoting(db, self.sql_mode) \
            if is_quoted_with_backticks(db, self.sql_mode) else db
        name = remove_backtick_quoting(name, self.sql_mode) \
            if is_quoted_with_backticks(name, self.sql_mode) else name

        if obj_type == _DATABASE:
            columns = 'SCHEMA_NAME, DEFAULT_CHARACTER_SET_NAME, ' + \
                      'DEFAULT_COLLATION_NAME, SQL_PATH'
            from_name = 'SCHEMATA'
            condition = "SCHEMA_NAME = '%s'" % name
        elif obj_type == _TABLE:
            columns = 'TABLE_SCHEMA, TABLE_NAME, ENGINE, AUTO_INCREMENT, ' + \
                      'AVG_ROW_LENGTH, CHECKSUM, TABLE_COLLATION, ' + \
                      'TABLE_COMMENT, ROW_FORMAT, CREATE_OPTIONS'
            from_name = 'TABLES'
            condition = "TABLE_SCHEMA = '%s' AND TABLE_NAME = '%s'" % \
                        (db, name)
        elif obj_type == _VIEW:
            columns = 'TABLE_SCHEMA, TABLE_NAME, VIEW_DEFINITION, ' + \
                      'CHECK_OPTION, DEFINER, SECURITY_TYPE'
            from_name = 'VIEWS'
            condition = "TABLE_SCHEMA = '%s' AND TABLE_NAME = '%s'" % \
                        (db, name)
        elif obj_type == _TRIG:
            columns = 'TRIGGER_SCHEMA, TRIGGER_NAME, EVENT_MANIPULATION, ' + \
                      'EVENT_OBJECT_TABLE, ACTION_STATEMENT, ' + \
                      'ACTION_TIMING, DEFINER'
            from_name = 'TRIGGERS'
            condition = "TRIGGER_SCHEMA = '%s' AND TRIGGER_NAME = '%s'" % \
                        (db, name)
        elif obj_type == _PROC or obj_type == _FUNC:
            columns = 'ROUTINE_SCHEMA, ROUTINE_NAME, ROUTINE_DEFINITION, ' + \
                      'ROUTINES.SQL_DATA_ACCESS, ROUTINES.SECURITY_TYPE, ' + \
                      'ROUTINE_COMMENT, ROUTINES.DEFINER, param_list, ' + \
                      'DTD_IDENTIFIER, ROUTINES.IS_DETERMINISTIC'
            from_name = 'ROUTINES JOIN mysql.proc ON ' + \
                        'ROUTINES.ROUTINE_SCHEMA = proc.db AND ' + \
                        'ROUTINES.ROUTINE_NAME = proc.name AND ' + \
                        'ROUTINES.ROUTINE_TYPE = proc.type '
            condition = "ROUTINE_SCHEMA = '%s' AND ROUTINE_NAME = '%s'" % \
                        (db, name)
            if obj_type == _PROC:
                typ = 'PROCEDURE'
            else:
                typ = 'FUNCTION'
            condition += " AND ROUTINE_TYPE = '%s'" % typ
        elif obj_type == _EVENT:
            columns = ('EVENT_SCHEMA, EVENT_NAME, DEFINER, EVENT_DEFINITION, '
                       'EVENT_TYPE, INTERVAL_FIELD, INTERVAL_VALUE, STATUS, '
                       'ON_COMPLETION, STARTS, ENDS')
            from_name = 'EVENTS'
            condition = "EVENT_SCHEMA = '%s' AND EVENT_NAME = '%s'" % \
                        (db, name)

        if from_name is None:
            raise UtilError('Attempting to get definition from unknown object '
                            'type = %s.' % obj_type)

        values = {
            'columns': columns,
            'table_name': from_name,
            'conditions': condition,
        }
        rows = self.source.exec_query(_DEFINITION_QUERY % values)
        if rows != []:
            # If this is a table, we need three types of information:
            # basic info, column info, and partitions info
            if obj_type == _TABLE:
                values['name'] = name
                values['db'] = db
                basic_def = rows[0]
                col_def = self.source.exec_query(_COLUMN_QUERY % values)
                part_def = self.source.exec_query(_PARTITION_QUERY % values)
                definition.append((basic_def, col_def, part_def))
            else:
                definition.append(rows[0])

        return definition

    def get_next_object(self):
        """Retrieve the next object in the database list.

        This method is an iterator for retrieving the objects in the database
        as specified in the init() method. You must call this method first.

        Returns next object in list or throws exception at EOL.
        """

        # Must call init() first!
        # Guard for init() prerequisite
        assert self.init_called, "You must call db.init() before db.copy()."

        for obj in self.objects:
            yield obj

    def __build_exclude_patterns(self, exclude_param):
        """Return a string to add to where clause to exclude objects.

        This method will add the conditions to exclude objects based on
        name if there is a dot notation or by a search pattern as specified
        by the options.

        exclude_param[in]  Name of column to check.

        Returns (string) String to add to where clause or ""
        """
        oper = 'NOT REGEXP' if self.use_regexp else 'NOT LIKE'
        string = ""
        for pattern in self.exclude_patterns:
            # Check use of qualified object names (with backtick support).
            if pattern.find(".") > 0:
                use_backtick = is_quoted_with_backticks(pattern, self.sql_mode)
                db, name = parse_object_name(pattern, self.sql_mode, True)
                if use_backtick:
                    # Remove backtick quotes.
                    db = remove_backtick_quoting(db, self.sql_mode)
                    name = remove_backtick_quoting(name, self.sql_mode)
                if db == self.db_name:  # Check if database name matches.
                    value = name  # Only use the object name to exclude.
                else:
                    value = pattern
            # Otherwise directly use the specified pattern.
            else:
                value = pattern
            if value:
                # Append exclude condition to previous one(s).
                string = "{0} AND {1} {2} {3}".format(string, exclude_param,
                                                      oper, obj2sql(value))

        return string

    def get_object_type(self, object_name):
        """Return the object type of an object

        This method attempts to locate the object name among the objects
        in the database. It returns the object type if found or None
        if not found.
        Note: different types of objects with the same name might exist in the
        database.

        object_name[in]    Name of the object to find

        Returns (list of strings) with the object types or None if not found
        """
        object_types = None

        # Remove object backticks if needed
        obj_name = remove_backtick_quoting(object_name, self.sql_mode) \
            if is_quoted_with_backticks(object_name, self.sql_mode) else \
            object_name

        res = self.source.exec_query(_OBJTYPE_QUERY %
                                     {'db_name': self.db_name,
                                      'obj_name': obj_name})

        if res:
            object_types = ['TABLE' if row[0] == 'BASE TABLE' else row[0]
                            for row in res]

        return object_types

    def get_db_objects(self, obj_type, columns='names', get_columns=False,
                       need_backtick=False):
        """Return a result set containing a list of objects for a given
        database based on type.

        This method returns either a list of names for the object type
        specified, a brief list of minimal columns for creating the
        objects, or the full list of columns from INFORMATION_SCHEMA. It can
        also provide the list of column names if desired.

        obj_type[in]       Type of object to retrieve
        columns[in]        Column mode - names (default), brief, or full
                           Note: not valid for GRANT objects.
        get_columns[in]    If True, return column names as first element
                           and result set as second element. If False,
                           return only the result set.
        need_backtick[in]  If True, it returns any identifiers, e.g. table and
                           column names, quoted with backticks.
                           By default, False.

        TODO: Change implementation to return classes instead of a result set.

        Returns mysql.connector result set
        """

        exclude_param = ""
        if obj_type == _TABLE:
            _NAMES = """
            SELECT DISTINCT TABLES.TABLE_NAME
            """
            names_pos_to_quote = (0,)
            _FULL = """
            SELECT TABLES.TABLE_CATALOG, TABLES.TABLE_SCHEMA,
                TABLES.TABLE_NAME, TABLES.TABLE_TYPE,
                TABLES.ENGINE, TABLES.VERSION, TABLES.ROW_FORMAT,
                TABLES.TABLE_ROWS, TABLES.AVG_ROW_LENGTH, TABLES.DATA_LENGTH,
                TABLES.MAX_DATA_LENGTH, TABLES.INDEX_LENGTH, TABLES.DATA_FREE,
                TABLES.AUTO_INCREMENT, TABLES.CREATE_TIME, TABLES.UPDATE_TIME,
                TABLES.CHECK_TIME, TABLES.TABLE_COLLATION, TABLES.CHECKSUM,
                TABLES.CREATE_OPTIONS, TABLES.TABLE_COMMENT,
                COLUMNS.ORDINAL_POSITION, COLUMNS.COLUMN_NAME,
                COLUMNS.COLUMN_TYPE, COLUMNS.IS_NULLABLE,
                COLUMNS.COLUMN_DEFAULT, COLUMNS.COLUMN_KEY,
                REFERENTIAL_CONSTRAINTS.CONSTRAINT_NAME,
                REFERENTIAL_CONSTRAINTS.REFERENCED_TABLE_NAME,
                REFERENTIAL_CONSTRAINTS.UNIQUE_CONSTRAINT_NAME,
                REFERENTIAL_CONSTRAINTS.UNIQUE_CONSTRAINT_SCHEMA,
                REFERENTIAL_CONSTRAINTS.UPDATE_RULE,
                REFERENTIAL_CONSTRAINTS.DELETE_RULE,
                KEY_COLUMN_USAGE.CONSTRAINT_NAME AS KEY_CONSTRAINT_NAME,
                KEY_COLUMN_USAGE.COLUMN_NAME AS COL_NAME,
                KEY_COLUMN_USAGE.REFERENCED_TABLE_SCHEMA,
                KEY_COLUMN_USAGE.REFERENCED_COLUMN_NAME
            """
            full_pos_to_quote = (1, 2, 22, 27, 28, 29, 30, 33, 34, 35, 36)
            full_pos_split_quote = (34, 36)
            _MINIMAL = """
            SELECT TABLES.TABLE_SCHEMA, TABLES.TABLE_NAME, TABLES.ENGINE,
                COLUMNS.ORDINAL_POSITION, COLUMNS.COLUMN_NAME,
                COLUMNS.COLUMN_TYPE, COLUMNS.IS_NULLABLE,
                COLUMNS.COLUMN_DEFAULT, COLUMNS.COLUMN_KEY,
                TABLES.TABLE_COLLATION,
                TABLES.CREATE_OPTIONS,
                REFERENTIAL_CONSTRAINTS.CONSTRAINT_NAME,
                REFERENTIAL_CONSTRAINTS.REFERENCED_TABLE_NAME,
                REFERENTIAL_CONSTRAINTS.UNIQUE_CONSTRAINT_NAME,
                REFERENTIAL_CONSTRAINTS.UPDATE_RULE,
                REFERENTIAL_CONSTRAINTS.DELETE_RULE,
                KEY_COLUMN_USAGE.CONSTRAINT_NAME AS KEY_CONSTRAINT_NAME,
                KEY_COLUMN_USAGE.COLUMN_NAME AS COL_NAME,
                KEY_COLUMN_USAGE.REFERENCED_TABLE_SCHEMA,
                KEY_COLUMN_USAGE.REFERENCED_COLUMN_NAME
            """
            minimal_pos_to_quote = (0, 1, 4, 11, 12, 13, 16, 17, 18, 19)
            minimal_pos_split_quote = (17, 19)
            _OBJECT_QUERY = """
            FROM INFORMATION_SCHEMA.TABLES JOIN INFORMATION_SCHEMA.COLUMNS ON
                TABLES.TABLE_SCHEMA = COLUMNS.TABLE_SCHEMA AND
                TABLES.TABLE_NAME = COLUMNS.TABLE_NAME
            LEFT JOIN INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS ON
                TABLES.TABLE_SCHEMA = REFERENTIAL_CONSTRAINTS.CONSTRAINT_SCHEMA
                AND
                TABLES.TABLE_NAME = REFERENTIAL_CONSTRAINTS.TABLE_NAME
            LEFT JOIN (
                  SELECT CONSTRAINT_SCHEMA, TABLE_NAME, CONSTRAINT_NAME,
                         GROUP_CONCAT(COLUMN_NAME ORDER BY ORDINAL_POSITION)
                         AS COLUMN_NAME, REFERENCED_TABLE_SCHEMA,
                         GROUP_CONCAT(REFERENCED_COLUMN_NAME ORDER BY
                         ORDINAL_POSITION) AS REFERENCED_COLUMN_NAME
                  FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
                  GROUP BY CONSTRAINT_SCHEMA, TABLE_NAME, CONSTRAINT_NAME,
                           REFERENCED_TABLE_SCHEMA
            ) AS KEY_COLUMN_USAGE ON
                TABLES.TABLE_SCHEMA = KEY_COLUMN_USAGE.CONSTRAINT_SCHEMA
                AND
                TABLES.TABLE_NAME = KEY_COLUMN_USAGE.TABLE_NAME
            WHERE TABLES.TABLE_SCHEMA = '%s' AND TABLE_TYPE <> 'VIEW' %s
            """
            _ORDER_BY_DEFAULT = """
            ORDER BY TABLES.TABLE_SCHEMA, TABLES.TABLE_NAME,
                     COLUMNS.ORDINAL_POSITION
            """
            _ORDER_BY_NAME = """
            ORDER BY TABLES.TABLE_NAME
            """
            exclude_param = "TABLES.TABLE_NAME"

        elif obj_type == _VIEW:
            _NAMES = """
            SELECT TABLE_NAME
            """
            names_pos_to_quote = (0,)
            _FULL = """
            SELECT TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME, VIEW_DEFINITION,
                   CHECK_OPTION, IS_UPDATABLE, DEFINER, SECURITY_TYPE,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION
            """
            full_pos_to_quote = (1, 2)
            full_pos_split_quote = ()
            _MINIMAL = """
            SELECT TABLE_SCHEMA, TABLE_NAME, DEFINER, SECURITY_TYPE,
                   VIEW_DEFINITION, CHECK_OPTION, IS_UPDATABLE,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION
            """
            minimal_pos_to_quote = (0, 1)
            minimal_pos_split_quote = ()
            _OBJECT_QUERY = """
            FROM INFORMATION_SCHEMA.VIEWS
            WHERE TABLE_SCHEMA = '%s' %s
            """
            _ORDER_BY_DEFAULT = ""
            _ORDER_BY_NAME = ""
            exclude_param = "VIEWS.TABLE_NAME"
        elif obj_type == _TRIG:
            _NAMES = """
            SELECT TRIGGER_NAME
            """
            names_pos_to_quote = (0,)
            _FULL = """
            SELECT TRIGGER_CATALOG, TRIGGER_SCHEMA, TRIGGER_NAME,
                   EVENT_MANIPULATION, EVENT_OBJECT_CATALOG,
                   EVENT_OBJECT_SCHEMA, EVENT_OBJECT_TABLE, ACTION_ORDER,
                   ACTION_CONDITION, ACTION_STATEMENT, ACTION_ORIENTATION,
                   ACTION_TIMING, ACTION_REFERENCE_OLD_TABLE,
                   ACTION_REFERENCE_NEW_TABLE, ACTION_REFERENCE_OLD_ROW,
                   ACTION_REFERENCE_NEW_ROW, CREATED, SQL_MODE, DEFINER,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION,
                   DATABASE_COLLATION
            """
            full_pos_to_quote = (1, 2, 5, 6)  # 9 ?
            full_pos_split_quote = ()
            _MINIMAL = """
            SELECT TRIGGER_NAME, DEFINER, EVENT_MANIPULATION,
                   EVENT_OBJECT_SCHEMA, EVENT_OBJECT_TABLE,
                   ACTION_ORIENTATION, ACTION_TIMING,
                   ACTION_STATEMENT, SQL_MODE,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION,
                   DATABASE_COLLATION
            """
            # Note: 7 (ACTION_STATEMENT) might require special handling
            minimal_pos_to_quote = (0, 3, 4)
            minimal_pos_split_quote = ()
            _OBJECT_QUERY = """
            FROM INFORMATION_SCHEMA.TRIGGERS
            WHERE TRIGGER_SCHEMA = '%s' %s
            """
            _ORDER_BY_DEFAULT = ""
            _ORDER_BY_NAME = ""
            exclude_param = "TRIGGERS.TRIGGER_NAME"
        elif obj_type == _PROC:
            _NAMES = """
            SELECT NAME
            """
            names_pos_to_quote = (0,)
            _FULL = """
            SELECT DB, NAME, TYPE, SPECIFIC_NAME, LANGUAGE, SQL_DATA_ACCESS,
                   IS_DETERMINISTIC, SECURITY_TYPE, PARAM_LIST, RETURNS, BODY,
                   DEFINER, CREATED, MODIFIED, SQL_MODE, COMMENT,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION, DB_COLLATION,
                   BODY_UTF8
            """
            full_pos_to_quote = (0, 1, 3)
            full_pos_split_quote = ()
            _MINIMAL = """
            SELECT NAME, LANGUAGE, SQL_DATA_ACCESS, IS_DETERMINISTIC,
                   SECURITY_TYPE, DEFINER, PARAM_LIST, RETURNS,
                   BODY, SQL_MODE,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION,
                   DB_COLLATION
            """
            minimal_pos_to_quote = (0,)
            minimal_pos_split_quote = ()
            _OBJECT_QUERY = """
            FROM mysql.proc
            WHERE DB = '%s' AND TYPE = 'PROCEDURE' %s
            """
            _ORDER_BY_DEFAULT = ""
            _ORDER_BY_NAME = ""
            exclude_param = "NAME"
        elif obj_type == _FUNC:
            _NAMES = """
            SELECT NAME
            """
            names_pos_to_quote = (0,)
            _FULL = """
            SELECT DB, NAME, TYPE, SPECIFIC_NAME, LANGUAGE, SQL_DATA_ACCESS,
                   IS_DETERMINISTIC, SECURITY_TYPE, PARAM_LIST, RETURNS, BODY,
                   DEFINER, CREATED, MODIFIED, SQL_MODE, COMMENT,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION, DB_COLLATION,
                   BODY_UTF8
            """
            full_pos_to_quote = (0, 1, 3)
            full_pos_split_quote = ()
            _MINIMAL = """
            SELECT NAME, LANGUAGE, SQL_DATA_ACCESS, IS_DETERMINISTIC,
                   SECURITY_TYPE, DEFINER, PARAM_LIST, RETURNS,
                   BODY, SQL_MODE,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION,
                   DB_COLLATION
            """
            minimal_pos_to_quote = (0,)
            minimal_pos_split_quote = ()
            _OBJECT_QUERY = """
            FROM mysql.proc
            WHERE DB = '%s' AND TYPE = 'FUNCTION' %s
            """
            _ORDER_BY_DEFAULT = ""
            _ORDER_BY_NAME = ""
            exclude_param = "NAME"
        elif obj_type == _EVENT:
            _NAMES = """
            SELECT NAME
            """
            names_pos_to_quote = (0,)
            _FULL = """
            SELECT DB, NAME, BODY, DEFINER, EXECUTE_AT, INTERVAL_VALUE,
                   INTERVAL_FIELD, CREATED, MODIFIED, LAST_EXECUTED, STARTS,
                   ENDS, STATUS, ON_COMPLETION, SQL_MODE, COMMENT, ORIGINATOR,
                   TIME_ZONE, CHARACTER_SET_CLIENT, COLLATION_CONNECTION,
                   DB_COLLATION, BODY_UTF8
            """
            full_pos_to_quote = (0, 1)
            full_pos_split_quote = ()
            _MINIMAL = """
            SELECT NAME, DEFINER, BODY, STATUS,
                   EXECUTE_AT, INTERVAL_VALUE, INTERVAL_FIELD, SQL_MODE,
                   STARTS, ENDS, STATUS, ON_COMPLETION, ORIGINATOR,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION,
                   DB_COLLATION
            """
            minimal_pos_to_quote = (0,)
            minimal_pos_split_quote = ()
            _OBJECT_QUERY = """
            FROM mysql.event
            WHERE DB = '%s' %s
            """
            _ORDER_BY_DEFAULT = ""
            _ORDER_BY_NAME = ""
            exclude_param = "NAME"
        elif obj_type == _GRANT:
            _OBJECT_QUERY = """
            (
                SELECT GRANTEE, PRIVILEGE_TYPE, TABLE_SCHEMA,
                       NULL as TABLE_NAME, NULL AS COLUMN_NAME,
                       NULL AS ROUTINE_NAME
                FROM INFORMATION_SCHEMA.SCHEMA_PRIVILEGES
                WHERE table_schema = '%s'
            ) UNION (
                SELECT grantee, privilege_type, table_schema, table_name,
                       NULL, NULL
                FROM INFORMATION_SCHEMA.TABLE_PRIVILEGES
                WHERE table_schema = '%s'
            ) UNION (
                SELECT grantee, privilege_type, table_schema, table_name,
                       column_name, NULL
                FROM INFORMATION_SCHEMA.COLUMN_PRIVILEGES
                WHERE table_schema = '%s'
            ) UNION (
                SELECT CONCAT('''', User, '''@''', Host, ''''),  Proc_priv, Db,
                       Routine_name, NULL, Routine_type
                FROM mysql.procs_priv WHERE Db = '%s'
            ) ORDER BY GRANTEE ASC, PRIVILEGE_TYPE ASC, TABLE_SCHEMA ASC,
                       TABLE_NAME ASC, COLUMN_NAME ASC, ROUTINE_NAME ASC
            """
        else:
            return None

        col_options = {
            'columns': get_columns
        }
        pos_to_quote = ()
        pos_split_quote = ()
        # pylint: disable=R0101
        if obj_type == _GRANT:
            query = _OBJECT_QUERY % (self.db_name, self.db_name,
                                     self.db_name, self.db_name)
            return self.source.exec_query(query, col_options)
        else:
            if columns == "names":
                prefix = _NAMES
                if need_backtick:
                    pos_to_quote = names_pos_to_quote
                sufix = _ORDER_BY_NAME
            elif columns == "full":
                prefix = _FULL
                if need_backtick:
                    pos_to_quote = full_pos_to_quote
                    pos_split_quote = full_pos_split_quote
                sufix = _ORDER_BY_DEFAULT
            else:
                prefix = _MINIMAL
                if need_backtick:
                    pos_to_quote = minimal_pos_to_quote
                    pos_split_quote = minimal_pos_split_quote
                sufix = _ORDER_BY_DEFAULT
            # Form exclusion string
            exclude_str = ""
            if self.exclude_patterns:
                exclude_str = self.__build_exclude_patterns(exclude_param)
            query = (prefix + _OBJECT_QUERY + sufix) % (self.db_name,
                                                        exclude_str)
            res = self.source.exec_query(query, col_options)

            # Quote required identifiers with backticks
            if need_backtick:
                new_rows = []
                for row in res[1]:
                    # Recreate row tuple quoting needed elements with backticks
                    # Note: handle elements that can hold multiple values
                    # quoting them separately (e.g., multiple column names).
                    r = []
                    for i, data in enumerate(row):
                        if data and i in pos_to_quote:
                            if i in pos_split_quote:
                                cols = data.split(',')
                                data = ','.join(
                                    [quote_with_backticks(col, self.sql_mode)
                                     for col in cols]
                                )
                                r.append(data)
                            else:
                                r.append(quote_with_backticks(data,
                                                              self.sql_mode))
                        else:
                            r.append(data)
                    new_rows.append(tuple(r))

                # set new result with with required data quoted with backticks
                res = (res[0], new_rows)

            if res and obj_type == _VIEW:
                res = self._get_views_sorted_by_dependencies(res, columns,
                                                             not need_backtick)

            return res

    def _check_user_permissions(self, uname, host, access):
        """Check user permissions for a given privilege

        uname[in]          user name to check
        host[in]           host name of connection
        access[in]         privilege to check (e.g. "SELECT")

        Returns True if user has permission, False if not
        """
        user = User(self.source, uname + '@' + host)
        result = user.has_privilege(access[0], '*', access[1])
        return result

    def check_read_access(self, user, host, options):
        """Check access levels for reading database objects

        This method will check the user's permission levels for copying a
        database from this server.

        It will also skip specific checks if certain objects are not being
        copied (i.e., views, procs, funcs, grants).

        user[in]           user name to check
        host[in]           host name to check
        options[in]        dictionary of values to include:
            skip_views     True = no views processed
            skip_proc      True = no procedures processed
            skip_func      True = no functions processed
            skip_grants    True = no grants processed
            skip_events    True = no events processed

        Returns True if user has permissions and raises a UtilDBError if the
                     user does not have permission with a message that includes
                     the server context.
        """

        # Build minimal list of privileges for source access
        source_privs = []
        priv_tuple = (self.db_name, "SELECT")
        source_privs.append(priv_tuple)
        # if views are included, we need SHOW VIEW
        if not options.get('skip_views', False):
            priv_tuple = (self.db_name, "SHOW VIEW")
            source_privs.append(priv_tuple)
        # if procs, funcs, events or grants are included, we need read on
        # mysql db
        if not options.get('skip_procs', False) or \
           not options.get('skip_funcs', False) or \
           not options.get('skip_events', False) or \
           not options.get('skip_grants', False):
            priv_tuple = ("mysql", "SELECT")
            source_privs.append(priv_tuple)
        # if events, we need event
        if not options.get('skip_events', False):
            priv_tuple = (self.db_name, "EVENT")
            source_privs.append(priv_tuple)
        # if triggers, we need trigger
        if not options.get('skip_triggers', False):
            priv_tuple = (self.db_name, "TRIGGER")
            source_privs.append(priv_tuple)

        # Check permissions on source
        for priv in source_privs:
            if not self._check_user_permissions(user, host, priv):
                raise UtilDBError("User %s on the %s server does not have "
                                  "permissions to read all objects in %s. " %
                                  (user, self.source.role, self.db_name) +
                                  "User needs %s privilege on %s." %
                                  (priv[1], priv[0]), -1, priv[0])

        return True

    def check_write_access(self, user, host, options, source_objects=None,
                           do_drop=False):
        """Check access levels for creating and writing database objects

        This method will check the user's permission levels for copying a
        database to this server.

        It will also skip specific checks if certain objects are not being
        copied (i.e., views, procs, funcs, grants).

        user[in]           user name to check
        host[in]           host name to check
        options[in]        dictionary of values to include:
            skip_views     True = no views processed
            skip_proc      True = no procedures processed
            skip_func      True = no functions processed
            skip_grants    True = no grants processed
            skip_events    True = no events processed
        source_objects[in] Dictionary containing the list of objects from
                           source database
        do_drop[in]        True if the user is using --drop-first option

        Returns True if user has permissions and raises a UtilDBError if the
                     user does not have permission with a message that includes
                     the server context.
        """
        if source_objects is None:
            source_objects = {}

        dest_privs = [(self.db_name, "CREATE"),
                      (self.db_name, "ALTER"),
                      (self.db_name, "SELECT"),
                      (self.db_name, "INSERT"),
                      (self.db_name, "UPDATE"),
                      (self.db_name, "LOCK TABLES")]

        # Check for the --drop-first
        if do_drop:
            dest_privs.append((self.db_name, "DROP"))

        extra_privs = []
        super_needed = False

        try:
            res = self.source.exec_query("SELECT CURRENT_USER()")
            dest_user = res[0][0]
        except UtilError as err:
            raise UtilError("Unable to execute SELECT current_user(). Error: "
                            "{0}".format(err.errmsg))

        # CREATE VIEW is needed for views
        if not options.get("skip_views", False):
            views = source_objects.get("views", None)
            if views:
                extra_privs.append("CREATE VIEW")
                for item in views:
                    # Test if DEFINER is equal to the current user
                    if item[6] != dest_user:
                        super_needed = True
                        break

        # CREATE ROUTINE and EXECUTE are needed for procedures
        if not options.get("skip_procs", False):
            procs = source_objects.get("procs", None)
            if procs:
                extra_privs.append("CREATE ROUTINE")
                extra_privs.append("EXECUTE")
                if not super_needed:
                    for item in procs:
                        # Test if DEFINER is equal to the current user
                        if item[11] != dest_user:
                            super_needed = True
                            break

        # CREATE ROUTINE and EXECUTE are needed for functions
        # pylint: disable=R0101
        if not options.get("skip_funcs", False):
            funcs = source_objects.get("funcs", None)
            if funcs:
                if "CREATE ROUTINE" not in extra_privs:
                    extra_privs.append("CREATE ROUTINE")
                if "EXECUTE" not in extra_privs:
                    extra_privs.append("EXECUTE")
                if not super_needed:
                    trust_function_creators = False
                    try:
                        res = self.source.show_server_variable(
                            "log_bin_trust_function_creators"
                        )
                        if res and isinstance(res, list) and \
                                res[0][1] in ("ON", "1"):
                            trust_function_creators = True
                        # If binary log is enabled and
                        # log_bin_trust_function_creators is 0, we need
                        # SUPER privilege
                        super_needed = self.source.binlog_enabled() and \
                            not trust_function_creators
                    except UtilError as err:
                        raise UtilDBError("ERROR: {0}".format(err.errmsg))

                    if not super_needed:
                        for item in funcs:
                            # Test if DEFINER is equal to the current user
                            if item[11] != dest_user:
                                super_needed = True
                                break

        # EVENT is needed for events
        if not options.get("skip_events", False):
            events = source_objects.get("events", None)
            if events:
                extra_privs.append("EVENT")
                if not super_needed:
                    for item in events:
                        # Test if DEFINER is equal to the current user
                        if item[3] != dest_user:
                            super_needed = True
                            break

        # TRIGGER is needed for events
        if not options.get("skip_triggers", False):
            triggers = source_objects.get("triggers", None)
            if triggers:
                extra_privs.append("TRIGGER")
                if not super_needed:
                    for item in triggers:
                        # Test if DEFINER is equal to the current user
                        if item[18] != dest_user:
                            super_needed = True
                            break

        # Add SUPER privilege if needed
        if super_needed:
            dest_privs.append(("*", "SUPER"))

        # Add extra privileges needed
        for priv in extra_privs:
            dest_privs.append((self.db_name, priv))

        if not options.get('skip_grants', False):
            priv_tuple = (self.db_name, "GRANT OPTION")
            dest_privs.append(priv_tuple)

        # Check privileges on destination
        for priv in dest_privs:
            if not self._check_user_permissions(user, host, priv):
                raise UtilDBError("User %s on the %s server does not "
                                  "have permissions to create all objects "
                                  "in %s. User needs %s privilege on %s." %
                                  (user, self.source.role, priv[0], priv[1],
                                   priv[0]), -1, priv[0])

        return True

    def check_auto_increment(self, tbl=None):
        """Check for any tables in the database with auto_increment values
        of 0. This will require a special sql_mode to copy or export. The
        method returns True if any table has an auto_increment value of 0.
        If tbl provided, use that table in the query otherwise check all
        tables.

        tbl[in]      If provided, use this table name

        Returns True if any table has 0 in auto_increment, False if not
        """
        FIND_AUTO_INC_COLS = """
            SELECT table_name, column_name FROM INFORMATION_SCHEMA.COLUMNS
            WHERE table_schema = '{0}' AND extra LIKE '%auto_increment%'
        """
        AUTO_INC_ZERO = "SELECT * FROM {0}.`{1}` WHERE {2} < 1;"
        # Watchout for weird tick marks in the name
        if self.db_name.count("`") > 0:
            query = FIND_AUTO_INC_COLS.format(self.q_db_name)
        else:
            query = FIND_AUTO_INC_COLS.format(self.db_name)
        if tbl:
            query = "{0} AND table_name = '{1}'".format(query, tbl)
        res = self.source.exec_query(query)
        for row in res:
            # Watchout for weird tick marks.
            column = row[1]
            # pylint: disable=W0125
            if (i in row[1] for i in ('`', '"', "'")):
                column = "`{0}`".format(row[1])
            query = AUTO_INC_ZERO.format(self.q_db_name, row[0], column)
            res = self.source.exec_query(query)
            if res:
                return True
        return False
#
# Copyright (c) 2011, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains the methods for checking consistency among two databases.
"""

import re
import tempfile
import difflib


# The following are the queries needed to perform table data consistency
# checking.

_COMPARE_TABLE_NAME = 'compare_{tbl}'

_COMPARE_TABLE_DROP = """
    DROP TABLE {db}.{compare_tbl};
"""

# The Length of key for the span index has been increased from 4 to 8 allow
# more accurate hits. This may slow the algorithm for big dbs, for future
# the key length could be calculated by the number of rows.
DEFAULT_SPAN_KEY_SIZE = 8

# Max allowed size for the span_key. Must be smaller or equal than the size of
# the key hash because it is a substring of it. Note: 32 = binary(16).
MAX_SPAN_KEY_SIZE = 32

# Note: Use a composed index (span, pk_hash) instead of only for column "span"
# due to the "ORDER BY pk_hash" in the _COMPARE_DIFF query.
_COMPARE_TABLE = """
    CREATE TEMPORARY TABLE {db}.{compare_tbl} (
        compare_sign binary(16) NOT NULL,
        pk_hash binary(16) NOT NULL,
        {pkdef}
        span binary({span_key_size}) NOT NULL,
        INDEX span_key (span, pk_hash)) ENGINE=MyISAM
"""

_COMPARE_INSERT = """
    INSERT INTO {db}.{compare_tbl}
        (compare_sign, pk_hash, {pkstr}, span)
    SELECT
        UNHEX(MD5(CONCAT_WS('/', {colstr}))),
        UNHEX(MD5(CONCAT_WS('/', {pkstr}))),
        {pkstr},
        UNHEX(LEFT(MD5(CONCAT_WS('/', {pkstr})), {span_key_size}))
    FROM {db}.{table}
"""

_COMPARE_SUM = """
    SELECT HEX(span), COUNT(*) as cnt,
        CONCAT(SUM(CONV(SUBSTRING(HEX(compare_sign),1,8),16,10)),
        SUM(CONV(SUBSTRING(HEX(compare_sign),9,8),16,10)),
        SUM(CONV(SUBSTRING(HEX(compare_sign),17,8),16,10)),
        SUM(CONV(SUBSTRING(HEX(compare_sign),25,8),16,10))) as sig
    FROM {db}.{compare_tbl}
    GROUP BY span
"""

# ORDER BY is used to ensure determinism for the order in which rows are
# returned between compared tables, otherwise rows might be returned in a
# different for server without the binlog enable (--log-bin option) leading to
# incorrect SQL diff statements (UPDATES).
_COMPARE_DIFF = """
    SELECT * FROM {db}.{compare_tbl}
    WHERE span = UNHEX('{span}') ORDER BY pk_hash
"""

_COMPARE_SPAN_QUERY = """
    SELECT * FROM {db}.{table} WHERE {where}
"""

_ERROR_NO_PRI_KEY = ("The table {tb} does not have an usable Index or "
                     "primary key.")

_WARNING_INDEX_NOT_USABLE = ("# Warning: Specified index {idx} for table {tb}"
                             " cannot be used. It contains at least one "
                             "column that accepts null values.")

_RE_EMPTY_ALTER_TABLE = "^ALTER TABLE {0};$"

_RE_DASHES_DIG = re.compile(r"^\-{3}\s\d+")

_RE_ASTERISK_DIG = re.compile(r"^\*{3}\s\d+")

_RE_ASTERISKS = re.compile(r"^\*{15}.{0,2}$")


def _get_objects(server, database, options):
    """Get all objects from the database (except grants)

    server[in]        connected server object
    database[in]      database names
    options[in]       global options

    Returns list - objects in database
    """
    options["skip_grants"] = True   # Tell db class to skip grants

    db_obj = Database(server, database, options)
    if not db_obj.exists():
        raise UtilDBError("The database does not exist: {0}".format(database))
    db_obj.init()
    db_objects = db_obj.objects
    db_objects.sort()

    return db_objects


def get_create_object(server, object_name, options, object_type):
    """Get the object's create statement.

    This method retrieves the object create statement from the database.

    server[in]        server connection
    object_name[in]   name of object in the form db.objectname
    options[in]       options: verbosity, quiet
    object_type[in]   type of the specified object (e.g, TABLE, PROCEDURE,
                      etc.).

    Returns string : create statement or raise error if object or db not exist
    """

    verbosity = options.get("verbosity", 0)
    quiet = options.get("quiet", False)

    # Get the sql_mode set on server
    sql_mode = server.select_variable("SQL_MODE")

    db_name, obj_name = parse_object_name(object_name, sql_mode)
    obj = [db_name]

    if db_name is None:
        raise UtilError(PARSE_ERR_OBJ_NAME_FORMAT.format(
            obj_name=object_name, option=object_type.lower()))
    db = Database(server, obj[0], options)

    # Error if database does not exist
    if not db.exists():
        raise UtilDBError("The database does not exist: {0}".format(obj[0]))

    if not obj_name or object_type == 'DATABASE':
        obj.append(db_name)
    else:
        obj.append(obj_name)

    create_stmt = db.get_create_statement(obj[0], obj[1], object_type)

    if verbosity > 0 and not quiet:
        if obj_name:
            print("\n# Definition for object {0}.{1}:"
                  "".format(remove_backtick_quoting(db_name, sql_mode),
                            remove_backtick_quoting(obj_name, sql_mode)))
        else:
            print("\n# Definition for object {0}:"
                  "".format(remove_backtick_quoting(db_name, sql_mode)))
        print create_stmt

    return create_stmt


def print_missing_list(item_list, first, second):
    """Print the list of items in the list.

    This method is used to display the list of objects that are missing
    from one of the databases in the compare.

    item_list[in]     list of items to print
    first[in]         name of first database
    second[in]        name of second database

    Returns bool True if items in the list, False if list is empty
    """
    if len(item_list) == 0:
        return False
    print "# WARNING: Objects in {0} but not in {1}:".format(first, second)
    for item in item_list:
        print "# {0:>12}: {1}".format(item[0], item[1][0])
    return True


def server_connect(server1_val, server2_val, object1, object2, options):
    """Connect to the servers

    This method connects to the servers and checks to see if the objects
    are different: db1.obj1 != db2.obj2 by name match.

    server1_val[in]    a dictionary containing connection information for the
                       first server including:
                       (user, password, host, port, socket)
    server2_val[in]    a dictionary containing connection information for the
                       second server including:
                       (user, password, host, port, socket)
    object1[in]        the first object in the compare
    object2[in]        the second object in the compare
    options[in]        a dictionary containing the options for the operation:
                       (quiet, verbosity)

    Returns tuple of Server objects (server1, server2)
    """
    quiet = options.get("quiet", False)
    charset = options.get("charset", None)

    conn_options = {
        'quiet': quiet,
        'src_name': "server1",
        'dest_name': "server2",
        'version': "5.1.30",
        'charset': charset,
    }
    servers = connect_servers(server1_val, server2_val, conn_options)
    server1 = servers[0]
    server2 = servers[1]
    if server2 is None:
        server2 = server1

    # Check if the specified objects and servers are the same.
    if object1 == object2 and server1.port == server2.port and \
       server1.is_alias(server2.host):
        raise UtilError("Comparing the same object on the same server.")

    return (server1, server2)


def get_common_lists(list1, list2):
    """Compare the items in two lists

    This method compares the items in two lists returning those items that
    appear in both lists as well as two lists that contain those unique items
    from the original lists.

    For example, given {s,b,c,d,e,f} and {a,b,c,d,e,z}, the lists returned are
        both = {b,c,d,e}
        in list1 not list2 = {s,f}
        in list2 not list1 = {a.z]

    list1[in]         first list
    list2[in]         second list

    Returns three lists
    """
    s1 = set(list1)
    s2 = set(list2)
    both = s1 & s2
    return(list(both), list(s1 - both), list(s2 - both))


def get_common_objects(server1, server2, db1, db2,
                       print_list=True, options=None):
    """Get a list of the common objects among two databases.

    server1[in]        first server connection
    server2[in]        second server connection
    object1[in]        the first object in the compare in the form: (db.name)
    object2[in]        the second object in the compare in the form: (db.name)
    print_list[in]     if True, print list of missing items
    options[in]        global options

    Returns (tuple) lists containing: items in both,
                                      items in db1 and not in db2,
                                      items in db2 not in db1
    """

    if options is None:
        options = {}
    db1_objects = _get_objects(server1, db1, options)
    db2_objects = _get_objects(server2, db2, options)

    in_both, in_db1_not_db2, in_db2_not_db1 = get_common_lists(db1_objects,
                                                               db2_objects)
    if print_list:
        server1_str = "server1." + db1
        if server1 == server2:
            server2_str = "server1." + db2
        else:
            server2_str = "server2." + db2
        print_missing_list(in_db1_not_db2, server1_str, server2_str)
        print_missing_list(in_db2_not_db1, server2_str, server1_str)

    return (in_both, in_db1_not_db2, in_db2_not_db1)


def _get_diff(list1, list2, object1, object2, difftype, compact=False):
    """Get the difference among two lists.

    This method finds the difference of two lists using either unified,
    context, or differ-style output.

    Note: We must strip not only \n but also trailing blanks due to change in
          Python 2.7.1 handling of difflib methods.

    list1[in]         The base list
    list2[in]         The list used for compare
    object1[in]       The 'from' or source
    object2[in]       The 'to' or difference destination
    difftype[in]      Difference type
    compact[in]       IF True, the resulting diff it will not contain all
                      the control lines, resulting in a fewer lines.

    Returns list - differences or []
    """
    diff_str = []

    # Generate unified is SQL is specified for use in reporting errors
    if difftype in ['unified', 'sql']:
        for line in difflib.unified_diff(list1, list2,
                                         fromfile=object1, tofile=object2):
            if compact:
                if not line.startswith("@@ "):
                    diff_str.append(line.strip('\n').rstrip(' '))
            else:
                diff_str.append(line.strip('\n').rstrip(' '))
    elif difftype == 'context':
        for line in difflib.context_diff(list1, list2,
                                         fromfile=object1, tofile=object2):
            if compact:
                if _RE_DASHES_DIG.match(line):
                    diff_str.append("---")
                elif _RE_ASTERISK_DIG.match(line):
                    diff_str.append("***")
                # Asterisks are used as row separators too
                elif not _RE_ASTERISKS.match(line):
                    diff_str.append(line.strip('\n').rstrip(' '))
            else:
                diff_str.append(line.strip('\n').rstrip(' '))
    else:
        has_diff = False
        for line in difflib.ndiff(list1, list2):
            diff_str.append(line.strip('\n').rstrip(' '))
            if line[0] in ['-', '+', '?']:
                has_diff = True

        if not has_diff:
            diff_str = []

    if compact and difftype != 'differ' and difftype != 'context':
        return diff_str[2:]
    # If objects names are the same, avoid print them
    elif (compact and difftype == 'context' and len(diff_str) > 0 and
          diff_str[0].endswith(diff_str[0][3:])):
        return diff_str[2:]

    return diff_str


def _get_transform(server1, server2, object1, object2, options,
                   object_type):
    """Get the transformation SQL statements

    This method generates the SQL statements to transform the destination
    object based on direction of the compare.

    server1[in]        first server connection
    server2[in]        second server connection
    object1            the first object in the compare in the form: (db.name)
    object2            the second object in the compare in the form: (db.name)
    options[in]        a dictionary containing the options for the operation:
                       (quiet, etc.)
    object_type[in]    type of the objects to be compared (e.g., TABLE,
                       PROCEDURE, etc.).

    Returns tuple - (bool - same db name?, list of transformation statements)
    """

    try:
        db1, name1 = parse_object_name(object1,
                                       server1.select_variable("SQL_MODE"))

        db2, name2 = parse_object_name(object2,
                                       server2.select_variable("SQL_MODE"))
    except:
        raise UtilError("Invalid object name arguments for _get_transform"
                        "(): %s, %s." % (object1, object2))
    # If the second part of the object qualified name is None, then the format
    # is not 'db_name.obj_name' for object1 and therefore must treat it as a
    # database name. (supports backticks and the use of '.' (dots) in names.)
    if not name1 or object_type == 'DATABASE':

        # We are working with databases so db and name need to be set
        # to the database name to tell the get_object_definition() method
        # to retrieve the database information.
        name1 = db1
        name2 = db2

    db_1 = Database(server1, db1, options)
    db_2 = Database(server2, db2, options)

    obj1 = db_1.get_object_definition(db1, name1, object_type)
    obj2 = db_2.get_object_definition(db2, name2, object_type)

    # Get the transformation based on direction.
    transform_str = []
    xform = SQLTransformer(db_1, db_2, obj1[0], obj2[0], object_type,
                           options.get('verbosity', 0), options)

    differences = xform.transform_definition()
    if differences and len(differences) > 0:
        transform_str.extend(differences)

    return transform_str


def _check_tables_structure(server1, server2, object1, object2, options,
                            diff_type):
    """Check if the tables have the same structure.

    This method compares the tables structure ignoring the order of the
    columns and retrieves the differences between the table options.

    server1[in]        first server connection.
    server2[in]        second server connection.
    object1            the first object in the compare in the form: (db.name).
    object2            the second object in the compare in the form: (db.name).
    options[in]        a dictionary containing the options for the operation:
                       (quiet, verbosity, difftype, width, suppress_sql).
    diff_type[in]      difference type.

    Returns a tuple (bool, list, bool) - The first tuple value is a boolean
    that indicates if both tables have the same structure (i.e. column
    definitions). The second returns the table options differences. Finally,
    the third is a boolean indicating if the partition options are the same.
    """
    try:
        db1, name1 = parse_object_name(object1,
                                       server1.select_variable("SQL_MODE"))

        db2, name2 = parse_object_name(object2,
                                       server2.select_variable("SQL_MODE"))
    except:
        raise UtilError("Invalid object name arguments for diff_objects(): "
                        "{0}, {1}.".format(object1, object2))

    compact_diff = options.get("compact", False)

    # If the second part of the object qualified name is None, then the format
    # is not 'db_name.obj_name' for object1 and therefore must treat it as a
    # database name.
    if not name1:
        return None, None, None

    db_1 = Database(server1, db1, options)
    db_2 = Database(server2, db2, options)

    # Get tables definitions.
    table_1 = db_1.get_object_definition(db1, name1, 'TABLE')[0]
    table_2 = db_2.get_object_definition(db2, name2, 'TABLE')[0]

    # Check table options.
    table1_opts = db_1.get_table_options(db1, name1)
    table2_opts = db_2.get_table_options(db2, name2)
    diff = _get_diff(table1_opts, table2_opts, object1, object2, diff_type,
                     compact=compact_diff)

    # Check if both tables have the same columns definition.
    # Discard column order.
    table_1_cols = [col[1:] for col in table_1[1]]
    table_2_cols = [col[1:] for col in table_2[1]]
    same_cols_def = set(table_1_cols) == set(table_2_cols)

    # Check if both tables have the same partition options.
    # Discard partition name.
    table_1_part = [part[1:] for part in table_1[2]]
    table_2_part = [part[1:] for part in table_2[2]]
    same_partition_opts = set(table_1_part) == set(table_2_part)

    # Return tables check results.
    return same_cols_def, diff, same_partition_opts


def build_diff_list(diff1, diff2, transform1, transform2,
                    first, second, options):
    """Build the list of differences

    This method builds a list of difference statements based on whether
    the lists are the result of an SQL statement generation, object definition
    differences, or data differences.

    Note: to specify a non-SQL difference for data, set
          options['data_diff'] = True

    diff1[in]              definition diff for first server
    diff2[in]              definition diff for second server
    transform1[in]         transformation for first server
    transform2[in]         transformation for second server
    first[in]              name of first server (e.g. server1)
    second[in]             name of second server (e.g. server2)
    options[in]            options for building the list

    Returns list = list of differences or transformations
    """
    # Don't build the list if there were no differences.
    if len(diff1) == 0:
        return []

    reverse = options.get('reverse', False)
    diff_list = []
    if options.get('difftype') == 'sql':
        if len(transform1) == 0:
            diff_list.append("\n# WARNING: Cannot generate SQL statements "
                             "for these objects.")
            diff_list.append("# Check the difference output for other "
                             "discrepencies.")
            diff_list.extend(diff1)
        else:
            diff_list.append("# Transformation for --changes-for=%s:\n#\n" %
                             first)
            diff_list.extend(transform1)
            diff_list.append("")
            if reverse and len(transform2) > 0:
                diff_list.append("#\n# Transformation for reverse changes "
                                 "(--changes-for=%s):\n#" % second)
                for row in transform2:
                    sub_rows = row.split('\n')
                    for sub_row in sub_rows:
                        diff_list.append("# %s" % sub_row)
                diff_list.append("#\n")
    else:
        # Don't print messages for a data difference (non-SQL)
        if not options.get('data_diff', False):
            diff_list.append("# Object definitions differ. "
                             "(--changes-for=%s)\n#\n" % first)
        diff_list.extend(diff1)
        if reverse and len(diff2) > 0:
            diff_list.append("")
            if not options.get('data_diff', False):
                diff_list.append("#\n# Definition diff for reverse changes "
                                 "(--changes-for=%s):\n#" % second)
            for row in diff2:
                diff_list.append("# %s" % row)
            diff_list.append("#\n")

    return diff_list


def diff_objects(server1, server2, object1, object2, options, object_type):
    """diff the definition (CREATE statement) of two objects

    Produce a diff in the form unified, context, or ndiff of two objects.
    Note: objects must exist else exception is thrown.

    With the transform option, the method will generate the transformation
    SQL statements in addition to the differences found in the CREATE
    statements.

    When the --difftype == 'sql', the method will print the sql statements
    to stdout. To suppress this, use options: quiet=True, suppress_sql=True.

    server1[in]        first server connection
    server2[in]        second server connection
    object1[in]        the first object in the compare in the form: (db.name)
    object2[in]        the second object in the compare in the form: (db.name)
    options[in]        a dictionary containing the options for the operation:
                       (quiet, verbosity, difftype, width, suppress_sql)
    object_type[in]    type of the objects to be compared (e.g., TABLE,
                       PROCEDURE, etc.).

    Returns None = objects are the same, diff[] = objects differ
    """
    quiet = options.get("quiet", False)
    difftype = options.get("difftype", "unified")
    width = options.get("width", 75)
    direction = options.get("changes-for", None)
    reverse = options.get("reverse", False)
    skip_table_opts = options.get("skip_table_opts", False)
    compact_diff = options.get("compact", False)

    # Get object CREATE statement.
    # Note: Table options are discarded if option skip_table_opts=True.
    object1_create = get_create_object(server1, object1, options, object_type)
    object2_create = get_create_object(server2, object2, options, object_type)

    # Only target CREATE DATABASE difference if decorations differ,
    # not just the database names. So we isolate the CREATE statement
    # without the names or +/- and compare. If different, print the
    # difference report otherwise, ignore it.
    if (object_type == "DATABASE") and (object1 != object2):
        quotes = ["'", '"', "`"]
        db1 = object1.translate(None, "".join(quotes))
        db2 = object2.translate(None, "".join(quotes))
        first = object1_create.replace(db1, "")[1::]
        second = object2_create.replace(db2, "")[1::]
        if first == second:
            object1_create = ""
            object2_create = ""

    if not quiet:
        msg = "# Comparing {0} to {1} ".format(object1, object2)
        print msg,
        linelen = width - (len(msg) + 10)
        print ' ' * linelen,

    object1_create_list = object1_create.split('\n')
    object2_create_list = object2_create.split('\n')

    diff_server1 = []
    diff_server2 = []
    transform_server1 = []
    transform_server2 = []

    # Get the difference based on direction.
    if direction == 'server1' or direction is None or reverse:
        diff_server1 = _get_diff(object1_create_list,
                                 object2_create_list,
                                 object1, object2, difftype,
                                 compact=compact_diff)
        # If there is a difference. Check for SQL output
        if difftype == 'sql' and len(diff_server1) > 0:
            transform_server1 = _get_transform(server1, server2,
                                               object1, object2, options,
                                               object_type)

    if direction == 'server2' or reverse:
        diff_server2 = _get_diff(object2_create_list,
                                 object1_create_list,
                                 object2, object1, difftype,
                                 compact=compact_diff)
        # If there is a difference. Check for SQL output
        if difftype == 'sql' and len(diff_server2) > 0:
            transform_server2 = _get_transform(server2, server1,
                                               object2, object1, options,
                                               object_type)

    # Build diff list
    if direction == 'server1' or direction is None:
        diff_list = build_diff_list(diff_server1, diff_server2,
                                    transform_server1, transform_server2,
                                    'server1', 'server2', options)
    else:
        diff_list = build_diff_list(diff_server2, diff_server1,
                                    transform_server2, transform_server1,
                                    'server2', 'server1', options)

    # Note: table structure check ignores columns order.
    same_tbl_def = None
    tbl_opts_diff = None
    same_part_def = None
    if object_type == 'TABLE':
        same_tbl_def, tbl_opts_diff, same_part_def = _check_tables_structure(
            server1, server2, object1, object2, options, difftype
        )

    # Check if ALTER TABLE statement have changes. If not, it is probably
    # because there are differences but they have no influence on the create
    # table, such as different order on indexes.
    if "ANSI_QUOTES" in server1.select_variable("SQL_MODE"):
        regex_pattern = REGEXP_QUALIFIED_OBJ_NAME_AQ
    else:
        regex_pattern = _RE_EMPTY_ALTER_TABLE.format(REGEXP_QUALIFIED_OBJ_NAME)
    if diff_list and same_tbl_def and same_part_def and \
       re.match(regex_pattern, diff_list[1]):
        print("[PASS]")
        return None

    if diff_list and direction is None and same_tbl_def and not tbl_opts_diff:
        if not quiet:
            print("[PASS]")
            print("# WARNING: The tables structure is the same, but the "
                  "columns order is different. Use --change-for to take the "
                  "order into account.")
        return None

    # Check for failure to generate SQL statements
    if (difftype == 'sql') and \
       ((direction == 'server1' and transform_server1 == [] and
         diff_server1 != []) or
        (direction == 'server2' and transform_server2 == [] and
         diff_server2 != [])):

        # Here we found no transformations. So either the change is nothing
        # more than the database name or we missed something. Send a
        # warning to the user.

        if not quiet:
            print "[FAIL]"

        for line in diff_list:
            print line

        print("# WARNING: Could not generate SQL statements for differences "
              "between {0} and {1}. No changes required or not supported "
              "difference.".format(object1, object2))

        return diff_list

    if len(diff_list) > 0:
        if not quiet:
            print "[FAIL]"

        if not quiet or \
           (not options.get("suppress_sql", False) and difftype == 'sql'):
            for line in diff_list:
                print line

            # Full ALTER TABLE for partition difference cannot be generated
            # (not supported). Notify the user.
            if same_part_def is False:
                print("# WARNING: Partition changes were not generated "
                      "(not supported).")

        return diff_list

    if not quiet:
        print("[PASS]")
        if skip_table_opts and tbl_opts_diff:
            print("# WARNING: Table options are ignored and differences were "
                  "found:")
            for diff in tbl_opts_diff:
                print("# {0}".format(diff))

    return None


def _drop_compare_object(server, db_name, tbl_name):
    """Drop the compare object table

    server[in]             Server instance
    db_name[in]            database name
    tbl_name[in]           table name
    """
    # Quote compare table appropriately with backticks
    sql_mode = server.select_variable("SQL_MODE")
    q_db_name = db_name if is_quoted_with_backticks(db_name, sql_mode) \
        else quote_with_backticks(db_name, sql_mode)
    if is_quoted_with_backticks(tbl_name, sql_mode):
        q_tbl_name = remove_backtick_quoting(tbl_name, sql_mode)
    else:
        q_tbl_name = tbl_name
    q_tbl_name = quote_with_backticks(
        _COMPARE_TABLE_NAME.format(tbl=q_tbl_name), sql_mode)

    try:
        # set autocommit=1 if it is 0, because CREATE TEMPORARY TABLE and
        # DROP TEMPORARY TABLE can be executed in a non-transactional context
        # only, and require that AUTOCOMMIT = 1.
        toggle_server = not server.autocommit_set()
        if toggle_server:
            server.toggle_autocommit(enable=True)
        server.exec_query(_COMPARE_TABLE_DROP.format(db=q_db_name,
                                                     compare_tbl=q_tbl_name))
        if toggle_server:
            server.toggle_autocommit(enable=False)
    except:
        pass


def _get_compare_objects(index_cols, table1,
                         span_key_size=DEFAULT_SPAN_KEY_SIZE):
    """Build the compare table and identify the primary index

    This method creates the compare table for use in forming the MD5 hash
    of rows and a hash of the primary key. It also forms the primary key
    list of columns.

    index_cols[in]    a list of columns that form the primary key in the form
                      (column_name, type)
    table1[in]        a Table instance of the original table

    span_key_size[in] the size of key used for the hash.

    Returns tuple (table create statement, concatenated string of the
                   primary index columns)
    """
    table = None

    # build primary key col definition
    index_str = ''.join("{0}, ".format(quote_with_backticks(col[0],
                                                            table1.sql_mode))
                        for col in index_cols)
    index_defn = ''.join("{0} {1}, ".
                         format(quote_with_backticks(col[0], table1.sql_mode),
                                col[1])
                         for col in index_cols)
    if index_defn == "":
        raise UtilError("Cannot generate index definition")
    else:
        # Quote compare table appropriately with backticks
        q_tbl_name = quote_with_backticks(
            _COMPARE_TABLE_NAME.format(tbl=table1.tbl_name), table1.sql_mode)

        table = _COMPARE_TABLE.format(db=table1.q_db_name,
                                      compare_tbl=q_tbl_name,
                                      pkdef=index_defn,
                                      span_key_size=span_key_size / 2)

    return (table, index_str)


def _setup_compare(table1, table2, span_key_size, use_indexes=None):
    """Create and populate the compare summary tables

    This method creates the condensed hash table used to compare groups
    (span) of records. It also creates the Table instance for each table
    and populates values in the table information dictionary for use
    in other methods.

    The method also checks to ensure the tables have primary keys and that
    the keys are the same (have the same columns). An error is raised if
    neither of these are met.

    table1[in]            table1 Table instance
    table2[in]            table2 Table instance
    span_key_size[in]     the size of key used for the hash.
    use_indexes[in]       a tuple of the indexes names that can be used as
                          an unique key, (for_table_1, for_table_2), they will
                          be tested for columns that not accept null.
    diag_msgs[out]       a list of debug and warning messages.

    Returns four-tuple - string representations of the primary index columns,
    the index_columns, the index name used and diagnostic messages.
    """

    def get_column_names_types_for_index(index, table):
        """Useful method to get the columns name and type used by an index
        """
        tb_columns = table.get_col_names_types()
        table_idx = [col_row for column in index.columns
                     for col_row in tb_columns if column[0] == col_row[0]]
        return table_idx

    def find_candidate_indexes(candidate_idexes, no_null_idxes_tb,
                               table_name, diag_msgs):
        """This method search the user's candidate indexes in the given list
        of unique indexes with no null columns. Table name is user to
        create the warning message if the candidate index has a column that
        accepts null values.
        """
        indexs_found = []
        for cte_index in candidate_idexes:
            for no_null_idx in no_null_idxes_tb:
                if no_null_idx.q_name == cte_index:
                    indexs_found.append((no_null_idx.q_name, no_null_idx))
                    break
            else:
                diag_msgs.append(
                    _WARNING_INDEX_NOT_USABLE.format(
                        idx=cte_index, tb=table_name)
                )
        return indexs_found

    diag_msgs = []
    server1 = table1.server
    server2 = table2.server

    # get not nullable indexes for tables
    table1.get_indexes()
    table2.get_indexes()
    no_null_idxes_tb1 = table1.get_not_null_unique_indexes()
    no_null_idxes_tb2 = table2.get_not_null_unique_indexes()

    # if table does not have non nullable unique keys, do not continue.
    if not no_null_idxes_tb1 or not no_null_idxes_tb2:
        raise UtilError(_ERROR_NO_PRI_KEY.format(tb=table1.tbl_name))

    table1_idx = []
    table2_idx = []
    # If user specified indexes with --use-indexes
    if use_indexes:
        # pylint: disable=W0633
        candidate_idxs_tb1, candidate_idxs_tb2 = use_indexes
        # Check if indexes exist,
        for cte_idx in candidate_idxs_tb1:
            if not table1.has_index(cte_idx):
                raise UtilError("The specified index {0} was not found in "
                                "table {1}".format(cte_idx, table1.table))
        for cte_idx in candidate_idxs_tb2:
            if not table2.has_index(cte_idx):
                raise UtilError("The specified index {0} was not found in "
                                "table {1}".format(cte_idx, table2.table))

        # Find the user index specified with --use-indexes
        unique_indexes_tb1 = find_candidate_indexes(
            candidate_idxs_tb1, no_null_idxes_tb1, table1.table, diag_msgs)

        unique_indexes_tb2 = find_candidate_indexes(
            candidate_idxs_tb2, no_null_idxes_tb2, table1.table, diag_msgs)

        if unique_indexes_tb1:
            table1_idx_name = unique_indexes_tb1[0][0]
            table1_idx = get_column_names_types_for_index(
                unique_indexes_tb1[0][1],
                table1
            )
        if unique_indexes_tb2:
            table2_idx = get_column_names_types_for_index(
                unique_indexes_tb2[0][1],
                table2
            )

    # If no user defined index or accepts nulls, use first unique not nullable
    if not table1_idx:
        table1_idx_name = no_null_idxes_tb1[0].name
        table1_idx = get_column_names_types_for_index(no_null_idxes_tb1[0],
                                                      table1)
    if not table2_idx:
        table2_idx = get_column_names_types_for_index(no_null_idxes_tb2[0],
                                                      table2)

    if len(table1_idx) != len(table2_idx):
        raise UtilError("Indexes are not the same.")

    # drop the temporary tables
    _drop_compare_object(server1, table1.db_name, table1.tbl_name)
    _drop_compare_object(server2, table2.db_name, table2.tbl_name)

    # Build the primary key hash if needed
    tbl1_table, pri_idx1 = _get_compare_objects(table1_idx, table1,
                                                span_key_size)
    tbl2_table, pri_idx2 = _get_compare_objects(table2_idx, table2,
                                                span_key_size)

    if tbl1_table is None or tbl2_table is None:
        raise UtilError("Cannot create compare table.")

    # Create the compare tables

    # set autocommit=1 if it is 0, because CREATE TEMPORARY TABLE and DROP
    # TEMPORARY TABLE can be executed in a non-transactional context only, and
    # require that AUTOCOMMIT = 1.

    # Check if server1 and server2 have autocommit=0, and if so set the flag
    #  to 1 and execute the create temporary table query.
    must_toggle_s1 = not server1.autocommit_set()
    if must_toggle_s1:
        server1.toggle_autocommit(enable=True)
    server1.exec_query(tbl1_table)

    must_toggle_s2 = not server2.autocommit_set()
    if must_toggle_s2:
        server2.toggle_autocommit(enable=True)
    server2.exec_query(tbl2_table)

    # if the autocommit flag was toggled, return it to its previous value.
    if must_toggle_s1:
        server1.toggle_autocommit(enable=False)
    if must_toggle_s2:
        server2.toggle_autocommit(enable=False)

    return (pri_idx1, pri_idx2, table1_idx, table1_idx_name, diag_msgs)


def _make_sum_rows(table, idx_str, span_key_size=8):
    """Populate the summary table

    This method inserts rows into the compare table from the original table
    then forms the summary table by combining a prefix of the primary key
    hash (group by).

    table[in]         Table instance
    idx_str[in]       string representation of primary key columns

    Returns result from
    """
    col_str = ", ".join(table.get_col_names(True))

    # Lock table first
    tbl_lock_list = [
        (table.table, 'READ'),
        ("%s.compare_%s" % (table.db_name, table.tbl_name), 'WRITE')
    ]
    my_lock = Lock(table.server, tbl_lock_list)

    # Quote compare table appropriately with backticks
    q_tbl_name = quote_with_backticks(
        _COMPARE_TABLE_NAME.format(tbl=table.tbl_name),
        table.sql_mode
    )

    table.server.exec_query(
        _COMPARE_INSERT.format(db=table.q_db_name, compare_tbl=q_tbl_name,
                               colstr=col_str.strip(", "),
                               pkstr=idx_str.strip(", "),
                               table=table.q_tbl_name,
                               span_key_size=span_key_size))

    res = table.server.exec_query(
        _COMPARE_SUM.format(db=table.q_db_name, compare_tbl=q_tbl_name))

    # Unlock table
    my_lock.unlock()

    return res


def _get_rows_span(table, span, index):
    """Get the rows corresponding to a list of span values

    This method returns the rows from the original table that match the
    span value presented.

    TODO: This may need refactoring to make it more efficient.
          For example, use a WHERE clause such as:
          WHERE some_col IN ('a','b')

    table[in]         Table instance
    span[in]          span value

    Returns rows from original table
    """
    server = table.server
    rows = []
    ukeys = [col[0] for col in index]
    # build WHERE clause
    for row in span:
        # Quote compare table appropriately with backticks
        q_tbl_name = quote_with_backticks(
            _COMPARE_TABLE_NAME.format(tbl=table.tbl_name),
            table.sql_mode
        )

        span_rows = server.exec_query(
            _COMPARE_DIFF.format(db=table.q_db_name, compare_tbl=q_tbl_name,
                                 span=row))
        # Loop through multiple rows with the same span value.
        for res_row in span_rows:
            pk = res_row[2:-1]
            where_clause = ' AND '.join("{0} = '{1}'".
                                        format(key, col)
                                        for key, col in zip(ukeys, pk))
            orig_rows = server.exec_query(
                _COMPARE_SPAN_QUERY.format(db=table.q_db_name,
                                           table=table.q_tbl_name,
                                           where=where_clause))
            rows.append(orig_rows[0])

    return rows


def _get_changed_rows_span(table1, table2, span, index):
    """Get the original changed rows corresponding to a list of span values.

    This method returns the changed rows from the original tables that match
    the given list of span keys. Several rows might be associated to each span,
    including unchanged, changed, missing or extra rows. This method takes all
    these situations into account, ignoring unchanged rows and separating
    changed rows from missing/extra rows for each table when retrieving the
    original data. This separation is required in order to generate the
    appropriate SQL diff statement (UPDATE, INSERT, DELETE) later.

    table1[in]      First table instance.
    table2[in]      Second table instance.
    span[in]        List of span keys.
    index[in]       Used table index (unique key).

    Returns the changed rows from original tables, i.e., a tuple with two
    elements containing the changes for each table. At its turn, the element
    for each table is another tuple where the first element contains the list
    of changed rows and the second the list of extra rows (compared to the
    other table).
    """
    # Get all span rows for table 1.
    server1 = table1.server
    full_span_data_1 = []
    for row in span:
        # Quote compare table appropriately with backticks
        q_tbl_name = quote_with_backticks(
            _COMPARE_TABLE_NAME.format(tbl=table1.tbl_name),
            table1.sql_mode
        )

        span_rows = server1.exec_query(
            _COMPARE_DIFF.format(db=table1.q_db_name, compare_tbl=q_tbl_name,
                                 span=row))
        # Auxiliary set with (compare_sign, pk_hash) tuples for table.
        cmp_signs = set([(row[0], row[1]) for row in span_rows])
        # Keep span rows and auxiliary data for table 1.
        full_span_data_1.append((span_rows, cmp_signs))

    # Get all span rows for table 2.
    server2 = table2.server
    full_span_data_2 = []
    for row in span:
        # Quote compare table appropriately with backticks
        q_tbl_name = quote_with_backticks(
            _COMPARE_TABLE_NAME.format(tbl=table2.tbl_name),
            table2.sql_mode
        )

        span_rows = server2.exec_query(
            _COMPARE_DIFF.format(db=table2.q_db_name, compare_tbl=q_tbl_name,
                                 span=row))
        # Auxiliary set with (compare_sign, pk_hash) tuples for table.
        cmp_signs = set([(row[0], row[1]) for row in span_rows])
        # Keep span rows and auxiliary data for table 1.
        full_span_data_2.append((span_rows, cmp_signs))

    # List of key columns
    ukeys = [col[0] for col in index]

    # Get the original diff rows for tables 1 and 2.
    changed_in1 = []
    extra_in1 = []
    changed_in2 = []
    extra_in2 = []
    for pos, span_data1 in enumerate(full_span_data_1):
        # Also get span data for table 2.
        # Note: specific span data is at the same position for both tables.
        span_data2 = full_span_data_2[pos]

        # Determine different rows for tables 1 and 2 (exclude unchanged rows).
        diff_rows_sign1 = span_data1[1] - span_data2[1]
        diff_rows_sign2 = span_data2[1] - span_data1[1]
        diff_pk_hash1 = set(cmp_sign[1] for cmp_sign in diff_rows_sign1)
        diff_pk_hash2 = set(cmp_sign[1] for cmp_sign in diff_rows_sign2)

        # Get the original diff rows for tables 1.
        for res_row in span_data1[0]:
            # Skip row if not in previously identified changed rows set.
            if (res_row[0], res_row[1]) in diff_rows_sign1:
                # Execute query to get the original row.
                pk = res_row[2:-1]
                where_clause = ' AND '.join("{0} = '{1}'".
                                            format(key, col)
                                            for key, col in zip(ukeys, pk))
                res = server1.exec_query(
                    _COMPARE_SPAN_QUERY.format(db=table1.q_db_name,
                                               table=table1.q_tbl_name,
                                               where=where_clause))

                # Determine if it is a changed or extra row.
                # Check if the same pk_hash is found in table 2.
                if res_row[1] in diff_pk_hash2:
                    # Store original changed row (to UPDATE).
                    changed_in1.append(res[0])
                else:
                    # Store original extra row (to DELETE).
                    extra_in1.append(res[0])

        # Get the original diff rows for table 2.
        for res_row in span_data2[0]:
            # Skip row if not in previously identified changed rows set.
            if (res_row[0], res_row[1]) in diff_rows_sign2:
                # Execute query to get the original row.
                pk = res_row[2:-1]
                where_clause = ' AND '.join("{0} = '{1}'".
                                            format(key, col)
                                            for key, col in zip(ukeys, pk))
                res = server2.exec_query(
                    _COMPARE_SPAN_QUERY.format(db=table2.q_db_name,
                                               table=table2.q_tbl_name,
                                               where=where_clause))

                # Determine if it is a changed or extra row.
                # Check if the same pk_hash is found in table 1.
                if res_row[1] in diff_pk_hash1:
                    # Store original changed row (to UPDATE).
                    changed_in2.append(res[0])
                else:
                    # Store original extra row (to ADD).
                    extra_in2.append(res[0])

    # Return a tuple with a tuple for each table, containing the changed and
    # extra original row for each table.
    return (changed_in1, extra_in1), (changed_in2, extra_in2)


def _get_formatted_rows(rows, table, fmt='GRID', col_widths=None):
    """Get a printable representation of the data rows

    This method generates a formatted view of the rows from a table. The output
    format can be in one of GRID, CSV, TAB, or VERTICAL. This output is
    returned as a list of strings for use in storing the output for later
    presentation.

    rows[in]          missing rows
    table[in]         a Table instance of the table
    obj1_str[in]      full table name for base table
    obj2_str[in]      full table name for other table
    fmt[in]           format to print
    col_widths[in]    column widths to use instead of actual col

    Returns list of formatted rows
    """
    result_rows = []
    if not col_widths:
        col_widths = []
    outfile = tempfile.TemporaryFile()
    to_sql = False
    if fmt.upper() == 'CSV':
        to_sql = True
    print_list(outfile, fmt, table.get_col_names(), rows, to_sql=to_sql,
               col_widths=col_widths)
    outfile.seek(0)
    for line in outfile.readlines():
        result_rows.append(line.strip('\n'))

    return result_rows


def _generate_data_diff_output(diff_data, table1, table2, used_index, options):
    """Generates the data difference output.

    This function generates the output data for the found data differences
    between two tables, according to the provided options (difftype and
    format).

    diff_data[in]   Tuple with three elements containing the data differences
                    between two tables. The first element contains the rows on
                    both tables but with different values, the second contains
                    the rows found in table1 but not in table2, and the third
                    contains the rows found in table2 but not in table1.
    table1[in]      First compared table (source).
    table2[in]      Second compared table (target).
    used_index[in]  Index (key) used to identify rows.
    options[in]     Dictionary of option (format, difftype, compact, etc.).

    Return a list of difference (strings) generated according to the
    specified options.
    """
    difftype = options.get('difftype', 'unified')
    fmt = options.get('format', 'grid')
    compact_diff = options.get("compact", False)
    table1_name = table1.q_table
    table2_name = table2.q_table
    changed_rows, extra1, extra2 = diff_data
    data_diffs = []

    def get_max_cols(tbl1_rows, tbl2_rows):
        """Get maximum columns for each set of rows

        Find maximum column widths for each column for a pair of tables.

        tbl1_rows[in]  first table rows
        tbl2_rows[in]  second table rows

        Return a list of the columns and the max width for each
        """
        # We need to turn the list of tuples to list of lists
        row_list = []
        for r in tbl1_rows:
            for i in r:
                row_list.append(list(i))
        t1_cols = get_col_widths(table1.get_col_names(), row_list)
        row_list = []
        for r in tbl2_rows:
            for i in r:
                row_list.append(list(i))
        t2_cols = get_col_widths(table2.get_col_names(), row_list)

        # Get max of each
        max_cols = []
        for i in range(0, len(t1_cols)):
            if t1_cols[i] > t2_cols[i]:
                max_cols.append(t1_cols[i])
            elif t1_cols[i] <= t2_cols[i]:
                max_cols.append(t2_cols[i])
        return max_cols

    if len(changed_rows) > 0:
        data_diffs.append("# Data differences found among rows:")
        # Get original changed/extra rows for each table within the given
        # span set 'changed_rows' (excluding unchanged rows).
        # Note: each span can refer to multiple rows.
        tbl1_rows, tbl2_rows = _get_changed_rows_span(table1, table2,
                                                      changed_rows,
                                                      used_index)

        if difftype == 'sql':
            # Compute SQL diff for changed rows.
            data_diffs.extend(transform_data(table1, table2, "UPDATE",
                                             (tbl1_rows[0], tbl2_rows[0])))
            # Compute SQL diff for extra rows in table 1.
            if tbl1_rows[1]:
                data_diffs.extend(transform_data(table1, table2,
                                                 "DELETE", tbl1_rows[1]))
            # Compute SQL diff for extra rows in table 2.
            if tbl2_rows[1]:
                data_diffs.extend(transform_data(table1, table2,
                                                 "INSERT", tbl2_rows[1]))
        else:
            # Ok, to make the comparison more uniform, we need to get the
            # max column widths for each table and use the higher of the
            # two to format the rows in the output.
            max_cols = get_max_cols(tbl1_rows, tbl2_rows)

            # Join changed and extra rows for table 1.
            tbl1_rows = tbl1_rows[0] + tbl1_rows[1]
            rows1 = _get_formatted_rows(tbl1_rows, table1, fmt, max_cols)
            # Join changed and extra rows for table 2.
            tbl2_rows = tbl2_rows[0] + tbl2_rows[1]
            rows2 = _get_formatted_rows(tbl2_rows, table2, fmt, max_cols)
            # Compute diff for all changes between table 1 and 2.
            diff_str = _get_diff(rows1, rows2, table1_name, table2_name,
                                 difftype, compact=compact_diff)
            if len(diff_str) > 0:
                data_diffs.extend(diff_str)

    if len(extra1) > 0:
        # Compute diff for extra rows in table 1.
        rows = _get_rows_span(table1, extra1, used_index)
        if difftype == 'sql':
            data_diffs.extend(transform_data(table1, table2,
                                             "DELETE", rows))
        else:
            data_diffs.append("\n# Rows in {0} not in {1}"
                              "".format(table1_name, table2_name))
            res = _get_formatted_rows(rows, table1, fmt)
            data_diffs.extend(res)

    if len(extra2) > 0:
        # Compute diff for extra rows in table 2.
        rows = _get_rows_span(table2, extra2, used_index)
        if difftype == 'sql':
            data_diffs.extend(transform_data(table1, table2,
                                             "INSERT", rows))
        else:
            data_diffs.append("\n# Rows in {0} not in {1}"
                              "".format(table2_name, table1_name))
            res = _get_formatted_rows(rows, table2, fmt)
            data_diffs.extend(res)

    return data_diffs


def check_consistency(server1, server2, table1_name, table2_name,
                      options=None, diag_msgs=None, reporter=None):
    """Check the data consistency of two tables

    This method performs a comparison of the data in two tables.

    Algorithm:

    This procedure uses a separate temporary compare table that
    contains an MD5 hash of the concatenated values of a row along with a
    MD5 hash of the concatenation of the primary key, the primary key columns,
    and a grouping column named span. By default, before executing this
    procedure the result of CHECKSUM TABLE is compared (which is faster when
    no differences are expected). The remaining algorithm to find row
    differences is only executed if this checksum table test fails or if it is
    skipped by the user.

    The process to calculate differences in table data is as follows:

    0. Compare the result of CHECKSUM TABLE for both tables. If the checksums
       match None is returned and the algorithm ends, otherwise the next steps
       to find row differences are executed.

       Note: The following steps are only executed if the table checksums are
             different or if this preliminary step is skipped by the user.

    1. If binary log on for the client (sql_log_bin = 1), turn it off.

    2. Create the temporary compare table for each table to compare
       (db1.table1, db2.table2)

    3. For each table, populate the compare table using an INSERT statement
       that calculates the MD5 hash for the row.

    4. For each table, a summary result is formed by summing the MD5 hash
       values broken into four parts. The MD5 hash is converted to decimal for
       a numerical sum. This summary query also groups the rows in the compare
       table by the span column which is formed from the first 4 positions of
       the primary key hash.

    5. The summary tables are compared using set methods to find rows (spans)
       that appear in both tables, those only in table1, and those only in
       table2. A set operation that does not match the rows means the summed
       hash is different therefore meaning one or more rows in the span have
       either a missing row in the other table or the data is different. If no
       differences found, skip to (9).

    6. The span values from the sets that contain rows that are different are
       then compared again using set operations. Those spans that are in both
       sets contain rows that have changed while the set of rows in one but not
       the other (and vice-versa) contain rows that are missing.

       Note: it is possible given sufficient density of the table for the
             changed rows span to contain missing rows. This is Ok because the
             output of the difference will still present the data as missing.

    7. The output of (6) that contain the same spans (changed rows) is then
       used to form a difference and this is saved for presentation to the
       user.

    8. The output of (7) that contain missing spans (missing rows) is then
       used to form a formatted list of the results for presentation to the
       user.

       Note: The differences output is generated considering the specified
       changes directions (for server1, server2, or both).

    9. The compare databases are destroyed and differences (if any) are
       returned according to the specified change direction in the options.
       A return value of (None, None) indicates the data is consistent.

    10. Turn binary logging on if turned off in step (1).

    Exceptions:

    server1[in]       first server Server instance
    server2[in]       second server Server instance
    table1_name[in]   name of first table in form 'db.name'
    table2_name[in]   name of second table in form 'db.name'
    options[in]       dictionary of options for the operation containing
                        'format'    : format for output of missing rows
                        'difftype'  : type of difference to show
                        'unique_key': column name for pseudo-key
    diag_msgs[out]    a list of diagnostic and warning messages.
    reporter[in]      Instance of the database compare reporter class.

    Returns tuple - string representations of the primary index columns

    Returns a tuple with the list of differences for server1 and/or server2
            according to the specified direction. If the data is consistent
            then the tuple (None, None) is returned.
    """
    if options is None:
        options = {}
    span_key_size = options.get('span_key_size', DEFAULT_SPAN_KEY_SIZE)
    use_indexes = options.get('use_indexes', None)
    direction = options.get('changes-for', 'server1')
    reverse = options.get('reverse', False)

    table1 = Table(server1, table1_name)
    table2 = Table(server2, table2_name)

    # First, check full table checksum if step is not skipped.
    if reporter:
        reporter.report_object("", "- Compare table checksum")
        reporter.report_state("")
        reporter.report_state("")
    if not options['no_checksum_table']:
        checksum1, err1 = server1.checksum_table(table1.q_table)
        checksum2, err2 = server2.checksum_table(table2.q_table)
        if err1 or err2:
            err_data = (server1.host, server1.port, err1) if err1 \
                else (server2.host, server2.port, err2)
            raise UtilError("Error executing CHECKSUM TABLE on '{0}@{1}': "
                            "{2}".format(*err_data))
        if checksum1 == checksum2:
            if reporter:
                reporter.report_state("pass")
            return None, None  # No data diffs (in any direction)
        else:
            if reporter:
                reporter.report_state("FAIL")
    else:
        if reporter:
            reporter.report_state("SKIP")

    # remove quotations to indexes
    unq_use_indexes = []
    if use_indexes:
        for tbl, index in use_indexes:
            unq_use_indexes.append((
                remove_backtick_quoting(tbl, table1.sql_mode),
                remove_backtick_quoting(index, table1.sql_mode)
            ))
            if table1.sql_mode != table2.sql_mode:
                unq_use_indexes.append((
                    remove_backtick_quoting(tbl, table2.sql_mode),
                    remove_backtick_quoting(index, table2.sql_mode)
                ))

    # if given get the unique_key for table_name
    table1_use_indexes = []
    if use_indexes:
        table1_use_indexes.extend(
            [quote_with_backticks(u_key, table1.sql_mode) for tb_name, u_key
             in unq_use_indexes if table1.tbl_name == tb_name]
        )
    table2_use_indexes = []
    if use_indexes:
        table2_use_indexes.extend(
            [quote_with_backticks(u_key, table2.sql_mode) for tb_name, u_key
             in unq_use_indexes if table2.tbl_name == tb_name]
        )

    if options.get('toggle_binlog', 'False'):
        binlog_server1 = server1.binlog_enabled()
        if binlog_server1:
            # Commit to avoid error setting sql_log_bin inside a transaction.
            server1.rollback()
            server1.toggle_binlog("DISABLE")
        binlog_server2 = server2.binlog_enabled()
        if binlog_server2:
            # Commit to avoid error setting sql_log_bin inside a transaction.
            server2.commit()
            server2.toggle_binlog("DISABLE")
    else:  # set to false to skip after actions to turn binlog back on
        binlog_server1 = False
        binlog_server2 = False

    data_diffs1 = None
    data_diffs2 = None

    # Now, execute algorithm to find row differences.
    if reporter:
        reporter.report_object("", "- Find row differences")
        reporter.report_state("")
        reporter.report_state("")
    # Setup the comparative tables and calculate the hashes
    pri_idx_str1, pri_idx_str2, used_index, used_index_name, msgs = (
        _setup_compare(table1, table2,
                       span_key_size,
                       use_indexes=(table1_use_indexes, table2_use_indexes))
    )

    # Add warnings to print them later.
    if diag_msgs is not None and isinstance(diag_msgs, list):
        diag_msgs.extend(msgs)
        diag_msgs.append("# INFO: for table {0} the index {1} is used to "
                         "compare.".format(table1.tbl_name, used_index_name))

    # Populate the compare tables and retrieve rows from each table
    tbl1_hash = _make_sum_rows(table1, pri_idx_str1, span_key_size)
    tbl2_hash = _make_sum_rows(table2, pri_idx_str2, span_key_size)

    # Compare results (between spans).
    _, in1_not2, in2_not1 = get_common_lists(tbl1_hash, tbl2_hash)

    # If mismatch found, go back to compare table and retrieve grouping.
    if len(in1_not2) != 0 or len(in2_not1) != 0:
        table1_diffs = []
        table2_diffs = []

        # Get keys for diffs on table1
        for row in in1_not2:
            table1_diffs.append(row[0])

        # Get keys for diffs on table2
        for row in in2_not1:
            table2_diffs.append(row[0])

        # Find changed and missing rows
        changed_rows, extra1, extra2 = get_common_lists(table1_diffs,
                                                        table2_diffs)

        # Generate data differences output according to direction.
        if direction == 'server1' or reverse:
            data_diffs1 = _generate_data_diff_output(
                (changed_rows, extra1, extra2), table1, table2, used_index,
                options
            )
        if direction == 'server2' or reverse:
            data_diffs2 = _generate_data_diff_output(
                (changed_rows, extra2, extra1), table2, table1, used_index,
                options
            )

    if binlog_server1:
        # Commit to avoid error setting sql_log_bin inside a transaction.
        server1.commit()
        server1.toggle_binlog("ENABLE")
    if binlog_server2:
        # Commit to avoid error setting sql_log_bin inside a transaction.
        server2.commit()
        server2.toggle_binlog("ENABLE")

    if reporter:
        if data_diffs1 or data_diffs2:
            reporter.report_state('FAIL')
        else:
            reporter.report_state('pass')
    return data_diffs1, data_diffs2
#
# Copyright (c) 2010, 2016 Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains helper methods for formatting output.

METHODS
    format_tabular_list - Format and write row data as a separated-value list
                          or as a grid layout like mysql client query results
                          Writes to a file specified (e.g. sys.stdout)
"""

import codecs
import csv
import os
import textwrap

try:
    import cStringIO as StringIO
except ImportError:
    import StringIO



_MAX_WIDTH = 78
_TWO_COLUMN_DISPLAY = "{0:{1}}  {2:{3}}"


class UnicodeWriter(object):
    """A CSV writer which will write rows to CSV file `f_out`,
    which is encoded in the given encoding.
    """

    def __init__(self, f_out, dialect="excel", encoding="utf-8", **kwds):
        """Contructor

        f_out[in]        file to print to (e.g. sys.stdout)
        dialect[in]      description of the dialect in use by the writer
        encoding[in]     encoding
        """
        # Redirect output to a queue
        self.queue = StringIO.StringIO()
        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)
        self.stream = f_out
        self.encoder = codecs.getincrementalencoder(encoding)()

    def writerow(self, row):
        """Write the row parameter to the writer's file object.

        row[in]     sequence of strings or numbers
        """
        self.writer.writerow([val.encode("utf-8") if isinstance(val, unicode)
                              else val for val in row])
        data = self.queue.getvalue()
        data = data.decode("utf-8")  # pylint: disable=R0204
        data = self.encoder.encode(data)
        self.stream.write(data)
        self.queue.truncate(0)

    def writerows(self, rows):
        """Write all rows parameter to the writer's file object.

        rows[in]     list of row objects
        """
        for row in rows:
            self.writerow(row)


def _format_col_separator(f_out, columns, col_widths, quiet=False):
    """Format a row of the header with column separators

    f_out[in]          file to print to (e.g. sys.stdout)
    columns[in]        list of column names
    col_widths[in]     width of each column
    quiet[in]          if True, do not print
    """
    if quiet:
        return
    stop = len(columns)
    for i in range(0, stop):
        width = int(col_widths[i] + 2)
        f_out.write('{0}{1:{1}<{2}}'.format("+", "-", width))
    f_out.write("+\n")


def _format_row_separator(f_out, columns, col_widths, row, quiet=False):
    """Format a row of data with column separators.

    f_out[in]          file to print to (e.g. sys.stdout)
    columns[in]        list of column names
    col_widths[in]     width of each column
    rows[in]           data to print
    quiet[in]          if True, do not print
    """
    i = 0
    if len(columns) == 1 and row != columns:
        row = [row]
    for i, _ in enumerate(columns):
        if not quiet:
            f_out.write("| ")
        val = row[i].encode("utf-8") if isinstance(row[i], unicode) \
            else row[i]
        if isinstance(val, str):
            val = u"{0:<{1}}".format(val.decode("utf-8"), col_widths[i] + 1)
            f_out.write(val.encode("utf-8"))
        else:
            f_out.write("{0:<{1}} ".format("%s" % val, col_widths[i]))

    if not quiet:
        f_out.write("|")
    f_out.write("\n")


def get_col_widths(columns, rows):
    """
    This function gets the maximum column width for a list of rows

    Returns: list - max column widths
    """
    # Calculate column width for each column
    col_widths = []
    for col in columns:
        size = len(col.decode("utf-8") if isinstance(col, str) else col)
        col_widths.append(size + 1)

    stop = len(columns)
    for row in rows:
        row = [val.encode("utf-8") if isinstance(val, unicode)
               else val for val in row]
        # if there is one column, just use row.
        if stop == 1:
            col_size = len(row[0].decode("utf-8")
                           if isinstance(row[0], str) else str(row[0]))
            col_size += 1
            if col_size > col_widths[0]:
                col_widths[0] = col_size
        else:
            for i in range(0, stop):
                col_size = len(row[i].decode("utf-8")
                               if isinstance(row[i], str) else str(row[i]))
                col_size += 1
                if col_size > col_widths[i]:
                    col_widths[i] = col_size
    return col_widths


def format_tabular_list(f_out, columns, rows, options=None):
    """Format a list in a pretty grid format.

    This method will format and write a list of rows in a grid or CSV list.

    f_out[in]          file to print to (e.g. sys.stdout)
    columns[in]        list of column names
    rows[in]           list of rows to print
    options[in]        options controlling list:
        print_header   if False, do not print header
        separator      if set, use the char specified for a CSV output
        quiet          if True, do not print the grid text (no borders)
        print_footer   if False, do not print footer
        none_to_null   if True converts None values to NULL
    """
    if options is None:
        options = {}
    print_header = options.get("print_header", True)
    separator = options.get("separator", None)
    quiet = options.get("quiet", False)
    print_footer = options.get("print_footer", True)
    none_to_null = options.get("none_to_null", False)
    convert_to_sql = options.get('to_sql', False)

    # do nothing if no rows.
    if len(rows) == 0:
        return
    if separator is not None:
        if os.name == "posix":
            # Use \n as line terminator in POSIX (non-Windows) systems.
            csv_writer = UnicodeWriter(f_out, delimiter=separator,
                                       lineterminator='\n')
        else:
            # Use the default line terminator '\r\n' on Windows.
            csv_writer = UnicodeWriter(f_out, delimiter=separator)
        if print_header:
            csv_writer.writerow(columns)
        for row in rows:
            row = [val.encode("utf-8") if isinstance(val, unicode)
                   else val for val in row]
            if convert_to_sql:
                # Convert value to SQL (i.e. add quotes if needed).
                row = ['NULL' if col is None else to_sql(col) for col in row]
            if none_to_null:
                # Convert None values to 'NULL'
                row = ['NULL' if val is None else val for val in row]
            csv_writer.writerow(row)
    else:
        # Calculate column width for each column
        col_widths = options.get('col_widths', None)
        if not col_widths:
            col_widths = get_col_widths(columns, rows)

        # print header
        if print_header:
            _format_col_separator(f_out, columns, col_widths, quiet)
            _format_row_separator(f_out, columns, col_widths, columns, quiet)
        _format_col_separator(f_out, columns, col_widths, quiet)
        for row in rows:
            # Note: lists need to be converted to tuple as expected by
            # next method (to handle single column rows correctly)
            if convert_to_sql:
                # Convert value to SQL (i.e. add quotes if needed).
                row = tuple(('NULL' if col is None else to_sql(col)
                             for col in row))
            if none_to_null:
                # Convert None values to 'NULL'
                row = tuple(('NULL' if val is None else val for val in row))
            _format_row_separator(f_out, columns, col_widths, row, quiet)
        if print_footer:
            _format_col_separator(f_out, columns, col_widths, quiet)


def format_vertical_list(f_out, columns, rows, options=None):
    r"""Format a list in a vertical format.

    This method will format and write a list of rows in a vertical format
    similar to the \G format in the mysql monitor.

    f_out[in]          file to print to (e.g. sys.stdout)
    columns[in]        list of column names
    rows[in]           list of rows to print
    options[in]        options controlling list:
        none_to_null   if True converts None values to NULL
    """
    if options is None:
        options = {}
    none_to_null = options.get("none_to_null", False)

    # do nothing if no rows.
    if len(rows) == 0:
        return

    max_colwidth = 0
    # Calculate maximum column width for all columns
    for col in columns:
        if len(col) + 1 > max_colwidth:
            max_colwidth = len(col) + 1

    stop = len(columns)
    row_num = 0
    for row in rows:
        row_num += 1
        f_out.write('{0:{0}<{1}}{2:{3}>{4}}. row {0:{0}<{1}}\n'.format("*", 25,
                                                                       row_num,
                                                                       ' ', 8))
        if none_to_null:
            # Convert None values to 'NULL'
            row = ['NULL' if not val else val for val in row]
        for i in range(0, stop):
            col = columns[i].decode("utf-8") \
                if isinstance(columns[i], str) else columns[i]
            val = row[i].decode("utf-8") \
                if isinstance(row[i], str) else row[i]
            out = u"{0:>{1}}: {2}\n".format(col, max_colwidth, val)
            f_out.write(out.encode("utf-8"))

    if row_num > 0:
        row_str = 'rows' if row_num > 1 else 'row'
        f_out.write("{0} {1}.\n".format(row_num, row_str))


def print_list(f_out, fmt, columns, rows, no_headers=False, sort=False,
               to_sql=False, col_widths=None):
    """Print a list< based on format.

    Prints a list of rows in the format chosen. Default is GRID.

    f_out[in]         file to print to (e.g. sys.stdout)
    fmt[in]           Format (GRID, CSV, TAB, VERTICAL)
    columns[in]       Column headings
    rows[in]          Rows to print
    no_headers[in]    If True, do not print headings (column names)
    sort[in]          If True, sort list before printing
    to_sql[out]       If True, converts columns to SQL format before
                      printing them to the output.
    col_widths[in]    col widths to use instead of actual col
    """

    if not col_widths:
        col_widths = []
    if sort:
        rows.sort()
    list_options = {
        'print_header': not no_headers,
        'to_sql': to_sql,
        'col_widths': col_widths,
    }
    if fmt == "vertical":
        format_vertical_list(f_out, columns, rows)
    elif fmt == "tab":
        list_options['separator'] = '\t'
        format_tabular_list(f_out, columns, rows, list_options)
    elif fmt == "csv":
        list_options['separator'] = ','
        format_tabular_list(f_out, columns, rows, list_options)
    else:  # default to table format
        format_tabular_list(f_out, columns, rows, list_options)


def _get_max_key_dict_list(dictionary_list, key, alias_key=None):
    """Get maximum key length for display calculation

    dictionary_list[in]   Dictionary to print
    key[in]               Name of the key
    use_alias[in]         If not None, add alias to width too

    Returns int - max width of key
    """
    def lcal(x):
        """ calculate string length """
        return len(str(x or ''))
    dl = dictionary_list
    tmp = [(lcal(item[key]), lcal(item.get(alias_key, 0))) for item in dl]
    return max([(x[0] + x[1] + 3) if x[1] else x[0] for x in tmp])


def print_dictionary_list(column_names, keys, dictionary_list,
                          max_width=_MAX_WIDTH, use_alias=True,
                          show_header=True):
    """Print a multiple-column list with text wrapping

    column_names[in]       Column headings
    keys[in]               Keys for dictionary items
    dictionary_list[in]    Dictionary to print (list of)
    max_width[in]          Max width
    use_alias[in]          If True, use keys[2] to print an alias
    """
    # max column size for the name
    max_name = _get_max_key_dict_list(dictionary_list, keys[0])
    if max_name < len(column_names[0]):
        max_name = len(column_names[0])
    min_value = 25  # min column size for the value
    max_value = max_width - 2 - max_name  # max column size for the value
    if max_value < min_value:
        max_value = min_value
        max_name = max_width - 2 - max_value

    if show_header:
        print(_TWO_COLUMN_DISPLAY.format(column_names[0], max_name,
                                         column_names[1], max_value))
        print(_TWO_COLUMN_DISPLAY.format('-' * (max_name), max_name,
                                         '-' * max_value, max_value))
    for item in dictionary_list:
        name = item[keys[0]]
        if len(name) > max_name:
            name = "{0}...".format(name[:(max_name - 3)])
        value = item[keys[1]]
        if isinstance(value, (bool, int)) or value is None:
            description = [str(value)]
        elif not value:
            description = ['']
        else:
            description = textwrap.wrap(value, max_value)

        if use_alias and len(keys) > 2 and len(item[keys[2]]) > 0:
            name += ' | ' + item[keys[2]]
        print(_TWO_COLUMN_DISPLAY.format(name, max_name,
                                         description[0], max_value))
        for i in range(1, len(description)):
            print(_TWO_COLUMN_DISPLAY.format('', max_name, description[i],
                                             max_value))


def convert_dictionary_list(dict_list):
    """Convert a dictionary to separated lists of keys and values.

    Convert the list of items of the given dictionary (i.e. pairs key, value)
    to a set of columns containing the keys and a set of rows containing the
    values.

    dict_list[in]    Dictionary with a list of items to convert

    Returns tuple - (columns, rows)
    """
    cols = []
    rows = []
    # First, get a list of the columns
    for node in dict_list:
        for key in node.keys():
            if key not in cols:
                cols.append(key)

    # Now form the rows replacing missing columns with None
    for node in dict_list:
        row = []
        for col in cols:
            row.append(node.get(col, None))
        rows.append(row)

    return (cols, rows)
#
# Copyright (c) 2013, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains a module to read .frm files and attempt to create a
facsimile of the CREATE TABLE command.
"""

import bisect
import os
import stat
import struct
import time


#
# Definitions and types for interpreting the .frm file values.
#

# Misc. constants
_PORTABLE_SIZEOF_CHAR_PTR = 8
_MY_CHARSET_BIN_NUM = 63
_HA_NOSAME = 1
_DIG2BYTES = [0, 1, 1, 2, 2, 3, 3, 4, 4, 4]
_DIG_PER_DEC1 = 9
_HEADER_LEN = 64
_TABLE_TYPE = 0x01fe   # Magic number for table .frm files
_VIEW_TYPE = 0x5954    # Magic number for view .frm files
_FIELD_NR_MASK = 16383
_HA_USES_COMMENT = 4096

# MySQL data type definitions
_MYSQL_TYPE_DECIMAL = 0
_MYSQL_TYPE_TINY = 1
_MYSQL_TYPE_SHORT = 2
_MYSQL_TYPE_LONG = 3
_MYSQL_TYPE_FLOAT = 4
_MYSQL_TYPE_DOUBLE = 5
_MYSQL_TYPE_NULL = 6
_MYSQL_TYPE_TIMESTAMP = 7
_MYSQL_TYPE_LONGLONG = 8
_MYSQL_TYPE_INT24 = 9
_MYSQL_TYPE_DATE = 10
_MYSQL_TYPE_TIME = 11
_MYSQL_TYPE_DATETIME = 12
_MYSQL_TYPE_YEAR = 13
_MYSQL_TYPE_NEWDATE = 14
_MYSQL_TYPE_VARCHAR = 15
_MYSQL_TYPE_BIT = 16
_MYSQL_TYPE_TIMESTAMP2 = 17
_MYSQL_TYPE_DATETIME2 = 18
_MYSQL_TYPE_TIME2 = 19
_MYSQL_TYPE_NEWDECIMAL = 246
_MYSQL_TYPE_ENUM = 247
_MYSQL_TYPE_SET = 248
_MYSQL_TYPE_TINY_BLOB = 249
_MYSQL_TYPE_MEDIUM_BLOB = 250
_MYSQL_TYPE_LONG_BLOB = 251
_MYSQL_TYPE_BLOB = 252
_MYSQL_TYPE_VAR_STRING = 253
_MYSQL_TYPE_STRING = 254
_MYSQL_TYPE_GEOMETRY = 255

# Mapping of field data types to data type names
_col_types = [
    {'value': _MYSQL_TYPE_DECIMAL, 'text': 'decimal', 'size': None},
    {'value': _MYSQL_TYPE_TINY, 'text': 'tinyint', 'size': 1},
    {'value': _MYSQL_TYPE_SHORT, 'text': 'smallint', 'size': 2},
    {'value': _MYSQL_TYPE_LONG, 'text': 'int', 'size': 4},
    {'value': _MYSQL_TYPE_FLOAT, 'text': 'float', 'size': 4},
    {'value': _MYSQL_TYPE_DOUBLE, 'text': 'double', 'size': 8},
    {'value': _MYSQL_TYPE_NULL, 'text': 'NULL', 'size': 0},
    {'value': _MYSQL_TYPE_TIMESTAMP, 'text': 'timestamp', 'size': 4},
    {'value': _MYSQL_TYPE_LONGLONG, 'text': 'bigint', 'size': 8},
    {'value': _MYSQL_TYPE_INT24, 'text': 'mediumint', 'size': 3},
    {'value': _MYSQL_TYPE_DATE, 'text': 'date', 'size': 4},
    {'value': _MYSQL_TYPE_TIME, 'text': 'time', 'size': 3},
    {'value': _MYSQL_TYPE_DATETIME, 'text': 'datetime', 'size': 8},
    {'value': _MYSQL_TYPE_YEAR, 'text': 'year', 'size': 1},
    {'value': _MYSQL_TYPE_NEWDATE, 'text': 'date', 'size': 3},
    # Size must be calculated
    {'value': _MYSQL_TYPE_VARCHAR, 'text': 'varchar', 'size': -1},
    # Size must be calculated
    {'value': _MYSQL_TYPE_BIT, 'text': 'bit', 'size': -2},
    {'value': _MYSQL_TYPE_TIMESTAMP2, 'text': 'timestamp', 'size': 4},
    {'value': _MYSQL_TYPE_DATETIME2, 'text': 'datetime', 'size': 8},
    {'value': _MYSQL_TYPE_TIME2, 'text': 'time', 'size': 3},
    {'value': _MYSQL_TYPE_NEWDECIMAL, 'text': 'decimal', 'size': None},
    {'value': _MYSQL_TYPE_ENUM, 'text': 'enum', 'size': 0},
    {'value': _MYSQL_TYPE_SET, 'text': 'set', 'size': 0},
    {'value': _MYSQL_TYPE_TINY_BLOB, 'text': 'tinyblob',
     'size': 1 + _PORTABLE_SIZEOF_CHAR_PTR},
    {'value': _MYSQL_TYPE_MEDIUM_BLOB, 'text': 'mediumblob',
     'size': 3 + _PORTABLE_SIZEOF_CHAR_PTR},
    {'value': _MYSQL_TYPE_LONG_BLOB, 'text': 'longblob',
     'size': 4 + _PORTABLE_SIZEOF_CHAR_PTR},
    {'value': _MYSQL_TYPE_BLOB, 'text': 'blob',
     'size': 2 + _PORTABLE_SIZEOF_CHAR_PTR},
    # Size must be calculated
    {'value': _MYSQL_TYPE_VAR_STRING, 'text': 'varchar', 'size': -1},
    {'value': _MYSQL_TYPE_STRING, 'text': 'char', 'size': None},
    {'value': _MYSQL_TYPE_GEOMETRY, 'text': 'geometry',
     'size': 4 + _PORTABLE_SIZEOF_CHAR_PTR},
]

_col_keys = [item['value'] for item in _col_types]

# Database/engine type definitions
_DB_TYPE_UNKNOWN = 0
_DB_TYPE_DIAB_ISAM = 1
_DB_TYPE_HASH = 2
_DB_TYPE_MISAM = 3
_DB_TYPE_PISAM = 4
_DB_TYPE_RMS_ISAM = 5
_DB_TYPE_HEAP = 6
_DB_TYPE_ISAM = 7
_DB_TYPE_MRG_ISAM = 8
_DB_TYPE_MYISAM = 9
_DB_TYPE_MRG_MYISAM = 10
_DB_TYPE_BERKELEY_DB = 11
_DB_TYPE_INNODB = 12
_DB_TYPE_GEMINI = 13
_DB_TYPE_NDBCLUSTER = 14
_DB_TYPE_EXAMPLE_DB = 15
_DB_TYPE_ARCHIVE_DB = 16
_DB_TYPE_CSV_DB = 17
_DB_TYPE_FEDERATED_DB = 18
_DB_TYPE_BLACKHOLE_DB = 19
_DB_TYPE_PARTITION_DB = 20
_DB_TYPE_BINLOG = 21
_DB_TYPE_SOLID = 22
_DB_TYPE_PBXT = 23
_DB_TYPE_TABLE_FUNCTION = 24
_DB_TYPE_MEMCACHE = 25
_DB_TYPE_FALCON = 26
_DB_TYPE_MARIA = 27
_DB_TYPE_PERFORMANCE_SCHEMA = 28
_DB_TYPE_FIRST_DYNAMIC = 42
_DB_TYPE_DEFAULT = 127

# Mapping of engine types to engine names
_engine_types = [
    {'value': _DB_TYPE_UNKNOWN, 'text': 'UNKNOWN'},
    {'value': _DB_TYPE_DIAB_ISAM, 'text': 'ISAM'},
    {'value': _DB_TYPE_HASH, 'text': 'HASH'},
    {'value': _DB_TYPE_MISAM, 'text': 'MISAM'},
    {'value': _DB_TYPE_PISAM, 'text': 'PISAM'},
    {'value': _DB_TYPE_RMS_ISAM, 'text': 'RMS_ISAM'},
    {'value': _DB_TYPE_HEAP, 'text': 'HEAP'},
    {'value': _DB_TYPE_ISAM, 'text': 'ISAM'},
    {'value': _DB_TYPE_MRG_ISAM, 'text': 'MERGE'},
    {'value': _DB_TYPE_MYISAM, 'text': 'MYISAM'},
    {'value': _DB_TYPE_MRG_MYISAM, 'text': 'MERGE'},
    {'value': _DB_TYPE_BERKELEY_DB, 'text': 'BDB'},
    {'value': _DB_TYPE_INNODB, 'text': 'INNODB'},
    {'value': _DB_TYPE_GEMINI, 'text': 'GEMINI'},
    {'value': _DB_TYPE_NDBCLUSTER, 'text': 'NDBCLUSTER'},
    {'value': _DB_TYPE_EXAMPLE_DB, 'text': 'EXAMPLE'},
    {'value': _DB_TYPE_ARCHIVE_DB, 'text': 'ARCHIVE'},
    {'value': _DB_TYPE_CSV_DB, 'text': 'CSV'},
    {'value': _DB_TYPE_FEDERATED_DB, 'text': 'FEDERATED'},
    {'value': _DB_TYPE_BLACKHOLE_DB, 'text': 'BLACKHOLE'},
    {'value': _DB_TYPE_PARTITION_DB, 'text': 'PARTITION'},
    {'value': _DB_TYPE_BINLOG, 'text': 'BINLOG'},
    {'value': _DB_TYPE_SOLID, 'text': 'SOLID'},
    {'value': _DB_TYPE_PBXT, 'text': 'PBXT'},
    {'value': _DB_TYPE_TABLE_FUNCTION, 'text': 'FUNCTION'},
    {'value': _DB_TYPE_MEMCACHE, 'text': 'MEMCACHE'},
    {'value': _DB_TYPE_FALCON, 'text': 'FALCON'},
    {'value': _DB_TYPE_MARIA, 'text': 'MARIA'},
    {'value': _DB_TYPE_PERFORMANCE_SCHEMA, 'text': 'PERFORMANCE_SCHEMA'},
    {'value': _DB_TYPE_FIRST_DYNAMIC, 'text': 'DYNAMIC'},
    {'value': _DB_TYPE_DEFAULT, 'text': 'DEFAULT'},
]
_engine_keys = [item['value'] for item in _engine_types]

# Key algorithms
_KEY_ALG = ['UNDEFINED', 'BTREE', 'RTREE', 'HASH', 'FULLTEXT']

# Format definitions
#                            1         2         3
#                  01234567890123456789012345678901
_HEADER_FORMAT = "<BBBBHHIHHIHHHHHBBIBBBBBIIIIBBBHH"
#                        11122222333333444445556666
#                  12346824602468023489012371590124
#                 ***   111111
#             0123456789012345
_COL_DATA = "<BBBBBBBBBBBBBBBH"
#             0123456789111111
#                       012345

# Various flags copied from server source code - some may not be used but
# may find a use as more esoteric table configurations are tested. These
# are derived from fields.h and all may not apply but are included for
# future expansion/features.
_FIELDFLAG_DECIMAL = 1
_FIELDFLAG_BINARY = 1
_FIELDFLAG_NUMBER = 2
_FIELDFLAG_ZEROFILL = 4
_FIELDFLAG_PACK = 120	              # Bits used for packing
_FIELDFLAG_INTERVAL = 256             # mangled with decimals!
_FIELDFLAG_BITFIELD = 512	          # mangled with decimals!
_FIELDFLAG_BLOB = 1024	              # mangled with decimals!
_FIELDFLAG_GEOM = 2048                # mangled with decimals!
_FIELDFLAG_TREAT_BIT_AS_CHAR = 4096   # use Field_bit_as_char
_FIELDFLAG_LEFT_FULLSCREEN = 8192
_FIELDFLAG_RIGHT_FULLSCREEN = 16384
_FIELDFLAG_FORMAT_NUMBER = 16384      # predit: ###,,## in output
_FIELDFLAG_NO_DEFAULT = 16384         # sql
_FIELDFLAG_SUM = 32768                # predit: +#fieldflag
_FIELDFLAG_MAYBE_NULL = 32768         # sql
_FIELDFLAG_HEX_ESCAPE = 0x10000
_FIELDFLAG_PACK_SHIFT = 3
_FIELDFLAG_DEC_SHIFT = 8
_FIELDFLAG_MAX_DEC = 31
_FIELDFLAG_NUM_SCREEN_TYPE = 0x7F01
_FIELDFLAG_ALFA_SCREEN_TYPE = 0x7800

# Additional flags
_NOT_NULL_FLAG = 1             # Field can't be NULL
_PRI_KEY_FLAG = 2              # Field is part of a primary key
_UNIQUE_KEY_FLAG = 4           # Field is part of a unique key
_MULTIPLE_KEY_FLAG = 8         # Field is part of a key
_BLOB_FLAG = 16                # Field is a blob
_UNSIGNED_FLAG = 32            # Field is unsigned
_HA_PACK_RECORD = 1            # Pack record?
_HA_FULLTEXT = 128             # For full-text search
_HA_SPATIAL = 1024             # For spatial search

# Row type definitions
_ROW_TYPE_DEFAULT, _ROW_TYPE_FIXED, _ROW_TYPE_DYNAMIC, _ROW_TYPE_COMPRESSED, \
    _ROW_TYPE_REDUNDANT, _ROW_TYPE_COMPACT, _ROW_TYPE_PAGE = range(0, 7)

# enum utypes from field.h
_NONE, _DATE, _SHIELD, _NOEMPTY, _CASEUP, _PNR, _BGNR, _PGNR, _YES, _NO, \
    _REL, _CHECK, _EMPTY, _UNKNOWN_FIELD, _CASEDN, _NEXT_NUMBER, \
    _INTERVAL_FIELD, _BIT_FIELD, _TIMESTAMP_OLD_FIELD, _CAPITALIZE, \
    _BLOB_FIELD, _TIMESTAMP_DN_FIELD, _TIMESTAMP_UN_FIELD, \
    _TIMESTAMP_DNUN_FIELD = range(0, 24)

# Array of field data types that can be unsigned
_UNSIGNED_FIELDS = ['TINYINT', 'SMALLINT', 'MEDIUMINT', 'INT', 'INTEGER',
                    'BIGINT', 'REAL', 'DOUBLE', 'FLOAT', 'DECIMAL', 'NUMERIC']

# Array of field data types that can have character set options
_CS_ENABLED = ['CHAR', 'VARCHAR', 'TINYBLOB', 'BLOB', 'MEDIUMBLOB', 'LONGBLOB',
               'ENUM', 'SET']

# Array of index (key) types
_KEY_TYPES = ['PRIMARY', 'UNIQUE', 'MULTIPLE', 'FULLTEXT', 'SPATIAL',
              'FOREIGN_KEY']

# Array of field data types that do not require parens for size
_NO_PARENS = ['TIMESTAMP', 'DATETIME', 'DATE', 'TIME',
              'TINYBLOB', 'BLOB', 'MEDIUMBLOB', 'LONGBLOB',
              'TINYTEXT', 'TEXT', 'MEDIUMTEXT', 'LONGTEXT']

# Array of field data types that are real data
_REAL_TYPES = ['REAL', 'DOUBLE', 'FLOAT', 'DECIMAL', 'NUMERIC']

# Array of blob data types
_BLOB_TYPES = [_MYSQL_TYPE_TINY_BLOB, _MYSQL_TYPE_MEDIUM_BLOB,
               _MYSQL_TYPE_LONG_BLOB, _MYSQL_TYPE_BLOB,
               _MYSQL_TYPE_GEOMETRY]

# Array of data types that do not use keysize for indexes
_NO_KEYSIZE = ['BIT', 'ENUM', 'SET', 'DECIMAL', 'NUMERIC',
               'TIMESTAMP', 'TIME', 'DATETIME']


def _is_decimal(col):
    """Check for decimal data types
    Returns bool - True if column is decimal or numeric.
    """
    return col['field_type_name'].upper() in ['DECIMAL', 'NUMERIC']


def _is_cs_enabled(col):
    """Check for data types that accept character set option
    Returns bool - True if column supports character set option.
    """
    return col['field_type_name'].upper() in _CS_ENABLED


def _is_unsigned(col):
    """Check for unsigned data types
    Returns bool - True if column is an unsigned type.
    """
    return col['field_type_name'].upper() in _UNSIGNED_FIELDS


def _is_real(col):
    """Check for real data types
    Returns bool - True if column is a real type.
    """
    return col['field_type_name'].upper() in _REAL_TYPES


def _is_blob(col):
    """Check for blob data types
    Returns bool - True if column is a blob.
    """
    return col['field_type'] in _BLOB_TYPES


def _is_geometry(flags):
    """Check for geometry field types
    Returns bool - True if geometry type.
    """
    print "flags: %0x" % flags
    return (flags & _FIELDFLAG_GEOM) == _FIELDFLAG_GEOM


def _no_keysize(col):
    """Check for data types that do not use keysize
    Returns bool - True if column is to be exluded from keysize.
    """
    return col['field_type_name'].upper() in _NO_KEYSIZE


def _print_default_values(values):
    """Print default values

    The method prints the default values 2 bytes at a time in hexidecimal
    and ASCII representation (similar to hexdump).

    values[in]         Array of default values
    """
    num_bytes = len(values)
    print "# Default values raw data:"
    i = 0
    while (i < num_bytes):
        def_str = ""
        j = 0
        print "#",
        while (j < 8) and (i < num_bytes):
            print "%02x" % ord(values[i]),
            def_str += values[i]
            i += 1
            j += 1
        print "",
        j = 0
        while (j < 8) and (i < num_bytes):
            print "%02x" % ord(values[i]),
            def_str += values[i]
            i += 1
            j += 1
        print " |",
        print def_str


def _get_pack_length(col):
    """Find the pack length for the field

    col[in]        Column data read for the column to operate

    Returns tuple - (pack_length, field_size)
    """
    size = _col_types[bisect.bisect_left(_col_keys,
                                         col['field_type'])]['size']
    if size == -1:
        col_len = col['bytes_in_col']
        return (1 if int(col_len) < 256 else 2), col_len
    if size == -2:
        col_len = col['bytes_in_col']
        return col_len / 8, col_len
    if size is None:
        return size, col['bytes_in_col']  # It's a string of some sort
    return 0, size


def _get_blob_text(col):
    """Form the correct field name string for blobs and text fields

    col[in]        Column data read for the column to operate

    Returns string - field name string
    """
    type_str = ""
    if col['field_type'] == _MYSQL_TYPE_TINY_BLOB:
        type_str = "tiny"
    elif col['field_type'] == _MYSQL_TYPE_MEDIUM_BLOB:
        type_str = "medium"
    elif col['field_type'] == _MYSQL_TYPE_LONG_BLOB:
        type_str = "long"
    if col['charset'] == _MY_CHARSET_BIN_NUM:
        type_str = "".join([type_str, "blob"])
    else:
        type_str = "".join([type_str, "text"])

    return type_str


def _format_default(col, col_flags, length, decimals):
    """Format a defaut value for printing

    col[in]        Column data dictionary
    col_flags[in]  Flags for column
    length[in]     Length of default value or integer part for floats
    decimals[in]   Number of decimal positions for floats

    Returns string - default clause for CREATE statement.
    """
    default = col['default']
    if isinstance(default, str):
        fmt_str = "'%s'"
    # Check for zerofill:
    elif col_flags & _FIELDFLAG_ZEROFILL:
        if _is_real(col):
            if decimals > 0 and decimals < length:
                if col['field_type_name'].upper() == "DECIMAL":
                    length += 1
                fmt_str = "'" + '%0' + "%s" % length + '.' + \
                          "%s" % decimals + 'f' + "'"
            else:
                fmt_str = "'" + '%0' + "%s" % length + '.' + 'f' + "'"
            if float(default) == 0.0:
                fmt_str = "%s"
                default = "NULL"
        else:
            fmt_str = "'" + '%0' + "%s" % length + 'd' + "'"
    else:
        if _is_real(col):
            if decimals > 0 and decimals < length:
                fmt_str = "'" + '%' + "%s" % (length - 1) + '.' + \
                          "%s" % decimals + 'f' + "'"
            elif decimals == 0:
                fmt_str = "'%d'"
                default = divmod(default, 1)[0]
            else:
                i, decm = divmod(default, 1)
                if decm == 0:
                    fmt_str = "'%d'"
                    default = i
                else:
                    fmt_str = "'%f'"
            if float(default) == 0.0:
                fmt_str = "%s"
                default = "NULL"
        else:
            fmt_str = "'%d'"

    return " DEFAULT " + fmt_str % default


class FrmReader(object):
    """
    This class implements an abstract of the .frm file format. It can be used
    to produce a likeness of the CREATE TABLE command. It is not a 100% match
    because some of the components are missing from the .frm file. For
    example, there are no character set or collation definitions stored so
    unless one has access to the server definitions, these cannot be
    determined.

    The class permits the following operations:

    - show_create_table_statement() - read a .frm file and print its CREATE
      statement. Optionally displays statistics for the .frm file.

    """

    def __init__(self, db_name, table, frm_path, options):
        """Constructor

        db[in]             the database (if known)
        table[in]          table name
        frm_path[in]       full path to .frm file
        options[in]        options for controlling behavior:
            verbosity      print extra data during operations (optional)
                           default value = 0
            quiet          suppress output except CREATE statement
                           default False
            server        path to server for server install
                           default None
            new_engine     substitute engine
                           default None
        """
        self.general_data = None
        self.key_data = None
        self.comment_str = None
        self.engine_str = None
        self.partition_str = None
        self.col_metadata = None
        self.column_data = None
        self.num_cols = 0
        self.default_values = None
        self.frm_file = None
        self.verbosity = options.get('verbosity', 0)
        self.quiet = options.get('quiet', False)
        self.server = options.get('server', None)
        self.new_engine = options.get('new_engine', None)
        self.show_stats = options.get("show_stats", False)
        self.db_name = db_name
        self.table = table
        self.frm_path = frm_path
        self.options = options

        if self.server is None:
            self.csi = None
        else:
            self.csi = CharsetInfo(options)

    def _read_header(self):
        """Read the header information from the file
        """
        try:
            # Skip to header position
            if self.verbosity > 1:
                print "# Skipping to header at : %0000x" % 2
            self.frm_file.seek(2, 0)
            data = self.frm_file.read(_HEADER_LEN)
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot read header.")

        # Read header
        header = struct.unpack(_HEADER_FORMAT, data)
        engine_name = _engine_types[bisect.bisect_left(_engine_keys,
                                                       header[1])]['text']
        self.general_data = {
            'frm_version': header[0],
            'legacy_db_type': engine_name,
            'IO_SIZE': header[4],
            'length': header[6],
            'tmp_key_length': header[7],
            'rec_length': header[8],
            'max_rows': header[10],
            'min_rows': header[11],
            'db_create_pack': header[12] >> 8,  # only want 1 byte
            'key_info_length': header[13],
            'create_options': header[14],
            'frm_file_ver': header[16],
            'avg_row_length': header[17],
            'default_charset': header[18],
            'row_type': header[20],
            'charset_low': header[21],
            'table_charset': (header[21] << 8) + header[18],
            'key_length': header[24],
            'MYSQL_VERSION_ID': header[25],
            'extra_size': header[26],
            'default_part_eng': header[29],
            'key_block_size': header[30],
        }
        # Fix storage engine string if partitioning engine specified
        if self.general_data['default_part_eng'] > 0 and \
           self.new_engine is None:
            self.engine_str = _engine_types[bisect.bisect_left(
                _engine_keys, header[29])]['text']

        return True

    def _read_keys(self):
        """Read key fields from the file
        """
        offset = self.general_data['IO_SIZE']
        try:
            # Skip ahead to key section
            if self.verbosity > 1:
                print "# Skipping to key data at : %0000x" % int(offset)
            self.frm_file.seek(offset, 0)
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot locate keys.")

        # Decipher key parts
        num_keys = struct.unpack("<B", self.frm_file.read(1))[0]
        if num_keys & 0x80:
            next_byte = struct.unpack("<B", self.frm_file.read(1))[0]
            num_keys = (next_byte << 7) | (num_keys & 0x7f)
            low = struct.unpack("<B", self.frm_file.read(1))[0]
            high = struct.unpack("<B", self.frm_file.read(1))[0]
            num_key_parts = low + (high << 8)
            self.frm_file.read(2)
        else:
            num_key_parts = struct.unpack("<B", self.frm_file.read(1))[0],
            self.frm_file.read(4)

        self.key_data = {
            'num_keys': num_keys,
            'num_key_parts': num_key_parts,
            'key_names': [],
            'keys': [],
        }

        for i in range(0, self.key_data['num_keys']):
            key_info = {
                'flags': struct.unpack("<H", self.frm_file.read(2))[0],
                'key_length': struct.unpack("<H", self.frm_file.read(2))[0],
                'num_parts': struct.unpack("<B", self.frm_file.read(1))[0],
                'algorithm': struct.unpack("<B", self.frm_file.read(1))[0],
                'block_size': struct.unpack("<H", self.frm_file.read(2))[0],
                'key_parts': [],
                'comment': "",
            }
            for j in range(0, key_info['num_parts']):
                if self.verbosity > 1:
                    print "# Reading key part %s." % j
                key_part_info = {
                    'field_num': struct.unpack(
                        "<H", self.frm_file.read(2))[0] & _FIELD_NR_MASK,
                    'offset': struct.unpack("<H",
                                            self.frm_file.read(2))[0] - 1,
                    'key_type': struct.unpack("<H",
                                              self.frm_file.read(2))[0],
                    'key_part_flag': struct.unpack("<B",
                                                   self.frm_file.read(1))[0],
                    'length': struct.unpack("<H",
                                            self.frm_file.read(2))[0],
                }
                key_info['key_parts'].append(key_part_info)
            self.key_data['keys'].append(key_info)

        terminator = struct.unpack("c", self.frm_file.read(1))[0]
        for i in range(0, self.key_data['num_keys']):
            key_name = ""
            # Read until the next 0xff
            char_read = ""
            while char_read != terminator:
                char_read = struct.unpack("c", self.frm_file.read(1))[0]
                if char_read != terminator:
                    key_name += str(char_read)
            self.key_data['key_names'].append(key_name)

        # Now find the key comments!
        self.frm_file.read(1)
        for i in range(0, self.key_data['num_keys']):
            if (self.key_data['keys'][i]['flags'] & _HA_USES_COMMENT) == \
               _HA_USES_COMMENT:
                k_len = struct.unpack("<H", self.frm_file.read(2))[0]
                com_str = struct.unpack("c" * k_len, self.frm_file.read(k_len))
                self.key_data['keys'][i]['comment'] = "".join(com_str)

        return True

    def _read_comment(self):
        """Read the table comments.
        """
        # Fields can be found 1 IO_SIZE more than what has been read to date
        # plus 46 bytes.
        io_size = self.general_data['IO_SIZE']
        record_offset = io_size + self.general_data['tmp_key_length'] + \
            self.general_data['rec_length']
        offset = (((record_offset / io_size) + 1) * io_size) + 46
        try:
            # Skip to column position
            if self.verbosity > 1:
                print "# Skipping to table comments at : %0000x" % int(offset)
            self.frm_file.seek(offset, 0)
            data = self.frm_file.read(1)
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot read table comment.")

        comment_len = struct.unpack("<B", data)[0]
        com_chars = struct.unpack("c" * comment_len,
                                  self.frm_file.read(comment_len))
        self.comment_str = "".join(com_chars)
        return True

    def _read_default_values(self):
        """Read the default values for all columns
        """
        offset = self.general_data['IO_SIZE'] + \
            self.general_data['tmp_key_length']
        try:
            # Skip ahead to key section
            if self.verbosity > 1:
                print "# Skipping to default data at : %0000x" % \
                    int(offset + 1)
            self.frm_file.seek(offset + 1, 0)
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot find default data.")

        num_bytes = self.general_data['rec_length']
        # allow overflow
        self.default_values = self.frm_file.read(num_bytes + 100)

    def _read_engine_data(self):
        """Read the storage engine data.
        """
        # We must calculate the location of the partition information by
        # locating the storage engine name and if it is 'partition' then read
        # the partition string following that.

        offset = self.general_data['IO_SIZE'] + \
            self.general_data['tmp_key_length'] + \
            self.general_data['rec_length']
        try:
            # Skip ahead to key section
            if self.verbosity > 1:
                print "# Skipping to keys at : %0000x" % int(offset + 2)
            self.frm_file.seek(offset + 2, 0)
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot find engine data.")

        engine_len = struct.unpack("<H", self.frm_file.read(2))[0]

        engine_str = "".join(struct.unpack("c" * engine_len,
                                           self.frm_file.read(engine_len)))

        # Save engine name unless user specified a new engine to use
        if self.engine_str is None:
            if self.new_engine is None:
                self.engine_str = engine_str
            else:
                self.engine_str = self.new_engine

        part_len = struct.unpack("<I", self.frm_file.read(4))[0]
        part_str = "".join(struct.unpack("c" * part_len,
                                         self.frm_file.read(part_len)))
        self.partition_str = " ".join(part_str.split('\n'))

        return True

    def _read_column_names(self, fields_per_screen):
        """Read the table column names.
        """
        # Column names start in 00002152.
        screens_read = 1

        cols = []
        col_in_screen = 0
        for i in range(0, self.num_cols):
            if (col_in_screen == fields_per_screen):
                screens_read += 1
                col_in_screen = 1
                # Do the skips
                self.frm_file.read(8)  # read ahead 8 bytes
                val = '\x20'
                while val == '\x20':  # skip the spaces
                    val = self.frm_file.read(1)
                self.frm_file.read(2)  # read past 2 more bytes
            else:
                col_in_screen += 1
            # get length byte
            col_len = struct.unpack("<B", self.frm_file.read(1))[0]
            col_str = ""
            # Don't copy trailing \x00
            j = 0
            while j < col_len - 1:
                char_found = struct.unpack("c", self.frm_file.read(1))[0]
                col_str += char_found
                j += 1
            # skip trailing \x00 and extra bits except for last col read
            if (i < self.num_cols - 1):
                self.frm_file.read(3)
            cols.append(col_str)
        return (screens_read, cols)

    def _get_decimal_value(self, recpos, col):
        """Get a decimal value from the default column data

        recpos[in]     Position in default row to find data
        col[in]        Column dictionary for the column data

        Returns float - default value retrieved
        """
        # Guard
        if not _is_decimal(col):
            return None
        col_flags = (int(col['flags_extra'] << 8) + col['flags'])
        length = col['bytes_in_col']
        decimals = (col_flags >> _FIELDFLAG_DEC_SHIFT) & _FIELDFLAG_MAX_DEC
        length = length - (1 if decimals else 0) - \
            (1 if (col_flags & _FIELDFLAG_DECIMAL) or (length == 0) else 0)

        # algorithm from bin2decimal()
        # int intg=precision-scale,
        #    intg0=intg/DIG_PER_DEC1, frac0=scale/DIG_PER_DEC1,
        #    intg0x=intg-intg0*DIG_PER_DEC1, frac0x=scale-frac0*DIG_PER_DEC1;
        #
        # return intg0*sizeof(dec1)+dig2bytes[intg0x]+
        #       frac0*sizeof(dec1)+dig2bytes[frac0x];

        intg = length - decimals
        intg0 = intg / _DIG_PER_DEC1
        frac0 = decimals / _DIG_PER_DEC1
        intg0x = intg - (intg0 * _DIG_PER_DEC1)
        frac0x = decimals - (frac0 * _DIG_PER_DEC1)
        int_len = (intg0 * 4 + _DIG2BYTES[intg0x]) - 1  # len of integer part
        frac_len = (frac0 * 4 + _DIG2BYTES[frac0x])   # len of fractional part
        int_val = 0
        shift_num = int_len - 1
        for i in range(0, int_len):
            int_val += ord(self.default_values[recpos + i + 1]) << \
                (shift_num * 8)
            shift_num -= 1
        frac_val = 0
        shift_num = frac_len - 1
        for i in range(0, frac_len):
            frac_val += ord(self.default_values[recpos + int_len + i + 1]) << \
                (shift_num * 8)
            shift_num -= 1
        return float("%s.%s" % (int_val, frac_val))

    def _get_field_defaults(self):
        """Retrieve the default values for the columns.
        """
        max_len = len(self.default_values)
        if self.verbosity > 2:
            _print_default_values(self.default_values)
        for i in range(0, len(self.column_data)):
            col = self.column_data[i]
            recpos = self.column_data[i]['recpos']
            recpos -= 2
            if recpos < 0:
                recpos = 0
            if recpos > max_len:  # safety net
                continue

            # Read default for decimal types
            if _is_decimal(col):
                col['default'] = self._get_decimal_value(recpos, col)
                continue
            len_pos, size = _get_pack_length(col)
            field_cs_num = (col['charset_low'] << 8) + col['charset']
            # Adjust size based on character set maximum length per char
            if _is_cs_enabled(col):
                if self.csi:
                    maxlen = self.csi.get_maxlen(field_cs_num)
                else:
                    maxlen = 1
                size = size / maxlen
            if len_pos is None:
                value = self.default_values[recpos:recpos + size]
            else:
                value = self.default_values[recpos:recpos + len_pos + size]

            # Read default for double type
            if col['field_type'] == _MYSQL_TYPE_DOUBLE:
                col['default'] = struct.unpack('d', value)[0]
                continue

            # Read default for float type
            if col['field_type'] == _MYSQL_TYPE_FLOAT:
                col['default'] = struct.unpack('f', value)[0]
                continue

            # Need to check for column type. Some are binary!
            if len_pos is None:  # Some form of string
                col_str = ""
                for col_def in range(0, len(value)):
                    if value[col_def] != '\x20':
                        col_str += value[col_def]
                col['default'] = '' if len(col_str) == 0 else col_str
            elif len_pos == 0:   # packed numeric
                len_pos = size
            if len_pos == 1:
                col['default'] = struct.unpack("<B", value[0:1])[0]
            elif len_pos == 2:
                col['default'] = struct.unpack("<H", value[0:2])[0]
            elif len_pos == 3:
                col['default'] = struct.unpack("<HB", value[0:3])[0]
            elif len_pos == 4:
                col['default'] = struct.unpack("<I", value[0:4])[0]
            elif len_pos == 8:
                col['default'] = struct.unpack("<Q", value[0:8])[0]

    def _read_column_metadata(self):
        """Read the column metadata (size, flags, etc.).

        Returns dictionary - column definition data
        """
        column_data = []
        # Skip ahead
        try:
            for i in range(0, self.num_cols):
                if self.verbosity > 1:
                    print "# Reading column metadata #%s" % i
                data = struct.unpack(_COL_DATA, self.frm_file.read(17))
                data_type = _col_types[bisect.bisect_left(_col_keys,
                                                          data[13])]
                col_def = {
                    'field_length': data[2],  # 1, +3
                    'bytes_in_col': int(data[3]) + (int(data[4]) << 8),
                    'recpos': (int(data[6]) << 8) +
                              (int(data[5])) + (int(data[4]) << 16),
                    'unireg': data[7],  # 1, +8
                    'flags': data[8],  # 1, +9
                    'flags_extra': data[9],  # 1, +10
                    'unireg_type': data[10],  # 1, +11
                    'charset_low': data[11],  # 1, +12
                    'interval_nr': data[12],  # 1, +13
                    'field_type': data[13],  # 1, +14
                    'field_type_name': data_type['text'],
                    'charset': data[14],  # 1, +15
                    'comment_length': data[15],  # 2, +17
                    'enums': [],
                    'comment': "",
                    'default': None,
                }
                column_data.append(col_def)
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot locate column data")
        return column_data

    def _read_column_data(self):
        """Read the column information from the file.

        This method builds the list of columns including defaults,
        data type, and determines enum and set values.
        """
        # Fields can be found 1 IO_SIZE more than what has been read to date
        # plus 258 bytes.
        io_size = self.general_data['IO_SIZE']
        record_offset = io_size + self.general_data['tmp_key_length'] + \
            self.general_data['rec_length']
        offset = ((((record_offset + self.general_data['key_info_length']) /
                    io_size) + 1) * io_size) + 258
        try:
            # Skip to column position
            if self.verbosity > 1:
                print "# Skipping to column data at : %0000x" % int(offset)
            self.frm_file.seek(offset, 0)
            data = struct.unpack("<HHHHHHHHHHHHH", self.frm_file.read(26))
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot read column header.")
        self.num_cols = data[0]
        self.col_metadata = {
            'num_cols': data[0],
            'pos': data[1],
            'unknown': data[2],
            'n_length': data[3],
            'interval_count': data[4],
            'interval_parts': data[5],
            'int_length': data[6],
            'com_length': data[8],
            'null_fields': data[12],
        }
        if self.verbosity > 1:
            pprint(self.col_metadata)

        # Skip ahead
        try:
            self.frm_file.read(7)
            fields_per_screen = struct.unpack("<B", self.frm_file.read(1))[0]
            if self.verbosity > 1:
                print "# Fields per screen =", fields_per_screen
            self.frm_file.read(46)
            col_names = self._read_column_names(fields_per_screen)[1]
            self.frm_file.read(1)  # skip 1 byte
            self.column_data = self._read_column_metadata()
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot read column data.")

        # TODO: Add ability to read defaults by modifying _get_field_defaults
        #       method to correctly read the default values. Currently, it
        #       does not read some non-character values correctly. When fixed,
        #       remove this comment and uncomment the following line.
        # self._get_field_defaults()

        # Skip column names
        col_len = 0
        for colname in col_names:
            col_len += len(colname)
        # Skip to enum section
        self.frm_file.read(len(col_names) + col_len + 2)
        intervals = []
        interval_num = 0
        # pylint: disable=R0101
        for i in range(0, len(col_names)):
            self.column_data[i]['name'] = col_names[i]
            # Here we read enums and match them to interval_nr.
            i_num = self.column_data[i]['interval_nr']
            if int(i_num) > 0:
                if interval_num < i_num:
                    interval_num += 1
                    cols = []
                    char_found = 99
                    col_str = ''
                    while char_found != 0:
                        char_found = struct.unpack("B",
                                                   self.frm_file.read(1))[0]
                        if char_found == 255:
                            if len(col_str):
                                cols.append(col_str)
                                col_str = ''
                        else:
                            col_str += chr(char_found)
                    intervals.append(cols)
                self.column_data[i]['enums'].extend(
                    intervals[interval_num - 1])

        # Now read column comments
        for i in range(0, len(col_names)):
            if self.verbosity > 1:
                print "# Column comment:", \
                    self.column_data[i]['comment_length']
            if self.column_data[i]['comment_length'] > 0:
                col_str = ''
                for j in range(0, self.column_data[i]['comment_length']):
                    if self.verbosity > 3:
                        print "# Reading column data %s." % j
                    char_found = struct.unpack("B", self.frm_file.read(1))[0]
                    col_str += chr(char_found)
                self.column_data[i]['comment'] = col_str

        return True

    def _get_charset_collation(self, col):
        """Get the character set and collation for column

        col[in]        Column data dictionary

        Returns list - option strings for charset and collation if needed
        """
        parts = []
        field_cs_num = (col['charset_low'] << 8) + col['charset']
        table_cs_num = self.general_data['table_charset']
        # If no character set information, add unknown tag to prompt user
        if self.csi is None:
            if field_cs_num is not None and table_cs_num is not None and \
               field_cs_num != 'binary' and table_cs_num != field_cs_num:
                parts.append(" CHARACTER SET <UNKNOWN>")
            return parts
        field_cs_name = self.csi.get_name(field_cs_num)
        table_cs_name = self.csi.get_name(table_cs_num)
        if field_cs_name is not None and table_cs_name is not None and \
           field_cs_name != 'binary' and table_cs_name != field_cs_name:
            parts.append(" CHARACTER SET `%s`" % field_cs_name)

        elif (field_cs_name is None or table_cs_name is None) and \
                not self.quiet:
            print "C",
            print "# WARNING: Cannot get character set name for id =", id
            parts.append(" CHARACTER SET <UNKNOWN>")
        else:
            parts.append("")

        # Get collation
        def_field_col = self.csi.get_default_collation(field_cs_num)
        field_col = self.csi.get_collation(field_cs_num)
        if def_field_col is not None and field_col is not None and \
           def_field_col[1] != field_col:
            parts.append(" COLLATE `%s`" % field_col)
        elif def_field_col is None and not self.quiet:
            print "# WARNING: Cannot get default collation for id =", id
        elif field_col is None and not self.quiet:
            print "# WARNING: Cannot get collation for id =", id
        else:
            parts.append("")

        return parts

    def _get_column_definitions(self):
        """Build the column definitions

        This method constructs the column definitions from the column data
        read from the file.

        Returns list of strings - column definitions
        """

        def _is_no_parens(col):
            """Check for column uses parens for size
            Returns bool - True if column needs parens for size.
            """
            # If the server version is 5.7.5 or before, we add YEAR to the
            # no parenthesis list. Otherwise, we print the length: YEAR(4)
            ver_str = str(self.general_data['MYSQL_VERSION_ID'])
            vers = (int(ver_str[0]), int(ver_str[1:3]), int(ver_str[3:]))
            # Check to see if it is in the list
            try:
                index_year = _NO_PARENS.index('YEAR')
            except ValueError:
                index_year = None
            if not ((vers[0] >= 5) and (vers[1] >= 7) and (vers[2] >= 5)):
                if not index_year:
                    _NO_PARENS.append("YEAR")
            elif index_year:
                _NO_PARENS.pop(_NO_PARENS.index('YEAR'))
            return col['field_type_name'].upper() in _NO_PARENS

        columns = []
        stop = len(self.column_data)
        for i in range(0, stop):
            col = self.column_data[i]
            col_flags = (int(col['flags_extra'] << 8) + col['flags'])
            length = int(col['bytes_in_col'])
            # Here we need to check for charset maxlen and adjust accordingly
            field_cs_num = (col['charset_low'] << 8) + col['charset']
            if self.csi:
                maxlen = self.csi.get_maxlen(field_cs_num)
            else:
                maxlen = 1
            # Only convert the length for character type fields
            if _is_cs_enabled(col):
                length = length / maxlen
            decimals = int((col_flags >> _FIELDFLAG_DEC_SHIFT) &
                           _FIELDFLAG_MAX_DEC)
            col_parts = []
            # name, data type, length
            # If enum or set values, put those in definition
            if col['enums']:
                col_str = "  `%s` %s(" % (col['name'], col['field_type_name'])
                col_str += ",".join(["'%s'" % i for i in col['enums']])
                col_str += ")"
                col_parts.append(col_str)
            elif _is_no_parens(col) and not _is_blob(col):
                col_parts.append("  `%s` %s" %
                                 (col['name'],
                                  col['field_type_name'].lower()))
            # for blobs
            elif _is_blob(col):
                col_parts.append("  `%s` %s" % (col['name'],
                                                _get_blob_text(col)))
            # for real types:
            elif _is_real(col):
                length_str = ""
                if _is_decimal(col):
                    length = length - (1 if decimals else 0) - \
                        (1 if (col_flags & _FIELDFLAG_DECIMAL) or
                         (length == 0) else 0)
                if decimals == _FIELDFLAG_MAX_DEC:
                    if col['field_type_name'].upper() not in \
                       ["FLOAT", "DOUBLE"]:
                        length_str = "(%s)" % length
                else:
                    length_str = "(%s,%s)" % (length, decimals)
                col_parts.append("  `%s` %s%s" %
                                 (col['name'],
                                  col['field_type_name'].lower(),
                                  length_str))
            else:
                col_parts.append(
                    "  `%s` %s(%s)" % (col['name'],
                                       col['field_type_name'].lower(),
                                       length)
                )

            # unsigned
            if col_flags & _FIELDFLAG_DECIMAL == 0 and _is_unsigned(col):
                col_parts.append(" unsigned")

            # zerofill
            if col_flags & _FIELDFLAG_ZEROFILL and _is_unsigned(col):
                col_parts.append(" zerofill")

            # character set and collation options
            if _is_cs_enabled(col):
                col_parts.extend(self._get_charset_collation(col))

            # null
            if col_flags & _FIELDFLAG_MAYBE_NULL:
                if not col['default']:
                    col_parts.append(" DEFAULT NULL")
            elif not _is_blob(col):
                col_parts.append(" NOT NULL")

            # default - Check the _FIELDFLAG_NO_DEFAULT flag. If this flag
            #           is set, there is no default.
            default = col['default']
            if col['field_type'] in [_MYSQL_TYPE_TIMESTAMP,
                                     _MYSQL_TYPE_TIMESTAMP2]:
                col_parts.append(" DEFAULT CURRENT_TIMESTAMP "
                                 "ON UPDATE CURRENT_TIMESTAMP")
            elif col_flags & _FIELDFLAG_NO_DEFAULT == 0 and \
                    default is not None:
                col_parts.append(_format_default(col, col_flags,
                                                 length, decimals))

            # auto increment
            if col['unireg_type'] == _NEXT_NUMBER:
                col_parts.append(" AUTO_INCREMENT")

            if len(col['comment']) > 0:
                col_parts.append(" comment '%s'" % col['comment'])

            # if not the last column or if there are keys, append comma
            if i < stop - 1 or self.key_data['num_keys'] > 0:
                col_parts.append(",")
            col_parts.append(" ")
            columns.append("".join(col_parts))

        return columns

    def _get_key_size(self, col, key_info, flags):
        """Get the key size option for column

        col[in]        Column data dictionary
        key_info[in]   Key information
        flags[in]      Key flags

        Returns string - string of (N) for size or None for no size information
        """
        size_info = None
        if _no_keysize(col) or self.csi is None:
            return size_info
        key_len = int(key_info['length'])
        pack_len = _get_pack_length(col)
        if col['field_type_name'].upper() == "VARCHAR":
            field_len = int(col['field_length'])
        elif (_is_real(col) or _is_unsigned(col) or _is_decimal(col)) and \
                pack_len[0]:
            field_len = int(pack_len[0])
        else:
            field_len = int(pack_len[1])
        field_cs_num = (col['charset_low'] << 8) + col['charset']
        if self.csi:
            maxlen = self.csi.get_maxlen(field_cs_num)
        else:
            maxlen = 1

        # Geometry is an exception
        if col['field_type'] == _MYSQL_TYPE_GEOMETRY:
            if self.csi:
                size_info = "(%d)" % key_len
            else:
                size_info = "(UNKNOWN)"

        elif field_len != key_len and \
                not int(flags) & _HA_FULLTEXT and not int(flags) & _HA_SPATIAL:
            if self.csi:
                size_info = "(%d)" % (key_len / maxlen)
            else:
                size_info = "(UNKNOWN)"
        return size_info

    def _get_key_columns(self):
        """Build the key column definitions

        This method constructs the key definitions from the column data
        read from the file.

        Returns list of strings - key column definitions
        """
        keys = []
        key_info = zip(self.key_data['key_names'], self.key_data['keys'])
        num_keys = len(key_info)
        i = 0
        for key, info in key_info:
            if key == "PRIMARY":
                key_prefix = "PRIMARY KEY"
            elif not info['flags'] & _HA_NOSAME:
                key_prefix = "UNIQUE KEY"
            else:
                key_prefix = "KEY"
            key_str = "%s `%s` (%s)"
            key_cols = ""
            for k in range(0, len(info['key_parts'])):
                key_part = info['key_parts'][k]
                col = self.column_data[key_part['field_num'] - 1]
                key_cols += "`%s`" % col['name']
                size_str = self._get_key_size(col, key_part, info['flags'])
                if size_str:
                    key_cols += size_str
                if k < len(info['key_parts']) - 1:
                    key_cols += ","
            algorithm = _KEY_ALG[info['algorithm']]
            if algorithm != 'UNDEFINED':
                key_str += " USING %s" % algorithm
            if i < num_keys - 1:
                key_str += ","
            keys.append(key_str % (key_prefix, key, key_cols))
            i += 1
        return keys

    def _get_table_options(self):
        """Read the general table options from the file.

        Returns string - options string for CREATE statement
        """
        options = []

        gen = self.general_data   # short name to save indent, space

        options.append(") ENGINE=%s" % self.engine_str)

        if self.partition_str is not None and len(self.partition_str):
            options.append("%s" % self.partition_str)

        if gen['avg_row_length'] > 0:
            options.append("AVG_ROW_LENGTH = %s" % gen['avg_row_length'])

        if gen['key_block_size'] > 0:
            options.append("KEY_BLOCK_SIZE = %s" % gen['key_block_size'])

        if gen['max_rows'] > 0:
            options.append("MAX_ROWS = %s" % gen['max_rows'])

        if gen['min_rows'] > 0:
            options.append("MIN_ROWS = %s" % gen['min_rows'])

        if gen['default_charset'] > 0:
            # If no character set information, add unknown tag to prompt user
            if self.csi:
                c_id = int(gen['default_charset'])
                cs_name = self.csi.get_name(c_id)
                if cs_name is not None:
                    options.append("DEFAULT CHARSET=%s" % cs_name)
                elif not self.quiet:
                    print "# WARNING: Cannot find character set by id =", c_id

                # collation
                def_col = self.csi.get_default_collation(c_id)
                col = self.csi.get_collation(c_id)
                if def_col is not None and col is not None and def_col != col:
                    options.append("COLLATE=`%s`" % col)
                elif def_col is None and not self.quiet:
                    print "# WARNING: Cannot find default collation " + \
                          "for table using id =", c_id
                elif col is None and not self.quiet:
                    print "# WARNING: Cannot find collation for table " + \
                        "using id =", c_id

        row_format = ""
        row_type = int(gen['row_type'])
        if row_type == _ROW_TYPE_FIXED:
            row_format = "FIXED"
        elif row_type == _ROW_TYPE_DYNAMIC:
            row_format = "DYNAMIC"
        elif row_type == _ROW_TYPE_COMPRESSED:
            row_format = "COMPRESSED"
        elif row_type == _ROW_TYPE_REDUNDANT:
            row_format = "REDUNDANT"
        elif row_type == _ROW_TYPE_COMPACT:
            row_format = "COMPACT"
        if len(row_format) > 0:
            options.append("ROW_FORMAT = %s" % row_type)

        if self.comment_str is not None and len(self.comment_str):
            options.append("COMMENT '%s'" % self.comment_str)

        if len(options) > 1:
            return options[0] + " " + ", ".join(options[1:]) + ";"
        return options[0] + ";"

    def _build_create_statement(self):
        """Build the create statement for the .frm file.

        This method builds the CREATE TABLE information as read from
        the file.

        Returns string - CREATE TABLE string
        """
        if self.general_data is None:
            raise UtilError("Header information missing.")

        # CREATE statement preamble
        parts = []

        # Create preamble
        preamble = "CREATE TABLE %s`%s` ("
        if self.db_name is not None and len(self.db_name) > 1:
            db_str = "`%s`." % self.db_name
        else:
            db_str = ""
        parts.append(preamble % (db_str, self.table))

        # Get columns
        parts.extend(self._get_column_definitions())

        # Get indexes
        parts.extend(self._get_key_columns())

        # Create postamble and table options
        parts.append(self._get_table_options())

        return "\n".join(parts)

    def get_type(self):
        """Return the file type - TABLE or VIEW
        """
        # Fail if we cannot read the file
        try:
            self.frm_file = open(self.frm_path, "rb")
        except Exception, error:
            raise UtilError("The file %s cannot be read.\n%s" %
                            (self.frm_path, error))

        # Read the file type
        file_type = struct.unpack("<H", self.frm_file.read(2))[0]

        # Close file and exit
        self.frm_file.close()

        # Take action based on file type
        if file_type == _TABLE_TYPE:
            return "TABLE"
        elif file_type == _VIEW_TYPE:
            return "VIEW"
        else:
            return "UNKNOWN"

    def show_statistics(self):
        """Show general file and table statistics
        """

        print "# File Statistics:"
        file_stats = os.stat(self.frm_path)
        file_info = {
            'Size': file_stats[stat.ST_SIZE],
            'Last Modified': time.ctime(file_stats[stat.ST_MTIME]),
            'Last Accessed': time.ctime(file_stats[stat.ST_ATIME]),
            'Creation Time': time.ctime(file_stats[stat.ST_CTIME]),
            'Mode': file_stats[stat.ST_MODE],
        }
        for value, data in file_info.iteritems():
            print "#%22s : %s" % (value, data)
        print

        # Fail if we cannot read the file
        try:
            self.frm_file = open(self.frm_path, "rb")
        except Exception, error:
            raise UtilError("The file %s cannot be read.\n%s" %
                            (self.frm_path, error))

        # Read the file type
        file_type = struct.unpack("<H", self.frm_file.read(2))[0]

        # Take action based on file type
        if file_type != _TABLE_TYPE:
            return

        # Read general information
        self._read_header()

        # Close file and exit
        self.frm_file.close()

        version = str(self.general_data['MYSQL_VERSION_ID'])
        ver_str = "%d.%d.%d" % (int(version[0]), int(version[1:3]),
                                int(version[3:]))
        def_part_eng = 'None'
        if self.general_data['default_part_eng'] > 0:
            def_part_eng = _engine_types[bisect.bisect_left(
                _engine_keys,
                self.general_data['default_part_eng'])]['text']
        print "# Table Statistics:"
        table_info = {
            'MySQL Version': ver_str,
            'frm Version': self.general_data['frm_version'],
            'Engine': self.general_data['legacy_db_type'],
            'IO_SIZE': self.general_data['IO_SIZE'],
            'frm File_Version': self.general_data['frm_file_ver'],
            'Def Partition Engine': def_part_eng,
        }
        for value, data in table_info.iteritems():
            print "#%22s : %s" % (value, data)
        print

    def show_create_table_statement(self):
        """Show the CREATE TABLE statement

        This method reads the .frm file specified in the constructor and
        builds a fascimile CREATE TABLE statement if the .frm file describes
        a table. For views, the method displays the CREATE VIEW statement
        contained in the file.
        """
        if not self.quiet:
            print "# Reading .frm file for %s:" % self.frm_path

        # Fail if we cannot read the file
        try:
            self.frm_file = open(self.frm_path, "rb")
        except Exception, error:
            raise UtilError("The file %s cannot be read.\n%s" %
                            (self.frm_path, error))

        # Read the file type
        file_type = struct.unpack("<H", self.frm_file.read(2))[0]

        # Take action based on file type
        if file_type == _TABLE_TYPE:
            if not self.quiet:
                print "# The .frm file is a TABLE."

            # Read general information
            self._read_header()
            if self.verbosity > 1:
                print "# General Data from .frm file:"
                pprint(self.general_data)

            # Read key information
            self._read_keys()
            if self.verbosity > 1:
                print "# Index (key) Data from .frm file:"
                pprint(self.key_data)

            # Read default field values information
            self._read_default_values()

            # Read partition information
            self._read_engine_data()
            if self.verbosity > 1:
                print "# Engine string:", self.engine_str
                print "# Partition string:", self.partition_str

            # Read column information
            self._read_column_data()
            if self.verbosity > 1:
                print "# Column Data from .frm file:"
                pprint(self.column_data)
                print "# Number of columns:", self.num_cols
                pprint(self.column_data[1:])

            # Read comment
            self._read_comment()
            if self.verbosity > 1:
                print "# Comment:", self.comment_str

            if self.csi is not None and self.verbosity > 2:
                print "# Character sets read from server:"
                self.csi.print_charsets()

            create_table_statement = self._build_create_statement()
            if not self.quiet:
                print "# CREATE TABLE Statement:\n"
            print create_table_statement
            print

        elif file_type == _VIEW_TYPE:
            # Skip heading
            self.frm_file.read(8)
            view_data = {}
            for line in self.frm_file.readlines():
                field, value = line.strip('\n').split("=", 1)
                view_data[field] = value
            if self.verbosity > 1:
                pprint(view_data)
            if not self.quiet:
                print "# CREATE VIEW Statement:\n"
            print view_data['query']
            print
        else:
            raise UtilError("Invalid file type. Magic bytes = %02x" %
                            file_type)

        # Close file and exit
        self.frm_file.close()

    def change_storage_engine(self):
        """Change the storage engine in an .frm file to MEMORY

        This method edits a .frm file to change the storage engine to the
        the MEMORY engine.

        CAUTION: Method will change the contents of the file.

        Returns tuple - (original engine type, original engine name,
                         sever version from the file)
        """
        # Here we must change the code in position 0x03 to the engine code
        # and the engine string in body of the file (Calculated location)
        if self.verbosity > 1 and not self.quiet:
            print "# Changing engine for .frm file %s:" % self.frm_path

        # Fail if we cannot read the file
        try:
            self.frm_file = open(self.frm_path, "r+b")
        except Exception, error:
            raise UtilError("The file %s cannot be read.\n%s" %
                            (self.frm_path, error))

        # Read the file type
        file_type = struct.unpack("<H", self.frm_file.read(2))[0]

        # Do nothing if this is a view.
        if file_type == _VIEW_TYPE:
            return None

        # Abort if not table.
        if file_type != _TABLE_TYPE:
            raise UtilError("Invalid file type. Magic bytes = %02x" %
                            file_type)

        # Replace engine value
        self.frm_file.read(1)  # skip 1 byte
        engine_type = struct.unpack("<B", self.frm_file.read(1))[0]

        # Read general information
        self._read_header()
        if self.verbosity > 1:
            print "# General Data from .frm file:"
            pprint(self.general_data)

        engine_str = ""
        server_version = str(self.general_data['MYSQL_VERSION_ID'])

        offset = self.general_data['IO_SIZE'] + \
            self.general_data['tmp_key_length'] + \
            self.general_data['rec_length']

        self.frm_file.seek(offset + 2, 0)

        engine_len = struct.unpack("<H", self.frm_file.read(2))[0]
        engine_str = "".join(struct.unpack("c" * engine_len,
                                           self.frm_file.read(engine_len)))
        if self.verbosity > 1:
            print "# Engine string:", engine_str

        # If this is a CSV storage engine, don't change the engine type
        # and instead create an empty .CSV file
        if engine_type == _DB_TYPE_CSV_DB:
            new_csv = os.path.splitext(self.frm_path)
            f_out = open(new_csv[0] + ".CSV", "w")
            f_out.close()
        elif engine_type == _DB_TYPE_ARCHIVE_DB:
            new_csv = os.path.splitext(self.frm_path)
            f_out = open(new_csv[0] + ".ARZ", "w")
            f_out.close()
        elif engine_type == _DB_TYPE_MRG_MYISAM:
            new_csv = os.path.splitext(self.frm_path)
            f_out = open(new_csv[0] + ".MRG", "w")
            f_out.close()
        elif engine_type == _DB_TYPE_BLACKHOLE_DB:
            pass  # Nothing to do for black hole storage engine
        else:
            # Write memory type
            self.frm_file.seek(3)
            self.frm_file.write(struct.pack("<B", 6))

            # Write memory name
            self.frm_file.seek(offset + 2, 0)
            self.frm_file.write(struct.pack("<H", 6))
            self.frm_file.write("MEMORY")

        # Close file and exit
        self.frm_file.close()

        return engine_type, engine_str, server_version
#
# Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains features to check which users hold privileges, specific or
not, over a given object/list of objects.
"""

from collections import defaultdict


_TABLE_PRIV_QUERY = ("SELECT GRANTEE, IS_GRANTABLE, "
                     "GROUP_CONCAT(PRIVILEGE_TYPE) "
                     "FROM INFORMATION_SCHEMA.TABLE_PRIVILEGES WHERE "
                     "TABLE_SCHEMA='{0}' AND TABLE_NAME='{1}' "
                     "GROUP BY GRANTEE, IS_GRANTABLE")

_DB_PRIVS_QUERY = ("SELECT GRANTEE, IS_GRANTABLE, "
                   "GROUP_CONCAT(PRIVILEGE_TYPE) "
                   "FROM INFORMATION_SCHEMA.SCHEMA_PRIVILEGES WHERE "
                   "TABLE_SCHEMA='{0}' GROUP BY GRANTEE, IS_GRANTABLE")

_GLOBAL_PRIV_QUERY = ("SELECT grantee, IS_GRANTABLE, "
                      "GROUP_CONCAT(privilege_type) FROM "
                      "information_schema.USER_PRIVILEGES GROUP BY GRANTEE,"
                      " IS_GRANTABLE")

_PROCS_PRIV_QUERY = ("SELECT User, Host, Proc_priv FROM "
                     "mysql.procs_priv WHERE db='{0}' AND "
                     "routine_name='{1}'")

_GLOBAL_ALL_PRIVS = set(['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE',
                         'DROP', 'RELOAD', 'SHUTDOWN', 'PROCESS', 'FILE',
                         'REFERENCES', 'INDEX', 'ALTER', 'SHOW DATABASES',
                         'SUPER', 'CREATE TEMPORARY TABLES', 'LOCK TABLES',
                         'EXECUTE', 'REPLICATION SLAVE', 'REPLICATION CLIENT',
                         'CREATE VIEW', 'SHOW VIEW', 'CREATE ROUTINE',
                         'ALTER ROUTINE', 'CREATE USER', 'EVENT', 'TRIGGER',
                         'CREATE TABLESPACE'])

_TABLE_ALL_PRIVS = set(['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE',
                        'DROP', 'REFERENCES', 'INDEX', 'ALTER', 'CREATE VIEW',
                        'SHOW VIEW', 'TRIGGER'])

_DB_ALL_PRIVS = set(['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE',
                     'DROP', 'REFERENCES', 'INDEX', 'ALTER',
                     'CREATE TEMPORARY TABLES', 'LOCK TABLES', 'EXECUTE',
                     'CREATE VIEW', 'SHOW VIEW', 'CREATE ROUTINE',
                     'ALTER ROUTINE', 'EVENT', 'TRIGGER'])

_ROUTINE_ALL_PRIVS = set(['EXECUTE', 'ALTER ROUTINE'])

DATABASE_TYPE = 'DATABASE'
TABLE_TYPE = 'TABLE'
PROCEDURE_TYPE = 'PROCEDURE'
ROUTINE_TYPE = 'ROUTINE'
FUNCTION_TYPE = 'FUNCTION'
GLOBAL_TYPE = 'GLOBAL'
GLOBAL_LEVEL = 3
DATABASE_LEVEL = 2
OBJECT_LEVEL = 1

ALL_PRIVS_LOOKUP_DICT = {PROCEDURE_TYPE: _ROUTINE_ALL_PRIVS,
                         ROUTINE_TYPE: _ROUTINE_ALL_PRIVS,
                         FUNCTION_TYPE: _ROUTINE_ALL_PRIVS,
                         TABLE_TYPE: _TABLE_ALL_PRIVS,
                         DATABASE_TYPE: _DB_ALL_PRIVS,
                         GLOBAL_TYPE: _GLOBAL_ALL_PRIVS}


def get_table_privs(server, db_name, table_name):
    """ Get the list of grantees and their privileges for a specific table.

    server[in]          Instance of Server class, where the query will be
                        executed.
    db_name[in]     Name of the database where the table belongs to.
    table_name[in]  Name of the table to check.

    Returns list of tuples (<Grantee>, <SET OF GRANTS>).
    """
    tpl_lst = []
    # Get sql_mode in server
    sql_mode = server.select_variable("SQL_MODE")
    # Remove backticks if necessary
    if is_quoted_with_backticks(db_name, sql_mode):
        db_name = remove_backtick_quoting(db_name, sql_mode)
    if is_quoted_with_backticks(table_name, sql_mode):
        table_name = remove_backtick_quoting(table_name, sql_mode)

    # Build query
    query = _TABLE_PRIV_QUERY.format(db_name, table_name)
    res = server.exec_query(query)
    for grantee, grant_option, grants in res:
        grants = set((grant.upper() for grant in grants.split(',')))
        # remove USAGE privilege since it does nothing.
        grants.discard('USAGE')
        if grants:
            if 'Y' in grant_option.upper():
                grants.add('GRANT OPTION')
            tpl_lst.append((grantee, grants))

    return tpl_lst


def get_db_privs(server, db_name):
    """ Get the list of grantees and their privileges for a database.

    server[in]          Instance of Server class, where the query will be
                        executed.
    db_name[in]  Name of the database to check.

    Returns list of tuples (<Grantee>, <SET OF GRANTS>).
    """
    tpl_lst = []
    # Get sql_mode in server
    sql_mode = server.select_variable("SQL_MODE")
    # remove backticks if necessary
    if is_quoted_with_backticks(db_name, sql_mode):
        db_name = remove_backtick_quoting(db_name, sql_mode)

    # Build query
    query = _DB_PRIVS_QUERY.format(db_name)
    res = server.exec_query(query)
    for grantee, grant_option, grants in res:
        grants = set((grant.upper() for grant in grants.split(',')))
        # remove USAGE privilege since it does nothing.
        grants.discard('USAGE')
        if grants:
            if 'Y' in grant_option.upper():
                grants.add('GRANT OPTION')
            tpl_lst.append((grantee, grants))

    return tpl_lst


def get_global_privs(server):
    """ Get the list of grantees and their list of global privileges.

    server[in]          Instance of Server class, where the query will be
                        executed.

    Returns list of tuples (<Grantee>, <SET OF GRANTS>).
    """
    tpl_lst = []
    query = _GLOBAL_PRIV_QUERY
    res = server.exec_query(query)
    for grantee, grant_option, grants in res:
        grants = set((grant.upper() for grant in grants.split(',')))
        # remove USAGE privilege since it does nothing.
        grants.discard('USAGE')
        if grants:
            if 'Y' in grant_option.upper():
                grants.add('GRANT OPTION')
            tpl_lst.append((grantee, grants))
    return tpl_lst


def get_routine_privs(server, db_name, routine_name):
    """ Get the list of grantees and their privileges for a routine.

    server[in]          Instance of Server class, where the query will be
                        executed.
    db_name[in]         Name of the database where the table belongs to.
    routine_name[in]    Name of the routine to check.

    Returns list of tuples (<GRANTEE>, <SET OF GRANTS>).
    """
    tpl_lst = []
    # Get sql_mode in server
    sql_mode = server.select_variable("SQL_MODE")
    # remove backticks if necesssary
    if is_quoted_with_backticks(db_name, sql_mode):
        db_name = remove_backtick_quoting(db_name, sql_mode)
    if is_quoted_with_backticks(routine_name, sql_mode):
        routine_name = remove_backtick_quoting(routine_name, sql_mode)

    # Build query
    query = _PROCS_PRIV_QUERY.format(db_name, routine_name)
    res = server.exec_query(query)
    for user, host, grants in res:
        grants = set((grant.upper() for grant in grants.split(',')))
        # remove USAGE privilege since it does nothing.
        grants.discard('USAGE')
        if grants:
            tpl_lst.append(("'{0}'@'{1}'".format(user, host), grants))
    return tpl_lst


def simplify_grants(grant_set, obj_type):
    """Replaces set of privileges with ALL PRIVILEGES, if possible

    grant_set[in]  set of privileges.
    obj_type[in]   type of the object to which these privileges apply.

    Returns a set with the simplified version of grant_set.
    """
    # Get set with all the privileges for the specified object type.
    all_privs = ALL_PRIVS_LOOKUP_DICT[obj_type]

    # remove USAGE privilege since it does nothing and is not on the global
    # all privileges set of any type
    grant_set.discard('USAGE')

    # Check if grant_set has grant option and remove if before checking
    # if given set of privileges contains all the privileges for the
    # specified type
    grant_opt_set = set(['GRANT OPTION', 'GRANT'])
    has_grant_opt = bool(grant_opt_set.intersection(grant_set))
    if has_grant_opt:
        # Remove grant option.
        grant_set = grant_set.difference(grant_opt_set)
    # Check if remaining privileges can be replaced with ALL PRIVILEGES.
    if all_privs == grant_set:
        grant_set = set(["ALL PRIVILEGES"])
    if has_grant_opt:
        # Insert GRANT OPTION PRIVILEGE again.
        grant_set.add("GRANT OPTION")
    return grant_set


def filter_grants(grant_set, obj_type_str):
    """This method returns a new set with just the grants that are valid to
    a given object type.

    grant_set[in]          Set of grants we want to 'filter'
    obj_type_str[in]       String with the type of the object that we are
                           working with, must be either 'ROUTINE', 'TABLE' or
                           'DATABASE'.

    Returns a new set with just the grants that apply.
    """
    # Get set with all the privs for obj_type
    all_privs_set = ALL_PRIVS_LOOKUP_DICT[obj_type_str]
    # Besides having all the privs from the obj_type, it can also have
    # 'ALL', 'ALL PRIVILEGES' and 'GRANT OPTION'
    all_privs_set = all_privs_set.union(['ALL', 'ALL PRIVILEGES',
                                         'GRANT OPTION'])

    # By intersecting the grants we have with the object type's valid set of
    # grants we will obtain just the set of valid grants.
    return grant_set.intersection(all_privs_set)


def _build_privilege_dicts(server, obj_type_dict, inherit_level=GLOBAL_LEVEL):
    """Builds TABLE, ROUTINE and DB dictionaries with grantee privileges

    server[in]        Server class instance
    obj_type_dict[in] dictionary with the list of objects to obtain the
                      grantee and respective grant information, organized
                      by object type
    inherit_level[in] Level of inheritance that should be taken into account.
                      It must be one of GLOBAL_LEVEL, DATABASE_LEVEL or
                      OBJECT_LEVEL

    This method builds and returns the 3 dictionaries with grantee
    information taking into account the grant hierarchy from mysql, i.e.
    global grants apply to all objects and database grants apply to all
    the database objects (tables, procedures and functions).
    """
    # Get the global Grants:
    global_grantee_lst = get_global_privs(server)
    # Build the Database level grants dict.
    # {db_name: {grantee: set(privileges)}}
    db_grantee_dict = defaultdict(lambda: defaultdict(set))
    for db_name, _ in obj_type_dict[DATABASE_TYPE]:
        db_privs_lst = get_db_privs(server, db_name)
        for grantee, priv_set in db_privs_lst:
            db_grantee_dict[db_name][grantee] = priv_set
        if inherit_level >= GLOBAL_LEVEL:
            # If global inheritance level is turned on, global privileges
            # also apply to the database level.
            for grantee, priv_set in global_grantee_lst:
                db_grantee_dict[db_name][grantee].update(
                    filter_grants(priv_set, DATABASE_TYPE))

    # Build the table Level grants dict.
    # {db_name: {tbl_name: {grantee: set(privileges)}}}
    table_grantee_dict = defaultdict(
        lambda: defaultdict(lambda: defaultdict(set)))

    for db_name, tbl_name in obj_type_dict[TABLE_TYPE]:
        tbl_privs_lst = get_table_privs(server, db_name, tbl_name)
        for grantee, priv_set in tbl_privs_lst:
            table_grantee_dict[db_name][tbl_name][grantee] = priv_set
        # Existing db and global_grantee level privileges also apply to
        # the table level if inherit level is database level or higher
        if inherit_level >= DATABASE_LEVEL:
            # If we already have the privileges for the database where the
            # table is at, we can use that information.
            if db_grantee_dict[db_name]:
                for grantee, priv_set in db_grantee_dict[db_name].iteritems():
                    table_grantee_dict[db_name][tbl_name][grantee].update(
                        filter_grants(priv_set, TABLE_TYPE))
            else:
                # Get the grant information for the db the table is at and
                # merge it together with database grants.
                db_privs_lst = get_db_privs(server, db_name)
                for grantee, priv_set in db_privs_lst:
                    table_grantee_dict[db_name][tbl_name][grantee].update(
                        filter_grants(priv_set, TABLE_TYPE))
                # Now do the same with global grants
                if inherit_level >= GLOBAL_LEVEL:
                    for grantee, priv_set in global_grantee_lst:
                        table_grantee_dict[db_name][tbl_name][grantee].update(
                            filter_grants(priv_set, TABLE_TYPE))

    # Build the ROUTINE Level grants dict.
    # {db_name: {proc_name: {user: set(privileges)}}}
    proc_grantee_dict = defaultdict(
        lambda: defaultdict(lambda: defaultdict(set)))
    for db_name, proc_name in obj_type_dict[ROUTINE_TYPE]:
        proc_privs_lst = get_routine_privs(server, db_name, proc_name)
        for grantee, priv_set in proc_privs_lst:
            proc_grantee_dict[db_name][proc_name][grantee] = priv_set
        # Existing db and global_grantee level privileges also apply to
        # the routine level if inherit level is database level or higher
        if inherit_level >= DATABASE_LEVEL:
            # If we already have the privileges for the database where the
            # routine is at, we can use that information.
            if db_grantee_dict[db_name]:
                for grantee, priv_set in db_grantee_dict[db_name].iteritems():
                    proc_grantee_dict[db_name][proc_name][grantee].update(
                        filter_grants(priv_set, ROUTINE_TYPE))
            else:
                # Get the grant information for the db the routine belongs to
                #  and merge it together with global grants.
                db_privs_lst = get_db_privs(server, db_name)
                for grantee, priv_set in db_privs_lst:
                    proc_grantee_dict[db_name][proc_name][grantee].update(
                        filter_grants(priv_set, ROUTINE_TYPE))
                # Now do the same with global grants.
                if inherit_level >= GLOBAL_LEVEL:
                    for grantee, priv_set in global_grantee_lst:
                        proc_grantee_dict[db_name][proc_name][grantee].update(
                            filter_grants(priv_set, ROUTINE_TYPE))

    # TODO Refactor the code below to remove code repetition.

    # Simplify sets of privileges for databases.
    for grantee_dict in db_grantee_dict.itervalues():
        for grantee, priv_set in grantee_dict.iteritems():
            grantee_dict[grantee] = simplify_grants(priv_set,
                                                    DATABASE_TYPE)

    # Simplify sets of privileges for tables.
    for tbl_dict in table_grantee_dict.itervalues():
        for grantee_dict in tbl_dict.itervalues():
            for grantee, priv_set in grantee_dict.iteritems():
                grantee_dict[grantee] = simplify_grants(priv_set,
                                                        TABLE_TYPE)

    # Simplify sets of privileges for routines.
    for proc_dict in proc_grantee_dict.itervalues():
        for grantee_dict in proc_dict.itervalues():
            for grantee, priv_set in grantee_dict.iteritems():
                grantee_dict[grantee] = simplify_grants(priv_set,
                                                        ROUTINE_TYPE)

    return db_grantee_dict, table_grantee_dict, proc_grantee_dict


def _has_all_privileges(query_priv_set, grantee_priv_set, obj_type):
    """Determines if a grantee has a certain set of privileges.

    query_priv_set[in]     set of privileges to be tested
    grantee_priv_set[in]   list of the privileges a grantee has over the
                           object
    obj_type[in]           string with the type of the object to be tested

    This method's purpose receives a set of privileges to test
    (query_priv_set), a set of privileges that a given grantee user
    possesses over a certain object(grantee_priv_set) and the type of that
    object. It returns True if the set of privileges that
    the user has over the object is a superset of query_priv_set.

    """
    # If the user has GRANT OPTION and and ALL PRIVILEGES, then we can
    # automatically return True
    if ("GRANT OPTION" in grantee_priv_set and
            ('ALL PRIVILEGES' in grantee_priv_set or
             'ALL' in grantee_priv_set)):
        return True

    # Remove USAGE privilege because it is the same has having nothing
    query_priv_set.discard('USAGE')

    # Also if query_priv_set contains ALL or ALL PRIVILEGES we can simply
    # discard the rest of the privileges on the set except for GRANT OPTION
    if 'ALL' in query_priv_set or 'ALL PRIVILEGES' in query_priv_set:
        query_priv_set = set(['ALL PRIVILEGES']).union(
            query_priv_set & set(['GRANT OPTION'])
        )
    else:
        # Remove privileges that do not apply to the type of object
        query_priv_set = query_priv_set.intersection(
            ALL_PRIVS_LOOKUP_DICT[obj_type].union(["GRANT OPTION"]))

    return query_priv_set.issubset(grantee_priv_set)


def get_grantees(server, valid_obj_type_dict, req_privileges=None,
                 inherit_level=GLOBAL_LEVEL):
    """Get grantees and respective grants for the specified objects.

    server[in]            Server class instance
    valid_obj_type_dict   Dict with list of valid object for server, sorted
                          by object type. We assume that each object exists
                          on the server
    req_privileges[in]    Optional set of required privileges
    inherit_level[in]     Level of inheritance that should be taken into
                          account. It must be one of GLOBAL_LEVEL,
                          DATABASE_LEVEL or OBJECT_LEVEL
    """

    # Build the privilege dicts
    db_dict, table_dict, proc_dict = _build_privilege_dicts(
        server, valid_obj_type_dict, inherit_level)

    # Build final dict with grantee/grant information, taking into account
    # required privileges
    # grantee_dict = {obj_type: {obj_name:{grantee:set_privs}}}
    grantee_dict = defaultdict(
        lambda: defaultdict(lambda: defaultdict(set)))

    # pylint: disable=R0101
    for obj_type in valid_obj_type_dict:
        for db_name, obj_name in valid_obj_type_dict[obj_type]:
            if obj_type == DATABASE_TYPE:
                for grantee, priv_set in db_dict[obj_name].iteritems():
                    if req_privileges is not None:
                        if _has_all_privileges(req_privileges,
                                               priv_set, obj_type):
                            grantee_dict[obj_type][obj_name][grantee] = \
                                priv_set
                    else:  # No need to check if it meets privileges
                        grantee_dict[obj_type][obj_name][grantee] = \
                            priv_set
            else:
                # It is either TABLE or ROUTINE and both have equal
                # structure dicts
                if obj_type == TABLE_TYPE:
                    type_dict = table_dict
                else:
                    type_dict = proc_dict

                for grantee, priv_set in \
                        type_dict[db_name][obj_name].iteritems():
                    # Get the full qualified name for the object
                    f_obj_name = "{0}.{1}".format(db_name, obj_name)
                    if req_privileges is not None:
                        if _has_all_privileges(
                                req_privileges, priv_set, obj_type):
                            grantee_dict[obj_type][f_obj_name][grantee] = \
                                priv_set
                    else:
                        grantee_dict[obj_type][f_obj_name][grantee] = \
                            priv_set

    return grantee_dict
#
# Copyright (c) 2014, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains function to manipulate GTIDs.
"""


def get_last_server_gtid(gtid_set, server_uuid):
    """Get the last GTID of the specified GTID set for the given server UUID.

    This function retrieves the last GTID from the specified set for the
    specified server UUID. In more detail, it returns the GTID with the greater
    sequence value that matches the specified UUID.

    Note: The method assumes that GTID sets are grouped by UUID (separated by
    comma ',') and intervals appear in ascending order (i.e., the last one is
    the greater one).

    gtid_set[in]        GTID set to search and get last (greater) GTID value.
    server_uuid[in]     Server UUID to match, as a GTID set might contain data
                        for different servers (UUIDs).

    Returns a string with the last GTID value in the set for the given server
    UUID in the format 'uuid:n'. If no GTID are found in the set for the
    specified server UUID then None is returned.
    """
    uuid_sets = gtid_set.split(',')
    for uuid_set in uuid_sets:
        uuid_set_elements = uuid_set.strip().split(':')
        # Note: UUID are case insensitive, but can appear with mixed cases for
        # some server versions (e.g., for 5.6.9, lower case in server_id
        # variable and upper case in GTID_EXECUTED set).
        if uuid_set_elements[0].lower() == server_uuid.lower():
            last_interval = uuid_set_elements[-1]
            try:
                _, end_val = last_interval.split('-')
                return '{0}:{1}'.format(server_uuid, end_val)
            except ValueError:
                # Error raised for single values (not an interval).
                return '{0}:{1}'.format(server_uuid, last_interval)
    return None


def gtid_set_cardinality(gtid_set):
    """Determine the cardinality of the specified GTID set.

    This function counts the number of elements in the specified GTID set.

    gtid_set[in]    target set of GTIDs to determine the cardinality.

    Returns the number of elements of the specified GTID set.
    """
    count = 0
    uuid_sets = gtid_set.split(',')
    for uuid_set in uuid_sets:
        intervals = uuid_set.strip().split(':')[1:]
        for interval in intervals:
            try:
                start_val, end_val = interval.split('-')
                count = count + int(end_val) - int(start_val) + 1
            except ValueError:
                # Error raised for single values (not an interval).
                count += 1
    return count


def gtid_set_union(gtid_set_a, gtid_set_b):
    """Perform the union of two GTID sets.

    This method computes the union of two GTID sets and returns the result of
    the operation.

    Note: This method support input GTID sets not in the normalized form,
    i.e., with unordered and repeated UUID sets and intervals, but with
    a valid syntax.

    gtid_set_a[in]      First GTID set (set A).
    gtid_set_b[in]      Second GTID set (set B).

    Returns a string with the result of the set union operation between the
    two given GTID sets.
    """
    def get_gtid_dict(gtid_a, gtid_b):
        """Get a dict representation of the specified GTID sets.

        Combine the given GTID sets into a single dict structure, removing
        duplicated UUIDs and string intervals.

        Return a dictionary (not normalized) with the GTIDs contained in both
        input GTID sets. For example, for the given (not normalized) GTID sets
        'uuid_a:2:5-7,uuid_b:4' and 'uuid_a:2:4-6:2,uuid_b:1-3' the follow dict
        will be returned:
        {'uuid_a': set(['2', '5-7', '4-6']), 'uuid_b': set(['4','1-3'])}
        """
        res_dict = {}
        uuid_sets_a = gtid_a.split(',')
        uuid_sets_b = gtid_b.split(',')
        uuid_sets = uuid_sets_a + uuid_sets_b
        for uuid_set in uuid_sets:
            uuid_set_values = uuid_set.split(':')
            uuid_key = uuid_set_values[0]
            if uuid_key in res_dict:
                res_dict[uuid_key] = \
                    res_dict[uuid_key].union(uuid_set_values[1:])
            else:
                res_dict[uuid_key] = set(uuid_set_values[1:])
        return res_dict

    # Create auxiliary dict representation of both input GTID sets.
    gtid_dict = get_gtid_dict(gtid_set_a, gtid_set_b)

    # Perform the union between the GTID sets.
    union_gtid_list = []
    for uuid in gtid_dict:
        intervals = gtid_dict[uuid]
        # Convert the set of string intervals into a single list of tuples
        # with integers, in order to be handled easily.
        intervals_list = []
        for values in intervals:
            interval = values.split('-')
            intervals_list.append((int(interval[0]), int(interval[-1])))
        # Compute the union of the tuples (intervals).
        union_set = []
        for start, end in sorted(intervals_list):
            # Note: no interval start before the next one (ordered list).
            if union_set and start <= union_set[-1][1] + 1:
                # Current interval intersects or is consecutive to the last
                # one in the results.
                if union_set[-1][1] < end:
                    # If the end of the interval is greater than the last one
                    # then augment it (set the new end), otherwise do nothing
                    # (meaning the interval is fully included in the last one).
                    union_set[-1] = (union_set[-1][0], end)
            else:
                # No interval in the results or the interval does not intersect
                # nor is consecutive to the last one, then add it to the end of
                # the results list.
                union_set.append((start, end))
        # Convert resulting union set to a valid string format.
        union_str = ":".join(
            ["{0}-{1}".format(vals[0], vals[1])
             if vals[0] != vals[1] else str(vals[0]) for vals in union_set]
        )
        # Concatenate UUID and add the to the result list.
        union_gtid_list.append("{0}:{1}".format(uuid, union_str))

    # GTID sets are sorted alphabetically, return the result accordingly.
    return ','.join(sorted(union_gtid_list))


def gtid_set_itemize(gtid_set):
    """Itemize the given GTID set.

    Decompose the given GTID set into a list of individual GTID items grouped
    by UUID.

    gtid_set[in]    GTID set to itemize.

    Return a list of tuples with the UUIDs and transactions number for all
    individual items in the GTID set. For example: 'uuid_a:1-3:5,uuid_b:4' is
    converted into [('uuid_a', [1, 2, 3, 5]), ('uuid_b', [4])].
    """
    gtid_list = []
    uuid_sets = gtid_set.split(',')
    for uuid_set in uuid_sets:
        uuid_set_elements = uuid_set.split(':')
        trx_num_list = []
        for interval in uuid_set_elements[1:]:
            try:
                start_val, end_val = interval.split('-')
                trx_num_list.extend(range(int(start_val), int(end_val) + 1))
            except ValueError:
                # Error raised for single values (not an interval).
                trx_num_list.append(int(interval))
        gtid_list.append((uuid_set_elements[0], trx_num_list))
    return gtid_list
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains the following methods design to support common operations
over the ip address or hostnames among the multiple utilities.

Methods:
  parse_connection()         Parse connection parameters
"""

import re
import os
import logging



log = logging.getLogger('ip_parser')

_BAD_CONN_FORMAT = (u"Connection '{0}' cannot be parsed. Please review the "
                    u"used connection string (accepted formats: "
                    u"<user>[:<password>]@<host>[:<port>][:<socket>] or "
                    u"<login-path>[:<port>][:<socket>])")

_BAD_QUOTED_HOST = u"Connection '{0}' has a malformed quoted host"

_MULTIPLE_CONNECTIONS = (u"It appears you are attempting to specify multiple "
                         u"connections. This option does not permit multiple "
                         u"connections")

_UNPARSED_CONN_FORMAT = ("Connection '{0}' not parsed completely. Parsed "
                         "elements '{1}', unparsed elements '{2}'")

_CONN_USERPASS = re.compile(
    r"(?P<fquote>[\'\"]?)"    # First quote
    r"(?P<user>.+?)"          # User name
    r"(?:(?P=fquote))"        # First quote match
    r"(?:\:"                  # Optional :
    r"(?P<squote>[\'\"]?)"    # Second quote
    r"(?P<passwd>.+)"         # Password
    r"(?P=squote))"           # Second quote match
    r"|(?P<sfquote>[\'\"]?)"  # Quote on single user name
    r"(?P<suser>.+)"          # Single user name
    r"(?:(?P=sfquote))"       # Quote match on single user name
)

_CONN_QUOTEDHOST = re.compile(
    r"((?:^[\'].*[\'])|(?:^[\"].*[\"]))"  # quoted host name
    r"(?:\:(\d+))?"                       # Optional port number
    r"(?:\:([\/\\w+.\w+.\-]+))?"          # Optional path to socket
)

_CONN_LOGINPATH = re.compile(
    r"((?:\\\"|[^:])+|(?:\\\'|[^:])+)"  # login-path
    r"(?:\:(\d+))?"                     # Optional port number
    r"(?:\:([\/\\w+.\w+.\-]+))?"        # Optional path to socket
)

_CONN_CONFIGPATH = re.compile(
    r"([\w\:]+(?:\\\"|[^[])+|(?:\\\'|[^[])+)"  # config-path
    r"(?:\[([^]]+))?",                         # group
    re.U
)

_CONN_ANY_HOST = re.compile(
    r"""([\w\.]*%)
       (?:\:{0,1}(.*))                   # capture all the rest
    """, re.VERBOSE)

_CONN_HOST_NAME = re.compile(
    r"""(
        (?:
           (?:
              (?:
                 (?!-)         # must not start with hyphen '-'
                 (?:[\w\d-])*  # must not end with the hyphen
                 [A-Za-z]      # starts with a character from the alphabet
                 (?:[\w\d-])*
                 (?:
                    (?<!-)     # end capturing if a '-' is followed by '.'
                 )
               ){1,63}         # limited length for segment
            )
         (?:                   # following segments
            (?:\.)?            # the segment separator  the dot '.'
            (?:
               (?!-)
               [\w\d-]{1,63}   # last segment
               (?<!-)          #shuld not end with hyphen
            )
          )*
         )
        )
       (.*)                    # capture all the rest
     """, re.VERBOSE)

_CONN_IPv4_NUM_ONLY = re.compile(
    r"""(
          (?:         # start of the IPv4 1st group
             25[0-5]  # this match numbers 250 to 255
                    | # or
             2[0-4]\d # this match numbers from 200 to 249
                    | # or
             1\d\d    # this match numbers from 100 to 199
                    | # or
             [1-9]{0,1}\d # this match numbers from 0 to 99
           )
          (?:         # start of the 3 next groups
             \.       # the prefix '.' like in '.255'
             (?:
                25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d
                      # same group as before
              )
           )
             {3}      # but it will match 3 times of it and prefixed by '.'
          )
          (?:\:{0,1}(.*))
          """, re.VERBOSE)

_CONN_port_ONLY = re.compile(
    r"""(?:
          \]{0,1}             # the ']' of IPv6 -optional
                 \:{0,1}      # the ':' for port number  -optional
                        (
                         \d*  # matches any sequence of numbers
                         )
         )          # end of port number group
        (?:\:{0,1}(.*))      # all the rest to extract the socket
        """, re.VERBOSE)

_CONN_socket_ONLY = re.compile(
    r"""(?:           # Not capturing group of ':'
           \:{0,1}
             ([      # Capturing '\' or '/' file name.ext
               \/\\w+.\w+.\-
               ]+    # to match a path
              )
        )?
       (.*)          # all the rest to advice the user.
    """, re.VERBOSE)

_CONN_IPv6 = re.compile(
    r"""
    \[{0,1}                   # the optional heading '['
    (
     (?!.*::.*::)              # Only a single whildcard allowed
     (?:(?!:)|:(?=:))          # Colon iff it would be part of a wildcard
     (?:                       # Repeat 6 times:
        [0-9a-f]{0,4}          # A group of at most four hexadecimal digits
        (?:(?<=::)|(?<!::):)   # Colon unless preceded by wildcard
     ){6}                      # expecting 6 groups
     (?:                       # Either
        [0-9a-f]{0,4}          # Another group
        (?:(?<=::)|(?<!::):)   # Colon unless preceded by wildcard
        [0-9a-f]{0,4}          # Last group
        (?:(?<=::)             # Colon iff preceded by exacly one colon
           |(?<!:)
           |(?<=:)(?<!::):
         )
      )
     )
     (?:
        \]{0,1}\:{0,1}(.*)     # optional closing ']' and group for the rest
      )
    """, re.VERBOSE)

# Type of address amd Key names for the dictionary IP_matchers
HN = "hostname"
ipv4 = "IPv4"
ipv6 = "IPv6"
ANY_LIKE = "host like"
# This list is used to set an order to the matchers.
IP_matchers_list = [ipv4, ipv6, ANY_LIKE, HN]
# This dictionary is used to identify the matched type..
IP_matchers = {
    ANY_LIKE: _CONN_ANY_HOST,
    HN: _CONN_HOST_NAME,
    ipv4: _CONN_IPv4_NUM_ONLY,
    ipv6: _CONN_IPv6
}


def hostname_is_ip(hostname):
    """Determine hostname is an IP address.

    Return bool - True = is IP address
    """
    if len(hostname.split(":")) <= 1:  # if fewer colons, must be IPv4
        grp = _CONN_IPv4_NUM_ONLY.match(hostname)
    else:
        grp = _CONN_IPv6.match(hostname)
    if not grp:
        return False
    return True


def handle_config_path(config_path, group=None, use_default=True):
    """Retrieve the data associated to the given group.

    config_path[in]    the path to the configuration file.
    group[in]          The group name to retrieve the data from, if None
                       the 'client' group will be use if found and if
                       use_default is True.
    use_default[in]    Use the default 'client' group name, True by default,
                       used if no group is given.

    Returns a dictionary with the options data.
    """
    # first verify if the configuration file exist on the given config_path
    # check config_path as near file as normalized path, then
    # at the default location file.

    if os.name == 'nt':
        default_loc = os.path.join('c:\\', config_path)
    else:
        default_loc = os.path.join('/etc/mysql/', config_path)

    # default group
    default_group = 'client'
    # first look at the given path, if not found look at the default location
    paths = [os.path.normpath(config_path), os.path.normpath(default_loc)]
    req_group = group
    # if not group is given use default.
    if not req_group and use_default:
        req_group = default_group
    for file_loc in paths:
        if os.path.exists(file_loc) and os.path.isfile(file_loc):
            opt_par = MySQLOptionsParser(file_loc)
            dict_grp = opt_par.get_groups_as_dict(req_group)
            if dict_grp:
                return dict_grp[req_group]
            else:
                if group:
                    raise UtilError("The given group '{0}' was not found on "
                                    "the configuration file '{1}'"
                                    "".format(group, file_loc))
                else:
                    raise UtilError("The default group '{0}' was not found "
                                    "on the configuration file '{1}'"
                                    "".format(req_group, file_loc))

    # No configuration file was found
    if paths[0] != paths[1]:
        raise UtilError("Could not find a configuration file neither in the "
                        "given path '{0}' nor the default path '{1}'."
                        "".format(*paths))
    raise UtilError("Could not find a configuration file in the given "
                    "path '{0}'.".format(paths[0]))


def parse_connection(connection_values, my_defaults_reader=None, options=None):
    """Parse connection values.

    The function parses a connection specification of one of the forms::

      - user[:password]@host[:port][:socket]
      - login-path[:port][:socket]

    A dictionary is returned containing the connection parameters. The
    function is designed so that it shall be possible to use it with a
    ``connect`` call in the following manner::

      options = parse_connection(spec)
      conn = mysql.connector.connect(**options)

    conn_values[in]         Connection values in the form:
                            user:password@host:port:socket
                            or login-path:port:socket
    my_defaults_reader[in]  Instance of MyDefaultsReader to read the
                            information of the login-path from configuration
                            files. By default, the value is None.
    options[in]             Dictionary of options (e.g. basedir), from the used
                            utility. By default, it set with an empty
                            dictionary. Note: also supports options values
                            from optparse.

    Notes:

    This method validates IPv4 addresses and standard IPv6 addresses.

    This method accepts quoted host portion strings. If the host is marked
    with quotes, the code extracts this without validation and assigns it to
    the host variable in the returned tuple. This allows users to specify host
    names and IP addresses that are outside of the supported validation.

    Returns dictionary (user, passwd, host, port, socket)
            or raise an exception if parsing error
    """
    if options is None:
        options = {}

    def _match(pattern, search_str):
        """Returns the groups from string search or raise FormatError if it
        does not match with the pattern.
        """
        grp = pattern.match(search_str)
        if not grp:
            raise FormatError(_BAD_CONN_FORMAT.format(connection_values))
        return grp.groups()

    # SSL options, must not be overwritten with those from options.
    ssl_ca = None
    ssl_cert = None
    ssl_key = None
    ssl = None

    # Split on the '@' to determine the connection string format.
    # The user/password may have the '@' character, split by last occurrence.
    conn_format = connection_values.rsplit('@', 1)

    if len(conn_format) == 1:
        # No '@' so try config-path and login-path

        # The config_path and login-path collide on their first element and
        # only differs on their secondary optional values.
        # 1. Try match config_path and his optional value group. If both
        #    matches and the connection data can be retrieved, return the data.
        #    If errors occurs in this step raise them immediately.
        # 2. If config_path matches but group does not, and connection data
        #    can not be retrieved, do not raise errors and try to math
        #    login_path on step 4.
        # 3. If connection data is retrieved on step 2, then try login_path on
        #    next step to overwrite values from the new configuration.
        # 4. If login_path matches, check is .mylogin.cnf exists, if it doesn't
        #    and data configuration was found verify it  for missing values and
        #    continue if they are not any missing.
        # 5. If .mylogin.cnf exists and data configuration is found, overwrite
        #    any previews value from config_path if there is any.
        # 6. If login_path matches a secondary value but the configuration data
        #    could not be retrieved, do not continue and raise any error.
        # 7. In case errors have occurred trying to get data from config_path,
        #    and group did not matched, and in addition no secondary value,
        #    matched from login_path (port of socket) mention that config_path
        #    and login_path were not able to retrieve the connection data.

        # try login_path and overwrite the values.
        # Handle the format: config-path[[group]]
        config_path, group = _match(_CONN_CONFIGPATH, conn_format[0])
        port = None
        socket = None
        config_path_data = None
        login_path_data = None
        config_path_err_msg = None
        login_path = None
        if config_path:
            try:
                # If errors occurs, and group matched: raise any errors as the
                # group is exclusive of config_path.
                config_path_data = handle_config_path(config_path, group)
            except UtilError as err:
                if group:
                    raise
                else:
                    # Convert first letter to lowercase to include in error
                    # message with the correct case.
                    config_path_err_msg = \
                        err.errmsg[0].lower() + err.errmsg[1:]

        if group is None:
            # the conn_format can still be a login_path so continue
            # No '@' then handle has in the format: login-path[:port][:socket]
            login_path, port, socket = _match(_CONN_LOGINPATH, conn_format[0])

            # Check if the login configuration file (.mylogin.cnf) exists
            if login_path and not my_login_config_exists():
                if not config_path_data:
                    util_err_msg = (".mylogin.cnf was not found at is default "
                                    "location: {0}. Please configure your "
                                    "login-path data before using it (use the "
                                    "mysql_config_editor tool)."
                                    "".format(my_login_config_path()))
                    if config_path_err_msg and not (port or socket):
                        util_err_msg = ("{0} In addition, {1}"
                                        "").format(util_err_msg,
                                                   config_path_err_msg)
                    raise UtilError(util_err_msg)

            else:
                # If needed, create a MyDefaultsReader and search for
                # my_print_defaults tool.
                if not my_defaults_reader:
                    try:
                        my_defaults_reader = MyDefaultsReader(options)
                    except UtilError as err:
                        if config_path_err_msg and not (port or socket):
                            util_err_msg = ("{0} In addition, {1}"
                                            "").format(err.errmsg,
                                                       config_path_err_msg)
                            raise UtilError(util_err_msg)
                        else:
                            raise

                elif not my_defaults_reader.tool_path:
                    my_defaults_reader.search_my_print_defaults_tool()

                # Check if the my_print_default tool is able to read a
                # login-path from the mylogin configuration file
                if not my_defaults_reader.check_login_path_support():
                    util_err_msg = ("the used my_print_defaults tool does not "
                                    "support login-path options: {0}. "
                                    "Please confirm that the location to a "
                                    "tool with login-path support is included "
                                    "in the PATH (at the beginning)."
                                    "".format(my_defaults_reader.tool_path))
                    if config_path_err_msg and not (port or socket):
                        util_err_msg = ("{0} In addition, {1}"
                                        "").format(util_err_msg,
                                                   config_path_err_msg)
                    raise UtilError(util_err_msg)

                # Read and parse the login-path data (i.e., user, password and
                # host)
                login_path_data = my_defaults_reader.get_group_data(login_path)

        if config_path_data or login_path_data:
            if config_path_data:
                if not login_path_data:
                    login_path_data = config_path_data
                else:
                    # Overwrite values from login_path_data
                    config_path_data.update(login_path_data)
                    login_path_data = config_path_data

            user = login_path_data.get('user', None)
            passwd = login_path_data.get('password', None)
            host = login_path_data.get('host', None)
            if not port:
                port = login_path_data.get('port', None)
            if not socket:
                socket = login_path_data.get('socket', None)

            if os.name == "posix" and socket is not None:
                # if we are on unix systems and used a socket, hostname can be
                # safely assumed as being localhost so it is not required
                required_options = ('user', 'socket')
                host = 'localhost' if host is None else host
            else:
                required_options = ('user', 'host', 'port')

            missing_options = [opt for opt in required_options
                               if locals()[opt] is None]
            # If we are on unix and port is missing, user might have specified
            # a socket instead
            if os.name == "posix" and "port" in missing_options:
                i = missing_options.index("port")
                if socket:  # If we have a socket, port is not needed
                    missing_options.pop(i)
                else:
                    # if we don't have neither a port nor a socket, we need
                    # either a port or a socket
                    missing_options[i] = "port or socket"

            if missing_options:
                message = ",".join(missing_options)
                if len(missing_options) > 1:
                    comma_idx = message.rfind(",")
                    message = "{0} and {1}".format(message[:comma_idx],
                                                   message[comma_idx + 1:])
                pluralize = "s" if len(missing_options) > 1 else ""
                raise UtilError("Missing connection value{0} for "
                                "{1} option{0}".format(pluralize, message))

            # optional options, available only on config_path_data
            if config_path_data:
                ssl_ca = config_path_data.get('ssl-ca', None)
                ssl_cert = config_path_data.get('ssl-cert', None)
                ssl_key = config_path_data.get('ssl-key', None)
                ssl = config_path_data.get('ssl', None)

        else:
            if login_path and not config_path:
                raise UtilError("No login credentials found for login-path: "
                                "{0}. Please review the used connection string"
                                ": {1}".format(login_path, connection_values))
            elif not login_path and config_path:
                raise UtilError("No login credentials found for config-path: "
                                "{0}. Please review the used connection string"
                                ": {1}".format(login_path, connection_values))
            elif login_path and config_path:
                raise UtilError("No login credentials found for either "
                                "login-path: '{0}' nor config-path: '{1}'. "
                                "Please review the used connection string: {2}"
                                "".format(login_path, config_path,
                                          connection_values))

    elif len(conn_format) == 2:

        # Check to see if the user attempted to pass a list of connections.
        # This is true if there is at least one comma and multiple @ symbols.
        if ((connection_values.find(',') > 0) and
                (connection_values.find('@') > 1)):
            raise FormatError(_MULTIPLE_CONNECTIONS.format(connection_values))

        # Handle as in the format: user[:password]@host[:port][:socket]
        userpass, hostportsock = conn_format

        # Get user, password
        match = _CONN_USERPASS.match(userpass)
        if not match:
            raise FormatError(_BAD_CONN_FORMAT.format(connection_values))
        user = match.group('user')
        if user is None:
            # No password provided
            user = match.group('suser').rstrip(':')
        passwd = match.group('passwd')

        # Handle host, port and socket
        if len(hostportsock) <= 0:
            raise FormatError(_BAD_CONN_FORMAT.format(connection_values))

        if hostportsock[0] in ['"', "'"]:
            # need to strip the quotes
            host, port, socket = _match(_CONN_QUOTEDHOST, hostportsock)
            if host[0] == '"':
                host = host.strip('"')
            if host[0] == "'":
                host = host.strip("'")

        else:
            host, port, socket, _ = parse_server_address(hostportsock)

    else:
        # Unrecognized format
        raise FormatError(_BAD_CONN_FORMAT.format(connection_values))

    # Get character-set from options
    if isinstance(options, dict):
        charset = options.get("charset", None)
        # If one SSL option was found before, not mix with those in options.
        if not ssl_cert and not ssl_ca and not ssl_key and not ssl:
            ssl_cert = options.get("ssl_cert", None)
            ssl_ca = options.get("ssl_ca", None)
            ssl_key = options.get("ssl_key", None)
            ssl = options.get("ssl", None)

    else:
        # options is an instance of optparse.Values
        try:
            charset = options.charset  # pylint: disable=E1103
        except AttributeError:
            charset = None
        # If one SSL option was found before, not mix with those in options.
        if not ssl_cert and not ssl_ca and not ssl_key and not ssl:
            try:
                ssl_cert = options.ssl_cert  # pylint: disable=E1103
            except AttributeError:
                ssl_cert = None
            try:
                ssl_ca = options.ssl_ca  # pylint: disable=E1103
            except AttributeError:
                ssl_ca = None
            try:
                ssl_key = options.ssl_key  # pylint: disable=E1103
            except AttributeError:
                ssl_key = None
            try:
                ssl = options.ssl  # pylint: disable=E1103
            except AttributeError:
                ssl = None

    # Set parsed connection values
    connection = {
        "user": user,
        "host": host,
        "port": int(port) if port else 3306,
        "passwd": passwd if passwd else ''
    }

    if charset:
        connection["charset"] = charset
    if ssl_cert:
        connection["ssl_cert"] = ssl_cert
    if ssl_ca:
        connection["ssl_ca"] = ssl_ca
    if ssl_key:
        connection["ssl_key"] = ssl_key
    if ssl:
        connection["ssl"] = ssl
    # Handle optional parameters. They are only stored in the dict if
    # they were provided in the specifier.
    if socket is not None and os.name == "posix":
        connection['unix_socket'] = socket

    return connection


def parse_server_address(connection_str):
    """Parses host, port and socket from the given connection string.

    Returns a tuple of (host, port, socket, add_type) where add_type is
    the name of the parser that successfully parsed the hostname from
    the connection string.
    """
    # Default values to return.
    host = None
    port = None
    socket = None
    address_type = None
    unparsed = None
    # From the matchers look the one that match a host.
    # pylint: disable=R0101
    for IP_matcher in IP_matchers_list:
        try:
            group = _match(IP_matchers[IP_matcher], connection_str)
            if group:
                host = group[0]
                if IP_matcher == ipv6:
                    host = "[%s]" % host

                if group[1]:
                    part2_port_socket = _match(_CONN_port_ONLY, group[1],
                                               trow_error=False)
                    if not part2_port_socket:
                        unparsed = group[1]
                    else:
                        port = part2_port_socket[0]
                        if part2_port_socket[1]:
                            part4 = _match(_CONN_socket_ONLY,
                                           part2_port_socket[1],
                                           trow_error=False)
                            if not part4:
                                unparsed = part2_port_socket[1]
                            else:
                                socket = part4[0]
                                unparsed = part4[1]

            # If host is match we stop looking as is the most significant.
            if host:
                address_type = IP_matcher
                break
        # ignore the error trying to match.
        except FormatError:
            pass
    # we must alert, that the connection could not be parsed.
    if host is None:
        raise FormatError(_BAD_CONN_FORMAT.format(connection_str))
    _verify_parsing(connection_str, host, port, socket, address_type, unparsed)

    return host, port, socket, address_type


def _verify_parsing(connection_str, host, port, socket, address_type,
                    unparsed):
    """Verify that the connection string was totally parsed and not parts of
    it where not matched, otherwise raise an error.
    """
    exp_connection_str = connection_str
    log.debug("exp_connection_str {0}".format(exp_connection_str))
    parsed_connection_list = []
    if host:
        log.debug("host {0}".format(host))
        if address_type == ipv6 and "[" not in connection_str:
            host = host.replace("[", "")
            host = host.replace("]", "")
        parsed_connection_list.append(host)
    if port:
        log.debug("port {0}".format(port))
        parsed_connection_list.append(port)
    if socket:
        log.debug("socket {0}".format(socket))
        parsed_connection_list.append(socket)
    parsed_connection = ":".join(parsed_connection_list)
    log.debug('parsed_connection {0}'.format(parsed_connection))
    diff = None
    if not unparsed:
        log.debug('not unparsed found, creating diff')
        diff = connection_str.replace(host, "")
        if port:
            diff = diff.replace(port, "")
        if socket:
            diff = diff.replace(socket, "")
        log.debug("diff {0}".format(diff))
    log.debug("unparsed {0}".format(unparsed))
    if unparsed or (exp_connection_str != parsed_connection and
                    (diff and diff != ":")):
        log.debug("raising exception")
        parsed_args = "host:%s, port:%s, socket:%s" % (host, port, socket)
        log.debug(_UNPARSED_CONN_FORMAT.format(connection_str,
                                               parsed_args,
                                               unparsed))
        raise FormatError(_UNPARSED_CONN_FORMAT.format(connection_str,
                                                       parsed_args,
                                                       unparsed))


def _match(pattern, connection_str, trow_error=True):
    """Tries to match a pattern with the connection string and returns the
    groups.
    """
    grp = pattern.match(connection_str)
    if not grp:
        if trow_error:
            raise FormatError(_BAD_CONN_FORMAT.format(connection_str))
        return False
    return grp.groups()


def clean_IPv6(host_address):
    """Clean IPv6 host address
    """
    if host_address:
        host_address = host_address.replace("[", "")
        host_address = host_address.replace("]", "")
    return host_address


def format_IPv6(host_address):
    """Format IPv6 host address
    """
    if host_address:
        if "]" not in host_address:
            host_address = "[{0}]".format(host_address)
    return host_address


def parse_login_values_config_path(login_values, quietly=True):
    """Parse the login values to retrieve the user and password from a
    configuration file.

    login_values[in]    The login values to be parsed.
    quietly[in]         Do not raise exceptions (Default True).

    returns parsed (user, password) tuple or (login_values, None) tuple.
    """
    try:
        matches = _match(_CONN_CONFIGPATH, login_values, trow_error=False)
        if matches:
            path = matches[0]
            group = matches[1]
            data = handle_config_path(path, group, use_default=False)
            user = data.get('user', None)
            passwd = data.get('password', None)
            return user, passwd
    except (FormatError, UtilError):
        if not quietly:
            raise
    return login_values, None


def find_password(value):
    """Search for password in a string

    value[in]           String to search for password
    """
    if not isinstance(value, str):
        return False
    # has to have an @ sign
    if '@' not in value:
        return False
    match = _CONN_USERPASS.match(value)
    if not match:
        return False
    if match.group('passwd'):
        return True
    return False
#
# Copyright (c) 2011, 2012, 2013, Oracle and/or its affiliates. All rights
# reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains the methods for checking consistency among two databases.
"""


# The following are the queries needed to perform table locking.

LOCK_TYPES = ['READ', 'WRITE']

_SESSION_ISOLATION_LEVEL = \
    "SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ"

_START_TRANSACTION = "START TRANSACTION WITH CONSISTENT SNAPSHOT"

_LOCK_WARNING = "WARNING: Lock in progress. You must call unlock() " + \
                "to unlock your tables."

_FLUSH_TABLES_READ_LOCK = "FLUSH TABLES WITH READ LOCK"


class Lock(object):
    """Lock
    """
    def __init__(self, server, table_list, options=None):
        """Constructor

        Lock a list of tables based on locking type. Locking types and their
        behavior is as follows:

           - (default) use consistent read with a single transaction
           - lock all tables without consistent read and no transaction
           - no locks, no transaction, no consistent read
           - flush (replication only) - issue a FTWRL command

        server[in]         Server instance of server to run locks
        table_list[in]     list of tuples (table_name, lock_type)
        options[in]        dictionary of options
                           locking = [snapshot|lock-all|no-locks|flush],
                           verbosity int
                           silent bool
                           rpl_mode string
        """
        if options is None:
            options = {}
        self.locked = False
        self.silent = options.get('silent', False)
        # Determine locking type
        self.locking = options.get('locking', 'snapshot')
        self.verbosity = options.get('verbosity', 0)
        if self.verbosity is None:
            self.verbosity = 0
        else:
            self.verbosity = int(self.verbosity)

        self.server = server
        self.table_list = table_list

        self.query_opts = {'fetch': False, 'commit': False}

        # If no locking, we're done
        if self.locking == 'no-locks':
            return

        elif self.locking == 'lock-all':
            # Check lock requests for validity
            table_locks = []
            for tablename, locktype in table_list:
                if locktype.upper() not in LOCK_TYPES:
                    raise UtilDBError("Invalid lock type '%s' for table '%s'."
                                      % (locktype, tablename))
                # Build LOCK TABLE command
                table_locks.append("%s %s" % (tablename, locktype))
            lock_str = "LOCK TABLE "
            lock_str += ', '.join(table_locks)

            if self.verbosity >= 3 and not self.silent:
                print '# LOCK STRING:', lock_str

            # Execute the lock
            self.server.exec_query(lock_str, self.query_opts)

            self.locked = True

        elif self.locking == 'snapshot':
            self.server.exec_query(_SESSION_ISOLATION_LEVEL, self.query_opts)
            self.server.exec_query(_START_TRANSACTION, self.query_opts)

        # Execute a FLUSH TABLES WITH READ LOCK for replication uses only
        elif self.locking == 'flush' and options.get("rpl_mode", None):
            if self.verbosity >= 3 and not self.silent:
                print "# LOCK STRING: %s" % _FLUSH_TABLES_READ_LOCK
            self.server.exec_query(_FLUSH_TABLES_READ_LOCK, self.query_opts)
            self.locked = True
        else:
            raise UtilError("Invalid locking type: '%s'." % self.locking)

    def __del__(self):
        """Destructor

        Returns string - warning if the lock has not been disengaged.
        """
        if self.locked:
            return _LOCK_WARNING

        return None

    def unlock(self, abort=False):
        """Release the table lock.
        """
        if not self.locked:
            return

        if self.verbosity >= 3 and not self.silent and \
           self.locking != 'no-locks':
            print "# UNLOCK STRING:",
        # Call unlock:
        if self.locking in ['lock-all', 'flush']:
            if self.verbosity >= 3 and not self.silent:
                print "UNLOCK TABLES"
            self.server.exec_query("UNLOCK TABLES", self.query_opts)
            self.locked = False

        # Stop transaction if locking == 0
        elif self.locking == 'snapshot':
            if not abort:
                if self.verbosity >= 3 and not self.silent:
                    print "COMMIT"
                self.server.exec_query("COMMIT", self.query_opts)
            else:
                self.server.exec_queery("ROLLBACK", self.query_opts)
                if self.verbosity >= 3 and not self.silent:
                    print "ROLLBACK"
#
# Copyright (c) 2013, 2016 Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains output string messages used by MySQL Utilities.
"""

EXTERNAL_SCRIPT_DOES_NOT_EXIST = ("'{path}' script cannot be found. Please "
                                  "check the path and filename for accuracy "
                                  "and try again.")

ERROR_ANSI_QUOTES_MIX_SQL_MODE = ("One or more servers have SQL mode set to "
                                  "ANSI_QUOTES, the {utility} requires to all "
                                  "or none of the servers to be set with the "
                                  "SQL mode set to ANSI_QUOTES.")

ERROR_USER_WITHOUT_PRIVILEGES = ("User '{user}' on '{host}@{port}' does not "
                                 "have sufficient privileges to "
                                 "{operation} (required: {req_privileges}).")

PARSE_ERR_DB_PAIR = ("Cannot parse the specified database(s): '{db_pair}'. "
                     "Please verify that the database(s) are specified in "
                     "a valid format (i.e., {db1_label}[:{db2_label}]) and "
                     "that backtick quotes are properly used when required.")

PARSE_ERR_DB_PAIR_EXT = ("%s The use of backticks is required if non "
                         "alphanumeric characters are used for database "
                         "names. Parsing the specified database results "
                         "in {db1_label} = '{db1_value}' and "
                         "{db2_label} = '{db2_value}'." % PARSE_ERR_DB_PAIR)

PARSE_ERR_DB_OBJ_PAIR = ("Cannot parse the specified database objects: "
                         "'{db_obj_pair}'. Please verify that the objects "
                         "are specified in a valid format (i.e., {db1_label}"
                         "[.{obj1_label}]:{db2_label}[.{obj2_label}]) and "
                         "that backtick quotes are properly used if "
                         "required.")

PARSE_ERR_DB_OBJ_PAIR_EXT = ("%s The use of backticks is required if non "
                             "alphanumeric characters are used for identifier "
                             "names. Parsing the specified objects results "
                             "in: {db1_label} = '{db1_value}', "
                             "{obj1_label} = '{obj1_value}', "
                             "{db2_label} = '{db2_value}' and "
                             "{obj2_label} = '{obj2_value}'."
                             % PARSE_ERR_DB_OBJ_PAIR)

PARSE_ERR_DB_OBJ_MISSING_MSG = ("Incorrect object compare argument, one "
                                "specific object is missing. Please verify "
                                "that both object are correctly specified. "
                                "{detail} Format should be: "
                                "{db1_label}[.{obj1_label}]"
                                ":{db2_label}[.{obj2_label}].")

PARSE_ERR_DB_OBJ_MISSING = ("No object has been specified for "
                            "{db_no_obj_label} '{db_no_obj_value}', while "
                            "object '{only_obj_value}' was specified for "
                            "{db_obj_label} '{db_obj_value}'.")

PARSE_ERR_DB_MISSING_CMP = ("You must specify at least one database to "
                            "compare or use the --all option to compare all "
                            "databases.")

PARSE_ERR_OBJ_NAME_FORMAT = ("Cannot parse the specified qualified name "
                             "'{obj_name}' for {option}. Please verify that a "
                             "valid format is used (i.e., <db_name>"
                             "[.<tbl_name>]) and that backtick quotes are "
                             "properly used if required.")

PARSE_ERR_SPAN_KEY_SIZE_TOO_HIGH = (
    "The value {s_value} specified for option --span-key-size is too big. It "
    "must be smaller or equal than {max} (size of the key hash values for "
    "comparison).")

PARSE_ERR_SPAN_KEY_SIZE_TOO_LOW = (
    "The value {s_value} specified for option --span-key-size is too small "
    "and would cause inaccurate results, please retry with a bigger value "
    "or the default value of {default}.")

PARSE_ERR_OPT_INVALID_CMD = "Invalid {opt} option for '{cmd}'."

PARSE_ERR_OPT_INVALID_CMD_TIP = ("%s Use {opt_tip} instead."
                                 % PARSE_ERR_OPT_INVALID_CMD)

PARSE_ERR_OPT_INVALID_DATE = "Invalid {0} date format (yyyy-mm-dd): {1}"

PARSE_ERR_OPT_INVALID_DATE_TIME = ("Invalid {0} date/time format "
                                   "(yyyy-mm-ddThh:mm:ss): {1}")

PARSE_ERR_OPT_INVALID_NUM_DAYS = ("Invalid number of days (must be an integer "
                                  "greater than zero) for {0} date: {1}")

PARSE_ERR_OPT_INVALID_VALUE = ("The value for option {option} is not valid: "
                               "'{value}'.")

PARSE_ERR_OPT_REQ_NON_NEGATIVE_VALUE = ("Option '{opt}' requires a "
                                        "non-negative value.")

PARSE_ERR_OPT_REQ_GREATER_VALUE = ("Option '{opt}' requires a value greater "
                                   "than {val}.")

PARSE_ERR_OPT_REQ_VALUE = "Option '{opt}' requires a non-empty value."

PARSE_ERR_OPT_REQ_OPT = ("Option {opt} requires the following option(s): "
                         "{opts}.")

PARSE_ERR_OPTS_EXCLD = ("Options {opt1} and {opt2} cannot be used "
                        "together.")

PARSE_ERR_OPTS_REQ = "Option '{opt}' is required."

PARSE_ERR_OPTS_REQ_BY_CMD = ("'{cmd}' requires the following option(s): "
                             "{opts}.")

PARSE_ERR_SLAVE_DISCO_REQ = ("Option --discover-slaves-login or --slaves is "
                             "required.")

PARSE_ERR_OPTS_REQ_GREATER_OR_EQUAL = ("The {opt} option requires a value "
                                       "greater than or equal to {value}.")

WARN_OPT_NOT_REQUIRED = ("WARNING: The {opt} option is not required for "
                         "'{cmd}' (option ignored).")

WARN_OPT_NOT_REQUIRED_ONLY_FOR = ("%s Only used with the {only_cmd} command."
                                  % WARN_OPT_NOT_REQUIRED)

WARN_OPT_NOT_REQUIRED_FOR_TYPE = (
    "# WARNING: The {opt} option is not required for the {type} type "
    "(option ignored).")

WARN_OPT_ONLY_USED_WITH = ("# WARNING: The {opt} option is only used with "
                           "{used_with} (option ignored).")

WARN_OPT_USING_DEFAULT = ("WARNING: Using default value '{default}' for "
                          "option {opt}.")

ERROR_SAME_MASTER = ("The specified new master {n_master_host}:{n_master_port}"
                     " is the same as the "
                     "actual master {master_host}:{master_port}.")

SLAVES = "slaves"

CANDIDATES = "candidates"

ERROR_MASTER_IN_SLAVES = ("The master {master_host}:{master_port} "
                          "and one of the specified {slaves_candidates} "
                          "are the same {slave_host}:{slave_port}.")

SCRIPT_THRESHOLD_WARNING = ("WARNING: You have chosen to use external script "
                            "return code checking. Depending on which script "
                            "fails, this can leave the operation in an "
                            "undefined state. Please check your results "
                            "carefully if the operation aborts.")

HOST_IP_WARNING = ("You may be mixing host names and IP addresses. This may "
                   "result in negative status reporting if your DNS services "
                   "do not support reverse name lookup.")

ERROR_MIN_SERVER_VERSIONS = ("The {utility} requires server versions greater "
                             "or equal than {min_version}. Server version for "
                             "'{host}:{port}' is not supported.")

PARSE_ERR_SSL_REQ_SERVER = ("Options --ssl-ca, --ssl-cert and --ssl-key "
                            "requires use of --server.")

WARN_OPT_SKIP_INNODB = ("The use of InnoDB is mandatory since MySQL 5.7. The "
                        "former options like '--innodb=0/1/OFF/ON' or "
                        "'--skip-innodb' are ignored.")

FILE_DOES_NOT_EXIST = "The following path is invalid, '{path}'."

INSUFFICIENT_FILE_PERMISSIONS = ("You do not have permission to {permissions} "
                                 "file '{path}'.")

MSG_UTILITIES_VERSION = "MySQL Utilities {utility} version {version}."

MSG_MYSQL_VERSION = "Server '{server}' is using MySQL version {version}."

USER_PASSWORD_FORMAT = ("Format of {0} option is incorrect. Use userid:passwd "
                        "or userid.")
#
# Copyright (c) 2013, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module provides features to read MySQL configuration files, wrapping the
tool my_print_defaults.
"""

import optparse
import os.path
import re
import subprocess
import tempfile


_MY_PRINT_DEFAULTS_TOOL = "my_print_defaults"
MYLOGIN_FILE = ".mylogin.cnf"


def my_login_config_path():
    """Return the default path of the mylogin file (.mylogin.cnf).
    """
    if os.name == 'posix':
        # File located in $HOME for non-Windows systems
        return os.path.expanduser('~')
    else:
        # File located in %APPDATA%\MySQL for Windows systems
        return r'{0}\MySQL'.format(os.environ['APPDATA'])


def my_login_config_exists():
    """Check if the mylogin file (.mylogin.cnf) exists.
    """

    my_login_fullpath = os.path.normpath(os.path.join(my_login_config_path(),
                                                      MYLOGIN_FILE))
    return os.path.isfile(my_login_fullpath)


class MyDefaultsReader(object):
    """The MyDefaultsReader class is used to read the data stored from a MySQL
    configuration file. This class provide methods to read the options data
    stored in configurations files, using the my_print_defaults tool. To learn
    more about my_print_defaults see:
    http://dev.mysql.com/doc/en/my-print-defaults.html
    """

    def __init__(self, options=None, find_my_print_defaults_tool=True):
        """Constructor

        options[in]                 dictionary of options (e.g. basedir). Note,
                                    allows options values from optparse to be
                                    passed directly to this parameter.
        find_my_print_defaults[in]  boolean value indicating if the tool
                                    my_print_defaults should be located upon
                                    initialization of the object.
        """
        if options is None:
            options = {}
        # _config_data is a dictionary of option groups containing a dictionary
        # of the options data read from the configuration file.
        self._config_data = {}

        # Options values from optparse can be directly passed, check if it is
        # the case and handle them correctly.
        if isinstance(options, optparse.Values):
            try:
                self._basedir = options.basedir  # pylint: disable=E1103
            except AttributeError:
                # if the attribute is not found, then set it to None (default).
                self._basedir = None
            try:
                # if the attribute is not found, then set it to 0 (default).
                self._verbosity = options.verbosity  # pylint: disable=E1103
            except AttributeError:
                self._verbosity = 0
        else:
            self._basedir = options.get("basedir", None)
            self._verbosity = options.get("verbosity", 0)

        if find_my_print_defaults_tool:
            self.search_my_print_defaults_tool()
        else:
            self._tool_path = None

    @property
    def tool_path(self):
        """Sets tool_path property
        """
        return self._tool_path

    def search_my_print_defaults_tool(self, search_paths=None):
        """Search for the tool my_print_defaults.
        """
        if not search_paths:
            search_paths = []

        # Set the default search paths (i.e., default location of the
        # .mylogin.cnf file).
        default_paths = [my_login_config_path()]

        # Extend the list of path to search with the ones specified.
        if search_paths:
            default_paths.extend(search_paths)

        # Search for the tool my_print_defaults.
        try:
            self._tool_path = get_tool_path(self._basedir,
                                            _MY_PRINT_DEFAULTS_TOOL,
                                            defaults_paths=default_paths,
                                            search_PATH=True)
        except UtilError as err:
            raise UtilError("Unable to locate MySQL Client tools. "
                            "Please confirm that the path to the MySQL client "
                            "tools are included in the PATH. Error: %s"
                            % err.errmsg)

    def check_show_required(self):
        """Check if the '--show' password option is required/supported by this
        version of the my_print_defaults tool.

        At MySQL Server 5.6.25 and 5.7.8, my_print_defaults' functionality
        changed to mask passwords by default and added the '--show' password
        option to display passwords in cleartext (BUG#19953365, BUG#20903330).
        As this module requires the password to be displayed as cleartext to
        extract the password, the use of the '--show' password option is also
        required starting on these version of the server, however the
        my_print_defaults tool version did not increase with this change, so
        this method looks at the output of the help text of my_print_defaults
        tool to determine if the '--show' password option is supported by the
        my_print_defaults tool available at _tool_path.

        Returns True if this version of the tool supports the'--show' password
        option, otherwise False.
        """
        # The path to the tool must have been previously found.
        assert self._tool_path, ("First, the required MySQL tool must be "
                                 "found. E.g., use method "
                                 "search_my_print_defaults_tool.")

        # Create a temporary file to redirect stdout
        out_file = tempfile.TemporaryFile()
        if self._verbosity > 0:
            subprocess.call([self._tool_path, "--help"], stdout=out_file)
        else:
            # Redirect stderr to null
            null_file = open(os.devnull, "w+b")
            subprocess.call([self._tool_path, "--help"], stdout=out_file,
                            stderr=null_file)

        # Read my_print_defaults help output text
        out_file.seek(0)
        lines = out_file.readlines()
        out_file.close()

        # find the "--show" option used to show passwords in plain text.
        for line in lines:
            if "--show" in line:
                return True

        # The option was not found in the tool help output.
        return False

    def check_tool_version(self, major_version, minor_version):
        """Check the version of the my_print_defaults tool.

        Returns True if the version of the tool is equal or above the one that
        is specified, otherwise False.
        """
        # The path to the tool must have been previously found.
        assert self._tool_path, ("First, the required MySQL tool must be "
                                 "found. E.g., use method "
                                 "search_my_print_defaults_tool.")

        # Create a temporary file to redirect stdout
        out_file = tempfile.TemporaryFile()
        if self._verbosity > 0:
            subprocess.call([self._tool_path, "--version"], stdout=out_file)
        else:
            # Redirect stderr to null
            null_file = open(os.devnull, "w+b")
            subprocess.call([self._tool_path, "--version"], stdout=out_file,
                            stderr=null_file)
        # Read --version output
        out_file.seek(0)
        line = out_file.readline()
        out_file.close()

        # Parse the version value
        match = re.search(r'(?:Ver )(\d)\.(\d)', line)
        if match:
            major, minor = match.groups()
            return (
                (major_version < int(major)) or
                (major_version == int(major) and
                 minor_version <= int(minor))
            )
        else:
            raise UtilError("Unable to determine tool version - %s" %
                            self._tool_path)

    def check_login_path_support(self):
        """Checks if the used my_print_defaults tool supports login-paths.
        """
        # The path to the tool must have been previously found.
        assert self._tool_path, ("First, the required MySQL tool must be "
                                 "found. E.g., use method "
                                 "search_my_print_defaults_tool.")

        # Create a temporary file to redirect stdout
        out_file = tempfile.TemporaryFile()
        if self._verbosity > 0:
            subprocess.call([self._tool_path, "--help"], stdout=out_file)
        else:
            # Redirect stderr to null
            null_file = open(os.devnull, "w+b")
            subprocess.call([self._tool_path, "--help"], stdout=out_file,
                            stderr=null_file)
        # Read --help output
        out_file.seek(0)
        help_output = out_file.read()
        out_file.close()

        # Check the existence of a "login-path" option
        return ('login-path' in help_output)

    def _read_group_data(self, group):
        """Read group options data using my_print_defaults tool.
        """
        # The path to the tool must have been previously found.
        assert self._tool_path, ("First, the required MySQL tool must be "
                                 "found. E.g., use method "
                                 "search_my_print_defaults_tool.")

        mp_cmd = [self._tool_path, group]
        if self.check_show_required():
            mp_cmd.append("--show")

        # Group not found; use my_print_defaults to get group data.
        out_file = tempfile.TemporaryFile()
        if self._verbosity > 0:
            subprocess.call(mp_cmd, stdout=out_file)
        else:
            # Redirect stderr to null
            null_file = open(os.devnull, "w+b")
            subprocess.call(mp_cmd, stdout=out_file, stderr=null_file)

        # Read and parse group options values.
        out_file.seek(0)
        results = []
        for line in out_file:
            # Parse option value; ignore starting "--"
            key_value = line[2:].split("=", 1)
            if len(key_value) == 2:
                # Handle option format: --key=value and --key=
                results.append((key_value[0], key_value[1].strip()))
            elif len(key_value) == 1:
                # Handle option format: --key
                results.append((key_value[0], True))
            else:
                raise UtilError("Invalid option value format for "
                                "group %s: %s" % (group, line))
        out_file.close()

        if len(results):
            self._config_data[group] = dict(results)
        else:
            self._config_data[group] = None

        return self._config_data[group]

    def get_group_data(self, group):
        """Retrieve the data associated to the given group.
        """
        # Returns group's data locally stored, if available.
        try:
            return self._config_data[group]
        except KeyError:
            # Otherwise, get it using my_print_defaults.
            return self._read_group_data(group)

    def get_option_value(self, group, opt_name):
        """Retrieve the value associated to the given opt_name in the group.
        """
        # Get option value, if group's data is available.
        grp_options = self.get_group_data(group)
        if grp_options:
            return grp_options.get(opt_name, None)
        else:
            return None
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains the following methods design to support common option
parsing among the multiple utilities.

Methods:
  setup_common_options()     Setup standard options for utilities
"""

import copy
import optparse
from optparse import Option as CustomOption, OptionValueError
import os.path
import re

from datetime import datetime
from ip_parser import find_password, parse_login_values_config_path
from mysql.connector.conversion import MySQLConverter


_PERMITTED_FORMATS = ["grid", "tab", "csv", "vertical"]
_PERMITTED_DIFFS = ["unified", "context", "differ"]
_PERMITTED_RPL_DUMP = ["master", "slave"]


class UtilitiesParser(optparse.OptionParser):
    """Special subclass of parser that allows showing of version information
       when --help is used.
    """

    def print_help(self, output=None):
        """Show version information before help
        """
        print self.version
        optparse.OptionParser.print_help(self, output)

    def format_epilog(self, formatter):
        return self.epilog if self.epilog is not None else ''


def prefix_check_choice(option, opt, value):
    """Check option values using case insensitive prefix compare

    This method checks to see if the value specified is a prefix of one of the
    choices. It converts the string provided by the user (value) to lower case
    to permit case insensitive comparison of the user input. If multiple
    choices are found for a prefix, an error is thrown. If the value being
    compared does not match the list of choices, an error is thrown.

    option[in]             Option class instance
    opt[in]                option name
    value[in]              the value provided by the user

    Returns string - valid option chosen
    """
    # String of choices
    choices = ", ".join([repr(choice) for choice in option.choices])

    # Get matches for prefix given
    alts = [alt for alt in option.choices if alt.startswith(value.lower())]
    if len(alts) == 1:   # only 1 match
        return alts[0]
    elif len(alts) > 1:  # multiple matches
        raise OptionValueError(
            ("option %s: there are multiple prefixes "
             "matching: %r (choose from %s)") % (opt, value, choices))

    # Doesn't match. Show user possible choices.
    raise OptionValueError("option %s: invalid choice: %r (choose from %s)"
                           % (opt, value, choices))


def license_callback(self, opt, value, parser, *args, **kwargs):
    """Show license information and exit.
    """
    print(LICENSE_FRM.format(program=parser.prog))
    parser.exit()


def path_callback(option, opt, value, parser):
    """Verify that the given path is an existing file. If it is then add it
    to the parser values.

    option[in]        option instance
    opt[in]           option name
    value[in]         given user value
    parser[in]        parser instance
    """
    if not os.path.exists(value):
        parser.error("the given path '{0}' in option {1} does not"
                     " exist or can not be accessed".format(value, opt))

    if not os.path.isfile(value):
        parser.error("the given path '{0}' in option {1} does not"
                     " correspond to a file".format(value, opt))

    setattr(parser.values, option.dest, value)


def ssl_callback(option, opt, value, parser):
    """Verify that the given path is an existing file. If it is then add it
    to the parser values.

    option[in]        option instance
    opt[in]           option name
    value[in]         given user value
    parser[in]        parser instance
    """
    if not (value == 0 or value == 1 or value == ''):
        parser.error("the given value '{0}' in option {1} is not"
                     " valid, valid values are 0 or 1.".format(value, opt))

    setattr(parser.values, option.dest, value)


def add_config_path_option(parser):
    """Add the config_path option.

    parser[in]        the parser instance
    """
    # --config-path option: config_path
    parser.add_option("--config-path", action="callback",
                      callback=path_callback,
                      type="string", help="The path to a MySQL option file "
                                          "with the login options")


def add_ssl_options(parser):
    """Add the ssl options.

    parser[in]        the parser instance
    """
    # --ssl options: ssl_ca, ssl_cert, ssl_key
    parser.add_option("--ssl-ca", action="callback",
                      callback=path_callback,
                      type="string", help="path to a file that contains "
                      "a list of trusted SSL CAs.")

    parser.add_option("--ssl-cert", action="callback",
                      callback=path_callback,
                      type="string", help="name of the SSL certificate "
                      "file to use for establishing a secure connection.")

    parser.add_option("--ssl-key", action="callback",
                      callback=path_callback,
                      type="string", help="name of the SSL key file to "
                      "use for establishing a secure connection.")

    parser.add_option("--ssl", action="callback", callback=ssl_callback,
                      type="int", help="specifies if the server "
                      "connection requires use of SSL. If an encrypted "
                      "connection cannot be established, the connection "
                      "attempt fails. By default 0 (SSL not required).")


class CaseInsensitiveChoicesOption(CustomOption):
    """Case insensitive choices option class

    This is an extension of the Option class. It replaces the check_choice
    method with the prefix_check_choice() method above to provide
    shortcut aware choice selection. It also ensures the choice compare is
    done with a case insensitve test.
    """
    TYPE_CHECKER = copy.copy(CustomOption.TYPE_CHECKER)
    TYPE_CHECKER["choice"] = prefix_check_choice

    def __init__(self, *opts, **attrs):
        if 'choices' in attrs:
            attrs['choices'] = [attr.lower() for attr in attrs['choices']]
        CustomOption.__init__(self, *opts, **attrs)


def setup_common_options(program_name, desc_str, usage_str,
                         append=False, server=True,
                         server_default="root@localhost:3306",
                         extended_help=None,
                         add_ssl=False):
    """Setup option parser and options common to all MySQL Utilities.

    This method creates an option parser and adds options for user
    login and connection options to a MySQL database system including
    user, password, host, socket, and port.

    program_name[in]   The program name
    desc_str[in]       The description of the utility
    usage_str[in]      A brief usage example
    append[in]         If True, allow --server to be specified multiple times
                       (default = False)
    server[in]         If True, add the --server option
                       (default = True)
    server_default[in] Default value for option
                       (default = "root@localhost:3306")
    extended_help[in]  Extended help (by default: None).
    add_ssl[in]        adds the --ssl-options, however these are added
                       automatically if server is True, (default = False)

    Returns parser object
    """

    program_name = program_name.replace(".py", "")
    parser = UtilitiesParser(
        version=VERSION_FRM.format(program=program_name),
        description=desc_str,
        usage=usage_str,
        add_help_option=False,
        option_class=CaseInsensitiveChoicesOption,
        epilog=extended_help,
        prog=program_name)
    parser.add_option("--help", action="help", help="display a help message "
                      "and exit")
    parser.add_option("--license", action='callback',
                      callback=license_callback,
                      help="display program's license and exit")

    if server:
        # Connection information for the first server
        if append:
            parser.add_option("--server", action="append", dest="server",
                              help="connection information for the server in "
                              "the form: <user>[:<password>]@<host>[:<port>]"
                              "[:<socket>] or <login-path>[:<port>]"
                              "[:<socket>] or <config-path>[<[group]>].")

        else:
            parser.add_option("--server", action="store", dest="server",
                              type="string", default=server_default,
                              help="connection information for the server in "
                              "the form: <user>[:<password>]@<host>[:<port>]"
                              "[:<socket>] or <login-path>[:<port>]"
                              "[:<socket>] or <config-path>[<[group]>].")

    if server or add_ssl:
        add_ssl_options(parser)

    return parser


def add_character_set_option(parser):
    """Add the --character-set option.

    parser[in]        the parser instance
    """
    parser.add_option("--character-set", action="store", dest="charset",
                      type="string", default=None,
                      help="sets the client character set. The default is "
                      "retrieved from the server variable "
                      "'character_set_client'.")


_SKIP_VALUES = (
    "tables", "views", "triggers", "procedures",
    "functions", "events", "grants", "data",
    "create_db"
)


def add_skip_options(parser):
    """Add the common --skip options for database utilties.

    parser[in]        the parser instance
    """
    parser.add_option("--skip", action="store", dest="skip_objects",
                      default=None, help="specify objects to skip in the "
                      "operation in the form of a comma-separated list (no "
                      "spaces). Valid values = tables, views, triggers, proc"
                      "edures, functions, events, grants, data, create_db")


def check_skip_options(skip_list):
    """Check skip options for validity

    skip_list[in]     List of items from parser option.

    Returns new skip list with items converted to upper case.
    """
    new_skip_list = []
    if skip_list is not None:
        items = skip_list.split(",")
        for item in items:
            obj = item.lower()
            if obj in _SKIP_VALUES:
                new_skip_list.append(obj)
            else:
                raise UtilError("The value %s is not a valid value for "
                                "--skip." % item)
    return new_skip_list


def add_format_option(parser, help_text, default_val, sql=False,
                      extra_formats=None):
    """Add the format option.

    parser[in]        the parser instance
    help_text[in]     help text
    default_val[in]   default value
    sql[in]           if True, add 'sql' format
                      default=False
    extra_formats[in] list with extra formats

    Returns corrected format value
    """
    formats = _PERMITTED_FORMATS
    if sql:
        formats.append('sql')
    if extra_formats:
        formats.extend(extra_formats)
    parser.add_option("-f", "--format", action="store", dest="format",
                      default=default_val, help=help_text, type="choice",
                      choices=formats)


def add_format_option_with_extras(parser, help_text, default_val,
                                  extra_formats):
    """Add the format option.

    parser[in]        the parser instance
    help_text[in]     help text
    default_val[in]   default value
    extra_formats[in] list of additional formats to support

    Returns corrected format value
    """
    formats = _PERMITTED_FORMATS
    formats.extend(extra_formats)
    parser.add_option("-f", "--format", action="store", dest="format",
                      default=default_val, help=help_text, type="choice",
                      choices=formats)


def add_no_headers_option(parser, restricted_formats=None, help_msg=None):
    """Add the --no-headers option.

    parser[in]              The parser instance.
    restricted_formats[in]  List of formats supported by this option (only
                            applies to them).
    help_msg[in]            Alternative help message to use, otherwise a
                            default one is used.
    """
    # Create the help message according to any format restriction.
    if restricted_formats:
        plural = "s" if len(restricted_formats) > 1 else ""
        formats_msg = (" (only applies to format{0}: "
                       "{1})").format(plural, ", ".join(restricted_formats))
    else:
        formats_msg = ""
    if help_msg:
        help_msg = "{0}{1}.".format(help_msg, formats_msg)
    else:
        help_msg = "do not show column headers{0}.".format(formats_msg)
    # Add the option.
    parser.add_option("-h", "--no-headers", action="store_true",
                      dest="no_headers", default=False, help=help_msg)


def add_verbosity(parser, quiet=True):
    """Add the verbosity and quiet options.

    parser[in]        the parser instance
    quiet[in]         if True, include the --quiet option
                      (default is True)

    """
    parser.add_option("-v", "--verbose", action="count", dest="verbosity",
                      help="control how much information is displayed. "
                      "e.g., -v = verbose, -vv = more verbose, -vvv = debug")
    if quiet:
        parser.add_option("-q", "--quiet", action="store_true", dest="quiet",
                          help="turn off all messages for quiet execution.",
                          default=False)


def check_verbosity(options):
    """Check to see if both verbosity and quiet are being used.
    """
    # Warn if quiet and verbosity are both specified
    if options.quiet is not None and options.quiet and \
       options.verbosity is not None and options.verbosity > 0:
        print "WARNING: --verbosity is ignored when --quiet is specified."
        options.verbosity = None


def add_changes_for(parser, default="server1"):
    """Add the changes_for option.

    parser[in]        the parser instance
    """
    parser.add_option("--changes-for", action="store", dest="changes_for",
                      type="choice", default=default, help="specify the "
                      "server to show transformations to match the other "
                      "server. For example, to see the transformation for "
                      "transforming server1 to match server2, use "
                      "--changes-for=server1. Valid values are 'server1' or "
                      "'server2'. The default is 'server1'.",
                      choices=['server1', 'server2'])


def add_reverse(parser):
    """Add the show-reverse option.

    parser[in]        the parser instance
    """
    parser.add_option("--show-reverse", action="store_true", dest="reverse",
                      default=False, help="produce a transformation report "
                      "containing the SQL statements to transform the object "
                      "definitions specified in reverse. For example if "
                      "--changes-for is set to server1, also generate the "
                      "transformation for server2. Note: the reverse changes "
                      "are annotated and marked as comments.")


def add_difftype(parser, allow_sql=False, default="unified"):
    """Add the difftype option.

    parser[in]        the parser instance
    allow_sql[in]     if True, allow sql as a valid option
                      (default is False)
    default[in]       the default option
                      (default is unified)
    """
    choice_list = ['unified', 'context', 'differ']
    if allow_sql:
        choice_list.append('sql')
    parser.add_option("-d", "--difftype", action="store", dest="difftype",
                      type="choice", default="unified", choices=choice_list,
                      help="display differences in context format in one of "
                      "the following formats: [%s] (default: unified)." %
                      '|'.join(choice_list))


def add_engines(parser):
    """Add the engine and default-storage-engine options.

    parser[in]        the parser instance
    """
    # Add engine
    parser.add_option("--new-storage-engine", action="store",
                      dest="new_engine", default=None, help="change all "
                      "tables to use this storage engine if storage engine "
                      "exists on the destination.")
    # Add default storage engine
    parser.add_option("--default-storage-engine", action="store",
                      dest="def_engine", default=None, help="change all "
                      "tables to use this storage engine if the original "
                      "storage engine does not exist on the destination.")


def check_engine_options(server, new_engine, def_engine,
                         fail=False, quiet=False):
    """Check to see if storage engines specified in options exist.

    This method will check to see if the storage engine in new exists on the
    server. If new_engine is None, the check is skipped. If the storage engine
    does not exist and fail is True, an exception is thrown else if quiet is
    False, a warning message is printed.

    Similarly, def_engine will be checked and if not present and fail is True,
    an exception is thrown else if quiet is False a warning is printed.

    server[in]         server instance to be checked
    new_engine[in]     new storage engine
    def_engine[in]     default storage engine
    fail[in]           If True, issue exception on failure else print warning
                       default = False
    quiet[in]          If True, suppress warning messages (not exceptions)
                       default = False
    """
    def _find_engine(server, target, message, fail, default):
        """Find engine
        """
        if target is not None:
            found = server.has_storage_engine(target)
            if not found and fail:
                raise UtilError(message)
            elif not found and not quiet:
                print message

    server.get_storage_engines()
    message = "WARNING: %s storage engine %s is not supported on the server."

    _find_engine(server, new_engine,
                 message % ("New", new_engine),
                 fail, quiet)
    _find_engine(server, def_engine,
                 message % ("Default", def_engine),
                 fail, quiet)


def add_all(parser, objects):
    """Add the --all option.

    parser[in]        the parser instance
    objects[in]       name of the objects for which all includes
    """
    parser.add_option("-a", "--all", action="store_true", dest="all",
                      default=False, help="include all %s" % objects)


def check_all(parser, options, args, objects):
    """Check to see if both all and specific arguments are used.

    This method will throw an exception if there are arguments listed and
    the all option has been turned on.

    parser[in]        the parser instance
    options[in]       command options
    args[in]          arguments list
    objects[in]       name of the objects for which all includes
    """
    if options.all and len(args) > 0:
        parser.error("You cannot use the --all option with a list of "
                     "%s." % objects)


def add_locking(parser):
    """Add the --locking option.

    parser[in]        the parser instance
    """
    parser.add_option("--locking", action="store", dest="locking",
                      type="choice", default="snapshot",
                      choices=['no-locks', 'lock-all', 'snapshot'],
                      help="choose the lock type for the operation: no-locks "
                      "= do not use any table locks, lock-all = use table "
                      "locks but no transaction and no consistent read, "
                      "snaphot (default): consistent read using a single "
                      "transaction.")


def add_exclude(parser, object_type="objects",
                example1="db1.t1", example2="db1.t% or db%.%"):
    """Add the --exclude option.

    parser[in]        the parser instance
    example1[in]
    example2[in]
    """
    parser.add_option("-x", "--exclude", action="append", dest="exclude",
                      type="string", default=None, help="exclude one or more "
                      "{0} from the operation using either a specific "
                      "name (e.g. {1}), a LIKE pattern (e.g. {2}) or a REGEXP "
                      "search pattern. To use a REGEXP search pattern for all "
                      "exclusions, you must also specify the --regexp option. "
                      "Repeat the --exclude option for multiple exclusions."
                      "".format(object_type, example1, example2))


def check_exclude_pattern(exclude_list, use_regexp):
    """Check the --exclude pattern to determine if there are special symbols
    that may be regexp symbols and the --use-regexp option is not specified.
    Prints warning if this is true.

    parser[in]        the parser instance
    use_regexp[in]    the option to use regexp
    """
    # ignore null lists
    if not exclude_list:
        return True
    for row in exclude_list:
        # replace _ and % and see if still not alnum()
        test = row.replace('_', '').replace('%', '').replace('`', '')
        test = test.replace("'", "").replace('.', '').replace('"', '')
        if len(test) > 0 and not test.isalnum() and not use_regexp:
            print "# WARNING: One or more of your --exclude patterns " \
                  "contains symbols that could be regexp patterns. You may " \
                  "need to include --regexp to ensure your exclude pattern " \
                  "is evaluated as REGEXP and not a SQL LIKE expression."
            return False
    return True


def add_regexp(parser):
    """Add the --regexp option.

    parser[in]        the parser instance
    """
    parser.add_option("-G", "--basic-regexp", "--regexp", dest="use_regexp",
                      action="store_true", default=False, help="use 'REGEXP' "
                      "operator to match pattern. Default is to use 'LIKE'.")


def add_rpl_user(parser):
    """Add the --rpl-user option.

    parser[in]        the parser instance
    """
    parser.add_option("--rpl-user", action="store", dest="rpl_user",
                      type="string",
                      help="the user and password for the replication "
                           "user requirement, in the form: <user>[:<password>]"
                           " or <login-path>. E.g. rpl:passwd")


def add_rpl_mode(parser, do_both=True, add_file=True):
    """Add the --rpl and --rpl-file options.

    parser[in]        the parser instance
    do_both[in]       if True, include the "both" value for the --rpl option
                      Default = True
    add_file[in]      if True, add the --rpl-file option
                      Default = True
    """
    rpl_mode_both = ""
    rpl_mode_options = _PERMITTED_RPL_DUMP
    if do_both:
        rpl_mode_options.append("both")
        rpl_mode_both = (", and 'both' = include 'master' and 'slave' options "
                         "where applicable")
    parser.add_option("--rpl", "--replication", dest="rpl_mode",
                      action="store", help="include replication information. "
                      "Choices: 'master' = include the CHANGE MASTER command "
                      "using the source server as the master, "
                      "'slave' = include the CHANGE MASTER command for "
                      "the source server's master (only works if the source "
                      "server is a slave){0}.".format(rpl_mode_both),
                      choices=rpl_mode_options)
    if add_file:
        parser.add_option("--rpl-file", "--replication-file", dest="rpl_file",
                          action="store", help="path and file name to place "
                          "the replication information generated. Valid on if "
                          "the --rpl option is specified.")


def check_rpl_options(parser, options):
    """Check replication dump options for validity

    This method ensures the optional --rpl-* options are valid only when
    --rpl is specified.

    parser[in]        the parser instance
    options[in]       command options
    """
    if options.rpl_mode is None:
        errors = []
        if parser.has_option("--comment-rpl") and options.rpl_file is not None:
            errors.append("--rpl-file")

        if options.rpl_user is not None:
            errors.append("--rpl-user")

        # It's Ok if the options do not include --comment-rpl
        if parser.has_option("--comment-rpl") and options.comment_rpl:
            errors.append("--comment-rpl")

        if len(errors) > 1:
            num_opt_str = "s"
        else:
            num_opt_str = ""

        if len(errors) > 0:
            parser.error("The %s option%s must be used with the --rpl "
                         "option." % (", ".join(errors), num_opt_str))


def add_discover_slaves_option(parser):
    """Add the --discover-slaves-login option.

    This method adds the --discover-slaves-login option that is used to
    discover the list of slaves associated to the specified login (user and
    password).

    parser[in]      the parser instance.
    """
    parser.add_option("--discover-slaves-login", action="store",
                      dest="discover", default=None, type="string",
                      help="at startup, query master for all registered "
                      "slaves and use the user name and password specified to "
                      "connect. Supply the user and password in the form "
                      "<user>[:<password>] or <login-path>. For example, "
                      "--discover-slaves-login=joe:secret will use 'joe' as "
                      "the user and 'secret' as the password for each "
                      "discovered slave.")


def add_log_option(parser):
    """Add the --log option.

    This method adds the --log option that is used the specify the target file
    for logging messages from the utility.

    parser[in]      the parser instance.
    """
    parser.add_option("--log", action="store", dest="log_file", default=None,
                      type="string", help="specify a log file to use for "
                      "logging messages")


def add_master_option(parser):
    """Add the --master option.

    This method adds the --master option that is used to specify the connection
    string for the server with the master role.

    parser[in]      the parser instance.
    """
    parser.add_option("--master", action="store", dest="master", default=None,
                      type="string", help="connection information for master "
                      "server in the form: <user>[:<password>]@<host>[:<port>]"
                      "[:<socket>] or <login-path>[:<port>][:<socket>]"
                      " or <config-path>[<[group]>].")


def add_slaves_option(parser):
    """Add the --slaves option.

    This method adds the --slaves option that is used to specify a list of
    slaves, more precisely their connection strings (separated by comma).

    parser[in]      the parser instance.
    """
    parser.add_option("--slaves", action="store", dest="slaves",
                      type="string", default=None,
                      help="connection information for slave servers in "
                      "the form: <user>[:<password>]@<host>[:<port>]"
                      "[:<socket>] or <login-path>[:<port>][:<socket>]"
                      " or <config-path>[<[group]>]. "
                      "List multiple slaves in comma-separated list.")


def add_failover_options(parser):
    """Add the common failover options.

    This adds the following options:

      --candidates
      --discover-slaves-login
      --exec-after
      --exec-before
      --log
      --log-age
      --master
      --max-position
      --ping
      --seconds-behind
      --slaves
      --timeout
      --script-threshold

    parser[in]        the parser instance
    """
    parser.add_option("--candidates", action="store", dest="candidates",
                      type="string", default=None,
                      help="connection information for candidate slave servers"
                      " for failover in the form: <user>[:<password>]@<host>[:"
                      "<port>][:<socket>] or <login-path>[:<port>][:<socket>]"
                      " or <config-path>[<[group]>]"
                      " Valid only with failover command. List multiple slaves"
                      " in comma-separated list.")

    add_discover_slaves_option(parser)

    parser.add_option("--exec-after", action="store", dest="exec_after",
                      default=None, type="string", help="name of script to "
                      "execute after failover or switchover")

    parser.add_option("--exec-before", action="store", dest="exec_before",
                      default=None, type="string", help="name of script to "
                      "execute before failover or switchover")

    add_log_option(parser)

    parser.add_option("--log-age", action="store", dest="log_age", default=7,
                      type="int", help="specify maximum age of log entries in "
                      "days. Entries older than this will be purged on "
                      "startup. Default = 7 days.")

    add_master_option(parser)

    parser.add_option("--max-position", action="store", dest="max_position",
                      default=0, type="int", help="used to detect slave "
                      "delay. The maximum difference between the master's "
                      "log position and the slave's reported read position of "
                      "the master. A value greater than this means the slave "
                      "is too far behind the master. Default is 0.")

    parser.add_option("--ping", action="store", dest="ping", default=None,
                      help="Number of ping attempts for detecting downed "
                      "server.")

    parser.add_option("--seconds-behind", action="store", dest="max_delay",
                      default=0, type="int", help="used to detect slave "
                      "delay. The maximum number of seconds behind the master "
                      "permitted before slave is considered behind the "
                      "master. Default is 0.")

    add_slaves_option(parser)

    parser.add_option("--timeout", action="store", dest="timeout", default=300,
                      help="maximum timeout in seconds to wait for each "
                      "replication command to complete. For example, timeout "
                      "for slave waiting to catch up to master. "
                      "Default = 300.")

    parser.add_option("--script-threshold", action="store", default=None,
                      dest="script_threshold",
                      help="Value for external scripts to trigger aborting "
                      "the operation if result is greater than or equal to "
                      "the threshold. Default = None (no threshold "
                      "checking).")


def check_server_lists(parser, master, slaves):
    """Check to see if master is listed in slaves list

    Returns bool - True = master not in slaves, issue error if it appears
    """
    if slaves:
        for slave in slaves.split(',', 1):
            if master == slave:
                parser.error("You cannot list the master as a slave.")

    return True


def obj2sql(obj):
    """Convert a Python object to an SQL object.

    This function convert Python objects to SQL values using the
    conversion functions in the database connector package."""
    return MySQLConverter().quote(obj)


def parse_user_password(userpass_values, my_defaults_reader=None,
                        options=None):
    """ This function parses a string with the user/password credentials.

    This function parses the login string, determines the used format, i.e.
    user[:password], config-path or login-path. If the ':' (colon) is not in
    the login string, the it can refer to a config-path, login-path or to a
    username (without a password). In this case, first it is assumed that the
    specified value is a config-path and tries to retrive the user and password
    from the configuration file secondly assume it is a login-path and the
    function attempts to retrieve the associated username and password, in a
    quiet way (i.e., without raising exceptions). If it fails to retrieve the
    login-path data, then the value is assumed to be a username.

    userpass_values[in]     String indicating the user/password credentials. It
                            must be in the form: user[:password] or login-path.
    my_defaults_reader[in]  Instance of MyDefaultsReader to read the
                            information of the login-path from configuration
                            files. By default, the value is None.
    options[in]             Dictionary of options (e.g. basedir), from the used
                            utility. By default, it set with an empty
                            dictionary. Note: also supports options values
                            from optparse.

    Returns a tuple with the username and password.
    """
    if options is None:
        options = {}
    # Split on the first ':' to determine if a login-path is used.
    login_values = userpass_values.split(':', 1)
    if len(login_values) == 1:
        # Format is config-path, login-path or user (without a password):
        # First check if the value is a config-path
        # The following method call also initializes the user and passwd with
        # default values in case the login_values are not from a config-path
        user, passwd = parse_login_values_config_path(login_values[0],
                                                      quietly=True)

        # Second assume it's a login-path and quietly try to retrieve the user
        # and password, in case of success overwrite the values previously set
        # and in case of failure return these ones instead.

        # Check if the login configuration file (.mylogin.cnf) exists
        if login_values[0] and not my_login_config_exists():
            return user, passwd

        if not my_defaults_reader:
            # Attempt to create the MyDefaultsReader
            try:
                my_defaults_reader = MyDefaultsReader(options)
            except UtilError:
                # Raise an UtilError when my_print_defaults tool is not found.
                return user, passwd
        elif not my_defaults_reader.tool_path:
            # Try to find the my_print_defaults tool
            try:
                my_defaults_reader.search_my_print_defaults_tool()
            except UtilError:
                # Raise an UtilError when my_print_defaults tool is not found.
                return user, passwd

        # Check if the my_print_default tool is able to read a login-path from
        # the mylogin configuration file
        if not my_defaults_reader.check_login_path_support():
            return user, passwd

        # Read and parse the login-path data (i.e., user and password)
        try:
            loginpath_data = my_defaults_reader.get_group_data(login_values[0])
            if loginpath_data:
                user = loginpath_data.get('user', None)
                passwd = loginpath_data.get('password', None)
                return user, passwd
            else:
                return user, passwd
        except UtilError:
            # Raise an UtilError if unable to get the login-path group data
            return user, passwd

    elif len(login_values) == 2:
        # Format is user:password; return a tuple with the user and password
        return login_values[0], login_values[1]
    else:
        # Invalid user credentials format
        raise FormatError("Unable to parse the specified user credentials "
                          "(accepted formats: <user>[:<password> or "
                          "<login-path>): %s" % userpass_values)


def add_basedir_option(parser):
    """ Add the --basedir option.
    """
    parser.add_option("--basedir", action="store", dest="basedir",
                      default=None, type="string",
                      help="the base directory for the server")


def check_dir_option(parser, opt_value, opt_name, check_access=False,
                     read_only=False):
    """ Check if the specified directory option is valid.

    Check if the value specified for the option is a valid directory, and if
    the user has appropriate access privileges. An appropriate  parser error
    is issued if the specified directory is invalid.

    parser[in]          Instance of the option parser (optparse).
    opt_value[in]       Value specified for the option.
    opt_name[in]        Option name (e.g., --basedir).
    check_access[in]    Flag specifying if the access privileges need to be
                        checked. By default, False (no access check).
    read_only[in]       Flag indicating if the access required is only for
                        read or read/write. By default, False (read/write
                        access). Note: only used if check_access=True.

    Return the absolute path for the specified directory or None if an empty
    value is specified.
    """
    # Check existence of specified directory.
    if opt_value:
        full_path = get_absolute_path(opt_value)
        if not os.path.isdir(full_path):
            parser.error("The specified path for {0} option is not a "
                         "directory: {1}".format(opt_name, opt_value))
        if check_access:
            mode = os.R_OK if read_only else os.R_OK | os.W_OK
            if not os.access(full_path, mode):
                parser.error("You do not have enough privileges to access the "
                             "folder specified by {0}.".format(opt_name))
        return full_path
    return None


def check_script_option(parser, opt_value, check_executable=True):
    """ Check if the specified script option is valid.

    Check if the script specified for the option exists, and if
    the user has appropriate access privileges to it. An appropriate parser
    error is issued if the specified directory does not exist or is not
    executable.

    parser[in]            Instance of the option parser (optparse).
    opt_value[in]         Value specified for the option.
    check_executable[in]  Flag specifying if the executable privileges need to
                          be checked. By default, True(needs to be executable).

    Return the absolute path for the specified script or None if an empty
    value is specified.
    """
    if opt_value:
        abs_path = os.path.abspath(opt_value)
        if not os.path.isfile(abs_path):
            parser.error(EXTERNAL_SCRIPT_DOES_NOT_EXIST.format(
                path=opt_value))

        if check_executable and not os.access(abs_path, os.X_OK):
            parser.error(INSUFFICIENT_FILE_PERMISSIONS.format(
                path=opt_value, permissions='execute'))
        return opt_value
    else:
        return None


def get_absolute_path(path):
    """ Returns the absolute path.
    """
    return os.path.abspath(os.path.expanduser(os.path.normpath(path)))


def db_objects_list_to_dictionary(parser, obj_list, option_desc,
                                  db_over_tables=True, sql_mode=''):
    """Process database object list and convert to a dictionary.

    Check the qualified name format of the given database objects and convert
    the given list of object to a dictionary organized by database names and
    sets of specific objects.

    Note: It is assumed that the given object list is obtained from the
    arguments or an option returned by the parser.

    parser[in]            Instance of the used option/arguments parser
    obj_list[in]          List of objects to process.
    option_desc[in]       Short description of the option for the object list
                          (e.g., "the --exclude option", "the database/table
                          arguments") to refer appropriately in any parsing
                          error.
    db_over_tables[in]    If True specifying a db alone overrides all
                          occurrences of table objects from that db (e.g.
                          if True and we have both db and db.table1, db.table1
                          is ignored).

    returns a dictionary with the objects grouped by database (without
    duplicates). None value associated to a database entry means that all
    objects are to be considered.
    E.g. {'db_name1': set(['table1','table2']), 'db_name2': None}.
    """
    db_objs_dict = {}
    for obj_name in obj_list:
        m_objs = parse_object_name(obj_name, sql_mode)
        if m_objs[0] is None:
            parser.error(PARSE_ERR_OBJ_NAME_FORMAT.format(
                obj_name=obj_name, option=option_desc
            ))
        else:
            db_name, obj_name = m_objs
            # Remove backtick quotes.
            db_name = remove_backtick_quoting(db_name, sql_mode) \
                if is_quoted_with_backticks(db_name, sql_mode) else db_name
            obj_name = remove_backtick_quoting(obj_name, sql_mode) \
                if obj_name and is_quoted_with_backticks(obj_name, sql_mode) \
                else obj_name
            # Add database object to result dictionary.
            if not obj_name:
                # If only the database is specified and db_over_tables is True,
                # then add entry with db name and value None (to include all
                # objects) even if a previous specific object was already
                # added, else if db_over_tables is False, add None value to the
                #  list, so that we know db was specified without any
                # table/routine.
                if db_name in db_objs_dict:
                    if db_objs_dict[db_name] and not db_over_tables:
                        db_objs_dict[db_name].add(None)
                    else:
                        db_objs_dict[db_name] = None
                else:
                    if db_over_tables:
                        db_objs_dict[db_name] = None
                    else:
                        db_objs_dict[db_name] = set([None])
            else:
                # If a specific object object is given add it to the set
                # associated to the database, except if the database entry
                # is None (meaning that all objects are included).
                if db_name in db_objs_dict:
                    if db_objs_dict[db_name]:
                        db_objs_dict[db_name].add(obj_name)
                else:
                    db_objs_dict[db_name] = set([obj_name])
    return db_objs_dict


def get_ssl_dict(parser_options=None):
    """Returns a dictionary with the SSL certificates

    parser_options[in]   options instance from the used option/arguments parser

    Returns a dictionary with the SSL certificates, each certificate name as
    the key with underscore instead of dash. If no certificate has been given
    by the user in arguments, returns an empty dictionary.

    Note: parser_options is a Values instance, that does not have method get as
    a dictionary instance.
    """
    conn_options = {}
    if parser_options is not None:
        certs_paths = {}
        if 'ssl_ca' in dir(parser_options):
            certs_paths['ssl_ca'] = parser_options.ssl_ca
        if 'ssl_cert' in dir(parser_options):
            certs_paths['ssl_cert'] = parser_options.ssl_cert
        if 'ssl_key' in dir(parser_options):
            certs_paths['ssl_key'] = parser_options.ssl_key
        if 'ssl' in dir(parser_options):
            certs_paths['ssl'] = parser_options.ssl
        conn_options.update(certs_paths)
    return conn_options


def get_value_intervals_list(parser, option_value, option_name, value_name):
    """Get and check the list of values for the given option.

    Convert the string value for the given option to the corresponding
    list of integer values and tuple of integers (for intervals). For example,
    converts the option_value '3,5-8,11' to the list [3, (5,8), 11].

    A parser error is issued if the used values or format are invalid.

    parser[in]          Instance of the used option/arguments parser.
    option_value[in]    Value specified for the option (e.g., '3,5-8,11').
    option_name[in]     Name of the option (e.g., '--status').
    value_name[in]      Name describing each option value (e.g., 'status').

    Returns a list of integers and tuple of integers (for intervals)
    representing the given option value string.
    """
    # Filter empty values and convert all to integers (values and intervals).
    values = option_value.split(",")
    values = [value for value in values if value]
    if len(values) <= 0:
        parser.error(PARSE_ERR_OPT_INVALID_VALUE.format(option=option_name,
                                                        value=option_value))
    res_list = []
    for value in values:
        interval = value.split('-')
        if len(interval) == 2:
            # Convert lower and higher value of the interval.
            try:
                lv = int(interval[0])
            except ValueError:
                parser.error("Invalid {0} value '{1}' (must be a "
                             "non-negative integer) for interval "
                             "'{2}'.".format(value_name, interval[0], value))
            try:
                hv = int(interval[1])
            except ValueError:
                parser.error("Invalid {0} value '{1}' (must be a "
                             "non-negative integer) for interval "
                             "'{2}'.".format(value_name, interval[1], value))
            # Add interval (tuple) to the list.
            res_list.append((lv, hv))
        elif len(interval) == 1:
            # Add single value to the status list.
            try:
                res_list.append(int(value))
            except ValueError:
                parser.error("Invalid {0} value '{1}' (must be a "
                             "non-negative integer).".format(value_name,
                                                             value))
        else:
            # Invalid format.
            parser.error("Invalid format for {0} interval (a single "
                         "dash must be used): '{1}'.".format(value_name,
                                                             value))
    return res_list


def check_date_time(parser, date_value, date_type, allow_days=False):
    """Check the date/time value for the given option.

    Check if the date/time value for the option is valid. The supported
    formats are 'yyyy-mm-ddThh:mm:ss' and 'yyyy-mm-dd'. If the allow days
    flag is ON then an integer valuse representing the number of days is
    also accepted.

    A parser error is issued if the date/time value is invalid.

    parser[in]        Instance of the used option/arguments parser.
    date_value[in]    Date/time value specified for the option.
    date_type[in]     Name describing the type of date being checked
                      (e.g., start, end, modified).
    allow_days[in]    Flag indicating if the specified value can also be an
                      integer representing the number of of days (> 0).

    Returns the date in the format 'yyyy-mm-ddThh:mm:ss' or an integer
    representing the number of days.
    """
    if allow_days:
        # Check if it is a valid number of days.
        try:
            days = int(date_value)
        except ValueError:
            # Not a valid integer (i.e., number of days).
            days = None
        if days:
            if days < 1:
                parser.error(PARSE_ERR_OPT_INVALID_NUM_DAYS.format(
                    date_type, date_value))
            return days
    # Check if it is a valid date/time format.
    _, _, time = date_value.partition("T")
    if time:
        try:
            dt_date = datetime.strptime(date_value, '%Y-%m-%dT%H:%M:%S')
        except ValueError:
            parser.error(PARSE_ERR_OPT_INVALID_DATE_TIME.format(date_type,
                                                                date_value))
    else:
        try:
            dt_date = datetime.strptime(date_value, '%Y-%m-%d')
        except ValueError:
            parser.error(PARSE_ERR_OPT_INVALID_DATE.format(date_type,
                                                           date_value))
    return dt_date.strftime('%Y-%m-%dT%H:%M:%S')


def check_gtid_set_format(parser, gtid_set):
    """Check the format of the GTID set given for the option.

    Perform some basic checks to verify the syntax of the specified string
    for the GTID set value. A parse error is issued if the format is incorrect.

    parser[in]      Instance of the used option/arguments parser.
    gtid_set[in]    GTID set value specified for the option.
    """

    # UUID format: hhhhhhhh-hhhh-hhhh-hhhh-hhhhhhhhhhhh
    re_uuid = re.compile(
        r"(?:[a-f]|\d){8}(?:-(?:[a-f]|\d){4}){3}-(?:[a-f]|\d){12}",
        re.IGNORECASE)
    # interval format: n[-n]
    re_interval = re.compile(r"(?:\d+)(?:-\d+)?")
    uuid_sets = gtid_set.split(',')
    for uuid_set in uuid_sets:
        uuid_set_elements = uuid_set.split(':')
        if len(uuid_set_elements) < 2:
            parser.error("Invalid GTID set '{0}' for option --gtid-set, "
                         "missing UUID or interval. Valid format: "
                         "uuid:interval[:interval].".format(uuid_set))
        # Check server UUID format.
        if not re_uuid.match(uuid_set_elements[0]):
            parser.error("Invalid UUID '{0}' for option --gtid-set. Valid "
                         "format: hhhhhhhh-hhhh-hhhh-hhhh-hhhhhhhhhhhh."
                         "".format(uuid_set_elements[0]))
        # Check intervals.
        for interval in uuid_set_elements[1:]:
            if not re_interval.match(interval):
                parser.error("Invalid interval '{0}' for option --gtid-set. "
                             "Valid format: n[-n].".format(interval))
            try:
                start_val, end_val = interval.split('-')
                if int(start_val) >= int(end_val):
                    parser.error(
                        "Invalid interval '{0}' for option --gtid-set. Start "
                        "value must be lower than the end value."
                        "".format(interval))
            except ValueError:
                # Error raised for intervals with a single value.
                pass  # Ignore no need to compare start and end value.


def check_password_security(options, args, prefix=""):
    """Check command line for passwords and report a warning.

    This method checks all options for passwords in the form ':%@'. If
    this pattern is found, the method with issue a warning to stdout and
    return True, else it returns False.

    Note: this allows us to make it possible to abort if command-line
          passwords are found (not the default...yet).

    options[in]     list of options
    args[in]        list of arguments
    prefix[in]      (optional) allows preface statement with # or something
                    for making the message a comment in-stream

    Returns - bool : False = no passwords, True = password found and msg shown
    """
    result = False
    for value in options.__dict__.values():
        if isinstance(value, list):
            for item in value:
                if find_password(item):
                    result = True
        else:
            if find_password(value):
                result = True
    for arg in args:
        if find_password(arg):
            result = True
    if result:
        print("{0}WARNING: Using a password on the command line interface"
              " can be insecure.".format(prefix))

    return result
#
# Copyright (c) 2014, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains the MySQLOptionsParser used to read the MySQL
configuration files.

This module belongs to Connector python, and it should be removed once
C/py v2.0.0 is released and in the meanwhile will be used from here.

"""

import codecs
import io
import os
import re
from ConfigParser import SafeConfigParser, MissingSectionHeaderError

DEFAULT_OPTION_FILES = {
    'nt': 'C:\\my.ini',
    'posix': '/etc/mysql/my.cnf'
}

DEFAULT_EXTENSIONS = {
    'nt': ('ini', 'cnf'),
    'posix': 'cnf'
}


class MySQLOptionsParser(SafeConfigParser):
    """This class implements methods to parse MySQL option files"""

    def __init__(self, files=None, keep_dashes=True):
        """Initialize

        files[in]       The files to parse searching for configuration items.
        keep_dashes[in] If False, dashes in options are replaced with
                        underscores.

        Raises ValueError if defaults is set to True but defaults files
        cannot be found.
        """

        # Regular expression to allow options with no value(For Python v2.6)
        self.OPTCRE = re.compile(           # pylint: disable=C0103
            r'(?P<option>[^:=\s][^:=]*)'
            r'\s*(?:'
            r'(?P<vi>[:=])\s*'
            r'(?P<value>.*))?$'
        )

        self._options_dict = {}

        SafeConfigParser.__init__(self)
        self.default_extension = DEFAULT_EXTENSIONS[os.name]
        self.keep_dashes = keep_dashes

        if not files:
            raise ValueError('files argument should be given')
        if isinstance(files, str):
            self.files = [files]
        else:
            self.files = files

        self._parse_options(list(self.files))
        self._sections = self.get_groups_as_dict()

    def optionxform(self, optionstr):
        """Converts option strings

        optionstr[in] input to be converted.

        Converts option strings to lower case and replaces dashes(-) with
        underscores(_) if keep_dashes variable is set.

        """
        if not self.keep_dashes:
            optionstr = optionstr.replace('-', '_')
        return optionstr.lower()

    def _parse_options(self, files):
        """Parse options from files given as arguments.
         This method checks for !include or !includedir directives and if there
         is any, those files included by these directives are also parsed
         for options.

         files[in]       The files to parse searching for configuration items.

        Raises ValueError if any of the included or file given in arguments
        is not readable.
        """
        index = 0
        err_msg = "Option file '{0}' being included again in file '{1}'"

        for file_ in files:
            try:
                with open(file_, 'r') as op_file:
                    for line in op_file.readlines():
                        if line.startswith('!includedir'):
                            _, dir_path = line.split(None, 1)
                            for entry in os.listdir(dir_path):
                                entry = os.path.join(dir_path, entry)
                                if entry in files:
                                    raise ValueError(err_msg.format(
                                        entry, file_))
                                if (os.path.isfile(entry) and
                                        entry.endswith(
                                            self.default_extension)):
                                    files.insert(index + 1, entry)

                        elif line.startswith('!include'):
                            _, filename = line.split(None, 1)
                            if filename in files:
                                raise ValueError(err_msg.format(
                                    filename, file_))
                            files.insert(index + 1, filename)

                        index += 1

            except (IOError, OSError) as exc:
                raise ValueError("Failed reading file '{0}': {1}".format(
                    file_, str(exc)))

        read_files = self.read(files)
        not_read_files = set(files) - set(read_files)
        if not_read_files:
            raise ValueError("File(s) {0} could not be read.".format(
                ', '.join(not_read_files)))

    def read(self, filenames):
        """Read and parse a filename or a list of filenames.

        Overridden from ConfigParser and modified so as to allow options
        which are not inside any section header

        filenames[in]    The file names to read.

        Return list of successfully read files.
        """
        # Get python version since we must use str() to read strings from
        # the file for older, 2.6 versions of Python
        py26 = check_python_version((2, 6, 0), (2, 6, 99), False,
                                    None, False, False, False)
        if isinstance(filenames, str):
            filenames = [filenames]
        read_ok = []
        for priority, filename in enumerate(filenames):
            try:
                out_file = io.StringIO()
                for line in codecs.open(filename, encoding='utf-8'):
                    line = line.strip()
                    match_obj = self.OPTCRE.match(line)
                    if not self.SECTCRE.match(line) and match_obj:
                        optname, delimiter, optval = match_obj.group('option',
                                                                     'vi',
                                                                     'value')
                        if optname and not optval and not delimiter:
                            out_file.write(line + "=\n")
                        else:
                            out_file.write(line + '\n')
                    else:
                        out_file.write(line + '\n')
                out_file.seek(0)
                self._read(out_file, filename)
            except IOError:
                continue
            try:
                self._read(out_file, filename)
                for group in self._sections.keys():
                    try:
                        self._options_dict[group]
                    except KeyError:
                        self._options_dict[group] = {}
                    for option, value in self._sections[group].items():
                        if py26:
                            self._options_dict[group][option] = (str(value),
                                                                 priority)
                        else:
                            self._options_dict[group][option] = (value,
                                                                 priority)

                self._sections = self._dict()

            except MissingSectionHeaderError:
                self._read(out_file, filename)
            out_file.close()
            read_ok.append(filename)
        return read_ok

    def get_groups(self, *args):
        """Returns options as a dictionary.

        Returns options from all the groups specified as arguments, returns
        the options from all groups if no argument provided. Options are
        overridden when they are found in the next group.

        *args[in]    Each group to be returned can be requested by providing
                     its name as an argument.

        Returns a dictionary
        """
        if len(args) == 0:
            args = self._options_dict.keys()

        options = {}
        for group in args:
            try:
                for option, value in self._options_dict[group].items():
                    if option not in options or options[option][1] <= value[1]:
                        options[option] = value
            except KeyError:
                pass

        for key in options.keys():
            if key == '__name__' or key.startswith('!'):
                del options[key]
            else:
                options[key] = options[key][0]
        return options

    def get_groups_as_dict_with_priority(self, *args):  # pylint: disable=C0103
        """Returns options as dictionary of dictionaries.

        Returns options from all the groups specified as arguments. For each
        group the option are contained in a dictionary. The order in which
        the groups are specified is unimportant. Also options are not
        overridden in between the groups.

        The value is a tuple with two elements, first being the actual value
        and second is the priority of the value which is higher for a value
        read from a higher priority file.

        *args[in]    Each group to be returned can be requested by providing
                     its name as an argument.

        Returns an dictionary of dictionaries
        """
        if len(args) == 0:
            args = self._options_dict.keys()

        options = dict()
        for group in args:
            try:
                options[group] = dict(self._options_dict[group])
            except KeyError:
                pass

        for group in options.keys():
            for key in options[group].keys():
                if key == '__name__' or key.startswith('!'):
                    del options[group][key]
        return options

    def get_groups_as_dict(self, *args):
        """Returns options as dictionary of dictionaries.

        Returns options from all the groups specified as arguments. For each
        group the option are contained in a dictionary. The order in which
        the groups are specified is unimportant. Also options are not
        overridden in between the groups.

        *args[in]    Each group to be returned can be requested by providing
                     its name as an argument.

        Returns an dictionary of dictionaries
        """
        if len(args) == 0:
            args = self._options_dict.keys()

        options = dict()
        for group in args:
            try:
                options[group] = dict(self._options_dict[group])
            except KeyError:
                pass

        for group in options.keys():
            for key in options[group].keys():
                if key == '__name__' or key.startswith('!'):
                    del options[group][key]
                else:
                    options[group][key] = options[group][key][0]
        return options
#
# Copyright (c) 2011, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#
"""Module with parsers for General and Slow Query Log.
"""

import re
import decimal
import datetime



_DATE_PAT = r"\d{6}\s+\d{1,2}:\d{2}:\d{2}"

_HEADER_VERSION_CRE = re.compile(
    r"(.+), Version: (\d+)\.(\d+)\.(\d+)(?:-(\S+))?")
_HEADER_SERVER_CRE = re.compile(r"Tcp port:\s*(\d+)\s+Unix socket:\s+(.*)")

_SLOW_TIMESTAMP_CRE = re.compile(r"#\s+Time:\s+(" + _DATE_PAT + r")")
_SLOW_USERHOST_CRE = re.compile(r"#\s+User@Host:\s+"
                                r"(?:([\w\d]+))?\s*"
                                r"\[\s*([\w\d]+)\s*\]\s*"
                                r"@\s*"
                                r"([\w\d\.\-]*)\s*"
                                r"\[\s*([\d.]*)\s*\]\s*"
                                r"(?:Id\:\s*(\d+)?\s*)?")
_SLOW_STATS_CRE = re.compile(r"#\sQuery_time:\s(\d*\.\d{1,6})\s*"
                             r"Lock_time:\s(\d*\.\d{1,6})\s*"
                             r"Rows_sent:\s(\d*)\s*"
                             r"Rows_examined:\s(\d*)")

_GENERAL_ENTRY_CRE = re.compile(
    r'(?:(' + _DATE_PAT + r'))?\s*'
    r'(\d+)\s([\w ]+)\t*(?:(.+))?$')


class LogParserBase(object):
    """Base class for parsing MySQL log files

    LogParserBase should be inherited to create parsers for MySQL log files.
    This class has the following capabilities:

    - Take a stream and check whether it is a file type
    - Retrieve next line from stream
    - Parse header information from a log file (for General or Slow Query Log)
    - Implements the iterator protocol

    This class should not be used directly, but inhereted and extended to
    match the log file which needs to be parsed.
    """
    def __init__(self, stream):
        """Constructor

        stream[in]          A file type

        The stream argument must be a valid file type supporting for
        example the readline()-method. For example, the return of the buildin
        function open() can be used:
            LogParserBase(open("/path/to/mysql.log"))

        Raises LogParserError on errors.
        """
        self._stream = None
        self._version = None
        self._program = None
        self._port = None
        self._socket = None
        self._start_datetime = None
        self._last_seen_datetime = None

        # Check if we got a file type
        line = None
        try:
            self._stream = stream
            line = self._get_next_line()
        except AttributeError:
            raise LogParserError("Need a file type")

        # Not every log file starts with a header
        if line is not None and line.endswith('started with:'):
            self._parse_header(line)
        else:
            self._stream.seek(0)

    def _get_next_line(self):
        """Get next line from the log file

        This method reads the next line from the stream. Trailing
        newline (\n) and carraige return (\r) are removed.

        Returns next line as string or None
        """
        line = self._stream.readline()
        if not line:
            return None
        return line.rstrip('\r\n')

    def _parse_header(self, line):
        """Parse the header of a MySQL log file

        line[in]        A string, usually result of self._get_next_line()

        This method parses the header of a MySQL log file, that is the header
        found in the General and Slow Query log files. It sets attributes
        _version, _program, _port and _socket.
        Note that headers can repeat in a log file, for example, after a
        restart of the MySQL server.

        Example header:
        /usr/sbin/mysqld, Version: 5.5.17-log (Source distribution). started
        with:
        Tcp port: 0  Unix socket: /tmp/mysql.sock
        Time                 Id Command    Argument

        Raises LogParserError on errors.
        """
        if line is None:
            return
        # Header line containing executable and version, example:
        # /raid0/mysql/mysql/bin/mysqld,
        # Version: 5.5.17-log (Source distribution). started with:
        info = _HEADER_VERSION_CRE.match(line)
        if not info:
            raise LogParserError("Could not read executable and version from "
                                 "header")
        program, major, minor, patch, extra = info.groups()

        # Header line with server information, example:
        # Tcp port: 3306  Unix socket: /tmp/mysql.sock
        line = self._get_next_line()
        info = _HEADER_SERVER_CRE.match(line)
        if not info:
            raise LogParserError("Malformed server header line: %s" % line)
        tcp_port, unix_socket = info.groups()

        # Throw away column header line, example:
        # Time                 Id Command    Argument
        self._get_next_line()

        self._version = (int(major), int(minor), int(patch), extra)
        self._program = program
        self._port = int(tcp_port)
        self._socket = unix_socket

    @property
    def version(self):
        """Returns the MySQL server version

        This property returns a tuple descriving the version of the
        MySQL server producing the log file. The tuple looks like this:
            (major, minor, patch, extra)

        The extra part is optional and when not available will be None.
        Examples:
            (5,5,17,'log')
            (5,1,57,None)

        Note that the version can change in the same log file.

        Returns a tuple or None.
        """
        return self._version

    @property
    def program(self):
        """Returns the executable which wrote the log file

        This property returns the full path to the executable which
        produced the log file.

        Note that the executable can change in the same log file.

        Returns a string or None.
        """
        return self._program

    @property
    def port(self):
        """Returns the MySQL server TCP/IP port

        This property returns the TCP/IP port on which the MySQL server
        was listening.

        Note that the TCP/IP port can change in the same log file.

        Returns an integer or None.
        """
        return self._port

    @property
    def socket(self):
        """Returns the MySQL server UNIX socket

        This property returns full path to UNIX socket used the MySQL server
        to accept incoming connections on UNIX-like servers.

        Note that the UNIX socket location can change in the same log file.

        Returns a string or None.
        """
        return self._socket

    @property
    def start_datetime(self):
        """Returns timestamp of first read log entry

        This property returns the timestamp of the first read log entry.

        Returns datetime.datetime-object or None.
        """
        return self._start_datetime

    @property
    def last_seen_datetime(self):
        """Returns timestamp of last read log entry

        This property returns the timestamp of the last read log entry.

        Returns datetime.datetime-object or None
        """
        return self._last_seen_datetime

    def __iter__(self):
        """Class is iterable

        Returns a LogParserBase-object.
        """
        return self

    def next(self):
        """Returns the next log entry

        Raises StopIteration when no more entries are available.

        Returns a LogEntryBase-object.
        """
        entry = self._parse_entry()
        if entry is None:
            raise StopIteration
        return entry

    def _parse_entry(self):
        """Returns a parsed log entry
        """
        pass

    def __str__(self):
        """String representation of LogParserBase
        """
        return "<%(clsname)s, MySQL v%(version)s>" % dict(
            clsname=self.__class__.__name__,
            version='.'.join([str(v) for v in self._version[0:3]]) +
            (self._version[3] or '')
        )


class GeneralQueryLog(LogParserBase):
    """Class implementing a parser for the MySQL General Query Log

    The GeneralQueryLog-class implements a parse for the MySQL General Query
    Log and has the following capabilities:
    - Parse General Query Log entries
    - Possibility to handle special commands
    - Keep track of MySQL sessions and remove them
    - Process log headers found later in the log file
    """
    def __init__(self, stream):
        """Constructor

        stream[in]      file type

        Raises LogParserError on errors.
        """
        super(GeneralQueryLog, self).__init__(stream)
        self._sessions = {}
        self._cached_logentry = None

        self._commands = {
            # 'Sleep': None,
            'Quit': self._handle_quit,
            'Init DB': self._handle_init_db,
            'Query': self._handle_multi_line,
            # 'Field List': None,
            # 'Create DB': None,
            # 'Drop DB': None,
            # 'Refresh': None,
            # 'Shutdown': None,
            # 'Statistics': None,
            # 'Processlist': None,
            'Connect': self._handle_connect,
            # 'Kill': None,
            # 'Debug': None,
            # 'Ping': None,
            # 'Time': None,
            # 'Delayed insert': None,
            # 'Change user': None,
            # 'Binlog Dump': None,
            # 'Table Dump': None,
            # 'Connect Out': None,
            # 'Register Slave': None,
            'Prepare': self._handle_multi_line,
            'Execute': self._handle_multi_line,
            # 'Long Data': None,
            # 'Close stmt': None,
            # 'Reset stmt': None,
            # 'Set option': None,
            'Fetch': self._handle_multi_line,
            # 'Daemon': None,
            # 'Error': None,
        }

    def _new_session(self, session_id):
        """Create a new session using the given session ID

        session_id[in]      integer presenting a MySQL session

        Returns a dictionary.
        """
        self._sessions[session_id] = dict(
            database=None,
            user=None,
            host=None,
            time_last_action=None,
            to_delete=False
        )
        return self._sessions[session_id]

    @staticmethod
    def _handle_connect(entry, session, argument):
        """Handle a 'Connect'-command

        entry[in]       a GeneralQueryLogEntry-instance
        session[in]     a dictionary with current session information,
                        element of self._sessions
        argument[in]    a string, last part of a log entry

        This method reads user and database information from the argument of
        a 'Connect'-command. It sets the user, host and database for the
        current session and also sets the argument for the entry.

        """
        # Argument can be as follows:
        # root@localhost on test
        # root@localhost on
        try:
            connection, _, database = argument.split(' ')
        except ValueError:
            connection = argument.replace(' on', '')
            database = None
        session['user'], session['host'] = connection.split('@')
        session['database'] = database
        entry['argument'] = argument

    @staticmethod
    def _handle_init_db(entry, session, argument):
        """Handle an 'Init DB'-command

        entry[in]       a GeneralQueryLogEntry-instance
        session[in]     a dictionary with current session information,
                        element of self._sessions
        argument[in]    a string, last part of a log entry

        The argument parameter is always the database name.
        """
        # Example (of full line):
        #           3 Init DB   mysql
        session['database'] = argument
        entry['argument'] = argument

    def _handle_multi_line(self, entry, session, argument):
        """Handle a command which can span multiple lines

        entry[in]       a GeneralQueryLogEntry-instance
        session[in]     a dictionary with current session information,
                        element of self._sessions
        argument[in]    a string, last part of a log entry

        The argument parameter passed to this function is the last part of a
        General Query Log entry and usually is already the full query.

        This function's main purpose is to read log entries which span multiple
        lines, such as the Query and Prepare-commands.
        """
        # Examples:
        # 111205 10:01:14       6 Query SELECT Name FROM time_zone_name
        #                       WHERE Time_zone_id = 417
        # 111205 10:03:28       6 Query SELECT Name FROM time_zone_name
        # WHERE Time_zone_id = 417
        argument_parts = [argument, ]
        line = self._get_next_line()
        # Next line is None if the end of the file is reached.
        # Note: empty lines can appear and should be read (i.e., line == '').
        while line is not None:
            # Stop if it is a header.
            if line.endswith('started with:'):
                self._cached_logentry = line
                break
            # Stop if a new log entry is found.
            info = _GENERAL_ENTRY_CRE.match(line)
            if info is not None:
                self._cached_logentry = info.groups()
                break
            # Otherwise, append line and read next.
            argument_parts.append(line)
            line = self._get_next_line()

        entry['argument'] = '\n'.join(argument_parts)

    @staticmethod
    def _handle_quit(entry, session, argument):
        """Handle the 'Quit'-command

        entry[in]       a GeneralQueryLogEntry-instance
        session[in]     a dictionary with current session information,
                        element of self._sessions
        argument[in]    a string, last part of a log entry

        This function sets a flag that the session can be removed from the
        session list.
        """
        # Example (of full line):
        # 111205 10:06:53       6 Quit
        session['to_delete'] = True

    def _parse_command(self, logentry, entry):
        """Parse a log entry from the General Query Log

        logentry[in]    a string or tuple
        entry[in]       an instance of GeneralQueryLogEntry

        The logentry-parameter is either a line read from the log file or
        the result of a previous attempt to read a command.
        The entry argument should be an instance of GeneralQueryLogEntry.
        It returns the entry or None if nothing could be read.

        Raises LogParserError on errors.

        Returns the GeneralQueryLogEntry-instance or None
        """
        if logentry is None:
            return None
        if isinstance(logentry, tuple):
            dt, session_id, command, argument = logentry
        elif logentry.endswith('started with:'):
            while logentry.endswith('started with:'):
                # We got a header
                self._parse_header(logentry)
                logentry = self._get_next_line()
                if logentry is None:
                    return None
            return self._parse_command(logentry, entry)
        else:
            info = _GENERAL_ENTRY_CRE.match(logentry)
            if info is None:
                raise LogParserError("Failed parsing command line: %s"
                                     % logentry)
            dt, session_id, command, argument = info.groups()
        self._cached_logentry = None

        session_id = int(session_id)
        entry['session_id'] = session_id
        try:
            session = self._sessions[session_id]
        except KeyError:
            session = self._new_session(session_id)

        entry['command'] = command
        if dt is not None:
            entry['datetime'] = datetime.datetime.strptime(dt,
                                                           "%y%m%d %H:%M:%S")
            session['time_last_action'] = entry['datetime']
        else:
            entry['datetime'] = session['time_last_action']

        try:
            self._commands[command](entry, session, argument)
        except KeyError:
            # Generic command
            entry['argument'] = argument

        for key in entry.keys():
            if key in session:
                entry[key] = session[key]

        if session['to_delete'] is True:
            del self._sessions[session_id]
            del session

        return entry

    def _parse_entry(self):
        """Returns a parsed log entry

        The method _parse_entry() uses _parse_command() to parse
        a General Query Log entry. It is used by the iterator protocol methods.

        Returns a GeneralQueryLogEntry-instance or None.
        """
        entry = GeneralQueryLogEntry()
        if self._cached_logentry is not None:
            self._parse_command(self._cached_logentry, entry)
            return entry
        else:
            line = self._get_next_line()
        if line is None:
            return None

        self._parse_command(line, entry)
        return entry


class SlowQueryLog(LogParserBase):
    """Class implementing a parser for the MySQL Slow Query Log

    The SlowQueryLog-class implements a parser for the MySQL Slow Query Log and
    has the following capabilities:
    - Parse Slow Query Log entries
    - Process log headers found later in the log file
    - Parse connection and temporal information
    - Get statistics of the slow query
    """
    def __init__(self, stream):
        """Constructor

        stream[in]      A file type

        The stream argument must be a valid file type supporting for
        example the readline()-method. For example, the return of the build-in
        function open() can be used:
            SlowQueryLog(open("/path/to/mysql-slow.log"))

        Raises LogParserError on errors.
        """
        super(SlowQueryLog, self).__init__(stream)
        self._cached_line = None
        self._current_database = None

    @staticmethod
    def _parse_line(regex, line):
        """Parses a log line using given regular expression

        regex[in]   a SRE_Match-object
        line[in]    a string

        This function takes a log line and matches the regular expresion given
        with the regex argument. It returns the result of
        re.MatchObject.groups(), which is a tuple.

        Raises LogParserError on errors.

        Returns a tuple.
        """
        info = regex.match(line)
        if info is None:
            raise LogParserError('Failed parsing Slow Query line: %s' %
                                 line[:30])
        return info.groups()

    def _parse_connection_info(self, line, entry):
        """Parses connection info

        line[in]    a string
        entry[in]   a SlowQueryLog-instance

        The line paramater should be a string, a line read from the Slow Query
        Log. The entry argument should be an instance of SlowQueryLogEntry.

        Raises LogParserError on failure.
        """
        # Example:
        # # User@Host: root[root] @ localhost [127.0.0.1]
        (priv_user,
         unpriv_user,
         host,
         ip,
         sid) = self._parse_line(_SLOW_USERHOST_CRE, line)

        entry['user'] = priv_user if priv_user else unpriv_user
        entry['host'] = host if host else ip
        entry['session_id'] = sid

    def _parse_timestamp(self, line, entry):
        """Parses a timestamp

        line[in]    a string
        entry[in]   a SlowQueryLog-instance

        The line paramater should be a string, a line read from the Slow Query
        Log. The entry argument should be an instance of SlowQueryLogEntry.

        Raises LogParserError on failure.
        """
        # Example:
        # # Time: 111206 11:55:54
        info = self._parse_line(_SLOW_TIMESTAMP_CRE, line)

        entry['datetime'] = datetime.datetime.strptime(info[0],
                                                       "%y%m%d %H:%M:%S")
        if self._start_datetime is None:
            self._start_datetime = entry['datetime']
            self._last_seen_datetime = entry['datetime']

    def _parse_statistics(self, line, entry):
        """Parses statistics information

        line[in]    a string
        entry[in]   a SlowQueryLog-instance

        The line paramater should be a string, a line read from the Slow Query
        Log. The entry argument should be an instance of SlowQueryLogEntry.

        Raises LogParserError on errors.
        """
        # Example statistic line:
        # Query_time: 0.101194  Lock_time: 0.000331 Rows_sent: 24
        # Rows_examined: 11624
        result = self._parse_line(_SLOW_STATS_CRE, line)

        entry['query_time'] = decimal.Decimal(result[0])
        entry['lock_time'] = decimal.Decimal(result[1])
        entry['rows_sent'] = int(result[2])
        entry['rows_examined'] = int(result[3])

    def _parse_query(self, line, entry):
        """Parses the query

        line[in]    a string
        entry[in]   a SlowQueryLog-instance

        The line paramater should be a string, a line read from the Slow Query
        Log. The entry argument should be an instance of SlowQueryLogEntry.

        Query entries in the Slow Query Log could span several lines. They can
        optionally start with a USE-command and have session variables, such as
        'timestamp', set before the actual query.
        """
        # Example:
        # SET timestamp=1323169459;
        # SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA
        #    WHERE SCHEMA_NAME = 'mysql';
        # # User@Host: root[root] @ localhost [127.0.0.1]
        query = []
        while True:
            if line is None:
                break
            if line.startswith('use'):
                entry['database'] = self._current_database = line.split(' ')[1]
            elif line.startswith('SET timestamp='):
                entry['datetime'] = datetime.datetime.fromtimestamp(
                    int(line[14:].strip(';')))
            elif (line.startswith('# Time:') or
                  line.startswith("# User@Host") or
                  line.endswith('started with:')):
                break
            query.append(line)
            line = self._get_next_line()

        if 'database' in entry:
            # This is not always correct: connections without current database
            # will get the database name of the previous query. However, it's
            # more likely current database is set. Fix would be that the server
            # includes a USE-statement for every entry.
            if (entry['database'] is None and
                    self._current_database is not None):
                entry['database'] = self._current_database
        entry['query'] = '\n'.join(query)
        self._cached_line = line

    def _parse_entry(self):
        """Parse and returns an entry of the Slow Query Log

        Each entry of the slow log consists of:
        1. An optional time line
        2. A connection information line with user, hostname and database
        3. A line containing statistics for the query
        4. An optional "use <database>" line
        5. A line setting the timestamp, insert_id, and last_insert_id
           session variables
        6. An optional administartor command line "# administator command"
        7. An optional SQL statement or the query

        Returns a SlowQueryLogEntry-instance or None
        """
        if self._cached_line is not None:
            line = self._cached_line
            self._cached_line = None
        else:
            line = self._get_next_line()
        if line is None:
            return None

        while line.endswith('started with:'):
            # We got a header
            self._parse_header(line)
            line = self._get_next_line()
            if line is None:
                return None

        entry = SlowQueryLogEntry()

        if line.startswith('# Time:'):
            self._parse_timestamp(line, entry)
            line = self._get_next_line()

        if line.startswith('# User@Host:'):
            self._parse_connection_info(line, entry)
            line = self._get_next_line()

        if line.startswith('# Query_time:'):
            self._parse_statistics(line, entry)
            line = self._get_next_line()

        self._parse_query(line, entry)

        return entry


class LogEntryBase(dict):
    """Class inherited by GeneralQueryEntryLog and SlowQueryEntryLog

    This class has the following capabilities:
    - Inherits from dict
    - Dictionary elements can be accessed using attributes. For example,
      logentry['database'] is accessible like logentry.database

    Should not be used directly.
    """
    def __init__(self):
        super(LogEntryBase, self).__init__()
        self['datetime'] = None
        self['database'] = None
        self['user'] = None
        self['host'] = None
        self['session_id'] = None

    def __getattr__(self, name):
        if name in self:
            return self[name]
        else:
            raise AttributeError("%s has no attribute '%s'" %
                                 (self.__class__.__name__, name))


class GeneralQueryLogEntry(LogEntryBase):
    """Class representing an entry of the General Query Log

    """
    def __init__(self):
        """Constructor

        GeneralQueryLogEntry inherits from LogEntryBase, which inherits from
        dict. Instances of GeneralQueryLogEntry can be used just like
        dictionaries.
        """
        super(GeneralQueryLogEntry, self).__init__()
        self['session_id'] = None
        self['command'] = None
        self['argument'] = None

    def __str__(self):
        """String representation of GeneralQueryLogEntry
        """
        param = self.copy()
        param['clsname'] = self.__class__.__name__
        try:
            if len(param['argument']) > 30:
                param['argument'] = param['argument'][:28] + '..'
        except TypeError:
            pass  # Nevermind when param['argument'] was not a string.
        try:
            param['datetime'] = param['datetime'].strftime("%Y-%m-%d %H:%M:%S")
        except AttributeError:
            param['datetime'] = ''
        return ("<%(clsname)s %(datetime)s [%(session_id)s]"
                " %(command)s: %(argument)s>" % param)


class SlowQueryLogEntry(LogEntryBase):
    """Class representing an entry of the Slow Query Log

    SlowQueryLogEntry inherits from LogEntryBase, which inherits from dict.
    Instances of SlowQueryLogEntry can be used just like dictionaries.
    """
    def __init__(self):
        """Constructor
        """
        super(SlowQueryLogEntry, self).__init__()
        self['query'] = None
        self['query_time'] = None
        self['lock_time'] = None
        self['rows_examined'] = None
        self['rows_sent'] = None

    def __str__(self):
        """String representation of SlowQueryLogEntry
        """
        param = self.copy()
        param['clsname'] = self.__class__.__name__
        try:
            param['datetime'] = param['datetime'].strftime("%Y-%m-%d %H:%M:%S")
        except AttributeError:
            param['datetime'] = ''
        return (
            "<%(clsname)s %(datetime)s [%(user)s@%(host)s] "
            "%(query_time)s/%(lock_time)s/%(rows_examined)s/%(rows_sent)s>"
        ) % param
#
# Copyright (c) 2012, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains auxiliary functions to handle pattern matching.
"""

import re


# Regular expression to match a database object identifier (support backticks)
REGEXP_OBJ_NAME = r'(`(?:[^`]|``)+`|\w+|\w+[\%\*]?|[\%\*])'

# Regular expression to match a database object identifier with ansi quotes
REGEXP_OBJ_NAME_AQ = r'("(?:[^"]|"")+"|\w+|\*)'

# Regular expression to match a qualified object identifier (with multiple
# parts). Example: db.obj, db or obj
REGEXP_QUALIFIED_OBJ_NAME = r'{0}(?:(?:\.){0})?'.format(REGEXP_OBJ_NAME)

# Same as the above but for use with ansi quotes
REGEXP_QUALIFIED_OBJ_NAME_AQ = r'{0}(?:(?:\.){0})?'.format(REGEXP_OBJ_NAME_AQ)


def convertSQL_LIKE2REGEXP(sql_like_pattern):
    """Convert a standard SQL LIKE pattern to a REGEXP pattern.

    Function that transforms a SQL LIKE pattern to a supported python
    regexp. Returns a python regular expression (i.e. regexp).

    sql_like_pattern[in] pattern in the SQL LIKE form to be converted.
    """
    # Replace '_' by equivalent regexp, except when precede by '\'
    # (escape character)
    regexp = re.sub(r'(?<!\\)_', '.', sql_like_pattern)
    # Replace '%' by equivalent regexp, except when precede by '\'
    # (escape character)
    regexp = re.sub(r'(?<!\\)%', '.*', regexp)
    # Set regexp to ignore cases; SQL patterns are case-insensitive by default.
    regexp = "(?i)^(" + regexp + ")$"
    return regexp


def parse_object_name(qualified_name, sql_mode='', wild=False):
    """Parses a qualified object name from the given string.

    qualified_name[in] MySQL object string (e.g. db.table)
    sql_mode[in]       The value of sql_mode from the server.
    wild[in]           Look for wildcards (stating at end of str)

    Returns tuple containing name split
    """
    if "ANSI_QUOTES" in sql_mode:
        regex_pattern = REGEXP_QUALIFIED_OBJ_NAME.replace("`", '"')
    else:
        regex_pattern = REGEXP_QUALIFIED_OBJ_NAME
    if wild:
        regex_pattern = regex_pattern + r'\Z'
    # Split the qualified name considering backtick quotes
    parts = re.match(regex_pattern, qualified_name)
    if parts:
        return parts.groups()
    else:
        return (None, None)
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains abstractions of MySQL replication functionality.
"""

import os
import time
import StringIO
import socket



_MASTER_INFO_COL = [
    'Master_Log_File', 'Read_Master_Log_Pos', 'Master_Host', 'Master_User',
    'Master_Password', 'Master_Port', 'Connect_Retry', 'Master_SSL_Allowed',
    'Master_SSL_CA_File', 'Master_SSL_CA_Path', 'Master_SSL_Cert',
    'Master_SSL_Cipher', 'Master_SSL_Key', 'Master_SSL_Verify_Server_Cert',
    'Heartbeat', 'Bind', 'Ignored_server_ids', 'Uuid', 'Retry_count',
    'SSL_CRL', 'SSL_CRL_Path', 'Enabled_auto_position', 'Channel_Name',
]

_SLAVE_IO_STATE, _SLAVE_MASTER_HOST, _SLAVE_MASTER_USER, _SLAVE_MASTER_PORT, \
    _SLAVE_MASTER_LOG_FILE, _SLAVE_MASTER_LOG_FILE_POS, _SLAVE_IO_RUNNING, \
    _SLAVE_SQL_RUNNING, _SLAVE_DO_DB, _SLAVE_IGNORE_DB, _SLAVE_DO_TABLE, \
    _SLAVE_IGNORE_TABLE, _SLAVE_WILD_DO_TABLE, _SLAVE_WILD_IGNORE_TABLE, \
    _SLAVE_DELAY, _SLAVE_REMAINING_DELAY, _SLAVE_IO_ERRORNO, _SLAVE_IO_ERROR, \
    _SLAVE_SQL_ERRORNO, _SLAVE_SQL_ERROR, _MASTER_UUID, _RETRIEVED_GTID_SET, \
    _EXECUTED_GTID_SET = \
    0, 1, 2, 3, 5, 6, 10, 11, 12, 13, 14, 15, 16, 17, 32, 33, 34, 35, 36, 37,\
    40, 51, 52

_PRINT_WIDTH = 75

_MASTER_DO_DB, _MASTER_IGNORE_DB = 2, 3

_RPL_USER_QUERY = """
    SELECT user, host, password = '' as has_password
    FROM mysql.user
    WHERE repl_slave_priv = 'Y'
"""
# Query for server versions >= 5.7.6.
_RPL_USER_QUERY_5_7_6 = """
    SELECT user, host, authentication_string = '' as has_password
    FROM mysql.user
    WHERE repl_slave_priv = 'Y'
"""

_WARNING = "# WARNING: %s"
_MASTER_BINLOG = "Server '%s' does not have binary logging turned on."
_NO_RPL_USER = "No --rpl-user specified and multiple users found with " + \
               "replication privileges."
_RPL_USER_PASS = "No --rpl-user specified and the user found with " + \
                 "replication privileges requires a password."

_GTID_EXECUTED = "SELECT @@GLOBAL.GTID_EXECUTED"
_GTID_WAIT = "SELECT WAIT_UNTIL_SQL_THREAD_AFTER_GTIDS('%s', %s)"


def _get_list(rows, cols):
    """Return a list of information in GRID format to stdout.

    rows[in]          rows of data
    cols[in]          column headings

    Returns list of strings
    """
    ostream = StringIO.StringIO()
    format_tabular_list(ostream, cols, rows)
    return ostream.getvalue().splitlines()


def negotiate_rpl_connection(server, is_master=True, strict=True,
                             options=None):
    """Determine replication connection

    This method attempts to determine if it is possible to build a CHANGE
    MASTER command based on the server passed. If it is possible, the method
    will return a CHANGE MASTER command. If there are errors and the strict
    option is turned on, it will throw errors if there is something missing.
    Otherwise, it will return the CHANGE MASTER command with warnings.

    If the server is a master, the following error checks will be performed.

      - if binary log is turned OFF, and strict = False, a warning message
        is added to the strings returned else an error is thrown

      - if the rpl_user option is missing, the method attempts to find a
        replication user. If more than one user is found or none are found, and
        strict = False, a warning message is added to the strings returned else
        an error is thrown

      - if a replication user is found but the user requires a password,
        the MASTER_USER and MASTER_PASSWORD options are commented out

    Note: the CHANGE MASTER command is formatted whereby each option is
          separated by a newline and indented two spaces

    Note: the make_change_master method does not support SSL connections

    server[in]        a Server class instance
    is_master[in]     if True, the server is acting as a master
                      Default = True
    strict[in]        if True, raise exception on errors
                      Default = True
    options[in]       replication options including rpl_user, quiet, multiline

    Returns list - strings containing the CHANGE MASTER command
    """
    if options is None:
        options = {}

    rpl_mode = options.get("rpl_mode", "master")
    rpl_user = options.get("rpl_user", None)
    quiet = options.get("quiet", False)

    # Copy options and add connected server
    new_opts = options.copy()
    new_opts["conn_info"] = server

    uname = None
    master_values = {}
    change_master = []

    # If server is a master, perform error checking
    # pylint: disable=R0101
    if is_master:
        master = Master(new_opts)
        master.connect()

        # Check master for binlog
        if not master.binlog_enabled():
            raise UtilError("Master must have binary logging turned on.")
        else:
            # Check rpl user
            if rpl_user is None:
                # Try to find the replication user
                res = master.get_rpl_users()
                if len(res) > 1:
                    uname = ""
                    passwd = ""
                    # Throw error if strict but not for rpl_mode = both
                    if strict and rpl_mode != 'both':
                        raise UtilRplError(_NO_RPL_USER)
                    else:
                        change_master.append(_WARNING % _NO_RPL_USER)
                else:
                    uname = res[0][0]
                    if res[0][2]:
                        # Throw error if strict but not for rpl_mode = both
                        if strict and rpl_mode != 'both':
                            raise UtilRplError(_RPL_USER_PASS)
                        else:
                            change_master.append(_WARNING % _RPL_USER_PASS)
                    passwd = res[0][1]
            else:
                # Parse username and password (supports login-paths)
                try:
                    uname, passwd = parse_user_password(rpl_user,
                                                        options=options)
                except FormatError:
                    raise UtilError(USER_PASSWORD_FORMAT.format("--rpl-user"))
                if not passwd:
                    passwd = ''

                # Check replication user privileges
                errors = master.check_rpl_user(uname, master.host)
                if errors != []:
                    raise UtilError(errors[0])

            res = master.get_status()
            if not res:
                raise UtilError("Cannot retrieve master status.")

            # Need to get the master values for the make_change_master command
            master_values = {
                'Master_Host': master.host,
                'Master_Port': master.port,
                'Master_User': uname,
                'Master_Password': passwd,
                'Master_Log_File': res[0][0],
                'Read_Master_Log_Pos': res[0][1],
            }

            if master.has_ssl:
                master_values['Master_SSL_Allowed'] = 1
                if master.ssl_ca:
                    master_values['Master_SSL_CA_File'] = master.ssl_ca
                if master.ssl_cert:
                    master_values['Master_SSL_Cert'] = master.ssl_cert
                if master.ssl_key:
                    master_values['Master_SSL_Key'] = master.ssl_key

    # Use slave class to get change master command
    slave = Slave(new_opts)
    slave.connect()
    cm_cmd = slave.make_change_master(False, master_values)

    if rpl_user is None and uname == "" and not quiet:
        cm_cmd = cm_cmd.replace("MASTER_PORT", "# MASTER_USER = '', "
                                "# MASTER_PASSWORD = '', MASTER_PORT")

    if options.get("multiline", False):
        cm_cmd = cm_cmd.replace(", ", ", \n  ") + ";"
        change_master.extend(cm_cmd.split("\n"))
    else:
        change_master.append(cm_cmd + ";")

    return change_master


class Replication(object):
    """
    The Replication class can be used to establish a replication connection
    between a master and a slave with the following utilities:

        - Create the replication user
        - Setup replication
        - Test prerequisites for replication
        - Conduct validation checks:
            - binlog
            - server ids
            - storage engine compatibility
            - innodb version compatibility
            - master binlog
            - lower case table name compatibility
            - slave connection to master
            - slave delay

    Replication prerequisite tests shall be constructed so that they return
    None if the check passes (no errors) or a list of strings containing the
    errors or warnings. They shall accept a dictionary of options set to
    options={}. This will allow for reduced code needed to call multiple tests.
    """

    def __init__(self, master, slave, options):
        """Constructor

        master[in]         Master Server object
        slave[in]          Slave Server object
        options[in]        Options for class
          verbose          print extra data during operations (optional)
                           default value = False
          master_log_file  master log file
                           default value = None
          master_log_pos   position in log file
                           default = -1 (no position specified)
          from_beginning   if True, start from beginning of logged events
                           default = False
        """
        self.verbosity = options.get("verbosity", 0)
        self.master_log_file = options.get("master_log_file", None)
        self.master_log_pos = options.get("master_log_pos", 0)
        self.from_beginning = options.get("from_beginning", False)
        self.ssl_ca = options.get("ssl_ca", None)
        self.ssl_cert = options.get("ssl_cert", None)
        self.ssl_key = options.get("ssl_key", None)
        self.ssl_opt = options.get("ssl", None)
        self.ssl = False
        if self.ssl_ca or self.ssl_cert or self.ssl_key or self.ssl_opt:
            self.ssl = True
        self.master = master
        self.slave = slave
        self.replicating = False
        self.query_options = {
            'fetch': False
        }

    def check_server_ids(self):
        """Check server ids on master and slave

        This method will check the server ids on the master and slave. It will
        raise exceptions for error conditions.

        Returns [] if compatible, list of errors if not compatible
        """
        master_server_id = self.master.get_server_id()
        slave_server_id = self.slave.get_server_id()
        if master_server_id == 0:
            raise UtilRplError("Master server_id is set to 0.")

        if slave_server_id == 0:
            raise UtilRplError("Slave server_id is set to 0.")

        # Check for server_id uniqueness
        if master_server_id == slave_server_id:
            raise UtilRplError("The slave's server_id is the same as the "
                               "master.")

        return []

    def check_server_uuids(self):
        """Check UUIDs on master and slave

        This method will check the UUIDs on the master and slave. It will
        raise exceptions for error conditions.

        Returns [] if compatible or no UUIDs used, list of errors if not
        """
        master_uuid = self.master.get_uuid()
        slave_uuid = self.slave.get_uuid()

        # Check for both not supporting UUIDs.
        if master_uuid is None and slave_uuid is None:
            return []

        # Check for unbalanced servers - one with UUID, one without
        if master_uuid is None or slave_uuid is None:
            raise UtilRplError("%s does not support UUIDs." %
                               "Master" if master_uuid is None else "Slave")

        # Check for uuid uniqueness
        if master_uuid == slave_uuid:
            raise UtilRplError("The slave's UUID is the same as the "
                               "master.")

        return []

    def check_innodb_compatibility(self, options):
        """Check InnoDB compatibility

        This method checks the master and slave to ensure they have compatible
        installations of InnoDB. It will print the InnoDB settings on the
        master and slave if quiet is not set. If pedantic is set, method
        will raise an error.

        options[in]   dictionary of options (verbose, pedantic)

        Returns [] if compatible, list of errors if not compatible
        """

        pedantic = options.get("pedantic", False)
        verbose = options.get("verbosity", 0) > 0

        errors = []

        master_innodb_stats = self.master.get_innodb_stats()
        slave_innodb_stats = self.slave.get_innodb_stats()

        if master_innodb_stats != slave_innodb_stats:
            if not pedantic:
                errors.append("WARNING: Innodb settings differ between master "
                              "and slave.")
            if verbose or pedantic:
                cols = ['type', 'plugin_version', 'plugin_type_version',
                        'have_innodb']
                rows = []
                rows.append(master_innodb_stats)
                errors.append("# Master's InnoDB Stats:")
                errors.extend(_get_list(rows, cols))
                rows = []
                rows.append(slave_innodb_stats)
                errors.append("# Slave's InnoDB Stats:")
                errors.extend(_get_list(rows, cols))
            if pedantic:
                for line in errors:
                    print line
                raise UtilRplError("Innodb settings differ between master "
                                   "and slave.")

        return errors

    def check_storage_engines(self, options):
        """Check compatibility of storage engines on master and slave

        This method checks that the master and slave have compatible storage
        engines. It will print the InnoDB settings on the master and slave if
        quiet is not set. If pedantic is set, method will raise an error.

        options[in]   dictionary of options (verbose, pedantic)

        Returns [] if compatible, list of errors if not compatible
        """

        pedantic = options.get("pedantic", False)
        verbose = options.get("verbosity", 0) > 0

        errors = []
        slave_engines = self.slave.get_storage_engines()
        results = self.master.check_storage_engines(slave_engines)
        if results[0] is not None or results[1] is not None:
            if not pedantic:
                errors.append("WARNING: The master and slave have differing "
                              "storage engine configurations!")
            if verbose or pedantic:
                cols = ['engine', 'support']
                if results[0] is not None:
                    errors.append("# Storage engine configuration on Master:")
                    errors.extend(_get_list(results[0], cols))
                if results[1] is not None:
                    errors.append("# Storage engine configuration on Slave:")
                    errors.extend(_get_list(results[1], cols))
            if pedantic:
                for line in errors:
                    print line
                raise UtilRplError("The master and slave have differing "
                                   "storage engine configurations!")

        return errors

    def check_master_binlog(self):
        """Check prerequisites for master for replication

        Returns [] if master ok, list of errors if binary logging turned off.
        """
        errors = []
        if not self.master.binlog_enabled():
            errors.append("Master must have binary logging turned on.")
        return errors

    def check_lctn(self):
        """Check lower_case_table_name setting

        Returns [] - no exceptions, list if exceptions found
        """
        errors = []
        slave_lctn = self.slave.get_lctn()
        master_lctn = self.master.get_lctn()
        if slave_lctn != master_lctn:
            return (master_lctn, slave_lctn)
        if slave_lctn == 1:
            msg = "WARNING: identifiers can have inconsistent case " + \
                  "when lower_case_table_names = 1 on the slave and " + \
                  "the master has a different value."
            errors.append(msg)

        return errors

    def get_binlog_exceptions(self):
        """Get any binary logging exceptions

        This method queries the master and slave status for the *-do-db and
        *-ignore-db settings. It returns the values of either of these for
        the master and slave.

        Returns [] - no exceptions, list if exceptions found
        """
        binlog_ex = []
        rows = []
        rows.extend(self.master.get_binlog_exceptions())
        rows.extend(self.slave.get_binlog_exceptions())
        if len(rows) > 0:
            cols = ['server', 'do_db', 'ignore_db']
            binlog_ex = _get_list(rows, cols)

        return binlog_ex

    def check_slave_connection(self):
        """Check to see if slave is connected to master

        This method will check the slave specified at instantiation to see if
        it is connected to the master specified. If the slave is connected
        to a different master, an error is returned. It will also raise an
        exception if the slave is stopped or if the server is not setup as a
        slave.

        Returns bool - True = slave connected to master
        """
        state = self.slave.get_io_running()
        if not state:
            raise UtilRplError("Slave is stopped.")
        if not self.slave.is_configured_for_master(self.master) or \
           state.upper() != "YES":
            return False
        return True

    def check_slave_delay(self):
        """Check to see if slave is behind master.

        This method checks slave_behind_master returning None if 0 or a
        message containing the value if non-zero. Also includes the slave's
        position as related to the master.

        Returns [] - no exceptions, list if exceptions found
        """
        m_log_file = None
        m_log_pos = 0
        errors = []
        res = self.master.get_status()
        if res != []:
            m_log_file = res[0][0]       # master's binlog file
            m_log_pos = res[0][1]        # master's binlog position
        else:
            raise UtilRplError("Cannot read master status.")
        delay_info = self.slave.get_delay()
        if delay_info is None:
            raise UtilRplError("The server specified as the slave is "
                               "not configured as a replication slave.")

        state, sec_behind, delay_remaining, \
            read_log_file, read_log_pos = delay_info

        if not state:
            raise UtilRplError("Slave is stopped.")
        if delay_remaining is None:  # if unknown, return the error
            errors.append("Cannot determine slave delay. Status: UNKNOWN.")
            return errors

        if sec_behind == 0:
            if m_log_file is not None and \
               (read_log_file != m_log_file or read_log_pos != m_log_pos):
                errors.append("Slave is behind master.")
                errors.append("Master binary log file = %s" % m_log_file)
                errors.append("Master binary log position = %s" % m_log_pos)
                errors.append("Slave is reading master binary log "
                              "file = %s" % read_log_file)
                errors.append("Slave is reading master binary log "
                              "position = %s" % read_log_pos)
            else:
                return errors
        else:
            errors.append("Slave is % seconds behind master." %
                          sec_behind)

        return errors

    def create_rpl_user(self, r_user, r_pass=None):
        """Create the replication user and grant privileges

        If the user exists, check privileges and add privileges as needed.
        Calls Master class method to execute.

        r_user[in]     user to create
        r_pass[in]     password for user to create (optional)

        Returns bool - True = success, False = errors
        """
        ssl = False
        if self.ssl:
            ssl = True
        return self.master.create_rpl_user(self.slave.host, self.slave.port,
                                           r_user, r_pass, self.verbosity, ssl)

    def setup(self, rpl_user, num_tries):
        """Setup replication among a slave and master.

        Note: Must have connected to a master and slave before calling this
        method.

        rpl_user[in]       Replication user in form user:passwd
        num_tries[in]      Number of attempts to wait for slave synch

        Returns True if success, False if error
        """
        if self.master is None or self.slave is None:
            print "ERROR: Must connect to master and slave before " \
                  "calling replicate()"
            return False

        result = True

        # Parse user and password (support login-paths)
        try:
            r_user, r_pass = parse_user_password(rpl_user)
        except FormatError:
            raise UtilError(USER_PASSWORD_FORMAT.format("--rpl-user"))

        # Check to see if rpl_user is present, else create her
        if not self.create_rpl_user(r_user, r_pass)[0]:
            return False

        # Read master log file information
        res = self.master.get_status()
        if not res:
            print "ERROR: Cannot retrieve master status."
            return False

        # If master log file, pos not specified, read master log file info
        read_master_info = False
        if self.master_log_file is None:
            res = self.master.get_status()
            if not res:
                print "ERROR: Cannot retrieve master status."
                return False

            read_master_info = True
            self.master_log_file = res[0][0]
            self.master_log_pos = res[0][1]
        else:
            # Check to make sure file is accessible and valid
            found = False
            res = self.master.get_binary_logs(self.query_options)
            for row in res:
                if row[0] == self.master_log_file:
                    found = True
                    break
            if not found:
                raise UtilError("Master binary log file not listed as a "
                                "valid binary log file on the master.")

        if self.master_log_file is None:
            raise UtilError("No master log file specified.")

        # Stop slave first
        res = self.slave.get_thread_status()
        if res is not None:
            if res[1] == "Yes" or res[2] == "Yes":
                res = self.slave.stop(self.query_options)

        # Connect slave to master
        if self.verbosity > 0:
            print "# Connecting slave to master..."
        master_values = {
            'Master_Host': self.master.host,
            'Master_Port': self.master.port,
            'Master_User': r_user,
            'Master_Password': r_pass,
            'Master_Log_File': self.master_log_file,
            'Read_Master_Log_Pos': self.master_log_pos,
        }

        # Use the options SSL certificates if defined,
        # else use the master SSL certificates if defined.
        if self.ssl:
            master_values['Master_SSL_Allowed'] = 1
            if self.ssl_ca:
                master_values['Master_SSL_CA_File'] = self.ssl_ca
            if self.ssl_cert:
                master_values['Master_SSL_Cert'] = self.ssl_cert
            if self.ssl_key:
                master_values['Master_SSL_Key'] = self.ssl_key

        elif self.master.has_ssl:
            master_values['Master_SSL_Allowed'] = 1
            master_values['Master_SSL_CA_File'] = self.master.ssl_ca
            master_values['Master_SSL_Cert'] = self.master.ssl_cert
            master_values['Master_SSL_Key'] = self.master.ssl_key

        change_master = self.slave.make_change_master(self.from_beginning,
                                                      master_values)
        res = self.slave.exec_query(change_master, self.query_options)
        if self.verbosity > 0:
            print "# %s" % change_master

        # Start slave
        if self.verbosity > 0:
            if not self.from_beginning:
                if read_master_info:
                    print "# Starting slave from master's last position..."
                else:
                    msg = "# Starting slave from master log file '%s'" % \
                          self.master_log_file
                    if self.master_log_pos >= 0:
                        msg += " using position %s" % self.master_log_pos
                    msg += "..."
                    print msg
            else:
                print "# Starting slave from the beginning..."
        res = self.slave.start(self.query_options)

        # Add commit because C/Py are auto_commit=0 by default
        self.slave.exec_query("COMMIT")

        # Check slave status
        i = 0
        while i < num_tries:
            time.sleep(1)
            res = self.slave.get_slaves_errors()
            status = res[0]
            sql_running = res[4]
            if self.verbosity > 0:
                io_errorno = res[1]
                io_error = res[2]
                io_running = res[3]
                sql_errorno = res[5]
                sql_error = res[6]
                print "# IO status: %s" % status
                print "# IO thread running: %s" % io_running
                # if io_errorno = 0 and error = '' -> no error
                if not io_errorno and not io_error:
                    print "# IO error: None"
                else:
                    print "# IO error: %s:%s" % (io_errorno, io_error)
                # if io_errorno = 0 and error = '' -> no error
                print "# SQL thread running: %s" % sql_running
                if not sql_errorno and not sql_error:
                    print "# SQL error: None"
                else:
                    print "# SQL error: %s:%s" % (io_errorno, io_error)
            if status == "Waiting for master to send event" and sql_running:
                break
            elif not sql_running:
                if self.verbosity > 0:
                    print "# Retry to start the slave SQL thread..."
                # SQL thread is not running, retry to start it
                res = self.slave.start_sql_thread(self.query_options)
            if self.verbosity > 0:
                print "# Waiting for slave to synchronize with master"
            i += 1
        if i == num_tries:
            print "ERROR: failed to sync slave with master."
            result = False

        if result is True:
            self.replicating = True

        return result

    def test(self, db, num_tries):
        """Test the replication setup.

        Requires a database name which is created on the master then
        verified it appears on the slave.

        db[in]             Name of a database to use in test
        num_tries[in]      Number of attempts to wait for slave synch
        """

        if not self.replicating:
            print "ERROR: Replication is not running among master and slave."
        print "# Testing replication setup..."
        if self.verbosity > 0:
            print "# Creating a test database on master named %s..." % db
        res = self.master.exec_query("CREATE DATABASE %s" % db,
                                     self.query_options)
        i = 0
        while i < num_tries:
            time.sleep(1)
            res = self.slave.exec_query("SHOW DATABASES")
            for row in res:
                if row[0] == db:
                    res = self.master.exec_query("DROP DATABASE %s" % db,
                                                 self.query_options)
                    print "# Success! Replication is running."
                    i = num_tries
                    break
            i += 1
            if i < num_tries and self.verbosity > 0:
                print "# Waiting for slave to synchronize with master"
        if i == num_tries:
            print "ERROR: Unable to complete testing."


class Master(Server):
    """The Slave class is a subclass of the Server class. It represents a
    MySQL server performing the role of a slave in a replication topology.
    The following utilities are provide in addition to the Server utilities:

        - check to see if replication user is defined and has privileges
        - get binary log exceptions
        - get master status
        - reset master

    """

    def __init__(self, options=None):
        """Constructor

        The method accepts one of the following types for options['conn_info']:

            - dictionary containing connection information including:
              (user, passwd, host, port, socket)
            - connection string in the form: user:pass@host:port:socket
            - an instance of the Server class

        options[in]        options for controlling behavior:
            conn_info      a dictionary containing connection information
                           (user, passwd, host, port, socket)
            role           Name or role of server (e.g., server, master)
            verbose        print extra data during operations (optional)
                           default value = False
            charset        Default character set for the connection.
                           (default latin1)
        """
        if options is None:
            options = {}

        assert options.get("conn_info") is not None

        self.options = options
        Server.__init__(self, options)

    def get_status(self):
        """Return the master status

        Returns result set
        """
        return self.exec_query("SHOW MASTER STATUS")

    def get_binlog_exceptions(self):
        """Get any binary logging exceptions

        This method queries the server status for the *-do-db and
        *-ignore-db settings.

        Returns [] - no exceptions, list if exceptions found
        """
        rows = []
        res = self.get_status()
        if res != []:
            do_db = res[0][_MASTER_DO_DB]
            ignore_db = res[0][_MASTER_IGNORE_DB]
            if len(do_db) > 0 or len(ignore_db) > 0:
                rows.append(('master', do_db, ignore_db))

        return rows

    def get_binlog_info(self):
        """Return the master's binary log information (file name and position).

        Returns a tuple with the binary log filename and position, or None if
        the server is not acting as a master.
        """
        res = self.get_status()
        if res:
            # Return binlog_file and binlog_pos.
            return res[0][0], res[0][1]
        else:
            # Status data is empty, server is not acting as a master.
            return None

    def get_rpl_users(self, options=None):
        """Attempts to find the users who have the REPLICATION SLAVE privilege

        options[in]    query options

        Returns tuple list - (string, string, bool) = (user, host,
                                                       has_password)
        """
        if options is None:
            options = {}
        # Use the correct query for server (changed for 5.7.6).
        if self.check_version_compat(5, 7, 6):
            query = _RPL_USER_QUERY_5_7_6
        else:
            query = _RPL_USER_QUERY
        return self.exec_query(query, options)

    def create_rpl_user(self, host, port, r_user, r_pass=None, verbosity=0,
                        ssl=False):
        """Create the replication user and grant privileges

        If the user exists, check privileges and add privileges as needed.

        host[in]       host of the slave
        port[in]       port of the slave
        r_user[in]     user to create
        r_pass[in]     password for user to create (optional)
        verbosity[in]  verbosity of output
                       Default = 0
        ssl[in]        If True the grant will include 'REQUIRE SSL'
                       (Default False).

        Returns tuple (bool, str) - (True, None) = success,
                                    (False, <error>) = error
        """

        grants_enabled = self.grant_tables_enabled()
        if not grants_enabled:
            return (True, None)

        if "]" in host:
            host = clean_IPv6(host)

        # Create user class instance
        user = User(self, "{0}:{1}@{2}:{3}".format(r_user, r_pass, host, port))
        if not user.exists():
            user.create()
            # Save current user for privilege checking
            user.current_user = "'{0}'@'{1}'".format(r_user, host)

        # Check privileges, but do not user the anonymous host
        if not user.has_privilege("*", "*", "REPLICATION SLAVE",
                                  globals_privs=False):
            if verbosity > 0:
                print "# Granting replication access to replication user..."
            query_str = ("GRANT REPLICATION SLAVE ON *.* TO "
                         "'{0}'@'{1}' ".format(r_user, host))
            if r_pass:
                query_str += "IDENTIFIED BY '{0}'".format(r_pass)

            if ssl:
                query_str = "{0} {1}".format(query_str, " REQUIRE SSL")
            try:
                self.exec_query(query_str)
            except UtilError:
                return (False, "ERROR: Cannot grant replication slave to "
                        "replication user.")

        return (True, None)

    def reset(self, options=None):
        """Reset the master

        options[in]    query options
        """
        if options is None:
            options = {}
        return self.exec_query("RESET MASTER", options)

    def check_rpl_health(self):
        """Check replication health of the master.

        This method checks to see if the master is setup correctly to
        operate in a replication environment. It returns a tuple with a
        bool to indicate if health is Ok (True), and a list to contain any
        errors encountered during the checks.

        Returns tuple (bool, []) - (True, []) = Ok,
                                   (False, error_list) = not setup correctly
        """
        errors = []
        rpl_ok = True

        if not self.is_alive():
            return (False, ["Cannot connect to server"])

        gtid_enabled = self.supports_gtid() == "ON"

        # Check for binlogging
        if not gtid_enabled and not self.binlog_enabled():
            errors.append("No binlog on master.")
            rpl_ok = False

        # See if there is at least one user with rpl privileges
        res = self.get_rpl_users()
        if len(res) == 0:
            errors.append("There are no users with replication privileges.")
            rpl_ok = False

        return (rpl_ok, errors)

    def _check_discovered_slave(self, conn_dict):
        """ Check discovered slave is configured to this master

        This method attempts to determine if the slave specified is
        configured to connect to this master.

        conn_dict[in]  dictionary of connection information

        Returns True if configured with this master otherwise an error is
        raised.
        """
        slave_conn = Slave(conn_dict)
        try:
            slave_conn.connect()
            # Skip discovered slaves that are not configured
            # to connect to the master
            return slave_conn.is_configured_for_master(self,
                                                       verify_state=False,
                                                       raise_error=True)
        finally:
            slave_conn.disconnect()

    def get_slaves(self, user, password):
        """Return the slaves registered for this master.

        This method returns a list of slaves (host, port) if this server is
        a master in a replication topology and has slaves registered.

        user[in]       user login
        password[in]   user password

        Returns list - [host:port, ...]
        """
        def _get_slave_info(host, port):
            """Return the slave info
            """
            if len(host) > 0:
                if ":" in host:
                    host = format_IPv6(host)
                slave_info = host
            else:
                slave_info = "unknown host"
            slave_info += ":%s" % port
            return slave_info

        slaves = []
        no_host_slaves = []
        connect_error_slaves = []
        res = self.exec_query("SHOW SLAVE HOSTS")
        verbose = self.options.get("verbose", False)
        if res != []:
            # Sort for conformity
            res.sort()  # pylint: disable=E1103

            for row in res:
                info = _get_slave_info(row[1], row[2])
                conn_dict = {
                    'conn_info': {'user': user, 'passwd': password,
                                  'host': row[1], 'port': row[2],
                                  'socket': None, 'ssl_ca': self.ssl_ca,
                                  'ssl_cert': self.ssl_cert,
                                  'ssl_key': self.ssl_key,
                                  'ssl': self.ssl},
                    'role': 'slave',
                    'verbose': verbose,
                }
                if not row[1]:
                    no_host_slaves.append(" - {0}".format(info))
                    break
                # Verify slave connection and configuration.
                try:
                    self._check_discovered_slave(conn_dict)
                    # Slave correctly configured.
                    slaves.append(info)
                except UtilError as err:
                    # Connection or configuration errors found.
                    connect_error_slaves.append(
                        " - {0}: {1}".format(info, err.errmsg)
                    )

        # Warn if slaves were found with configuration/connection issues.
        hint = ":" if verbose else " (--verbose for more details)."
        if no_host_slaves:
            print("WARNING: There are slaves that have not been registered"
                  " with --report-host or --report-port{0}".format(hint))
            if verbose:
                for row in no_host_slaves:
                    print(row)
        if connect_error_slaves:
            print("\nWARNING: Cannot connect to some slaves{0}".format(hint))
            if verbose:
                for row in connect_error_slaves:
                    print(row)

        return slaves

    def get_gtid_purged_statement(self):
        """General the SET @@GTID_PURGED statement for backup

        Returns string - statement for slave if GTID=ON, else None
        """
        if self.supports_gtid == "ON":
            gtid_executed = self.exec_query("SELECT @@GLOBAL.GTID_EXECUTED")[0]
            return "SET @@GLOBAL.GTID_PURGED = '{0}'".format(gtid_executed)
        else:
            return None


class MasterInfo(object):
    """The MasterInfo is an abstraction of the mechanism for storing the
    master information for slave servers. It is designed to return an
    implementation neutral representation of the information for use in
    newer servers that use a table to store the information as well as
    older servers that use a file to store the information.
    """

    def __init__(self, slave, options):
        """Constructor

        The method accepts one of the following types for options['conn_info']:

            - dictionary containing connection information including:
              (user, passwd, host, port, socket)
            - connection string in the form: user:pass@host:port:socket
            - an instance of the Server class

        options[in]        options for controlling behavior:
          filename         filename for master info file - valid only for
                           servers with master-info-repository=FILE or
                           versions prior to 5.6.5.
          verbosity        determines level of output. Default = 0.
          quiet            turns off all messages except errors.
                           Default is False.
        """

        assert slave is not None, "MasterInfo requires an instance of Slave."
        self.slave = slave
        self.filename = options.get("master_info", "master.info")
        self.quiet = options.get("quiet", False)
        self.verbosity = options.get("verbosity", 0)
        self.values = {}      # internal dictionary of the values
        self.repo = "FILE"
        if self.slave is not None:
            res = self.slave.show_server_variable("master_info_repository")
            if res is not None and res != [] and \
               res[0][1].upper() == "TABLE":
                self.repo = "TABLE"

    def read(self):
        """Read the master information

        This method reads the master information either from a file or a
        table depending on the availability of and setting for
        master-info-repository. If missing (server version < 5.6.5), it
        defaults to reading from a file.

        Returns bool - True = success
        """
        if self.verbosity > 2:
            print "# Reading master information from a %s." % self.repo.lower()
        if self.repo == "FILE":
            # Check host name of this host. If not the same, issue error.
            if self.slave.is_alias(socket.gethostname()):
                return self._read_master_info_file()
            else:
                raise UtilRplWarn("Cannot read master information file "
                                  "from a remote machine.")
        else:
            return self._read_master_info_table()

    def _check_read(self, refresh=False):
        """Check if master information has been read

        refresh[in]    if True, re-read the master information.
                       Default is False.

        If the master information has not been read, read it and populate
        the dictionary.
        """
        # Read the values if not already read or user says to refresh them.
        if self.values is None or self.values == {} or refresh:
            self.read()

    def _build_dictionary(self, rows):
        """Build the internal dictionary of values.

        rows[in]       Rows as read from the file or table
        """
        for i in range(0, len(rows)):
            self.values[_MASTER_INFO_COL[i]] = rows[i]

    def _read_master_info_file(self):
        """Read the contents of the master.info file.

        This method will raise an error if the file is missing or cannot be
        read by the user.

        Returns bool - success = True
        """
        contents = []
        res = self.slave.show_server_variable('datadir')
        if res is None or res == []:
            raise UtilRplError("Cannot get datadir.")
        datadir = res[0][1]
        if self.filename == 'master.info':
            self.filename = os.path.join(datadir, self.filename)

        if not os.path.exists(self.filename):
            raise UtilRplError("Cannot find master information file: "
                               "%s." % self.filename)
        try:
            mfile = open(self.filename, 'r')
            num = int(mfile.readline())
            # Protect overrun of array if master_info file length is
            # changed (more values added).
            if num > len(_MASTER_INFO_COL):
                num = len(_MASTER_INFO_COL)
        except:
            raise UtilRplError("Cannot read master information file: "
                               "%s.\nUser needs to have read access to "
                               "the file." % self.filename)
        # Build the dictionary
        i = 1
        while i < num:
            contents.append(mfile.readline().strip('\n'))
            i += 1
        self._build_dictionary(contents)
        mfile.close()

        return True

    def _read_master_info_table(self):
        """Read the contents of the slave_master_info table.

        This method will raise an error if the file is missing or cannot be
        read by the user.

        Returns bool - success = True
        """
        res = None
        try:
            res = self.slave.exec_query("SELECT * FROM "
                                        "mysql.slave_master_info")
        except UtilError, e:
            raise UtilRplError("Unable to read the slave_master_info table. "
                               "Error: %s" % e.errmsg)
        if res is None or res == []:
            return False

        # Protect overrun of array if the master_info table size has changed
        # (more rows than expected).
        num = len(res[0][1:])
        if num > len(_MASTER_INFO_COL):
            num = len(_MASTER_INFO_COL)
        # Build dictionary for the information with column information
        rows = []
        for i in range(0, num):
            rows.append(res[0][i + 1])
        self._build_dictionary(rows)

        return True

    def show_master_info(self, refresh=False):
        """Display the contents of the master information.

        refresh[in]    if True, re-read the master information.
                       Default is False.
        """
        # Check to see if we need to read the information
        self._check_read(refresh)
        stop = len(self.values)
        for i in range(0, stop):
            print "{0:>30} : {1}".format(_MASTER_INFO_COL[i],
                                         self.values[_MASTER_INFO_COL[i]])

    def check_master_info(self, refresh=False):
        """Check to see if master info file matches slave status

        This method will return a list of discrepancies if the master.info
        file does not match slave status. It will also raise errors if there
        are problem accessing the master.info file.

        refresh[in]    if True, re-read the master information.
                       Default is False.

        Returns [] - no exceptions, list if exceptions found
        """
        # Check to see if we need to read the information
        self._check_read(refresh)
        errors = []
        res = self.slave.get_status()
        if res != []:
            state = res[0][_SLAVE_IO_STATE]
            if not state:
                raise UtilRplError("Slave is stopped.")
            m_host = res[0][_SLAVE_MASTER_HOST]
            m_port = res[0][_SLAVE_MASTER_PORT]
            rpl_user = res[0][_SLAVE_MASTER_USER]
            if m_host != self.values['Master_Host'] or \
               int(m_port) != int(self.values['Master_Port']) or \
               rpl_user != self.values['Master_User']:
                errors.append("Slave is connected to master differently "
                              "than what is recorded in the master "
                              "information file. Master information file "
                              "= user=%s, host=%s, port=%s." %
                              (self.values['Master_User'],
                               self.values['Master_Host'],
                               self.values['Master_Port']))

        return errors

    def get_value(self, key, refresh=False):
        """Returns the value found for the key or None if key not found.

        refresh[in]    if True, re-read the master information.
                       Default is False.

        Returns value - Value found for the key or None if key missing
        """
        # Check to see if we need to read the information
        self._check_read(refresh)
        try:
            return self.values[key]
        except:
            return None

    def get_master_info(self, refresh=False):
        """Returns the master information dictionary.

        refresh[in]    if True, re-read the master information.
                       Default is False.

        Returns dict - master information
        """
        # Check to see if we need to read the information
        self._check_read(refresh)
        return self.values


class Slave(Server):
    """The Slave class is a subclass of the Server class. It represents a
    MySQL server performing the role of a slave in a replication topology.
    The following utilities are provide in addition to the Server utilities:

        - get methods to return status, binary log exceptions, slave delay,
          thread status, io error, and master information
        - form the change master command with either known master or user-
          supplied values
        - check to see if slave is connected to a master
        - display slave status
        - show master information
        - verify master information matches currently connected master
        - start, stop, and reset slave

    """

    def __init__(self, options=None):
        """Constructor

        The method accepts one of the following types for options['conn_info']:

            - dictionary containing connection information including:
              (user, passwd, host, port, socket)
            - connection string in the form: user:pass@host:port:socket
            - an instance of the Server class

        options[in]        options for controlling behavior:
            conn_info      a dictionary containing connection information
                           (user, passwd, host, port, socket)
            role           Name or role of server (e.g., server, master)
            verbose        print extra data during operations (optional)
                           default value = False
            charset        Default character set for the connection.
                           (default latin1)
        """
        if options is None:
            options = {}

        assert options.get("conn_info") is not None

        self.options = options
        Server.__init__(self, options)
        self.master_info = None

    def get_status(self, col_options=None):
        """Return the slave status

        col_options[in]    options for displaying columns (optional)

        Returns result set
        """
        if not col_options:
            col_options = {}
        return self.exec_query("SHOW SLAVE STATUS", col_options)

    def get_retrieved_gtid_set(self):
        """Get any events (gtids) read but not executed

        Returns a string with the list of gtids in Executed_Gtid_Set.

        Note: an empty string is returned if the server is not acting as a
              slave.
        """
        res = self.get_status()
        if res != []:
            return res[0][_RETRIEVED_GTID_SET]
        return ''

    def get_executed_gtid_set(self):
        """Get any events (gtids) executed

        Returns a string with the list of gtids in Executed_Gtid_Set.

        Note: an empty string is returned if the server is not acting as a
              slave.
        """
        res = self.get_status()
        if res:
            return res[0][_EXECUTED_GTID_SET]

        return ''

    def get_binlog_exceptions(self):
        """Get any binary logging exceptions

        This method queries the server status for the *-do-db and
        *-ignore-db settings.

        Returns [] - no exceptions, list if exceptions found
        """
        rows = []
        res = self.get_status()
        if res != []:
            do_db = res[0][_SLAVE_DO_DB]
            ignore_db = res[0][_SLAVE_IGNORE_DB]
            if len(do_db) > 0 or len(ignore_db) > 0:
                rows.append(('slave', do_db, ignore_db))

        return rows

    def get_master_host_port(self):
        """Get the slave's connected master host and port

        Returns tuple - (master host, master port) or
                        None if not acting as slave
        """
        res = self.get_status()
        if res == []:
            return None
        m_host = res[0][_SLAVE_MASTER_HOST]
        m_port = res[0][_SLAVE_MASTER_PORT]

        return (m_host, m_port)

    def is_connected(self):
        """Check to see if slave is connected to master

        This method will check the slave to see if it is connected to a master
        by checking if his I/O Thread is running.

        Returns bool - True = slave is connected
        """
        res = self.get_status()
        if res == []:
            return False
        return res[0][_SLAVE_IO_RUNNING].upper() == "YES"

    def get_rpl_master_user(self):
        """Get the rpl master user from the slave status

        Returns the slave_master_user as string or False if there is
        no slave status.
        """
        res = self.get_status()
        if not res:
            return False
        return res[0][_SLAVE_MASTER_USER]

    def get_master_uuid(self):
        """Get the master_uuid from the slave status.

        Return the master UUID or None if not an acting slave.
        """
        res = self.get_status()
        if not res:
            return None
        return res[0][_MASTER_UUID]

    def get_state(self):
        """Get the slave's connection state

        Returns state or None if not acting as slave
        """
        res = self.get_status()
        if res == []:
            return None
        state = res[0][_SLAVE_IO_STATE]

        return state

    def get_io_running(self):
        """Get the slave's IO thread status

        Returns IO_THREAD state or None if not acting as slave
        """
        res = self.get_status()
        if res == []:
            return None
        return res[0][_SLAVE_IO_RUNNING]

    def get_sql_running(self):
        """Get the slave's SQL thread status

        Returns SQL_THREAD state or None if not acting as slave
        """
        res = self.get_status()
        if res == []:
            return None
        return res[0][_SLAVE_SQL_RUNNING]

    def get_delay(self):
        """Return slave delay values

        This method retrieves the slave's delay parameters.

        Returns tuple - slave delay values or None if not connected
        """
        res = self.get_status()
        if res == []:
            return None

        # slave IO state
        state = res[0][_SLAVE_IO_STATE]
        # seconds behind master
        if res[0][_SLAVE_DELAY] is None:
            sec_behind = 0
        else:
            sec_behind = int(res[0][_SLAVE_DELAY])
        # remaining delay
        delay_remaining = res[0][_SLAVE_REMAINING_DELAY]
        # master's log file read
        read_log_file = res[0][_SLAVE_MASTER_LOG_FILE]
        # position in master's binlog
        read_log_pos = res[0][_SLAVE_MASTER_LOG_FILE_POS]

        return (state, sec_behind, delay_remaining,
                read_log_file, read_log_pos)

    def get_thread_status(self):
        """Return the slave threads status

        Returns tuple - (slave_io_state, slave_io_running, slave_sql_running)
                        or None if not connected
        """
        res = self.get_status()
        if res == []:
            return None

        # slave IO state
        state = res[0][_SLAVE_IO_STATE]
        # slave_io_running
        io_running = res[0][_SLAVE_IO_RUNNING]
        # slave_sql_running
        sql_running = res[0][_SLAVE_SQL_RUNNING]

        return (state, io_running, sql_running)

    def get_io_error(self):
        """Return the slave slave io error status

        Returns tuple - (slave_io_state, io_errorno, io_error)
                        or None if not connected
        """
        res = self.get_status()
        if res == []:
            return None

        state = res[0][_SLAVE_IO_STATE]
        io_errorno = int(res[0][_SLAVE_IO_ERRORNO])
        io_error = res[0][_SLAVE_IO_ERROR]

        return (state, io_errorno, io_error)

    def get_sql_error(self):
        """Return the slave slave sql error status

        Returns tuple - (sql_running, sql_errorno, sql_error)
                        or None if not connected
        """
        res = self.get_status()
        if not res:
            return None

        sql_running = res[0][_SLAVE_SQL_RUNNING]
        sql_errorno = int(res[0][_SLAVE_SQL_ERRORNO])
        sql_error = res[0][_SLAVE_SQL_ERROR]

        return (sql_running, sql_errorno, sql_error)

    def get_slaves_errors(self):
        """Return the slave slave io and sql error status

        Returns tuple - (slave_io_state, io_errorno, io_error, io_running,
                         sql_running, sql_errorno, sql_error)
                        or None if not connected
        """
        res = self.get_status()
        if not res:
            return None

        state = res[0][_SLAVE_IO_STATE]
        io_errorno = int(res[0][_SLAVE_IO_ERRORNO])
        io_error = res[0][_SLAVE_IO_ERROR]
        io_running = res[0][_SLAVE_IO_RUNNING]
        sql_running = res[0][_SLAVE_SQL_RUNNING]
        sql_errorno = int(res[0][_SLAVE_SQL_ERRORNO])
        sql_error = res[0][_SLAVE_SQL_ERROR]

        return (state, io_errorno, io_error, io_running, sql_running,
                sql_errorno, sql_error)

    def get_slave_rpl_filters(self):
        """Get the replication filter options for the slave.

        Get the replication filter information from the slave status.

        Returns a tuple with the replication filter options (Replicate_Do_DB,
        Replicate_Ignore_DB, Replicate_Do_Table, Replicate_Ignore_Table,
        Replicate_Wild_Do_Table, Replicate_Wild_Ignore_Table). An empty tuple
        () is returned if no filter is defined and None if the slave status is
        not available.
        """
        res = self.get_status()
        if not res:
            return None

        rpl_do_db = res[0][_SLAVE_DO_DB]
        rpl_ignore_db = res[0][_SLAVE_IGNORE_DB]
        rpl_do_table = res[0][_SLAVE_DO_TABLE]
        rpl_ignore_table = res[0][_SLAVE_IGNORE_TABLE]
        rpl_wild_do_table = res[0][_SLAVE_WILD_DO_TABLE]
        rpl_wild_ignore_table = res[0][_SLAVE_WILD_IGNORE_TABLE]

        if (rpl_do_db or rpl_ignore_db or rpl_do_table or rpl_ignore_table or
                rpl_wild_do_table or rpl_wild_ignore_table):
            return (rpl_do_db, rpl_ignore_db, rpl_do_table, rpl_ignore_table,
                    rpl_wild_do_table, rpl_wild_ignore_table)
        else:
            return ()

    def show_status(self):
        """Display the slave status from the slave server
        """
        col_options = {
            'columns': True
        }
        res = self.get_status(col_options)
        if res != [] and res[1] != []:
            stop = len(res[0])
            cols = res[0]
            rows = res[1]
            for i in range(0, stop):
                print "{0:>30} : {1}".format(cols[i], rows[0][i])
        else:
            raise UtilRplError("Cannot get slave status or slave is "
                               "not configured as a slave or not "
                               "started.")

    def get_rpl_user(self):
        """Return the master user from the master info record.

        Returns - tuple = (user, password) or (None, None) if errors
        """
        self.master_info = MasterInfo(self, self.options)
        m_host = self.master_info.get_value("Master_User")
        m_passwd = self.master_info.get_value("Master_Password")
        if m_host is not None:
            return (m_host, m_passwd)
        return (None, None)

    def start(self, options=None, autocommit_fix=True, until_gtid_set=None,
              sql_after_gtid=True, only_sql_thread=False):
        """Start the slave.

        Execute the START SLAVE statement (to start the IO and/or SQL threads),
        according to the used parameters.

        options[in]         query options
        autocommit_fix[in]  If True, turn off AUTOCOMMIT before start command.
                            True by default to always apply the fix.
        until_gtid_set[in]  GTID set to use to execute START SLAVE UNTIL. By
                            default None, until option is not applied.
        sql_after_gtid[in]  Indicates if the until option SQL_AFTER_GTIDS is
                            used or in alternative SQL_BEFORE_GTIDS. Only
                            applied if until_gtid_set is specified. By default
                            True, SQL_AFTER_GTIDS is used.
        only_sql_thread[in] If True only the SQL thread is started, otherwise
                            both (by default).
        """
        if options is None:
            options = {}

        # Temporary workaround for BUG#16533802 - remove when fixed (part 1/2).
        if autocommit_fix:
            autocommit_value = self.autocommit_set()
            # If disabled, turn it on.
            if not autocommit_value:
                self.toggle_autocommit(True)

        query = "START SLAVE"
        if only_sql_thread:
            query = "{0} SQL_THREAD".format(query)
        if until_gtid_set:
            # Use until option.
            until_type = (
                'SQL_AFTER_GTIDS' if sql_after_gtid else 'SQL_BEFORE_GTIDS'
            )
            query = "{0} UNTIL {1} = '{2}'".format(query, until_type,
                                                   until_gtid_set)
        res = self.exec_query(query, options)

        # Temporary workaround for BUG#16533802 - remove when fixed (part 2/2).
        if autocommit_fix:
            # If disabled originally, turn it off.
            if not autocommit_value:
                self.toggle_autocommit(False)

        return res

    def start_sql_thread(self, options=None):
        """Start the slave SQL thread

        options[in]    query options
        """
        if options is None:
            options = {}
        return self.exec_query("START SLAVE SQL_THREAD", options)

    def stop(self, options=None):
        """Stop the slave

        options[in]    query options
        """
        if options is None:
            options = {}
        return self.exec_query("STOP SLAVE", options)

    def stop_sql_thread(self, options=None):
        """Stop the slave SQL thread.

        options[in]    query options
        """
        if options is None:
            options = {}
        return self.exec_query("STOP SLAVE SQL_THREAD", options)

    def reset(self, options=None):
        """Reset the slave

        options[in]    query options
        """
        if options is None:
            options = {}
        return self.exec_query("RESET SLAVE", options)

    def reset_all(self, options=None):
        """Reset all information on this slave.

        options[in]    query options
        """
        if options is None:
            options = {}
        # Must be sure to do stop first
        self.stop()
        # RESET SLAVE ALL was implemented in version 5.5.16 and later
        if not self.check_version_compat(5, 5, 16):
            return self.reset()
        return self.exec_query("RESET SLAVE ALL", options)

    def wait_checksum_and_start(self, tbl_name, wait_timeout=30,
                                wait_interval=3, checksum_timeout=0,
                                options=None):
        """Checksum specified table and start slave.

        tbl_name[in]        Name of the table to perform the checksum.
        wait_timeout[in]    Timeout value to wait for the slave to stop SQL
                            thread (automatically stopped after catching up
                            with master). By default 30 seconds.
        wait_interval[in]   Wait interval to perform the next polling (check
                            if SQL thread is stopped) By default 3 seconds..
        options[in]     Query options.

        Returns the result of the table checksum,more precisely a tuple with
        the checksum and an error description. If the checksum is computed it
        returns (checksum, None), otherwise (None, <skip error description>)
        where <skip error description> is a brief description of the motive why
        the checksum was not computed.
        """
        # Wait for slave to stop (if timeout > 0).
        tick = 0
        checksum = None
        skip_checksum = True if wait_timeout > 0 else False
        while tick < wait_timeout:
            status = self.get_slaves_errors()
            io_running = status[3].upper() == 'YES'
            sql_running = status[4].upper() == 'YES'
            # Only check if SQl thread is running since START SLAVE UNTIL does
            # not stop the IO thread.
            if sql_running:
                time.sleep(wait_interval)
                tick += wait_interval
            else:
                skip_checksum = False
                # Report if replication was stopped due to an error.
                if not io_running and status[2]:
                    print("# IO thread ERROR found for {0}:{1}: {2} - "
                          "{3}".format(self.host, self.port, status[1],
                                       status[2]))
                if not sql_running and status[6]:
                    print("# SQL thread ERROR found for {0}:{1}: {2} - "
                          "{3}".format(self.host, self.port, status[5],
                                       status[6]))
                break

        if skip_checksum:
            # Checksum skipped.
            skip_error = "timeout catching up with master"
            self.stop_sql_thread(options)
        else:
            # Compute checksum.
            checksum, skip_error = self.checksum_table(
                tbl_name, exec_timeout=checksum_timeout
            )

        # Resume replication, start slave.
        self.start_sql_thread(options)

        return checksum, skip_error

    def num_gtid_behind(self, master_gtids):
        """Get the number of transactions the slave is behind the master.

        master_gtids[in]  the master's GTID_EXECUTED list

        Returns int - number of trans behind master
        """
        slave_gtids = self.exec_query(_GTID_EXECUTED)[0][0]
        gtids = self.exec_query("SELECT GTID_SUBTRACT('%s','%s')" %
                                (master_gtids[0][0], slave_gtids))[0]
        # Init gtid_behind count (if no GTIDs behind then 0 is returned)
        gtid_behind = 0
        # Check if there are GTIDs behind
        # (i.e. string with GTIDs set is not equal to '')
        if gtids[0]:
            gtids_list = gtids[0].split("\n")
            # Extract the interval for each GTID and compute its length
            for gtid_item in gtids_list:
                interval_list = gtid_item.rstrip(', ')
                interval_list = interval_list.split(':')[1:]
                for interval_str in interval_list:
                    interval = interval_str.split('-')
                    if len(interval) == 1:
                        # Interval has only one element
                        gtid_behind += 1
                    else:
                        # Compute interval size and sum to total GTIDs behind.
                        num_gtids = int(interval[1]) - int(interval[0]) + 1
                        gtid_behind += num_gtids
        return gtid_behind

    def wait_for_slave(self, binlog_file, binlog_pos, timeout=300):
        """Wait for the slave to read the master's binlog to specified position

        binlog_file[in]  master's binlog file
        binlog_pos[in]   master's binlog file position
        timeout[in]      maximum number of seconds to wait for event to occur

        Returns bool - True = slave has read to the file and pos,
                       False = slave is behind.
        """
        # Wait for slave to read the master log file
        _MASTER_POS_WAIT = "SELECT MASTER_POS_WAIT('%s', %s, %s)"
        res = self.exec_query(_MASTER_POS_WAIT % (binlog_file,
                                                  binlog_pos, timeout))
        if res is None or (res[0][0] is not None and int(res[0][0]) < 0):
            return False
        return True

    def wait_for_slave_gtid(self, master_gtid, timeout=300, verbose=False):
        """Wait for the slave to read the master's GTIDs.

        This method requires that the server supports GTIDs.

        master_gtid[in]  the list of gtids from the master
                         obtained via SELECT @@GLOBAL.GTID_EXECUTED on master
        timeout[in]      timeout for waiting for slave to catch up
                         Note: per GTID call. Default is 300 seconds (5 min.).
        verbose[in]      if True, print query used.
                         Default is False

        Returns bool - True = slave has read all GTIDs
                       False = slave is behind
        """
        master_gtids = master_gtid[0][0].split('\n')
        slave_wait_ok = True
        for gtid in master_gtids:
            try:
                if verbose:
                    print "# Slave %s:%s:" % (self.host, self.port)
                    print "# QUERY =", _GTID_WAIT % (gtid.strip(','), timeout)
                res = self.exec_query(_GTID_WAIT % (gtid.strip(','), timeout))
                if verbose:
                    print "# Return Code =", res[0][0]
                if res is None or res[0] is None or res[0][0] is None or \
                   int(res[0][0]) < 0:
                    slave_wait_ok = False
            except UtilRplError, e:
                raise UtilRplError("Error executing %s: %s" %
                                   ((_GTID_WAIT % (gtid.strip(','), timeout)),
                                    e.errmsg))
        return slave_wait_ok

    def make_change_master(self, from_beginning=False, master_values=None):
        """Make the CHANGE MASTER command.

        This method forms the CHANGE MASTER command based on the current
        settings of the slave. If the user supplies a dictionary of options,
        the method will use those values provided by the user if present
        otherwise it will use current settings.

        Note: the keys used in the dictionary are defined in the
              _MASTER_INFO_COL list defined above.

        from_beginning[in] if True, omit specification of master's binlog info
        master_values[in] if provided, use values in the dictionary

        Returns string - CHANGE MASTER command
        """
        if not master_values:
            master_values = {}
        if master_values == {} and not self.is_connected():
            raise UtilRplError("Cannot generate CHANGE MASTER command. The "
                               "slave is not connected to a master and no "
                               "master information was provided.")
        elif self.is_connected():
            m_info = MasterInfo(self, self.options)
            master_info = m_info.get_master_info()
            if master_info is None and master_values == {}:
                raise UtilRplError("Cannot create CHANGE MASTER command.")
        else:
            master_info = None

        # Form values for command.
        # If we cannot get the master info information, try the values passed
        if master_info is None:
            master_host = master_values['Master_Host']
            if "]" in master_host:
                master_host = clean_IPv6(master_host)
            master_port = master_values['Master_Port']
            master_user = master_values['Master_User']
            master_passwd = master_values['Master_Password']
            master_log_file = master_values['Master_Log_File']
            master_log_pos = master_values['Read_Master_Log_Pos']
            master_ssl = master_values.get('Master_SSL_Allowed', None)
            master_ssl_ca = master_values.get('Master_SSL_CA_File', None)
            master_ssl_cert = master_values.get('Master_SSL_Cert', None)
            master_ssl_key = master_values.get('Master_SSL_Key', None)
            if master_ssl and master_ssl_ca is None:
                master_ssl_ca = ''
        else:
            master_host = master_values.get('Master_Host',
                                            master_info['Master_Host'])
            master_port = master_values.get('Master_Port',
                                            master_info['Master_Port'])
            master_user = master_values.get('Master_User',
                                            master_info['Master_User'])
            master_passwd = master_values.get('Master_Password',
                                              master_info['Master_Password'])
            master_log_file = master_values.get('Master_Log_File',
                                                master_info['Master_Log_File'])
            master_log_pos = master_values.get(
                'Read_Master_Log_Pos',
                master_info['Read_Master_Log_Pos']
            )
            master_ssl = master_values.get(
                'Master_SSL_Allowed',
                master_info['Master_SSL_Allowed']
            )
            master_ssl_ca = master_values.get(
                'Master_SSL_CA_File',
                master_info['Master_SSL_CA_File']
            )
            master_ssl_cert = master_values.get(
                'Master_SSL_Cert',
                master_info['Master_SSL_Cert']
            )
            master_ssl_key = master_values.get(
                'Master_SSL_Key',
                master_info['Master_SSL_Key']
            )

        change_master = "CHANGE MASTER TO MASTER_HOST = '%s', " % master_host
        if master_user:
            change_master += "MASTER_USER = '%s', " % master_user
        # To rewrite a current password with blank password, not check against
        # empty string.
        if master_passwd is not None:
            change_master += "MASTER_PASSWORD = '%s', " % master_passwd
        change_master += "MASTER_PORT = %s" % master_port
        if master_ssl and master_ssl not in ('0', 'OFF'):
            change_master = "{0}, MASTER_SSL = {1}".format(change_master, 1)
        if master_ssl_ca is not None:
            change_master = (
                "{0}, MASTER_SSL_CA = '{1}'"
            ).format(change_master, master_ssl_ca)
        if master_ssl_cert:
            change_master = (
                "{0}, MASTER_SSL_CERT = '{1}'"
            ).format(change_master, master_ssl_cert)
        if master_ssl_key:
            change_master = (
                "{0}, MASTER_SSL_KEY = '{1}'"
            ).format(change_master, master_ssl_key)
        if self.supports_gtid() == "ON":
            change_master += ", MASTER_AUTO_POSITION=1"
        elif not from_beginning:
            change_master += ", MASTER_LOG_FILE = '%s'" % master_log_file
            if master_log_pos >= 0:
                change_master += ", MASTER_LOG_POS = %s" % master_log_pos

        return change_master

    def is_configured_for_master(self, master, verify_state=False,
                                 raise_error=False):
        """Check that slave is connected to the master at host, port.

        master[in]          Instance of the master.
        verify_state[in]    Flag to verify the state of the slave.
                            By default False, state verification ignored.
        raise_error[in]     Indicate if an Error is raised instead of
                            returning false (not configured for master).
                            By default False, return a boolean value.

        Returns bool - True = is connected
        """
        res = self.get_status()
        if res == [] or not res[0]:
            if raise_error:
                raise UtilRplError("Server '{0}:{1}' is not acting as a slave "
                                   "(slave status is empty)"
                                   ".".format(self.host, self.port))
            return False
        # We must not assume there is one and only one master for a slave.
        # Starting with 5.7.6, multi-master means a slave could have many
        # masters, each connected via a replication channel. Thus, we must
        # loop through the rows in the SHOW SLAVE STATUS and check every
        # master listed. If no matches to this master is found, we can
        # declare the slave not connected to the master otherwise, we can
        # stop the loop when the master is found.
        m_host = ""
        m_port = None
        master_found = False
        for row in res:
            # pylint: disable=W0633
            m_host = row[_SLAVE_MASTER_HOST]  # get master host
            m_port = row[_SLAVE_MASTER_PORT]  # get master port
            # Suppose the state is True for "Waiting for master to send event"
            # so we can ignore it if verify_state is not given as True.
            if verify_state:
                state = (row[_SLAVE_IO_STATE] ==
                         "Waiting for master to send event")
                if not state:
                    if raise_error:
                        raise UtilRplError("Slave '{0}:{1}' is not waiting"
                                           " for events from master."
                                           "".format(self.host, self.port))
                    return False
            # If we find a match, stop.
            if master.is_alias(m_host) and int(m_port) == int(master.port):
                master_found = True
                break
        # If no master found, report what we did find or in the case of
        # multi-master (more than one row in SHOW SLAVE STATUS), state this
        # master is not among the masters listed for the slave.
        if not master_found:
            if raise_error:
                if len(res) > 1:
                    raise UtilRplError("The list of masters for slave "
                                       "'{0}:{1}' does not include master"
                                       " '{2}:{3}'"
                                       ".".format(self.host, self.port,
                                                  master.host, master.port))
                else:
                    raise UtilRplError("Slave '{0}:{1}' is configured for "
                                       "master '{2}:{3}' and not '{4}:{5}'"
                                       ".".format(self.host, self.port,
                                                  m_host, m_port,
                                                  master.host, master.port))
            return False
        return True

    def check_rpl_health(self, master, master_log, master_log_pos,
                         max_delay, max_pos, verbosity):
        """Check replication health of the slave.

        This method checks to see if the slave is setup correctly to
        operate in a replication environment. It returns a tuple with a
        bool to indicate if health is Ok (True), and a list to contain any
        errors encountered during the checks.

        master[in]         Master class instance
        master_log[in]     master's log file
        master_log_pos[in] master's log file position
        max_delay[in]      if the slave delay (in seconds) is greater than this
                           value, the slave health is not Ok
        max_pos[in]        maximum position difference from master to slave to
                           determine if slave health is not Ok
        verbosity[in]      if > 1, return detailed errors else return only
                           short phrases

        Returns tuple (bool, []) - (True, []) = Ok,
                                   (False, error_list) = not setup correctly
        """
        errors = []
        rpl_ok = True

        if not self.is_alive():
            return (False, ["Cannot connect to server"])

        res = self.get_status()
        if res != [] and res[0] != []:
            res = res[0]
            self.get_master_host_port()
            m_log = res[_SLAVE_MASTER_LOG_FILE]
            m_log_pos = res[_SLAVE_MASTER_LOG_FILE_POS]
            io_running = res[_SLAVE_IO_RUNNING]
            sql_running = res[_SLAVE_SQL_RUNNING]
            s_delay = res[_SLAVE_DELAY]
            delay = s_delay if s_delay is not None else 0
            remaining_delay = res[_SLAVE_REMAINING_DELAY]
            io_error_num = res[_SLAVE_IO_ERRORNO]
            io_error_text = res[_SLAVE_IO_ERROR]

            # Check to see that slave is connected to the right master
            if not self.is_configured_for_master(master):
                return (False, ["Not connected to correct master."])

            # Check slave status for errors, threads activity
            if io_running.upper() != "YES":
                errors.append("IO thread is not running.")
                rpl_ok = False
            if sql_running.upper() != "YES":
                errors.append("SQL thread is not running.")
                rpl_ok = False
            if int(io_error_num) > 0:
                errors.append(io_error_text)
                rpl_ok = False

            # Check slave delay with threshhold of SBM, and master's log pos
            if int(delay) > int(max_delay):
                errors.append("Slave delay is %s seconds behind master." %
                              delay)
                if len(remaining_delay):
                    errors.append(remaining_delay)
                rpl_ok = False

            # Check master position
            if self.supports_gtid() != "ON":
                if m_log != master_log:
                    errors.append("Wrong master log file.")
                    rpl_ok = False
                elif (int(m_log_pos) + int(max_pos)) < int(master_log_pos):
                    errors.append("Slave's master position exceeds maximum.")
                    rpl_ok = False

            # Check GTID trans behind.
            elif self.supports_gtid() == "ON":
                master_gtids = master.exec_query(_GTID_EXECUTED)
                num_gtids_behind = self.num_gtid_behind(master_gtids)
                if num_gtids_behind > 0:
                    errors.append("Slave has %s transactions behind master." %
                                  num_gtids_behind)
                    rpl_ok = False

        else:
            errors.append("Not connected")
            rpl_ok = False

        if len(errors) > 1:
            errors = [", ".join(errors)]

        return (rpl_ok, errors)

    def get_rpl_details(self):
        """Return slave status variables for health reporting

        This method retrieves the slave's parameters for checking relationship
        with master.

        Returns tuple - slave values or None if not connected
        """
        res = self.get_status()
        if res == []:
            return None

        res = res[0]
        read_log_file = res[_SLAVE_MASTER_LOG_FILE]
        read_log_pos = res[_SLAVE_MASTER_LOG_FILE_POS]
        io_thread = res[_SLAVE_IO_RUNNING]
        sql_thread = res[_SLAVE_SQL_RUNNING]

        # seconds behind master
        if res[_SLAVE_DELAY] is None:
            sec_behind = 0
        else:
            sec_behind = int(res[_SLAVE_DELAY])
        delay_remaining = res[_SLAVE_REMAINING_DELAY]

        io_error_num = res[_SLAVE_IO_ERRORNO]
        io_error_text = res[_SLAVE_IO_ERROR]
        sql_error_num = res[_SLAVE_SQL_ERRORNO]
        sql_error_text = res[_SLAVE_SQL_ERROR]

        return (read_log_file, read_log_pos, io_thread, sql_thread, sec_behind,
                delay_remaining, io_error_num, io_error_text, sql_error_num,
                sql_error_text)

    def switch_master(self, master, user, passwd="", from_beginning=False,
                      master_log_file=None, master_log_pos=None,
                      show_command=False):
        """Switch slave to a new master

        This method stops the slave and issues a new change master command
        to the master specified then starts the slave. No prerequisites are
        checked and it does not wait to see if slave catches up to the master.

        master[in]           Master class instance
        user[in]             replication user
        passwd[in]           replication user password
        from_beginning[in]   if True, start from beginning of logged events
                             Default = False
        master_log_file[in]  master's log file (not needed for GTID)
        master_log_pos[in]   master's log file position (not needed for GTID)
        show_command[in]     if True, display the change master command
                             Default = False

        returns bool - True = success
        """
        hostport = "%s:%s" % (self.host, self.port)

        master_values = {
            'Master_Host': master.host,
            'Master_Port': master.port,
            'Master_User': user,
            'Master_Password': passwd,
            'Master_Log_File': master_log_file,
            'Read_Master_Log_Pos': master_log_pos,
        }
        if master.has_ssl:
            master_values['Master_SSL_Allowed'] = 1
            if master.ssl_ca:
                master_values['Master_SSL_CA_File'] = master.ssl_ca
            if master.ssl_cert:
                master_values['Master_SSL_Cert'] = master.ssl_cert
            if master.ssl_key:
                master_values['Master_SSL_Key'] = master.ssl_key
        change_master = self.make_change_master(from_beginning, master_values)
        if show_command:
            print "# Change master command for %s:%s" % (self.host, self.port)
            print "#", change_master
        try:
            self.exec_query(change_master)
        except UtilError as err:
            raise UtilRplError("Slave {0} change master failed. "
                               "{1}".format(hostport, err.errmsg))
        return True
#
# Copyright (c) 2014, 2016 Oracle and/or its affiliates. All rights
# reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains the multi-source replication utility. It is used to setup
replication among a slave and multiple masters.
"""

import os
import sys
import time
import logging


_MIN_SERVER_VERSION = (5, 6, 9)
_GTID_LISTS = ["Transactions executed on the servers:",
               "Transactions purged from the servers:",
               "Transactions owned by another server:"]
_GEN_UUID_COLS = ["host", "port", "role", "uuid"]
_GEN_GTID_COLS = ["host", "port", "role", "gtid"]


class ReplicationMultiSource(Daemon):
    """Setup replication among a slave and multiple masters.

    This class implements a multi-source replication using a round-robin
    scheduling for setup replication among all masters and slave.

    This class also implements a POSIX daemon.
    """
    def __init__(self, slave_vals, masters_vals, options):
        """Constructor.

        slave_vals[in]     Slave server connection dictionary.
        master_vals[in]    List of master server connection dictionaries.
        options[in]        Options dictionary.
        """
        pidfile = options.get("pidfile", None)
        if pidfile is None:
            pidfile = "./rplms_daemon.pid"
        super(ReplicationMultiSource, self).__init__(pidfile)

        self.slave_vals = slave_vals
        self.masters_vals = masters_vals
        self.options = options
        self.quiet = self.options.get("quiet", False)
        self.logging = self.options.get("logging", False)
        self.rpl_user = self.options.get("rpl_user", None)
        self.verbosity = options.get("verbosity", 0)
        self.interval = options.get("interval", 15)
        self.switchover_interval = options.get("switchover_interval", 60)
        self.format = self.options.get("format", False)
        self.topology = None
        self.report_values = [
            report.lower() for report in
            self.options["report_values"].split(",")
        ]

        # A sys.stdout copy, that can be used later to turn on/off stdout
        self.stdout_copy = sys.stdout
        self.stdout_devnull = open(os.devnull, "w")

        # Disable stdout when running --daemon with start, stop or restart
        self.daemon = options.get("daemon")
        if self.daemon:
            if self.daemon in ("start", "nodetach"):
                self._report("Starting multi-source replication daemon...",
                             logging.INFO, False)
            elif self.daemon == "stop":
                self._report("Stopping multi-source replication daemon...",
                             logging.INFO, False)
            else:
                self._report("Restarting multi-source replication daemon...",
                             logging.INFO, False)

            # Disable stdout
            sys.stdout = self.stdout_devnull
        else:
            self._report("# Starting multi-source replication...",
                         logging.INFO)
            print("# Press CTRL+C to quit.")

        # Check server versions
        try:
            self._check_server_versions()
        except UtilError as err:
            raise UtilRplError(err.errmsg)

        # Check user privileges
        try:
            self._check_privileges()
        except UtilError as err:
            msg = "Error checking user privileges: {0}".format(err.errmsg)
            self._report(msg, logging.CRITICAL, False)
            raise UtilRplError(err.errmsg)

    @staticmethod
    def _reconnect_server(server, pingtime=3):
        """Tries to reconnect to the server.

        This method tries to reconnect to the server and if connection fails
        after 3 attemps, returns False.

        server[in]      Server instance.
        pingtime[in]    Interval between connection attempts.
        """
        if server and server.is_alive():
            return True
        is_connected = False
        i = 0
        while i < 3:
            try:
                server.connect()
                is_connected = True
                break
            except UtilError:
                pass
            time.sleep(pingtime)
            i += 1
        return is_connected

    def _get_slave(self):
        """Get the slave server instance.

        Returns a Server instance of the slave from the replication topology.
        """
        return self.topology.slaves[0]["instance"]

    def _get_master(self):
        """Get the current master server instance.

        Returns a Server instance of the current master from the replication
        topology.
        """
        return self.topology.master

    def _check_server_versions(self):
        """Checks the server versions.
        """
        if self.verbosity > 0:
            print("# Checking server versions.\n#")

        # Connection dictionary
        conn_dict = {
            "conn_info": None,
            "quiet": True,
            "verbose": self.verbosity > 0,
        }

        # Check masters version
        for master_vals in self.masters_vals:
            conn_dict["conn_info"] = master_vals
            master = Master(conn_dict)
            master.connect()
            if not master.check_version_compat(*_MIN_SERVER_VERSION):
                raise UtilRplError(
                    ERROR_MIN_SERVER_VERSIONS.format(
                        utility="mysqlrplms",
                        min_version=".".join([str(val) for val in
                                              _MIN_SERVER_VERSION]),
                        host=master.host,
                        port=master.port
                    )
                )
            master.disconnect()

        # Check slave version
        conn_dict["conn_info"] = self.slave_vals
        slave = Slave(conn_dict)
        slave.connect()
        if not slave.check_version_compat(*_MIN_SERVER_VERSION):
            raise UtilRplError(
                ERROR_MIN_SERVER_VERSIONS.format(
                    utility="mysqlrplms",
                    min_version=".".join([str(val) for val in
                                          _MIN_SERVER_VERSION]),
                    host=slave.host,
                    port=slave.port
                )
            )
        slave.disconnect()

    def _check_privileges(self):
        """Check required privileges to perform the multi-source replication.

        This method check if the used users for the slave and masters have
        the required privileges to perform the multi-source replication.
        The following privileges are required:
            - on slave: SUPER, SELECT, INSERT, UPDATE, REPLICATION
                        SLAVE AND GRANT OPTION;
            - on the master: SUPER, SELECT, INSERT, UPDATE, REPLICATION SLAVE
                             AND GRANT OPTION.
        An exception is thrown if users doesn't have enough privileges.
        """
        if self.verbosity > 0:
            print("# Checking users privileges for replication.\n#")

        # Connection dictionary
        conn_dict = {
            "conn_info": None,
            "quiet": True,
            "verbose": self.verbosity > 0,
        }

        # Check privileges for master.
        master_priv = [('SUPER',), ('SELECT',), ('INSERT',), ('UPDATE',),
                       ('REPLICATION SLAVE',), ('GRANT OPTION',)]
        master_priv_str = ("SUPER, SELECT, INSERT, UPDATE, REPLICATION SLAVE "
                           "AND GRANT OPTION")

        for master_vals in self.masters_vals:
            conn_dict["conn_info"] = master_vals
            master = Master(conn_dict)
            master.connect()

            user_obj = User(master, "{0}@{1}".format(master.user, master.host))
            for any_priv_tuple in master_priv:
                has_privilege = any(
                    [user_obj.has_privilege('*', '*', priv)
                     for priv in any_priv_tuple]
                )
                if not has_privilege:
                    msg = ERROR_USER_WITHOUT_PRIVILEGES.format(
                        user=master.user, host=master.host, port=master.port,
                        operation='perform replication',
                        req_privileges=master_priv_str
                    )
                    self._report(msg, logging.CRITICAL, False)
                    raise UtilRplError(msg)
            master.disconnect()

        # Check privileges for slave
        slave_priv = [('SUPER',), ('SELECT',), ('INSERT',), ('UPDATE',),
                      ('REPLICATION SLAVE',), ('GRANT OPTION',)]
        slave_priv_str = ("SUPER, SELECT, INSERT, UPDATE, REPLICATION SLAVE "
                          "AND GRANT OPTION")

        conn_dict["conn_info"] = self.slave_vals
        slave = Slave(conn_dict)
        slave.connect()

        user_obj = User(slave, "{0}@{1}".format(slave.user, slave.host))
        for any_priv_tuple in slave_priv:
            has_privilege = any(
                [user_obj.has_privilege('*', '*', priv)
                 for priv in any_priv_tuple]
            )
            if not has_privilege:
                msg = ("User '{0}' on '{1}@{2}' does not have sufficient "
                       "privileges to perform replication (required: {3})."
                       "".format(slave.user, slave.host, slave.port,
                                 slave_priv_str))
                self._report(msg, logging.CRITICAL, False)
                raise UtilRplError(msg)
        slave.disconnect()

    def _check_host_references(self):
        """Check to see if using all host or all IP addresses.

        Returns bool - True = all references are consistent.
        """
        uses_ip = hostname_is_ip(self.topology.master.host)
        slave = self._get_slave()
        host_port = slave.get_master_host_port()
        host = None
        if host_port:
            host = host_port[0]
        if (not host or uses_ip != hostname_is_ip(slave.host) or
                uses_ip != hostname_is_ip(host)):
            return False
        return True

    def _setup_replication(self, master_vals, use_rpl_setup=True):
        """Setup replication among a master and a slave.

        master_vals[in]      Master server connection dictionary.
        use_rpl_setup[in]    Use Replication.setup() if True otherwise use
                             switch_master() on the slave. This is used to
                             control the first pass in the masters round-robin
                             scheduling.
        """
        conn_options = {
            "src_name": "master",
            "dest_name": "slave",
            "version": "5.0.0",
            "unique": True,
        }
        (master, slave,) = connect_servers(master_vals, self.slave_vals,
                                           conn_options)
        rpl_options = self.options.copy()
        rpl_options["verbosity"] = self.verbosity > 0

        # Start from beginning only on the first pass
        if rpl_options.get("from_beginning", False) and not use_rpl_setup:
            rpl_options["from_beginning"] = False

        # Create an instance of the replication object
        rpl = Replication(master, slave, rpl_options)

        if use_rpl_setup:
            # Check server ids
            errors = rpl.check_server_ids()
            for error in errors:
                self._report(error, logging.ERROR, True)

            # Check for server_id uniqueness
            errors = rpl.check_server_uuids()
            for error in errors:
                self._report(error, logging.ERROR, True)

            # Check InnoDB compatibility
            errors = rpl.check_innodb_compatibility(self.options)
            for error in errors:
                self._report(error, logging.ERROR, True)

            # Checking storage engines
            errors = rpl.check_storage_engines(self.options)
            for error in errors:
                self._report(error, logging.ERROR, True)

            # Check master for binary logging
            errors = rpl.check_master_binlog()
            if errors != []:
                raise UtilRplError(errors[0])

            # Setup replication
            if not rpl.setup(self.rpl_user, 10):
                msg = "Cannot setup replication."
                self._report(msg, logging.CRITICAL, False)
                raise UtilRplError(msg)
        else:
            # Parse user and password (support login-paths)
            try:
                (r_user, r_pass,) = parse_user_password(self.rpl_user)
            except FormatError:
                raise UtilError(USER_PASSWORD_FORMAT.format("--rpl-user"))

            # Switch master and start slave
            slave.switch_master(master, r_user, r_pass)
            slave.start({'fetch': False})

        # Disconnect from servers
        master.disconnect()
        slave.disconnect()

    def _switch_master(self, master_vals, use_rpl_setup=True):
        """Switches replication to a new master.

        This method stops replication with the old master if exists and
        starts the replication with a new one.

        master_vals[in]      Master server connection dictionary.
        use_rpl_setup[in]    Used to control the first pass in the masters
                             round-robin scheduling.
        """
        if self.topology:
            # Stop slave
            master = self._get_master()
            if master.is_alive():
                master.disconnect()
            slave = self._get_slave()
            if not slave.is_alive() and not self._reconnect_server(slave):
                msg = "Failed to connect to the slave."
                self._report(msg, logging.CRITICAL, False)
                raise UtilRplError(msg)
            slave.stop()
            slave.disconnect()

        self._report("# Switching to master '{0}:{1}'."
                     "".format(master_vals["host"],
                               master_vals["port"]), logging.INFO, True)

        try:
            # Setup replication on the new master
            self._setup_replication(master_vals, use_rpl_setup)

            # Create a Topology object
            self.topology = Topology(master_vals, [self.slave_vals],
                                     self.options)
        except UtilError as err:
            msg = "Error while switching master: {0}".format(err.errmsg)
            self._report(msg, logging.CRITICAL, False)
            raise UtilRplError(err.errmsg)

        # Only works for GTID_MODE=ON
        if not self.topology.gtid_enabled():
            msg = ("Topology must support global transaction ids and have "
                   "GTID_MODE=ON.")
            self._report(msg, logging.CRITICAL, False)
            raise UtilRplError(msg)

        # Check for mixing IP and hostnames
        if not self._check_host_references():
            print("# WARNING: {0}".format(HOST_IP_WARNING))
            self._report(HOST_IP_WARNING, logging.WARN, False)

    def _report(self, message, level=logging.INFO, print_msg=True):
        """Log message if logging is on.

        This method will log the message presented if the log is turned on.
        Specifically, if options['log_file'] is not None. It will also
        print the message to stdout.

        message[in]      Message to be printed.
        level[in]        Level of message to log. Default = INFO.
        print_msg[in]    If True, print the message to stdout. Default = True.
        """
        # First, print the message.
        if print_msg and not self.quiet:
            print(message)
        # Now log message if logging turned on
        if self.logging:
            logging.log(int(level), message.strip("#").strip(" "))

    def _format_health_data(self):
        """Return health data from topology.

        Returns tuple - (columns, rows).
        """
        if self.topology:
            try:
                health_data = self.topology.get_health()
                current_master = self._get_master()

                # Get data for the remaining masters
                for master_vals in self.masters_vals:
                    # Discard the current master
                    if master_vals["host"] == current_master.host and \
                       master_vals["port"] == current_master.port:
                        continue

                    # Connect to the master
                    conn_dict = {
                        "conn_info": master_vals,
                        "quiet": True,
                        "verbose": self.verbosity > 0,
                    }
                    master = Master(conn_dict)
                    master.connect()

                    # Get master health
                    rpl_health = master.check_rpl_health()

                    master_data = [
                        master.host,
                        master.port,
                        "MASTER",
                        get_server_state(master, master.host, 3,
                                         self.verbosity > 0),
                        master.supports_gtid(),
                        "OK" if rpl_health[0] else ", ".join(rpl_health[1]),
                    ]

                    # Get master status
                    master_status = master.get_status()
                    if len(master_status):
                        master_log, master_log_pos = master_status[0][0:2]
                    else:
                        master_log = None
                        master_log_pos = 0

                    # Show additional details if verbosity is turned on
                    if self.verbosity > 0:
                        master_data.extend([master.get_version(), master_log,
                                            master_log_pos, "", "", "", "", "",
                                            "", "", "", ""])
                    health_data[1].append(master_data)
                return health_data
            except UtilError as err:
                msg = "Cannot get health data: {0}".format(err)
                self._report(msg, logging.ERROR, False)
                raise UtilRplError(msg)
        return ([], [])

    def _format_uuid_data(self):
        """Return the server's uuids.

        Returns tuple - (columns, rows).
        """
        if self.topology:
            try:
                return (_GEN_UUID_COLS, self.topology.get_server_uuids())
            except UtilError as err:
                msg = "Cannot get UUID data: {0}".format(err)
                self._report(msg, logging.ERROR, False)
                raise UtilRplError(msg)
        return ([], [])

    def _format_gtid_data(self):
        """Return the GTID information from the topology.

        Returns tuple - (columns, rows).
        """
        if self.topology:
            try:
                return (_GEN_GTID_COLS, self.topology.get_gtid_data())
            except UtilError as err:
                msg = "Cannot get GTID data: {0}".format(err)
                self._report(msg, logging.ERROR, False)
                raise UtilRplError(msg)
        return ([], [])

    def _log_data(self, title, labels, data, print_format=True):
        """Helper method to log data.

        title[in]     Title to log.
        labels[in]    List of labels.
        data[in]      List of data rows.
        """
        self._report("# {0}".format(title), logging.INFO)
        for row in data:
            msg = ", ".join(
                ["{0}: {1}".format(*col) for col in zip(labels, row)]
            )
            self._report("# {0}".format(msg), logging.INFO, False)
        if print_format:
            print_list(sys.stdout, self.format, labels, data)

    def _log_master_status(self, master):
        """Logs the master information.

        master[in]    Master server instance.

        This method logs the master information from SHOW MASTER STATUS.
        """
        # If no master present, don't print anything.
        if master is None:
            return

        print("#")
        self._report("# {0}:".format("Current Master Information"),
                     logging.INFO)

        try:
            status = master.get_status()[0]
        except UtilError:
            msg = "Cannot get master status"
            self._report(msg, logging.ERROR, False)
            raise UtilRplError(msg)

        cols = ("Binary Log File", "Position", "Binlog_Do_DB",
                "Binlog_Ignore_DB")
        rows = (status[0] or "N/A", status[1] or "N/A", status[2] or "N/A",
                status[3] or "N/A")

        print_list(sys.stdout, self.format, cols, [rows])

        self._report("# {0}".format(
            ", ".join(["{0}: {1}".format(*item) for item in zip(cols, rows)]),
        ), logging.INFO, False)

        # Display gtid executed set
        master_gtids = []
        for gtid in status[4].split("\n"):
            if gtid:
                # Add each GTID to a tuple to match the required format to
                # print the full GRID list correctly.
                master_gtids.append((gtid.strip(","),))

        try:
            if len(master_gtids) > 1:
                gtid_executed = "{0}[...]".format(master_gtids[0][0])
            else:
                gtid_executed = master_gtids[0][0]
        except IndexError:
            gtid_executed = "None"

        self._report("# GTID Executed Set: {0}".format(gtid_executed),
                     logging.INFO)

    def stop_replication(self):
        """Stops multi-source replication.

        Stop the slave if topology is available.
        """
        if self.topology:
            # Get the slave instance
            slave = self._get_slave()
            # If slave is not connected, try to reconnect and stop replication
            if self._reconnect_server(slave):
                slave.stop()
                slave.disconnect()
        if self.daemon:
            self._report("Multi-source replication daemon stopped.",
                         logging.INFO, False)
        else:
            print("")
            self._report("# Multi-source replication stopped.",
                         logging.INFO, True)

    def stop(self):
        """Stops the daemon.

        Stop slave if topology is available and then stop the daemon.
        """
        self.stop_replication()
        super(ReplicationMultiSource, self).stop()

    def run(self):
        """Run the multi-source replication using the round-robin scheduling.

        This method implements the multi-source replication by using time
        slices for each master.
        """
        num_masters = len(self.masters_vals)
        use_rpl_setup = True

        # pylint: disable=R0101
        while True:
            # Round-robin scheduling on the masters
            for idx in range(num_masters):
                # Get the new master values and switch for the next one
                try:
                    master_vals = self.masters_vals[idx]
                    self._switch_master(master_vals, use_rpl_setup)
                except UtilError as err:
                    msg = ("Error while switching master: {0}"
                           "".format(err.errmsg))
                    self._report(msg, logging.CRITICAL, False)
                    raise UtilRplError(msg)

                # Get the new master and slave instances
                master = self._get_master()
                slave = self._get_slave()

                switchover_timeout = time.time() + self.switchover_interval

                while switchover_timeout > time.time():
                    # If servers not connected, try to reconnect
                    if not self._reconnect_server(master):
                        msg = ("Failed to connect to the master '{0}:{1}'."
                               "".format(master_vals["host"],
                                         master_vals["port"]))
                        self._report(msg, logging.CRITICAL, False)
                        raise UtilRplError(msg)

                    if not self._reconnect_server(slave):
                        msg = "Failed to connect to the slave."
                        self._report(msg, logging.CRITICAL, False)
                        raise UtilRplError(msg)

                    # Report
                    self._log_master_status(master)
                    if "health" in self.report_values:
                        (health_labels, health_data,) = \
                            self._format_health_data()
                        if health_data:
                            print("#")
                            self._log_data("Health Status:", health_labels,
                                           health_data)
                    if "gtid" in self.report_values:
                        (gtid_labels, gtid_data,) = self._format_gtid_data()
                        for i, row in enumerate(gtid_data):
                            if row:
                                print("#")
                                self._log_data("GTID Status - {0}"
                                               "".format(_GTID_LISTS[i]),
                                               gtid_labels, row)

                    if "uuid" in self.report_values:
                        (uuid_labels, uuid_data,) = self._format_uuid_data()
                        if uuid_data:
                            print("#")
                            self._log_data("UUID Status:", uuid_labels,
                                           uuid_data)

                    # Disconnect servers
                    master.disconnect()
                    slave.disconnect()

                    # Wait for reporting interval
                    time.sleep(self.interval)

            # Use Replication.setup() only for the first round
            use_rpl_setup = False
#
# Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains features to check the data consistency in a replication
topology (i.e., between the master and its slaves, or only slaves), providing
synchronization features to perform the check over the (supposed) same data of
a system with replication active (running).
"""
import re
import sys

from multiprocessing.pool import ThreadPool


# Regular expression to handle the server version format.
_RE_VERSION_FORMAT = r'^(\d+\.\d+(\.\d+)*).*$'


class RPLSynchronizer(object):
    """Class to manage the features of the replication synchronization checker.

    The RPLSynchronizer class is used to manage synchronization check between
    servers of a replication topology, namely between the master and its
    slaves or only between slaves. It provides functions to determine the
    slaves missing transactions (i.e., missing GTIDs) and check data
    consistency.
    """

    def __init__(self, master_cnx_dic, slaves_cnx_dic_lst, options):
        """Constructor.

        options[in]       dictionary of options (e.g., discover, timeouts,
                          verbosity).
        """
        self._verbosity = options.get('verbosity')
        self._rpl_timeout = options.get('rpl_timeout')
        self._checksum_timeout = options.get('checksum_timeout')
        self._interval = options.get('interval')

        self._rpl_topology = Topology(master_cnx_dic, slaves_cnx_dic_lst,
                                      options)
        self._slaves = self._rpl_topology.get_slaves_dict()

        # Verify all the servers in the topology has or does not sql_mode set
        # to 'ANSI_QUOTES'.
        match_group, unmatch_group = \
            self._rpl_topology.get_servers_with_different_sql_mode(
                'ANSI_QUOTES'
            )
        # List and Raise an error if just some of the server has sql_mode set
        # to 'ANSI_QUOTES' instead of all or none.
        if match_group and unmatch_group:
            sql_mode = match_group[0].select_variable("SQL_MODE")
            if sql_mode == '':
                sql_mode = '""'
            sql_mode = sql_mode.replace(',', ', ')
            print("# The SQL mode in the following servers is set to "
                  "ANSI_QUOTES: {0}".format(sql_mode))
            for server in match_group:
                sql_mode = server.select_variable("SQL_MODE")
                if sql_mode == '':
                    sql_mode = '""'
                sql_mode = sql_mode.replace(',', ', ')
                print("# {0}:{1} sql_mode={2}"
                      "".format(server.host, server.port, sql_mode))
            print("# The SQL mode in the following servers is not set to "
                  "ANSI_QUOTES:")
            for server in unmatch_group:
                sql_mode = server.select_variable("SQL_MODE")
                if sql_mode == '':
                    sql_mode = '""'
                print("# {0}:{1} sql_mode={2}"
                      "".format(server.host, server.port, sql_mode))

            raise UtilError(ERROR_ANSI_QUOTES_MIX_SQL_MODE.format(
                utility='mysqlrplsync'
            ))

        # Set base server used as reference for comparisons.
        self._base_server = None
        self._base_server_key = None
        self._set_base_server()

        # Check user permissions to perform the consistency check.
        self._check_privileges()

        # Check usage of replication filters.
        self._master_rpl_filters = {}
        self._slaves_rpl_filters = {}
        self._check_rpl_filters()

    def _set_base_server(self):
        """Set the base server used for comparison in the internal state.

        Set the master if used or the first slave from the topology as the
        base server. The base server is the one used as a reference for
        comparison with the others. This method sets two instance variables:
        _base_server with the Server instance, and _base_server_key with the
        string identifying the server (format: 'host@port').

        Note: base server might need to be changed (set again) if it is
        removed from the topology for some reason (e.g. GTID disabled).
        """
        master = self._get_master()
        self._base_server = master if master \
            else self._rpl_topology.slaves[0]['instance']
        self._base_server_key = "{0}@{1}".format(self._base_server.host,
                                                 self._base_server.port)

    def _get_slave(self, slave_key):
        """Get the slave server instance for the specified key 'host@port'.

        This function retrieves the Server instance of for a slave from the
        internal state by specifying the key that uniquely identifies it,
        i.e. 'host@port'.

        slave_key[in]   String with the format 'host@port' that uniquely
                        identifies a server.

        Returns a Server instance of the slave with the specified key value
        (i.e., 'host@port').
        """
        slave_dict = self._slaves[slave_key]
        return slave_dict['instance']

    def _get_master(self):
        """Get the master server instance.

        This function retrieves the Server instance of the master (in the
        replication topology).

        Returns a Server instance of the master.
        """
        return self._rpl_topology.master

    def _check_privileges(self):
        """Check required privileges to perform the synchronization check.

        This method check if the used users for the master and slaves possess
        the required privileges to perform the synchronization check. More
        specifically, the following privileges are required:
            - on the master: SUPER or REPLICATION CLIENT, LOCK TABLES and
                             SELECT;
            - on slaves: SUPER and SELECT.
        An exception is thrown if users doesn't have enough privileges.
        """
        if self._verbosity:
            print("# Checking users permission to perform consistency check.\n"
                  "#")

        # Check privileges for master.
        master_priv = [('SUPER', 'REPLICATION CLIENT'), ('LOCK TABLES',),
                       ('SELECT',)]
        master_priv_str = "SUPER or REPLICATION CLIENT, LOCK TABLES and SELECT"
        if self._get_master():
            server = self._get_master()
            user_obj = User(server, "{0}@{1}".format(server.user, server.host))
            for any_priv_tuple in master_priv:
                has_privilege = any(
                    [user_obj.has_privilege('*', '*', priv)
                     for priv in any_priv_tuple]
                )
                if not has_privilege:
                    raise UtilError(ERROR_USER_WITHOUT_PRIVILEGES.format(
                        user=server.user, host=server.host, port=server.port,
                        operation='perform the synchronization check',
                        req_privileges=master_priv_str
                    ))

        # Check privileges for slaves.
        slave_priv = [('SUPER',), ('SELECT',)]
        slave_priv_str = "SUPER and SELECT"
        for slave_key in self._slaves:
            server = self._get_slave(slave_key)
            user_obj = User(server, "{0}@{1}".format(server.user, server.host))
            for any_priv_tuple in slave_priv:
                has_privilege = any(
                    [user_obj.has_privilege('*', '*', priv)
                     for priv in any_priv_tuple]
                )
                if not has_privilege:
                    raise UtilError(
                        "User '{0}' on '{1}@{2}' does not have sufficient "
                        "privileges to perform the synchronization check "
                        "(required: {3}).".format(server.user, server.host,
                                                  server.port, slave_priv_str)
                    )

    def _check_rpl_filters(self):
        """Check usage of replication filters.

        Check the usage of replication filtering option on the master (if
        defined) and slaves, and set the internal state with the found options
        (to check later).
        """
        # Get binlog filtering option for the master.
        if self._get_master():
            m_filters = self._get_master().get_binlog_exceptions()
            if m_filters:
                # Set filtering option for master.
                self._master_rpl_filters['binlog_do_db'] = \
                    m_filters[0][1].split(',') if m_filters[0][1] else None
                self._master_rpl_filters['binlog_ignore_db'] = \
                    m_filters[0][2].split(',') if m_filters[0][2] else None

        # Get replication filtering options for each slave.
        for slave_key in self._slaves:
            slave = self._get_slave(slave_key)
            s_filters = slave.get_slave_rpl_filters()
            if s_filters:
                # Handle known server issues with some replication filters,
                # leading to inconsistent GTID sets. Sync not supported for
                # server with those issues.
                issues = [(0, 'replicate_do_db'), (1, 'replicate_ignore_db'),
                          (4, 'replicate_wild_do_table')]
                for index, rpl_opt in issues:
                    if s_filters[index]:
                        raise UtilError(
                            "Use of {0} option is not supported. There is a "
                            "known issue with the use this replication filter "
                            "and GTID for some server versions. Issue "
                            "detected for '{1}'.".format(rpl_opt, slave_key))
                # Set map (dictionary) with the slave filtering options.
                filters_map = {
                    'replicate_do_db':
                    s_filters[0].split(',') if s_filters[0] else None,
                    'replicate_ignore_db':
                    s_filters[1].split(',') if s_filters[1] else None,
                    'replicate_do_table':
                    s_filters[2].split(',') if s_filters[2] else None,
                    'replicate_ignore_table':
                    s_filters[3].split(',') if s_filters[3] else None,
                }
                # Handle wild-*-table filters differently to create
                # corresponding regexp.
                if s_filters[4]:
                    wild_list = s_filters[4].split(',')
                    filters_map['replicate_wild_do_table'] = wild_list
                    # Create auxiliary list with compiled regexp to match.
                    regexp_list = []
                    for wild in wild_list:
                        regexp = re.compile(convertSQL_LIKE2REGEXP(wild))
                        regexp_list.append(regexp)
                    filters_map['regexp_do_table'] = regexp_list
                else:
                    filters_map['replicate_wild_do_table'] = None
                    filters_map['regexp_do_table'] = None
                if s_filters[5]:
                    wild_list = s_filters[5].split(',')
                    filters_map['replicate_wild_ignore_table'] = wild_list
                    # Create auxiliary list with compiled regexp to match.
                    regexp_list = []
                    for wild in wild_list:
                        regexp = re.compile(convertSQL_LIKE2REGEXP(wild))
                        regexp_list.append(regexp)
                    filters_map['regexp_ignore_table'] = regexp_list
                else:
                    filters_map['replicate_wild_ignore_table'] = None
                    filters_map['regexp_ignore_table'] = None
                # Set filtering options for the slave.
                self._slaves_rpl_filters[slave_key] = filters_map

        # Print warning if filters are found.
        if self._master_rpl_filters or self._slaves_rpl_filters:
            print("# WARNING: Replication filters found on checked "
                  "servers. This can lead data consistency issues "
                  "depending on how statements are evaluated.\n"
                  "# More information: "
                  "http://dev.mysql.com/doc/en/replication-rules.html")
            if self._verbosity:
                # Print filter options in verbose mode.
                if self._master_rpl_filters:
                    print("# Master '{0}@{1}':".format(
                        self._get_master().host, self._get_master().port
                    ))
                    for rpl_filter in self._master_rpl_filters:
                        if self._master_rpl_filters[rpl_filter]:
                            print("#   - {0}: {1}".format(
                                rpl_filter,
                                ', '.join(
                                    self._master_rpl_filters[rpl_filter]
                                )
                            ))
                if self._slaves_rpl_filters:
                    for slave_key in self._slaves_rpl_filters:
                        print("# Slave '{0}':".format(slave_key))
                        filters_map = self._slaves_rpl_filters[slave_key]
                        for rpl_filter in filters_map:
                            if (rpl_filter.startswith('replicate') and
                                    filters_map[rpl_filter]):
                                print("#   - {0}: {1}".format(
                                    rpl_filter,
                                    ', '.join(filters_map[rpl_filter])
                                ))

    def _is_rpl_filtered(self, db_name, tbl_name=None, slave=None):
        """ Check if the given object is to be filtered by replication.

        This method checks if the given database or table name is
        supposed to be filtered by replication (i.e., not replicated),
        according to the defined replication filters for the master or
        the specified slave.

        db_name[in]     Name of the database to check (not backtick quoted) or
                        associated to the table to check..
        tbl_name[in]    Name of the table to check (not backtick quoted).
                        Table level filtering rules are only checked if this
                        value is not None. By default None, meaning that only
                        the database level rules are checked.
        slave[in]       Identification of the slave in the format 'host@port'
                        to check, determining which filtering rules will be
                        checked. If None only the master filtering rules are
                        checked, otherwise the rule of the specified slaves
                        are used. By default: None.

        Returns a boolean value indicating if the given database or table is
        supposed to be filtered by the replication  or not. More precisely,
        if True then updates associated to the object are (supposedly) not
        replicated, otherwise they are replicated.
        """
        def match_regexp(name, regex_list):
            """ Check if 'name' matches one of the regex in the given list.
            """
            for regex in regex_list:
                if regex.match(name):
                    return True
            return False

        # Determine object to check and set full qualified name.
        is_db = tbl_name is None
        obj_name = db_name if is_db else '{0}.{1}'.format(db_name, tbl_name)

        # Match replication filter for Master.
        if not slave and is_db and self._master_rpl_filters:
            if self._master_rpl_filters['binlog_do_db']:
                if obj_name in self._master_rpl_filters['binlog_do_db']:
                    return False
                else:
                    return True
            elif self._master_rpl_filters['binlog_ignore_db']:
                if obj_name in self._master_rpl_filters['binlog_ignore_db']:
                    return True

        # Match replication filters for the specified slave.
        if slave and slave in self._slaves_rpl_filters:
            rpl_filter = self._slaves_rpl_filters[slave]
            if is_db:
                if rpl_filter['replicate_do_db']:
                    if obj_name in rpl_filter['replicate_do_db']:
                        return False
                    else:
                        return True
                elif (rpl_filter['replicate_ignore_db'] and
                      obj_name in rpl_filter['replicate_ignore_db']):
                    return True
            else:
                if (rpl_filter['replicate_do_table'] and
                        obj_name in rpl_filter['replicate_do_table']):
                    return False
                if (rpl_filter['replicate_ignore_table'] and
                        obj_name in rpl_filter['replicate_ignore_table']):
                    return True
                if (rpl_filter['replicate_wild_do_table'] and
                        match_regexp(obj_name,
                                     rpl_filter['regexp_do_table'])):
                    return False
                if (rpl_filter['replicate_wild_ignore_table'] and
                        match_regexp(obj_name,
                                     rpl_filter['regexp_ignore_table'])):
                    return True
                if (rpl_filter['replicate_do_table'] or
                        rpl_filter['replicate_wild_do_table']):
                    return True

        # Do not filter replication for object (if no filter rule matched).
        return False

    def _apply_for_all_slaves(self, slaves, function, args=(), kwargs=None,
                              multithreading=False):
        """Apply specified function to all given slaves.

        This function allow the execution (concurrently or not) of the
        specified function with the given arguments on all the specified
        slaves.

        slaves[in]          List of slaves to apply the function. It is assumed
                            that the list is composed by strings with the
                            format 'host@port', identifying each slave.
        function[in]        Name of the function (string) to apply on all
                            slaves.
        args[in]            Tuple with all the function arguments (except
                            keyword arguments).
        kwargs[in]          Dictionary with all the function keyword arguments.
        multithreading[in]  Boolean value indicating if the function will be
                            applied concurrently on all slaves. By default
                            False, no concurrency.

        Return a list of tuples composed by two elements: a string identifying
        the slave ('host@port') and the result of the execution of the target
        function for the corresponding slave.
        """
        if kwargs is None:
            kwargs = {}
        if multithreading:
            # Create a pool of threads to execute the method for each slave.
            pool = ThreadPool(processes=len(slaves))
            thread_res_lst = []
            for slave_key in slaves:
                slave = self._get_slave(slave_key)
                thread_res = pool.apply_async(getattr(slave, function), args,
                                              kwargs)
                thread_res_lst.append((slave_key, thread_res))
            pool.close()
            # Wait for all threads to finish here to avoid RuntimeErrors when
            # waiting for the result of a thread that is already dead.
            pool.join()
            # Get the result from each slave and return the results.
            res = []
            for slave_key, thread_res in thread_res_lst:
                res.append((slave_key, thread_res.get()))
            return res
        else:
            res = []
            for slave_key in slaves:
                slave = self._get_slave(slave_key)
                slave_res = getattr(slave, function)(*args, **kwargs)
                res.append((slave_key, slave_res))
            return res

    def check_server_versions(self):
        """Check server versions.

        Check all server versions and report version differences.
        """
        srv_versions = {}
        # Get the server version of the master if used.
        master = self._get_master()
        if master:
            master_version = master.get_version()
            match = re.match(_RE_VERSION_FORMAT, master_version.strip())
            if match:
                # Add .0 as release version if not provided.
                if not match.group(2):
                    master_version = "{0}.0".format(match.group(1))
                else:
                    master_version = match.group(1)
            master_id = '{0}@{1}'.format(master.host, master.port)
            # Store the master version.
            srv_versions[master_version] = [master_id]

        # Get the server version for all slaves.
        for slave_key in self._slaves:
            slave = self._get_slave(slave_key)
            version = slave.get_version()
            match = re.match(_RE_VERSION_FORMAT, version.strip())
            if match:
                # Add .0 as release version if not provided.
                if not match.group(2):
                    version = "{0}.0".format(match.group(1))
                else:
                    version = match.group(1)
            # Store the slave version.
            if version in srv_versions:
                srv_versions[version].append(slave_key)
            else:
                srv_versions[version] = [slave_key]

        # Check the servers versions and issue a warning if different.
        if len(srv_versions) > 1:
            print("# WARNING: Servers using different versions:")
            for version in srv_versions:
                servers_str = ",".join(srv_versions[version])
                print("# - {0} for {1}.".format(version, servers_str))
            print("#")

    def check_gtid_sync(self):
        """Check GTIDs synchronization.

        Perform several GTID checks (enabled and errant transactions). If the
        master is available (was specified) then it also checks if GTIDs are
        in sync between master and its slaves and report the amount of
        transaction (i.e., GTIDs) behind the master for each slave.

        GTID differences might be an indicator of the existence of data
        consistency issues.

        Note: The master may not be specified, its use is not mandatory.
        """
        # Check if GTIDs are enabled on the topology.
        if self._get_master():  # Use of Master is not mandatory.
            # GTIDs must be enabled on the master.
            if self._get_master().supports_gtid().upper() != 'ON':
                raise UtilError(
                    "Master must support GTIDs and have GTID_MODE=ON."
                )
        # Skip slaves without GTID enabled and warn user.
        reset_base_srv = False
        for slave_key, slave_dict in self._slaves.items():
            slave = slave_dict['instance']
            support_gtid = slave.supports_gtid().upper()
            if support_gtid != 'ON':
                reason = "GTID_MODE=OFF" if support_gtid == 'OFF' \
                    else "not support GTIDs"
                print("# WARNING: Slave '{0}' will be skipped - "
                      "{1}.".format(slave_key, reason))
                print("#")
                del self._slaves[slave_key]
                self._rpl_topology.remove_slave(slave_dict)
                if slave_key == self._base_server_key:
                    reset_base_srv = True
        # At least on slave must have GTIDs enabled.
        if len(self._slaves) == 0:
            raise UtilError("No slaves found with GTID support and "
                            "GTID_MODE=ON.")
        # Reset base server if needed (it must have GTID_MODE=ON).
        if reset_base_srv:
            self._set_base_server()

        # Check the set of executed GTIDs and report differences, only if the
        # master is specified.
        if self._get_master():
            master_gtids = self._get_master().get_gtid_executed()
            slaves_gtids_data = \
                self._rpl_topology.slaves_gtid_subtract_executed(
                    master_gtids, multithreading=True
                )
            print("#\n# GTID differences between Master and Slaves:")
            for host, port, gtids_missing in slaves_gtids_data:
                slave_key = '{0}@{1}'.format(host, port)
                gtid_size = gtid_set_cardinality(gtids_missing)
                if gtid_size:
                    plural = 's' if gtid_size > 1 else ''
                    print("# - Slave '{0}' is {1} transaction{2} behind "
                          "Master.".format(slave_key, gtid_size, plural))
                    if self._verbosity:
                        print("#       Missing GTIDs: "
                              "{0}".format(gtids_missing))
                else:
                    print("# - Slave '{0}' is up-to-date.".format(slave_key))

        print("#")

    @staticmethod
    def _exist_in_obj_list(obj_name, obj_type, obj_list):
        """Check if object (name and type) exists in the given list.

        This function checks if the database object for the specified name and
        type exists in the specified list of database objects.

        obj_name[in]    Name of the object to check.
        obj_type[in]    Type of the object to check.
        obj_list[in]    List of objects to check. It is assumed that the list
                        has the format of the ones returned by the function
                        mysql.utilities.command.dbcompare.get_common_objects().
                        More precisely with the format:
                        [(obj_type1, (obj_name1,))..(obj_typeN, (obj_nameN,))]

        Returns a boolean value indicating if object with the specified name
        and type exists in the specified list of objects.
        """
        for obj_row in obj_list:
            if obj_row[0] == obj_type and obj_row[1][0] == obj_name:
                return True
        return False

    def _split_active_slaves(self, slaves):
        """Get the list of slaves with replication running and not.

        This method separates the list of given slaves into active (with the
        IO and SQL thread running) and non active slaves (with one of the
        threads stopped).

        slaves[in]      List of target slaves to separate.

        Returns a tuple with two elements, first with the list of active slaves
        and the second with the list of not active ones.
        """
        # Get slaves status.
        slaves_state = self._apply_for_all_slaves(slaves, 'get_slaves_errors',
                                                  multithreading=True)

        # Store IO and SQL thread status.
        active_slaves = []
        not_active_slaves = []
        for slave_key, state in slaves_state:
            # Locally store IO and SQL threads status.
            io_running = state[3].upper() == 'YES'
            self._slaves[slave_key]['IO_Running'] = io_running
            sql_running = state[4].upper() == 'YES'
            self._slaves[slave_key]['SQL_Running'] = sql_running
            if io_running and sql_running:
                active_slaves.append(slave_key)
            else:
                not_active_slaves.append(slave_key)
                print("#   WARNING: Slave not active '{0}' - "
                      "Sync skipped.".format(slave_key))
                if self._verbosity:
                    # Print warning if slave is stopped due to an error.
                    if not io_running and state[2]:
                        print("#    - IO thread stopped: ERROR {0} - "
                              "{1}".format(state[1], state[2]))
                    if not sql_running and state[6]:
                        print("#    - SQL thread stopped: ERROR {0} - "
                              "{1}".format(state[5], state[6]))

        # Return separated list of active and non active replication slaves.
        return active_slaves, not_active_slaves

    def _compute_sync_point(self, active_slaves=None, master_uuid=None):
        """Compute the GTID synchronization point.

        This method computes the GTID synchronization point based based on the
        GTID_EXECUTED set. If a master is available for synchronization the
        last GTID from the GTID_EXECUTED set is used as sync point  If no
        master is available the union of the GTID_EXECUTED sets among all
        active slaves is used as the sync point.

        active_slaves[in]   List of active slaves to consider. Only required
                            if the master is not available. It is assumed
                            that the list is composed by strings with the
                            format 'host@port', identifying each slave.
        master_uuid[in]     UUID of the master server used to compute its last
                            GTID (sync point). If not provided it is
                            determined, but can lead to issues for servers
                            >= 5.7.6 if specific tables are locked previously.

        Return a GTID set representing to synchronization point (to wait for
        slaves to catch up and stop).
        """
        if self._get_master():
            gtid_set = self._get_master().get_gtid_executed()
            master_uuid = master_uuid if master_uuid \
                else self._get_master().get_server_uuid()
            return get_last_server_gtid(gtid_set, master_uuid)
        else:
            # Get GTID_EXECUTED on all slaves.
            all_gtid_executed = self._apply_for_all_slaves(
                active_slaves, 'get_gtid_executed', multithreading=True
            )

            # Compute the union of all GTID sets for each UUID among slaves.
            gtid_sets_by_uuid = {}
            for _, gtid_executed in all_gtid_executed:
                gtids_list = gtid_executed.split("\n")
                for gtid in gtids_list:
                    gtid_set = gtid.rstrip(', ')
                    uuid = gtid_set.split(':')[0]
                    if uuid not in gtid_sets_by_uuid:
                        gtid_sets_by_uuid[uuid] = gtid_set
                    else:
                        union_set = gtid_set_union(gtid_sets_by_uuid[uuid],
                                                   gtid_set)
                        gtid_sets_by_uuid[uuid] = union_set

            # Return union of all know executed GTID.
            return ",".join(gtid_sets_by_uuid.itervalues())

    def _sync_slaves(self, slaves, gtid):
        """Set synchronization point (specified GTID set) for the given slaves.

        The method set the synchronization point for the given slaves by
        (concurrently) stopping and immediately executing START SLAVE UNTIL
        on all given slaves in order to stop upon reaching the given GTID set
        (i.e., committing all corresponding transactions for the given GTID
        sync point).

        slaves[in]      List of target slaves to synchronize (i.e., instruct
                        to stop upon reaching the synchronization point).
        gtid[in]        GTID set used as the synchronization point.
        """
        # Make running slaves stop until sync point (GTID) is reached.
        if self._verbosity:
            print("#   Setting data synchronization point for slaves.")
        # STOP slave (only SQL thread).
        self._apply_for_all_slaves(slaves, 'stop_sql_thread',
                                   multithreading=True)
        # START slave UNTIL sync point is reached.
        # Note: Only the SQL thread is stopped when the condition is reached.
        until_ops = {'until_gtid_set': gtid, 'sql_after_gtid': True,
                     'only_sql_thread': True}
        self._apply_for_all_slaves(slaves, 'start', (), until_ops,
                                   multithreading=True)

    def _checksum_and_resume_rpl(self, not_sync_slaves, sync_slave, table):
        """Checksum table and resume replication on slaves.

        This method computes (concurrently) the table checksum of the given
        slaves lists (those synced and not synced). For the list of not synced
        slaves the table checksum is immediately computed. For the list of
        synced slaves, first it waits for them to catch up and the sync point
        and only then compute the table checksum and resume replication.

        not_sync_slaves[in] List of not synced slaves.
        sync_slave[in]      List of (previously) synced slaves.
        table[in]           Target table to compute the checksum.

        Returns a list of tuples, each tuple containing the identification of
        the server and the corresponding checksum result.
        """
        if self._verbosity:
            print("#   Compute checksum on slaves (wait to catch up and resume"
                  " replication).")
            sys.stdout.flush()
        not_sync_checksum = []
        if not_sync_slaves:
            not_sync_checksum = self._apply_for_all_slaves(
                not_sync_slaves, 'checksum_table', (table,),
                {'exec_timeout': self._checksum_timeout},
                multithreading=True
            )
        sync_checksum = []
        if sync_slave:
            sync_checksum = self._apply_for_all_slaves(
                sync_slave, 'wait_checksum_and_start', (table,),
                {'wait_timeout': self._rpl_timeout,
                 'wait_interval': self._interval,
                 'checksum_timeout': self._checksum_timeout},
                multithreading=True
            )
        return not_sync_checksum + sync_checksum

    def _check_table_data_sync(self, table, slaves):
        """Check table data synchronization for specified slaves.

        This method check the data consistency for the specified table between
        the base server (master or slave) and the specified salves. This
        operation requires the definition of a "synchronization point" in order
        to ensure that the "supposed" same data is compared between servers.
        This coordination process is based on GTIDs (checking that all data
        until a given GTID has been processed on the slaves). A different
        algorithm is used to set the "synchronization point" depending if the
        master is used or not. The data consistency is checked relying on the
        CHECKSUM TABLE query.

        If an error occur during this process, any locked table must be
        unlocked and both master and slaves should resume their previous
        activity.

        Important note: this method assumes that the table exists on the base
        server and all specified slaves, therefore checking the existence of
        the table as well as other integrity checks (server versions, GTID
        definitions, etc.) need to be performed outside the scope of this
        method.

        table[in]       Qualified name of the table to check (quoted with
                        backticks).
        slaves[in]      List of slaves to check. Each element of the list must
                        be a string with the format 'host@port'.

        Returns the number of data consistency found.
        """
        success = False
        checksum_issues = 0
        # If no master used then add base server (slave) to slaves to sync.
        if not self._get_master():
            slaves = slaves + [self._base_server_key]

        # Separate active from non active slaves.
        active_slaves, not_active_slaves = self._split_active_slaves(slaves)

        if self._get_master():
            # Get uuid of the master server
            master_uuid = self._get_master().get_server_uuid()

            # Lock the table on the master to get GTID synchronization point
            # and perform the table checksum.
            try:
                self._get_master().exec_query(
                    "LOCK TABLES {0} READ".format(table)
                )

                last_exec_gtid = self._compute_sync_point(
                    master_uuid=master_uuid
                )
                if self._verbosity > 2:
                    print("#   Sync point GTID: {0}".format(last_exec_gtid))

                # Immediately instruct active slaves to stop on sync point.
                if active_slaves:
                    self._sync_slaves(active_slaves, last_exec_gtid)

                # Perform table checksum on master.
                base_server_checksum = self._get_master().checksum_table(
                    table, self._checksum_timeout
                )
                if base_server_checksum[0]:
                    success = True  # Successful checksum for base server.
                    if self._verbosity > 2:
                        print("#   Checksum on base server (Master): "
                              "{0}".format(base_server_checksum[0][1]))
                else:
                    print("#   [SKIP] {0} checksum on base server (Master) - "
                          "{1}".format(table, base_server_checksum[1]))
            finally:
                # Unlock table.
                self._get_master().exec_query("UNLOCK TABLES")
        elif active_slaves:
            # Perform sync without master, only based on active slave (if any).
            try:
                # Stop all active slaves to get the GTID synchronization point.
                self._apply_for_all_slaves(
                    active_slaves, 'stop_sql_thread', multithreading=True
                )

                sync_gtids = self._compute_sync_point(active_slaves)
                if self._verbosity > 2:
                    print("#   Sync point GTID: {0}".format(sync_gtids))

                # Instruct active slaves to stop on sync point.
                self._sync_slaves(active_slaves, sync_gtids)

            except UtilError:
                # Try to restart the slaves in case an error occurs.
                self._apply_for_all_slaves(
                    active_slaves, 'star_sql_thread', multithreading=True
                )

        # Compute checksum on all slaves and return to previous state.
        slaves_checksum = self._checksum_and_resume_rpl(not_active_slaves,
                                                        active_slaves, table)

        # Check if checksum for base server was successfully computed.
        if not self._get_master():
            for slave_key, checksum in slaves_checksum:
                if slave_key == self._base_server_key:
                    if checksum[0]:
                        success = True  # Successful checksum for base server.
                        base_server_checksum = checksum
                        slaves_checksum.remove((slave_key, checksum))
                        if self._verbosity > 2:
                            print("#   Checksum on base server: "
                                  "{0}".format(base_server_checksum[0][1]))
                    else:
                        print("#   [SKIP] {0} checksum on base server - "
                              "{1}".format(table, checksum[1]))
                    break

        # Compare checksum and report results.
        if success and slaves_checksum:
            for slave_key, checksum_res in slaves_checksum:
                if checksum_res[0] is None:
                    print("#   [SKIP] {0} checksum for Slave '{1}' - "
                          "{2}.".format(table, slave_key, checksum_res[1]))
                else:
                    if self._verbosity > 2:
                        checksum_val = ': {0}'.format(checksum_res[0][1])
                    else:
                        checksum_val = ''
                    if checksum_res[0] != base_server_checksum[0]:
                        print("#   [DIFF] {0} checksum for server '{1}'"
                              "{2}.".format(table, slave_key, checksum_val))
                        checksum_issues += 1
                    else:
                        print("#   [OK] {0} checksum for server '{1}'"
                              "{2}.".format(table, slave_key, checksum_val))

        return checksum_issues

    def check_data_sync(self, options, data_to_include, data_to_exclude):
        """Check data synchronization.

        Check if the data (in all tables) is in sync between the checked
        servers (master and its slaves, or only slaves). It reports structure
        difference database/tables missing or with a different definition and
        data differences between a base server and the others.

        Note: A different algorithm is applied to perform the synchronization,
        depending if the master is specified (available) or not.

        options[in]         Dictionary of options.
        data_to_include[in] Dictionary of data (set of tables) by database to
                            check.
        data_to_exclude[in] Dictionary of data (set of tables) by database to
                            exclude from check.

        Returns the number of consistency issues found (comparing database
        definitions and data).
        """
        issues_count = 0

        # Skip all database objects, except tables.
        options['skip_views'] = True
        options['skip_triggers'] = True
        options['skip_procs'] = True
        options['skip_funcs'] = True
        options['skip_events'] = True
        options['skip_grants'] = True

        diff_options = {}
        diff_options.update(options)
        diff_options['quiet'] = True  # Do not print messages.
        diff_options['suppress_sql'] = True  # Do not print SQL statements.
        diff_options['skip_table_opts'] = True  # Ignore AUTO_INCREMENT diffs.

        # Check the server version requirement to support sync features.
        # Slave servers of version >= 5.6.14 are required due to a known issue
        # for START SLAVE UNTIL with the SQL_AFTER_GTIDS option. More info:
        # https://dev.mysql.com/doc/refman/5.6/en/start-slave.html
        for slave_key in self._slaves:
            if not self._get_slave(slave_key).check_version_compat(5, 6, 14):
                raise UtilError(
                    "Server '{0}' version must be 5.6.14 or greater. Sync is "
                    "not supported for versions prior to 5.6.14 due to a "
                    "known issue with START SLAVE UNTIL and the "
                    "SQL_AFTER_GTIDS option.".format(slave_key))

        print("# Checking data consistency.\n#")
        base_srv_type = 'Master' if self._get_master() else 'Slave'
        print("# Using {0} '{1}' as base server for comparison."
              "".format(base_srv_type, self._base_server_key))

        # Get all databases from the base server.
        db_rows = self._base_server.get_all_databases()
        base_server_dbs = set([row[0] for row in db_rows])

        # Process databases to include/exclude from check.
        db_to_include = set()
        if data_to_include:
            db_to_include = set([db for db in data_to_include])
            base_server_dbs = base_server_dbs & db_to_include
            not_exist_db = db_to_include - base_server_dbs
            if not_exist_db:
                plurals = ('s', '') if len(not_exist_db) > 1 else ('', 'es')
                print('# WARNING: specified database{0} to check do{1} not '
                      'exist on base server and will be skipped: '
                      '{2}.'.format(plurals[0], plurals[1],
                                    ", ".join(not_exist_db)))
        db_to_exclude = set()
        if data_to_exclude:
            db_to_exclude = set(
                [db for db in data_to_exclude if not data_to_exclude[db]]
            )
            base_server_dbs = base_server_dbs - db_to_exclude

        # Check databases on slaves (except the base server).
        slaves_except_base = [key for key in self._slaves
                              if key != self._base_server_key]
        for slave_key in slaves_except_base:
            slave = self._get_slave(slave_key)
            db_rows = slave.get_all_databases()
            slave_dbs = set([row[0] for row in db_rows])
            # Process databases to include/exclude.
            if db_to_include:
                slave_dbs = slave_dbs & db_to_include
            if db_to_exclude:
                slave_dbs = slave_dbs - db_to_exclude
            # Add slave databases set to internal state.
            self._slaves[slave_key]['databases'] = slave_dbs
            # Report databases not on base server and filtered by replication.
            dbs_not_in_base_srv = slave_dbs - base_server_dbs
            filtered_dbs = set(
                [db for db in dbs_not_in_base_srv
                 if self._is_rpl_filtered(db, slave=self._base_server_key)]
            )
            dbs_not_in_base_srv -= filtered_dbs
            for db in filtered_dbs:
                print("# [SKIP] Database '{0}' - filtered by replication "
                      "rule on base server.".format(db))
            if dbs_not_in_base_srv:
                issues_count += len(dbs_not_in_base_srv)
                plural = 's' if len(dbs_not_in_base_srv) > 1 else ''
                print("# [DIFF] Database{0} NOT on base server but found on "
                      "'{1}': {2}".format(plural, slave_key,
                                          ",".join(dbs_not_in_base_srv)))

        # Determine server to check base replication filtering options.
        filter_srv = None if self._get_master() else self._base_server_key

        # Check data consistency for each table on the base server.
        # pylint: disable=R0101
        for db_name in base_server_dbs:
            # Skip database if filtered by defined replication rules.
            if self._is_rpl_filtered(db_name, slave=filter_srv):
                print("# [SKIP] Database '{0}' check - filtered by "
                      "replication rule.".format(db_name))
                continue
            print("# Checking '{0}' database...".format(db_name))
            slaves_to_check = {}
            # Check if database exists on slaves (except the base server).
            for slave_key in slaves_except_base:
                # Skip database if filtered by defined replication rules.
                if self._is_rpl_filtered(db_name, slave=slave_key):
                    print("# [SKIP] Database '{0}' check for '{1}' - filtered "
                          "by replication rule.".format(db_name, slave_key))
                    continue
                if db_name in self._slaves[slave_key]['databases']:
                    # Store slave database instance and common objects.
                    slave_db = Database(self._get_slave(slave_key), db_name,
                                        options)
                    slave_db.init()
                    slave_dic = {'db': slave_db}
                    in_both, in_basesrv, not_in_basesrv = get_common_objects(
                        self._base_server, self._get_slave(slave_key),
                        db_name, db_name, False, options)
                    # Process tables to include/exclude from check (on slaves).
                    if (data_to_include and db_name in data_to_include and
                            data_to_include[db_name]):
                        in_both = [
                            obj_row for obj_row in in_both
                            if obj_row[1][0] in data_to_include[db_name]
                        ]
                        in_basesrv = [
                            obj_row for obj_row in in_basesrv
                            if obj_row[1][0] in data_to_include[db_name]
                        ]
                        not_in_basesrv = [
                            obj_row for obj_row in not_in_basesrv
                            if obj_row[1][0] in data_to_include[db_name]
                        ]
                    if (data_to_exclude and db_name in data_to_exclude and
                            data_to_exclude[db_name]):
                        in_both = [
                            obj_row for obj_row in in_both
                            if obj_row[1][0] not in data_to_exclude[db_name]
                        ]
                        in_basesrv = [
                            obj_row for obj_row in in_basesrv
                            if obj_row[1][0] not in data_to_exclude[db_name]
                        ]
                        not_in_basesrv = [
                            obj_row for obj_row in not_in_basesrv
                            if obj_row[1][0] not in data_to_exclude[db_name]
                        ]
                    slave_dic['in_both'] = in_both
                    slave_dic['in_basesrv'] = in_basesrv
                    slaves_to_check[slave_key] = slave_dic
                    # Report tables not on base server and filtered by
                    # replication.
                    tbls_not_in = set(
                        [obj_row[1][0] for obj_row in not_in_basesrv
                         if obj_row[0] == 'TABLE']
                    )
                    filtered_tbls = set(
                        [tbl for tbl in tbls_not_in if self._is_rpl_filtered(
                            db_name, tbl_name=tbl, slave=self._base_server_key
                        )]
                    )
                    tbls_not_in -= filtered_tbls
                    for tbl in filtered_tbls:
                        print("# [SKIP] Table '{0}' - filtered by replication "
                              "rule on base server.".format(tbl))
                    if tbls_not_in:
                        plural = 's' if len(tbls_not_in) > 1 else ''
                        print("#   [DIFF] Table{0} NOT on base server but "
                              "found on '{1}': "
                              "{2}".format(plural, slave_key,
                                           ", ".join(tbls_not_in)))
                        issues_count += len(tbls_not_in)
                else:
                    print("#   [DIFF] Database '{0}' NOT on server "
                          "'{1}'.".format(db_name, slave_key))
                    issues_count += 1
            # Only check database if at least one slave has it.
            if slaves_to_check:
                db = Database(self._base_server, db_name, options)
                db.init()
                for db_obj in db.get_next_object():
                    obj_type = db_obj[0]
                    obj_name = db_obj[1][0]
                    # Process tables to include/exclude from check (on base
                    # server).
                    if (data_to_include and db_name in data_to_include and
                            data_to_include[db_name] and
                            obj_name not in data_to_include[db_name]):
                        # Skip to the next object if not in data to include.
                        continue
                    if (data_to_exclude and db_name in data_to_exclude and
                            data_to_exclude[db_name] and
                            obj_name in data_to_exclude[db_name]):
                        # Skip to the next object if in data to exclude.
                        continue
                    checksum_task = []
                    # Check object data on all valid slaves.
                    for slave_key in slaves_to_check:
                        # Skip table if filtered by defined replication rules.
                        if (obj_type == 'TABLE' and
                                self._is_rpl_filtered(db_name, obj_name,
                                                      slave=slave_key)):
                            print("# [SKIP] Table '{0}' check for '{1}' - "
                                  "filtered by replication rule."
                                  "".format(obj_name, slave_key))
                            continue
                        slave_dic = slaves_to_check[slave_key]
                        # Check if object does not exist on Slave.
                        if self._exist_in_obj_list(obj_name, obj_type,
                                                   slave_dic['in_basesrv']):
                            print("#   [DIFF] {0} '{1}.{2}' NOT on server "
                                  "'{3}'.".format(obj_type.capitalize(),
                                                  db_name, obj_name,
                                                  slave_key))
                            issues_count += 1
                            continue

                        # Quote object name with backticks.
                        q_obj = '{0}.{1}'.format(
                            quote_with_backticks(db_name, db.sql_mode),
                            quote_with_backticks(obj_name, db.sql_mode)
                        )

                        # Check object definition.
                        def_diff = diff_objects(
                            self._base_server, self._get_slave(slave_key),
                            q_obj, q_obj, diff_options, obj_type
                        )
                        if def_diff:
                            print("#   [DIFF] {0} {1} definition is "
                                  "different on '{2}'."
                                  "".format(obj_type.capitalize(), q_obj,
                                            slave_key))
                            issues_count += 1
                            if self._verbosity:
                                for diff in def_diff[3:]:
                                    print("#       {0}".format(diff))
                            continue

                        # Add slave to table checksum task.
                        checksum_task.append(slave_key)

                    # Perform table checksum on valid slaves.
                    if checksum_task and obj_type == 'TABLE':
                        print("# - Checking '{0}' table data..."
                              "".format(obj_name))
                        num_issues = self._check_table_data_sync(q_obj,
                                                                 checksum_task)
                        issues_count += num_issues

        print("#\n#...done.\n#")
        str_issues_count = 'No' if issues_count == 0 else str(issues_count)
        plural = 's' if issues_count > 1 else ''
        print("# SUMMARY: {0} data consistency issue{1} found.\n"
              "#".format(str_issues_count, plural))
        return issues_count
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains an abstraction of a MySQL server object used
by multiple utilities. It also contains helper methods for common
server operations used in multiple utilities.
"""

import os
import re
import socket
import string
import subprocess
import tempfile
import threading
import logging

import mysql.connector
from mysql.connector.constants import ClientFlag

from mysql.connector.errorcode import CR_SERVER_LOST


_FOREIGN_KEY_SET = "SET foreign_key_checks = {0}"
_AUTOCOMMIT_SET = "SET AUTOCOMMIT = {0}"
_GTID_ERROR = ("The server %s:%s does not comply to the latest GTID "
               "feature support. Errors:")


def tostr(value):
    """Cast value to str except when None

    value[in]          Value to be cast to str

    Returns value as str instance or None.
    """
    return None if value is None else str(value)


class MySQLUtilsCursorRaw(mysql.connector.cursor.MySQLCursorRaw):
    """
    Cursor for Connector/Python v2.0, returning str instead of bytearray
    """
    def fetchone(self):
        row = self._fetch_row()
        if row:
            return tuple([tostr(v) for v in row])
        return None

    def fetchall(self):
        rows = []
        all_rows = super(MySQLUtilsCursorRaw, self).fetchall()
        for row in all_rows:
            rows.append(tuple([tostr(v) for v in row]))
        return rows


class MySQLUtilsCursorBufferedRaw(
        mysql.connector.cursor.MySQLCursorBufferedRaw):
    """
    Cursor for Connector/Python v2.0, returning str instead of bytearray
    """
    def fetchone(self):
        row = self._fetch_row()
        if row:
            return tuple([tostr(v) for v in row])
        return None

    def fetchall(self):
        if self._rows is None:
            raise mysql.connector.InterfaceError(
                "No result set to fetch from."
            )

        rows = []
        all_rows = [r for r in self._rows[self._next_row:]]
        for row in all_rows:
            rows.append(tuple([tostr(v) for v in row]))
        return rows


def get_connection_dictionary(conn_info, ssl_dict=None):
    """Get the connection dictionary.

    The method accepts one of the following types for conn_info:

        - dictionary containing connection information including:
          (user, passwd, host, port, socket)
        - connection string in the form: user:pass@host:port:socket or
                                         login-path:port:socket
        - an instance of the Server class

    conn_info[in]          Connection information
    ssl_dict[in]           A dictionary with the ssl certificates
                           (ssl_ca, ssl_cert and ssl_key).

    Returns dict - dictionary for connection (user, passwd, host, port, socket)
    """
    if conn_info is None:
        return conn_info

    conn_val = {}
    if isinstance(conn_info, dict) and 'host' in conn_info:
        # Not update conn_info if already has any ssl certificate.
        if (ssl_dict is not None and
                not (conn_info.get("ssl_ca", None) or
                     conn_info.get("ssl_cert", None) or
                     conn_info.get("ssl_key", None) or
                     conn_info.get("ssl", None))):
            conn_info.update(ssl_dict)
        conn_val = conn_info
    elif isinstance(conn_info, Server):
        # get server's dictionary
        conn_val = conn_info.get_connection_values()
    elif isinstance(conn_info, basestring):
        # parse the string
        conn_val = parse_connection(conn_info, options=ssl_dict)
    else:
        raise ConnectionValuesError("Cannot determine connection information"
                                    " type.")

    return conn_val


def set_ssl_opts_in_connection_info(ssl_opts, connection_info):
    """Sets the ssl options in a connection information to be used with C/py.

    ssl_opts[in]           A dictionary with the ssl options (ssl_ca, ssl_cert
                           and ssl_key).
    connection_info[out]   A dictionary to set the ssl options after validate
                           them.

    The ssl options will be set on the connection_info if they are not None.
    In addition the SSL client flags are added if at least one ssl option is
    set.
    """
    # Add SSL parameters ONLY if they are not None
    add_ssl_flag = False
    if ssl_opts.get('ssl_ca') is not None:
        connection_info['ssl_ca'] = ssl_opts.get('ssl_ca')
        add_ssl_flag = True
    if ssl_opts.get('ssl_cert') is not None:
        connection_info['ssl_cert'] = ssl_opts.get('ssl_cert')
        add_ssl_flag = True
    if ssl_opts.get('ssl_key') is not None:
        connection_info['ssl_key'] = ssl_opts.get('ssl_key')
        add_ssl_flag = True
    if ssl_opts.get('ssl'):
        add_ssl_flag = True

    # When at least one of cert, key or ssl options are specified, the ca
    # option is not required for establishing the encrypted connection,
    # but C/py will not allow the None value for the ca option, so we use an
    # empty string i.e '' to avoid an error from C/py about ca option being
    # the None value.
    if ('ssl_cert' in connection_info.keys() or
            'ssl_key' in connection_info.keys() or
            ssl_opts.get('ssl')) and \
            'ssl_ca' not in connection_info.keys():
        connection_info['ssl_ca'] = ''

    # The ca certificate is verified only if the ssl option is also specified.
    if ssl_opts.get('ssl') and connection_info['ssl_ca']:
        connection_info['ssl_verify_cert'] = True

    if add_ssl_flag:
        cpy_flags = [ClientFlag.SSL,
                     ClientFlag.SSL_VERIFY_SERVER_CERT]
        connection_info['client_flags'] = cpy_flags


def _print_connection(prefix, conn_info):
    """Print connection information

    The method accepts one of the following types for conn_info:

        - dictionary containing connection information including:
          (user, passwd, host, port, socket)
        - connection string in the form: user:pass@host:port:socket or
                                         login-path:port:socket
        - an instance of the Server class

    conn_info[in]          Connection information
    """
    conn_val = get_connection_dictionary(conn_info)
    print "# %s on %s: ..." % (prefix, conn_val["host"]),


def get_local_servers(all_proc=False, start=3306, end=3333,
                      datadir_prefix=None):
    """Check to see if there are any servers running on the local host.

    This method attempts to locate all running servers. If provided, it will
    also limit the search to specific ports of datadirectory prefixes.

    This method uses ps for posix systems and netstat for Windows machines
    to determine the list of running servers.

    For posix, it matches on the datadir and if datadir is the path for the
    test directory, the server will be added to the list.

    For nt, it matches on the port in the range starting_port,
    starting_port + 10.

    all_proc[in]        If True, find all processes else only user processes
    start[in]           For Windows/NT systems: Starting port value to search.
                        Default = 3306
    end[in]             For Windows/NT systems: Ending port value to search.
                        Default = 3333
    datadir_prefix[in]  For posix systems, if not None, find only those servers
                        whose datadir starts with this prefix.

    Returns list - tuples of the form: (process_id, [datadir|port])
    """
    processes = []
    if os.name == "posix":
        tmp_file = tempfile.TemporaryFile()
        if all_proc:
            subprocess.call(["ps", "-A"], stdout=tmp_file)
        else:
            subprocess.call(["ps", "-f"], stdout=tmp_file)
        tmp_file.seek(0)
        for line in tmp_file.readlines():
            mysqld_safe = False
            mysqld = False
            datadir = False
            grep = False
            datadir_arg = ""
            proginfo = string.split(line)
            for arg in proginfo:
                if "datadir" in arg:
                    datadir = True
                    datadir_arg = arg
                if "mysqld" in arg:
                    mysqld = True
                if "mysqld_safe" in arg:
                    mysqld_safe = True
                if "grep" in arg:
                    grep = True
            # Check to see if this is a mysqld server and not mysqld_safe proc
            if ((mysqld and datadir) or (mysqld and not grep)) and \
               not mysqld_safe:
                # If provided, check datadir prefix
                if all_proc:
                    proc_id = proginfo[0]
                else:
                    proc_id = proginfo[1]
                if datadir_prefix is not None:
                    if datadir_prefix in datadir_arg:
                        processes.append((proc_id, datadir_arg[10:]))
                else:
                    processes.append((proc_id, datadir_arg[10:]))
    elif os.name == "nt":
        f_out = open("portlist", 'w+')
        execute_script("netstat -anop tcp", "portlist")
        f_out = open("portlist", 'r')
        for line in f_out.readlines():
            proginfo = string.split(line)
            if proginfo:
                # Look for port on either local or foreign address
                port = proginfo[1][proginfo[1].find(":") + 1:]
                if proginfo[1][0] == '0' and port.isdigit():
                    if int(port) >= int(start) and int(port) <= int(end):
                        processes.append((proginfo[4], port))
                        break
                if len(proginfo) > 2:
                    port = proginfo[2][proginfo[2].find(":") + 1:]
                    if port.isdigit() and \
                       int(port) >= int(start) and int(port) <= int(end):
                        processes.append((proginfo[4], port))
                        break
        f_out.close()
        os.unlink("portlist")
    return processes


def get_server(name, values, quiet, verbose=False):
    """Connect to a server and return Server instance

    If the name is 'master' or 'slave', the connection will be made via the
    Master or Slave class else a normal Server class shall be used.

    name[in]        Name of the server.
    values[in]      Dictionary of connection values.
    quiet[in]       If True, do not print messages.
    verbose[in]     Verbose value used by the returned server instances.
                    By default False.

    Returns Server class instance
    """
    from mysql.utilities.common.replication import Master, Slave

    server_conn = None

    # Try to connect to the MySQL database server.
    if not quiet:
        _print_connection(name, values)

    server_options = {
        'conn_info': values,
        'role': name,
        'verbose': verbose,
    }
    if name.lower() == 'master':
        server_conn = Master(server_options)
    elif name.lower() == 'slave':
        # pylint: disable=R0204
        server_conn = Slave(server_options)
    else:
        # pylint: disable=R0204
        server_conn = Server(server_options)
    try:
        server_conn.connect()
    except:
        if not quiet:
            print("")
        raise

    return server_conn


def _require_version(server, version):
    """Check version of server

    server[in]         Server instance
    version[in]        minimal version of the server required

    Returns boolean - True = version Ok, False = version < required
    """
    if version is not None and server is not None:
        major, minor, rel = version.split(".")
        if not server.check_version_compat(major, minor, rel):
            return False
    return True


def get_server_state(server, host, pingtime=3, verbose=False):
    """Return the state of the server.

    This method returns one of the following states based on the
    criteria shown.

      UP   - server is connected
      WARN - server is not connected but can be pinged
      DOWN - server cannot be pinged nor is connected

    server[in]     Server class instance
    host[in]       host name to ping if server is not connected
    pingtime[in]   timeout in seconds for ping operation
                   Default = 3 seconds
    verbose[in]    if True, show ping status messages
                   Default = False

    Returns string - state
    """
    if verbose:
        print "# Attempting to contact %s ..." % host,
    if server is not None and server.is_alive():
        if verbose:
            print "Success"
        return "UP"
    elif ping_host(host, pingtime):
        if verbose:
            print "Server is reachable"
        return "WARN"
    if verbose:
        print "FAIL"
    return "DOWN"


def connect_servers(src_val, dest_val, options=None):
    """Connect to a source and destination server.

    This method takes two groups of --server=user:password@host:port:socket
    values and attempts to connect one as a source connection and the other
    as the destination connection. If the source and destination are the
    same server and the unique parameter is False, destination is set to None.

    The method accepts one of the following types for the src_val and dest_val:

        - dictionary containing connection information including:
          (user, passwd, host, port, socket)
        - connection string in the form: user:pass@host:port:socket or
                                         login-path:port:socket or
                                         config-path[group]
        - an instance of the Server class

    src_val[in]        source connection information
    dest_val[in]       destination connection information
    options[in]        options to control behavior:
        quiet          do not print any information during the operation
                       (default is False)
        version        if specified (default is None), perform version
                       checking and fail if server version is < version
                       specified - an exception is raised
        src_name       name to use for source server
                       (default is "Source")
        dest_name      name to use for destination server
                       (default is "Destination")
        unique         if True, servers must be different when dest_val is
                       not None (default is False)
        verbose        Verbose value used by the returned server instances
                       (default is False).

    Returns tuple (source, destination) where
            source = connection to source server
            destination = connection to destination server (set to None)
                          if source and destination are same server
            if error, returns (None, None)
    """
    if options is None:
        options = {}
    quiet = options.get("quiet", False)
    src_name = options.get("src_name", "Source")
    dest_name = options.get("dest_name", "Destination")
    version = options.get("version", None)
    charset = options.get("charset", None)
    verbose = options.get('verbose', False)

    ssl_dict = {}
    if options.get("ssl_cert", None) is not None:
        ssl_dict['ssl_cert'] = options.get("ssl_cert")
    if options.get("ssl_ca", None) is not None:
        ssl_dict['ssl_ca'] = options.get("ssl_ca", None)
    if options.get("ssl_key", None) is not None:
        ssl_dict['ssl_key'] = options.get("ssl_key", None)
    if options.get("ssl", None) is not None:
        ssl_dict['ssl'] = options.get("ssl", None)

    source = None
    destination = None

    # Get connection dictionaries
    src_dict = get_connection_dictionary(src_val, ssl_dict)
    if "]" in src_dict['host']:
        src_dict['host'] = clean_IPv6(src_dict['host'])
    dest_dict = get_connection_dictionary(dest_val)
    if dest_dict and "]" in dest_dict['host']:
        dest_dict['host'] = clean_IPv6(dest_dict['host'])

    # Add character set
    if src_dict and charset:
        src_dict["charset"] = charset
    if dest_dict and charset:
        dest_dict["charset"] = charset

    # Check for uniqueness - dictionary
    if options.get("unique", False) and dest_dict is not None:
        dupes = False
        if "unix_socket" in src_dict and "unix_socket" in dest_dict:
            dupes = (src_dict["unix_socket"] == dest_dict["unix_socket"])
        else:
            dupes = (src_dict["port"] == dest_dict["port"]) and \
                    (src_dict["host"] == dest_dict["host"])
        if dupes:
            raise UtilError("You must specify two different servers "
                            "for the operation.")

    # If we're cloning so use same server for faster copy
    cloning = dest_dict is None or (src_dict == dest_dict)

    # Connect to the source server and check version
    if isinstance(src_val, Server):
        source = src_val
    else:
        source = get_server(src_name, src_dict, quiet, verbose=verbose)
        if not quiet:
            print "connected."
    if not _require_version(source, version):
        raise UtilError("The %s version is incompatible. Utility "
                        "requires version %s or higher." %
                        (src_name, version))

    # If not cloning, connect to the destination server and check version
    if not cloning:
        if isinstance(dest_val, Server):
            destination = dest_val
        else:
            destination = get_server(dest_name, dest_dict, quiet,
                                     verbose=verbose)
            if not quiet:
                print "connected."
        if not _require_version(destination, version):
            raise UtilError("The %s version is incompatible. Utility "
                            "requires version %s or higher." %
                            (dest_name, version))
    elif not quiet and dest_dict is not None and \
            not isinstance(dest_val, Server):
        try:
            _print_connection(dest_name, src_dict)
            print "connected."
        except:
            print("")
            raise
    return (source, destination)


def test_connect(conn_info, throw_errors=False, ssl_dict=None):
    """Test connection to a server.

    The method accepts one of the following types for conn_info:

        - dictionary containing connection information including:
          (user, passwd, host, port, socket)
        - connection string in the form: user:pass@host:port:socket or
                                         login-path:port:socket or
                                         config-path[group]
        - an instance of the Server class

    conn_info[in]          Connection information

    throw_errors           throw any errors found during the test,
                           false by default.
    ssl_dict[in]           A dictionary with the ssl certificates
                           (ssl_ca, ssl_cert and ssl_key).

    Returns True if connection success, False if error
    """
    # Parse source connection values
    try:
        src_val = get_connection_dictionary(conn_info, ssl_dict)
    except Exception as err:
        raise ConnectionValuesError("Server connection values invalid: {0}."
                                    "".format(err))
    try:
        conn_options = {
            'quiet': True,
            'src_name': "test",
            'dest_name': None,
        }
        s = connect_servers(src_val, None, conn_options)
        s[0].disconnect()
    except UtilError:
        if throw_errors:
            raise
        return False
    return True


def get_port(server1_vals):
    """Get the port for a connection using a socket.

    This method attempts to connect to a server to retrieve
    its port. It is used to try and update local connection
    values with a valid port number for servers connected
    via a socket.

    server1_vals[in]   connection dictionary for server1

    Returns string - port for server or None if cannot connect
                     or server is not connected via socket
    """
    socket = server1_vals.get('unix_socket', None)
    if socket:
        try:
            server1 = Server({'conn_info': server1_vals})
            server1.connect()
            port = server1.port
            server1.disconnect()
            return port
        except:
            pass
    return None


def check_hostname_alias(server1_vals, server2_vals):
    """Check to see if the servers are the same machine by host name.

    This method will attempt to compare two servers to see
    if they are the same host and port. However, if either is
    using a unix socket, it will connect to the server and attempt
    so that the port is updated.

    server1_vals[in]   connection dictionary for server1
    server2_vals[in]   connection dictionary for server2

    Returns bool - true = server1 and server2 are the same host
    """
    server1 = Server({'conn_info': server1_vals})
    server2 = Server({'conn_info': server2_vals})
    server1_socket = server1_vals.get('unix_socket', None)
    server2_socket = server1_vals.get('unix_socket', None)
    if server1_socket:
        server1.connect()
        server1.disconnect()
    if server2_socket:
        server2.connect()
        server2.disconnect()

    return (server1.is_alias(server2.host) and
            int(server1.port) == int(server2.port))


def stop_running_server(server, wait=10, drop=True):
    """Stop a running server.

    This method will stop a server using the mysqladmin utility to
    shutdown the server. It also destroys the datadir.

    server[in]          Server instance to clone
    wait[in]            Number of wait cycles for shutdown
                        default = 10
    drop[in]            If True, drop datadir

    Returns - True = server shutdown, False - unknown state or error
    """
    # Nothing to do if server is None
    if server is None:
        return True

    # Build the shutdown command
    res = server.show_server_variable("basedir")
    mysqladmin_client = "mysqladmin"
    if os.name != 'posix':
        mysqladmin_client = "mysqladmin.exe"
    mysqladmin_path = os.path.normpath(os.path.join(res[0][1], "bin",
                                                    mysqladmin_client))
    if not os.path.exists(mysqladmin_path):
        mysqladmin_path = os.path.normpath(os.path.join(res[0][1], "client",
                                                        mysqladmin_client))
    if not os.path.exists(mysqladmin_path) and os.name != 'posix':
        mysqladmin_path = os.path.normpath(os.path.join(res[0][1],
                                                        "client/debug",
                                                        mysqladmin_client))
    if not os.path.exists(mysqladmin_path) and os.name != 'posix':
        mysqladmin_path = os.path.normpath(os.path.join(res[0][1],
                                                        "client/release",
                                                        mysqladmin_client))
    if os.name == 'posix':
        cmd = "'{0}'".format(mysqladmin_path)
    else:
        cmd = '"{0}"'.format(mysqladmin_path)
    if server.socket is None and server.host == 'localhost':
        server.host = '127.0.0.1'
    cmd = "{0} shutdown --user={1} --host={2} ".format(cmd, server.user,
                                                       server.host)
    if server.passwd:
        cmd = "{0} --password={1} ".format(cmd, server.passwd)
    # Use of server socket only works with 'localhost' (not with 127.0.0.1).
    if server.socket and server.host == 'localhost':
        cmd = "{0} --socket={1} ".format(cmd, server.socket)
    else:
        cmd = "{0} --port={1} ".format(cmd, server.port)
    if server.has_ssl:
        if server.ssl_cert is not None:
            cmd = "{0} --ssl-cert={1} ".format(cmd, server.ssl_cert)
        if server.ssl_ca is not None:
            cmd = "{0} --ssl-ca={1} ".format(cmd, server.ssl_ca)
        if server.ssl_key is not None:
            cmd = "{0} --ssl-key={1} ".format(cmd, server.ssl_key)

    res = server.show_server_variable("datadir")
    datadir = os.path.normpath(res[0][1])
    # Kill all connections so shutdown will work correctly
    res = server.exec_query("SHOW PROCESSLIST")
    for row in res:
        if not row[7] or not row[7].upper().startswith("SHOW PROCESS"):
            try:
                server.exec_query("KILL CONNECTION %s" % row[0])
            except UtilDBError:  # Ok to ignore KILL failures
                pass

    # disconnect user
    server.disconnect()

    # Stop the server
    f_null = os.devnull
    f_out = open(f_null, 'w')
    proc = subprocess.Popen(cmd, shell=True,
                            stdout=f_out, stderr=f_out)
    ret_val = proc.wait()
    f_out.close()

    # if shutdown doesn't work, exit.
    if int(ret_val) != 0:
        return False

    # If datadir exists, delete it
    if drop:
        delete_directory(datadir)

    if os.path.exists("cmd.txt"):
        try:
            os.unlink("cmd.txt")
        except:
            pass

    return True


def log_server_version(server, level=logging.INFO):
    """Log server version message.

    This method will log the server version message.
    If no log file is provided it will also print the message to stdout.

    server[in]           Server instance.
    level[in]            Level of message to log. Default = INFO.
    print_version[in]    If True, print the message to stdout. Default = True.
    """
    host_port = "{host}:{port}".format(**get_connection_dictionary(server))
    version_msg = MSG_MYSQL_VERSION.format(server=host_port,
                                           version=server.get_version())
    logging.log(level, version_msg)


class Server(object):
    """The Server class can be used to connect to a running MySQL server.
    The following utilities are provided:

        - Connect to the server
        - Retrieve a server variable
        - Execute a query
        - Return list of all databases
        - Return a list of specific objects for a database
        - Return list of a specific objects for a database
        - Return list of all indexes for a table
        - Read SQL statements from a file and execute
    """

    def __init__(self, options=None):
        """Constructor

        The method accepts one of the following types for options['conn_info']:

            - dictionary containing connection information including:
              (user, passwd, host, port, socket)
            - connection string in the form: user:pass@host:port:socket or
                                             login-path:port:socket
            - an instance of the Server class

        options[in]        options for controlling behavior:
            conn_info      a dictionary containing connection information
                           (user, passwd, host, port, socket)
            role           Name or role of server (e.g., server, master)
            verbose        print extra data during operations (optional)
                           default value = False
            charset        Default character set for the connection.
                           (default None)
        """
        if options is None:
            options = {}

        assert options.get("conn_info") is not None

        self.verbose = options.get("verbose", False)
        self.db_conn = None
        self.host = None
        self.role = options.get("role", "Server")
        self.has_ssl = False
        conn_values = get_connection_dictionary(options.get("conn_info"))
        try:
            self.host = conn_values["host"]
            self.user = conn_values["user"]
            self.passwd = conn_values["passwd"] \
                if "passwd" in conn_values else None
            self.socket = conn_values["unix_socket"] \
                if "unix_socket" in conn_values else None
            self.port = 3306
            if conn_values["port"] is not None:
                self.port = int(conn_values["port"])
            self.charset = options.get("charset",
                                       conn_values.get("charset", None))
            # Optional values
            self.ssl_ca = conn_values.get('ssl_ca', None)
            self.ssl_cert = conn_values.get('ssl_cert', None)
            self.ssl_key = conn_values.get('ssl_key', None)
            self.ssl = conn_values.get('ssl', False)
            if self.ssl_cert or self.ssl_ca or self.ssl_key or self.ssl:
                self.has_ssl = True
        except KeyError:
            raise UtilError("Dictionary format not recognized.")
        self.connect_error = None
        # Set to TRUE when foreign key checks are ON. Check with
        # foreign_key_checks_enabled.
        self.fkeys = None
        self.autocommit = None
        self.read_only = False
        self.aliases = set()
        self.grants_enabled = None
        self._version = None

    @classmethod
    def fromServer(cls, server, conn_info=None):
        """ Create a new server instance from an existing one

        Factory method that will allow the creation of a new server instance
        from an existing server.

        server[in]       instance object that must be instance of the Server
                         class or a subclass.
        conn_info[in]    A dictionary with the connection information to
                         connect to the server

        Returns an instance of the calling class as a result.
        """

        if isinstance(server, Server):
            options = {"role": server.role,
                       "verbose": server.verbose,
                       "charset": server.charset}
            if conn_info is not None and isinstance(conn_info, dict):
                options["conn_info"] = conn_info
            else:
                options["conn_info"] = server.get_connection_values()

            return cls(options)
        else:
            raise TypeError("The server argument's type is neither Server nor "
                            "a subclass of Server")

    def is_alive(self):
        """Determine if connection to server is still alive.

        Returns bool - True = alive, False = error or cannot connect.
        """
        res = True
        try:
            if self.db_conn is None:
                res = False
            else:
                # ping and is_connected only work partially, try exec_query
                # to make sure connection is really alive
                retval = self.db_conn.is_connected()
                if retval:
                    self.exec_query("SHOW DATABASES")
                else:
                    res = False
        except:
            res = False
        return res

    def _update_alias(self, ip_or_hostname, suffix_list):
        """Update list of aliases for the given IP or hostname.

        Gets the list of aliases for host *ip_or_hostname*. If any
        of them matches one of the server's aliases, then update
        the list of aliases (self.aliases). It also receives a list (tuple)
        of suffixes that can be ignored when checking if two hostnames are
        the same.

        ip_or_hostname[in] IP or hostname to test.
        suffix_list[in]    Tuple with list of suffixes that can be ignored.

        Returns True if ip_or_hostname is a server alias, otherwise False.
        """
        host_or_ip_aliases = self._get_aliases(ip_or_hostname)
        host_or_ip_aliases.add(ip_or_hostname)

        # Check if any of aliases matches with one the servers's aliases
        common_alias = self.aliases.intersection(host_or_ip_aliases)
        if common_alias:  # There are common aliases, host is the same
            self.aliases.update(host_or_ip_aliases)
            return True
        else:  # Check with and without suffixes
            no_suffix_server_aliases = set()
            no_suffix_host_aliases = set()

            for suffix in suffix_list:
                # Add alias with and without suffix from self.aliases
                for alias in self.aliases:
                    if alias.endswith(suffix):
                        try:
                            host, _ = alias.rsplit('.', 1)
                            no_suffix_host_aliases.add(host)
                        except:
                            pass  # Ok if parts don't split correctly
                    no_suffix_server_aliases.add(alias)
                # Add alias with and without suffix from host_aliases
                for alias in host_or_ip_aliases:
                    if alias.endswith(suffix):
                        try:
                            host, _ = alias.rsplit('.', 1)
                            no_suffix_host_aliases.add(host)
                        except:
                            pass  # Ok if parts don't split correctly
                    no_suffix_host_aliases.add(alias)
            # Check if there is any alias in common
            common_alias = no_suffix_host_aliases.intersection(
                no_suffix_server_aliases)
            if common_alias:  # Same host, so update self.aliases
                self.aliases.update(
                    no_suffix_host_aliases.union(no_suffix_server_aliases)
                )
                return True

        return False

    def _get_aliases(self, host):
        """Gets the aliases for the given host
        """
        aliases = set([clean_IPv6(host)])
        if hostname_is_ip(clean_IPv6(host)):  # IP address
            try:
                my_host = socket.gethostbyaddr(clean_IPv6(host))
                aliases.add(my_host[0])
                # socket.gethostbyname_ex() does not work with ipv6
                if (my_host[0].count(":") >= 1 or
                        my_host[0] != "ip6-localhost"):
                    host_ip = socket.gethostbyname_ex(my_host[0])
                else:
                    addrinfo = socket.getaddrinfo(my_host[0], None)
                    host_ip = ([socket.gethostbyaddr(addrinfo[0][4][0])],
                               [fiveple[4][0] for fiveple in addrinfo],
                               [addrinfo[0][4][0]])
            except (socket.gaierror, socket.herror,
                    socket.error) as err:
                host_ip = ([], [], [])
                if self.verbose:
                    print("WARNING: IP lookup by address failed for {0},"
                          "reason: {1}".format(host, err.strerror))
        else:
            try:
                # server may not really exist.
                host_ip = socket.gethostbyname_ex(host)
            except (socket.gaierror, socket.herror,
                    socket.error) as err:
                if self.verbose:
                    print("WARNING: hostname: {0} may not be reachable, "
                          "reason: {1}".format(host, err.strerror))
                return aliases
            aliases.add(host_ip[0])
            addrinfo = socket.getaddrinfo(host, None)
            local_ip = None
            error = None
            for addr in addrinfo:
                try:
                    local_ip = socket.gethostbyaddr(addr[4][0])
                    break
                except (socket.gaierror, socket.herror,
                        socket.error) as err:
                    error = err

            if local_ip:
                host_ip = ([local_ip[0]],
                           [fiveple[4][0] for fiveple in addrinfo],
                           [addrinfo[0][4][0]])
            else:
                host_ip = ([], [], [])
                if self.verbose:
                    print("WARNING: IP lookup by name failed for {0},"
                          "reason: {1}".format(host, error.strerror))
        aliases.update(set(host_ip[1]))
        aliases.update(set(host_ip[2]))
        return aliases

    def is_alias(self, host_or_ip):
        """Determine if host_or_ip is an alias for this host

        host_or_ip[in] host or IP number to check

        Returns bool - True = host_or_ip is an alias
        """
        # List of possible suffixes
        suffixes = ('.local', '.lan', '.localdomain')

        host_or_ip = clean_IPv6(host_or_ip.lower())

        # for quickness, verify in the existing aliases, if they exist.
        if self.aliases:
            if host_or_ip.lower() in self.aliases:
                return True
            else:
                # get the alias for the given host_or_ip
                return self._update_alias(host_or_ip, suffixes)

        # no previous aliases information
        # First, get the local information
        hostname_ = socket.gethostname()
        try:
            local_info = socket.gethostbyname_ex(hostname_)
            local_aliases = set([local_info[0].lower()])
            # if dotted host name, take first part and use as an alias
            try:
                local_aliases.add(local_info[0].split('.')[0])
            except:
                pass
            local_aliases.update(['127.0.0.1', 'localhost', '::1', '[::1]'])
            local_aliases.update(local_info[1])
            local_aliases.update(local_info[2])
            local_aliases.update(self._get_aliases(hostname_))
        except (socket.herror, socket.gaierror, socket.error) as err:
            if self.verbose:
                print("WARNING: Unable to find aliases for hostname"
                      " '{0}' reason: {1}".format(hostname_, str(err)))
            # Try with the basic local aliases.
            local_aliases = set(['127.0.0.1', 'localhost', '::1', '[::1]'])

        # Get the aliases for this server host
        self.aliases = self._get_aliases(self.host)

        # Check if this server is local
        for host in self.aliases.copy():
            if host in local_aliases:
                # Is local then save the local aliases for future.
                self.aliases.update(local_aliases)
                break
            # Handle special suffixes in hostnames.
            for suffix in suffixes:
                if host.endswith(suffix):
                    # Remove special suffix and attempt to match with local
                    # aliases.
                    host, _ = host.rsplit('.', 1)
                    if host in local_aliases:
                        # Is local then save the local aliases for future.
                        self.aliases.update(local_aliases)
                        break

        # Check if the given host_or_ip is alias of the server host.
        if host_or_ip in self.aliases:
            return True

        # Check if any of the aliases of ip_or_host is also an alias of the
        # host server.
        return self._update_alias(host_or_ip, suffixes)

    def user_host_exists(self, user, host_or_ip):
        """Check to see if a user, host exists

        This method attempts to see if a user name matches the users on the
        server and that any user, host pair can match the host or IP address
        specified. This attempts to resolve wildcard matches.

        user[in]       user name
        host_or_ip[in] host or IP address

        Returns string - host from server that matches the host_or_ip or
                         None if no match.
        """
        res = self.exec_query("SELECT host FROM mysql.user WHERE user = '%s' "
                              "AND '%s' LIKE host " % (user, host_or_ip))
        if res:
            return res[0][0]
        return None

    def get_connection_values(self):
        """Return a dictionary of connection values for the server.

        Returns dictionary
        """
        conn_vals = {
            "user": self.user,
            "host": self.host
        }
        if self.passwd:
            conn_vals["passwd"] = self.passwd
        if self.socket:
            conn_vals["socket"] = self.socket
        if self.port:
            conn_vals["port"] = self.port
        if self.ssl_ca:
            conn_vals["ssl_ca"] = self.ssl_ca
        if self.ssl_cert:
            conn_vals["ssl_cert"] = self.ssl_cert
        if self.ssl_key:
            conn_vals["ssl_key"] = self.ssl_key
        if self.ssl:
            conn_vals["ssl"] = self.ssl

        return conn_vals

    def connect(self, log_version=False):
        """Connect to server

        Attempts to connect to the server as specified by the connection
        parameters.

        log_version[in]      If True, log server version. Default = False.

        Note: This method must be called before executing queries.

        Raises UtilError if error during connect
        """
        try:
            self.db_conn = self.get_connection()
            if log_version:
                log_server_version(self)
            # If no charset provided, get it from the "character_set_client"
            # server variable.
            if not self.charset:
                res = self.show_server_variable('character_set_client')
                self.db_conn.set_charset_collation(charset=res[0][1])
                self.charset = res[0][1]
            if self.ssl:
                res = self.exec_query("SHOW STATUS LIKE 'Ssl_cipher'")
                if res[0][1] == '':
                    raise UtilError("Can not encrypt server connection.")
            # if we connected via a socket, get the port
            if os.name == 'posix' and self.socket:
                res = self.show_server_variable('port')
                if res:
                    self.port = res[0][1]
        except UtilError:
            # Reset any previous value if the connection cannot be established,
            # before raising an exception. This prevents the use of a broken
            # database connection.
            self.db_conn = None
            raise
        self.connect_error = None
        # Valid values are ON and OFF, not boolean.
        self.read_only = self.show_server_variable("READ_ONLY")[0][1] == "ON"

    def get_connection(self):
        """Return a new connection to the server.

        Attempts to connect to the server as specified by the connection
        parameters and returns a connection object.

        Return the resulting MySQL connection object or raises an UtilError if
        an error occurred during the server connection process.
        """
        try:
            parameters = {
                'user': self.user,
                'host': self.host,
                'port': self.port,
            }
            if self.socket and os.name == "posix":
                parameters['unix_socket'] = self.socket
            if self.passwd and self.passwd != "":
                parameters['passwd'] = self.passwd
            if self.charset:
                parameters['charset'] = self.charset
            parameters['host'] = parameters['host'].replace("[", "")
            parameters['host'] = parameters['host'].replace("]", "")

            # Add SSL parameters ONLY if they are not None
            if self.ssl_ca is not None:
                parameters['ssl_ca'] = self.ssl_ca
            if self.ssl_cert is not None:
                parameters['ssl_cert'] = self.ssl_cert
            if self.ssl_key is not None:
                parameters['ssl_key'] = self.ssl_key

            # When at least one of cert, key or ssl options are specified,
            # the ca option is not required for establishing the encrypted
            # connection, but C/py will not allow the None value for the ca
            # option, so we use an empty string i.e '' to avoid an error from
            # C/py about ca option being the None value.
            if ('ssl_cert' in parameters.keys() or
                    'ssl_key' in parameters.keys() or
                    self.ssl) and \
                    'ssl_ca' not in parameters:
                parameters['ssl_ca'] = ''

            # The ca certificate is verified only if the ssl option is also
            # specified.
            if self.ssl and parameters['ssl_ca']:
                parameters['ssl_verify_cert'] = True

            if self.has_ssl:
                cpy_flags = [ClientFlag.SSL, ClientFlag.SSL_VERIFY_SERVER_CERT]
                parameters['client_flags'] = cpy_flags

            db_conn = mysql.connector.connect(**parameters)
            # Return MySQL connection object.
            return db_conn
        except mysql.connector.Error as err:
            raise UtilError(err.msg, err.errno)
        except AttributeError as err:
            raise UtilError(str(err))

    def disconnect(self):
        """Disconnect from the server.
        """
        try:
            self.db_conn.disconnect()
        except:
            pass

    def get_version(self):
        """Return version number of the server.

        Get the server version. The respective instance variable is set with
        the result after querying the server the first time. The version is
        immediately returned when already known, avoiding querying the server
        at each time.

        Returns string - version string or None if error
        """
        # Return the local version value if already known.
        if self._version:
            return self._version

        # Query the server for its version.
        try:
            res = self.show_server_variable("VERSION")
            if res:
                self._version = res[0][1]
        except UtilError:
            # Ignore errors and return _version, initialized with None.
            pass

        return self._version

    def check_version_compat(self, t_major, t_minor, t_rel):
        """Checks version of the server against requested version.

        This method can be used to check for version compatibility.

        t_major[in]        target server version (major)
        t_minor[in]        target server version (minor)
        t_rel[in]          target server version (release)

        Returns bool True if server version is GE (>=) version specified,
                     False if server version is LT (<) version specified
        """
        version_str = self.get_version()
        if version_str is not None:
            match = re.match(r'^(\d+\.\d+(\.\d+)*).*$', version_str.strip())
            if match:
                version = [int(x) for x in match.group(1).split('.')]
                version = (version + [0])[:3]  # Ensure a 3 elements list
                return version >= [int(t_major), int(t_minor), int(t_rel)]
            else:
                return False
        return True

    def exec_query(self, query_str, options=None, exec_timeout=0):
        """Execute a query and return result set

        This is the singular method to execute queries. It should be the only
        method used as it contains critical error code to catch the issue
        with mysql.connector throwing an error on an empty result set.

        Note: will handle exception and print error if query fails

        Note: if fetchall is False, the method returns the cursor instance

        query_str[in]      The query to execute
        options[in]        Options to control behavior:
            params         Parameters for query
            columns        Add column headings as first row
                           (default is False)
            fetch          Execute the fetch as part of the operation and
                           use a buffered cursor
                           (default is True)
            raw            If True, use a buffered raw cursor
                           (default is True)
            commit         Perform a commit (if needed) automatically at the
                           end (default: True).
        exec_timeout[in]   Timeout value in seconds to kill the query execution
                           if exceeded. Value must be greater than zero for
                           this feature to be enabled. By default 0, meaning
                           that the query will not be killed.

        Returns result set or cursor
        """
        if options is None:
            options = {}
        params = options.get('params', ())
        columns = options.get('columns', False)
        fetch = options.get('fetch', True)
        raw = options.get('raw', True)
        do_commit = options.get('commit', True)

        # Guard for connect() prerequisite
        assert self.db_conn, "You must call connect before executing a query."

        # If we are fetching all, we need to use a buffered
        if fetch:
            if raw:
                if mysql.connector.__version_info__ < (2, 0):
                    cur = self.db_conn.cursor(buffered=True, raw=True)
                else:
                    cur = self.db_conn.cursor(
                        cursor_class=MySQLUtilsCursorBufferedRaw)
            else:
                cur = self.db_conn.cursor(buffered=True)
        else:
            if mysql.connector.__version_info__ < (2, 0):
                cur = self.db_conn.cursor(raw=True)
            else:
                cur = self.db_conn.cursor(cursor_class=MySQLUtilsCursorRaw)

        # Execute query, handling parameters.
        q_killer = None
        try:
            if exec_timeout > 0:
                # Spawn thread to kill query if timeout is reached.
                # Note: set it as daemon to avoid waiting for it on exit.
                q_killer = QueryKillerThread(self, query_str, exec_timeout)
                q_killer.daemon = True
                q_killer.start()
            # Execute query.
            if params == ():
                cur.execute(query_str)
            else:
                cur.execute(query_str, params)
        except mysql.connector.Error as err:
            cur.close()
            if err.errno == CR_SERVER_LOST and exec_timeout > 0:
                # If the connection is killed (because the execution timeout is
                # reached), then it attempts to re-establish it (to execute
                # further queries) and raise a specific exception to track this
                # event.
                # CR_SERVER_LOST = Errno 2013 Lost connection to MySQL server
                # during query.
                self.db_conn.reconnect()
                raise UtilError("Timeout executing query", err.errno)
            else:
                raise UtilDBError("Query failed. {0}".format(err))
        except Exception:
            cur.close()
            raise UtilError("Unknown error. Command: {0}".format(query_str))
        finally:
            # Stop query killer thread if alive.
            if q_killer and q_killer.is_alive():
                q_killer.stop()

        # Fetch rows (only if available or fetch = True).
        # pylint: disable=R0101
        if cur.with_rows:
            if fetch or columns:
                try:
                    results = cur.fetchall()
                    if columns:
                        col_headings = cur.column_names
                        col_names = []
                        for col in col_headings:
                            col_names.append(col)
                        # pylint: disable=R0204
                        results = col_names, results
                except mysql.connector.Error as err:
                    raise UtilDBError("Error fetching all query data: "
                                      "{0}".format(err))
                finally:
                    cur.close()
                return results
            else:
                # Return cursor to fetch rows elsewhere (fetch = false).
                return cur
        else:
            # No results (not a SELECT)
            try:
                if do_commit:
                    self.db_conn.commit()
            except mysql.connector.Error as err:
                raise UtilDBError("Error performing commit: {0}".format(err))
            finally:
                cur.close()
            return cur

    def commit(self):
        """Perform a COMMIT.
        """
        # Guard for connect() prerequisite
        assert self.db_conn, "You must call connect before executing a query."

        self.db_conn.commit()

    def rollback(self):
        """Perform a ROLLBACK.
        """
        # Guard for connect() prerequisite
        assert self.db_conn, "You must call connect before executing a query."

        self.db_conn.rollback()

    def show_server_variable(self, variable):
        """Returns one or more rows from the SHOW VARIABLES command.

        variable[in]       The variable or wildcard string

        Returns result set
        """

        return self.exec_query("SHOW VARIABLES LIKE '%s'" % variable)

    def select_variable(self, var_name, var_type=None):
        """Get server system variable value using SELECT statement.

        This function displays the value of system variables using the SELECT
        statement. This can be used as a workaround for variables with very
        long values, as SHOW VARIABLES is subject to a version-dependent
        display-width limit.

        Note: Some variables may not be available using SELECT @@var_name, in
        such cases use SHOW VARIABLES LIKE 'var_name'.

        var_name[in]    Name of the variable to display.
        var_type[in]    Type of the variable ('session' or 'global'). By
                        default no type is used, meaning that the session
                        value is returned if it exists and the global value
                        otherwise.

        Return the value for the given server system variable.
        """
        if var_type is None:
            var_type = ''
        elif var_type.lower() in ('global', 'session', ''):
            var_type = '{0}.'.format(var_type)  # Add dot (.)
        else:
            raise UtilDBError("Invalid variable type: {0}. Supported types: "
                              "'global' and 'session'.".format(var_type))
        # Execute SELECT @@[var_type.]var_name.
        # Note: An error is issued if the given variable is not known.
        res = self.exec_query("SELECT @@{0}{1}".format(var_type, var_name))
        return res[0][0]

    def flush_logs(self, log_type=None):
        """Execute the FLUSH [log_type] LOGS statement.

        Reload internal logs cache and closes and reopens all log files, or
        only of the specified log_type.

        Note: The log_type option is available from MySQL 5.5.3.

        log_type[in]    Type of the log files to be flushed. Supported values:
                        BINARY, ENGINE, ERROR, GENERAL, RELAY, SLOW.
        """
        if log_type:
            self.exec_query("FLUSH {0} LOGS".format(log_type))
        else:
            self.exec_query("FLUSH LOGS")

    def get_uuid(self):
        """Return the uuid for this server if it is GTID aware.

        Returns uuid or None if server is not GTID aware.
        """
        if self.supports_gtid() != "NO":
            res = self.show_server_variable("server_uuid")
            return res[0][1]
        return None

    def supports_gtid(self):
        """Determine if server supports GTIDs

        Returns string - 'ON' = gtid supported and turned on,
                         'OFF' = supported but not enabled,
                         'NO' = not supported
        """
        # Check servers for GTID support
        version_ok = self.check_version_compat(5, 6, 5)
        if not version_ok:
            return "NO"
        try:
            res = self.exec_query("SELECT @@GLOBAL.GTID_MODE")
        except:
            return "NO"

        return res[0][0]

    def check_gtid_version(self):
        """Determine if server supports latest GTID changes

        This method checks the server to ensure it contains the latest
        changes to the GTID variables (from version 5.6.9).

        Raises UtilRplError when errors occur.
        """
        errors = []
        if self.supports_gtid() != "ON":
            errors.append("    GTID is not enabled.")
        if not self.check_version_compat(5, 6, 9):
            errors.append("    Server version must be 5.6.9 or greater.")
        if errors:
            error_str = "\n".join(errors)
            error_str = "\n".join([_GTID_ERROR % (self.host, self.port),
                                   error_str])
            raise UtilRplError(error_str)

    def check_gtid_executed(self, operation="copy"):
        """Check to see if the gtid_executed variable is clear

        If the value is not clear, raise an error with appropriate instructions
        for the user to correct the issue.

        operation[in]  Name of the operation (copy, import, etc.)
                       default = copy
        """
        res = self.exec_query("SHOW GLOBAL VARIABLES LIKE 'gtid_executed'")[0]
        if res[1].strip() == '':
            return
        err = ("The {0} operation contains GTID statements "
               "that require the global gtid_executed system variable on the "
               "target to be empty (no value). The gtid_executed value must "
               "be reset by issuing a RESET MASTER command on the target "
               "prior to attempting the {0} operation. "
               "Once the global gtid_executed value is cleared, you may "
               "retry the {0}.").format(operation)
        raise UtilRplError(err)

    def get_gtid_executed(self, skip_gtid_check=True):
        """Get the executed GTID set of the server.

        This function retrieves the (current) GTID_EXECUTED set of the server.

        skip_gtid_check[in]     Flag indicating if the check for GTID support
                                will be skipped or not. By default 'True'
                                (check is skipped).

        Returns a string with the GTID_EXECUTED set for this server.
        """
        if not skip_gtid_check:
            # Check server for GTID support.
            gtid_support = self.supports_gtid() == "NO"
            if gtid_support == 'NO':
                raise UtilRplError("Global Transaction IDs are not supported.")
            elif gtid_support == 'OFF':
                raise UtilError("Global Transaction IDs are not enabled.")
        # Get GTID_EXECUTED.
        try:
            return self.exec_query("SELECT @@GLOBAL.GTID_EXECUTED")[0][0]
        except UtilError:
            if skip_gtid_check:
                # Query likely failed because GTIDs are not supported,
                # therefore skip error in this case.
                return ""
            else:
                # If GTID check is not skipped re-raise exception.
                raise
        except IndexError:
            # If no rows are returned by query then return an empty string.
            return ''

    def gtid_subtract(self, gtid_set, gtid_subset):
        """Subtract given GTID sets.

        This function invokes GTID_SUBTRACT function on the server to retrieve
        the GTIDs from the given gtid_set that are not in the specified
        gtid_subset.

        gtid_set[in]        Base GTID set to subtract the subset from.
        gtid_subset[in]     GTID subset to be subtracted from the base set.

        Return a string with the GTID set resulting from the subtraction of the
        specified gtid_subset from the gtid_set.
        """
        try:
            return self.exec_query(
                "SELECT GTID_SUBTRACT('{0}', '{1}')".format(gtid_set,
                                                            gtid_subset)
            )[0][0]
        except IndexError:
            # If no rows are returned by query then return an empty string.
            return ''

    def gtid_subtract_executed(self, gtid_set):
        """Subtract GTID_EXECUTED to the given GTID set.

        This function invokes GTID_SUBTRACT function on the server to retrieve
        the GTIDs from the given gtid_set that are not in the GTID_EXECUTED
        set.

        gtid_set[in]        Base GTID set to subtract the GTID_EXECUTED.

        Return a string with the GTID set resulting from the subtraction of the
        GTID_EXECUTED set from the specified gtid_set.
        """
        from mysql.utilities.common.topology import _GTID_SUBTRACT_TO_EXECUTED
        try:
            result = self.exec_query(
                _GTID_SUBTRACT_TO_EXECUTED.format(gtid_set)
            )[0][0]
            # Remove newlines (\n and/or \r) from the GTID set string returned
            # by the server.
            return result.replace('\n', '').replace('\r', '')
        except IndexError:
            # If no rows are returned by query then return an empty string.
            return ''

    def inject_empty_trx(self, gtid, gtid_next_automatic=True):
        """ Inject an empty transaction.

        This method injects an empty transaction on the server for the given
        GTID.

        Note: SUPER privilege is required for this operation, more precisely
        to set the GTID_NEXT variable.

        gtid[in]                    GTID for the empty transaction to inject.
        gtid_next_automatic[in]     Indicate if the GTID_NEXT is set to
                                    AUTOMATIC after injecting the empty
                                    transaction. By default True.
        """
        self.exec_query("SET GTID_NEXT='{0}'".format(gtid))
        self.exec_query("BEGIN")
        self.commit()
        if gtid_next_automatic:
            self.exec_query("SET GTID_NEXT='AUTOMATIC'")

    def set_gtid_next_automatic(self):
        """ Set GTID_NEXT to AUTOMATIC.
        """
        self.exec_query("SET GTID_NEXT='AUTOMATIC'")

    def checksum_table(self, tbl_name, exec_timeout=0):
        """Compute checksum of specified table (CHECKSUM TABLE tbl_name).

        This function executes the CHECKSUM TABLE statement for the specified
        table and returns the result. The CHECKSUM is aborted (query killed)
        if a timeout value (greater than zero) is specified and the execution
        takes longer than the specified time.

        tbl_name[in]        Name of the table to perform the checksum.
        exec_timeout[in]    Maximum execution time (in seconds) of the query
                            after which it will be killed. By default 0, no
                            timeout.

        Returns a tuple with the checksum result for the target table. The
        first tuple element contains the result from the CHECKSUM TABLE query
        or None if an error occurred (e.g. execution timeout reached). The
        second element holds any error message or None if the operation was
        successful.
        """
        try:
            return self.exec_query(
                "CHECKSUM TABLE {0}".format(tbl_name),
                exec_timeout=exec_timeout
            )[0], None
        except IndexError:
            # If no rows are returned by query then return None.
            return None, "No data returned by CHECKSUM TABLE"
        except UtilError as err:
            # Return None if the query is killed (exec_timeout reached).
            return None, err.errmsg

    def get_gtid_status(self):
        """Get the GTID information for the server.

        This method attempts to retrieve the GTID lists. If the server
        does not have GTID turned on or does not support GTID, the method
        will throw and exception.

        Returns [list, list, list]
        """
        # Check servers for GTID support
        if self.supports_gtid() == "NO":
            raise UtilError("Global Transaction IDs are not supported.")

        res = self.exec_query("SELECT @@GLOBAL.GTID_MODE")
        if res[0][0].upper() == 'OFF':
            raise UtilError("Global Transaction IDs are not enabled.")

        gtid_data = [self.exec_query("SELECT @@GLOBAL.GTID_EXECUTED")[0],
                     self.exec_query("SELECT @@GLOBAL.GTID_PURGED")[0],
                     self.exec_query("SELECT @@GLOBAL.GTID_OWNED")[0]]

        return gtid_data

    def check_rpl_user(self, user, host):
        """Check replication user exists and has the correct privileges.

        user[in]      user name of rpl_user
        host[in]      host name of rpl_user

        Returns [] - no exceptions, list if exceptions found
        """
        errors = []
        ipv6 = False
        if "]" in host:
            ipv6 = True
            host = clean_IPv6(host)
        result = self.user_host_exists(user, host)
        if ipv6:
            result = format_IPv6(result)
        if result is None or result == []:
            errors.append("The replication user %s@%s was not found "
                          "on %s:%s." % (user, host, self.host, self.port))
        else:
            rpl_user = User(self, "%s@" % user + result)
            if not rpl_user.has_privilege('*', '*',
                                          'REPLICATION SLAVE'):
                errors.append("Replication user does not have the "
                              "correct privilege. She needs "
                              "'REPLICATION SLAVE' on all replicated "
                              "databases.")

        return errors

    def supports_plugin(self, plugin):
        """Check if the given plugin is supported.

        Check to see if the server supports a plugin. Return True if
        plugin installed and active.

        plugin[in]     Name of plugin to check

        Returns True if plugin is supported, and False otherwise.
        """
        _PLUGIN_QUERY = ("SELECT * FROM INFORMATION_SCHEMA.PLUGINS "
                         "WHERE PLUGIN_NAME ")
        res = self.exec_query("".join([_PLUGIN_QUERY, "LIKE ",
                                       "'%s" % plugin, "%'"]))
        if not res:
            return False
        # Now see if it is active.
        elif res[0][2] != 'ACTIVE':
            return False
        return True

    def get_all_databases(self, ignore_internal_dbs=True):
        """Return a result set containing all databases on the server
        except for internal databases (mysql, INFORMATION_SCHEMA,
        PERFORMANCE_SCHEMA).

        Note: New internal database 'sys' added by default for MySQL 5.7.7+.

        Returns result set
        """

        if ignore_internal_dbs:
            _GET_DATABASES = """
            SELECT SCHEMA_NAME
            FROM INFORMATION_SCHEMA.SCHEMATA
            WHERE SCHEMA_NAME != 'INFORMATION_SCHEMA'
            AND SCHEMA_NAME != 'PERFORMANCE_SCHEMA'
            AND SCHEMA_NAME != 'mysql'
            """
            # Starting from MySQL 5.7.7, sys schema is installed by default.
            if self.check_version_compat(5, 7, 7):
                _GET_DATABASES = "{0} AND SCHEMA_NAME != 'sys'".format(
                    _GET_DATABASES)
        else:
            _GET_DATABASES = """
            SELECT SCHEMA_NAME
            FROM INFORMATION_SCHEMA.SCHEMATA
            """

        return self.exec_query(_GET_DATABASES)

    def get_storage_engines(self):
        """Return list of storage engines on this server.

        Returns (list) (engine, support, comment)
        """

        _QUERY = """
            SELECT UPPER(engine), UPPER(support)
            FROM INFORMATION_SCHEMA.ENGINES
            ORDER BY engine
        """
        return self.exec_query(_QUERY)

    def check_storage_engines(self, other_list):
        """Compare storage engines from another server.

        This method compares the list of storage engines for the current
        server against a list supplied as **other_list**. It returns two
        lists - one for the storage engines on this server not on the other
        list, and another for the storage engines on the other list not on this
        server.

        Note: type case sensitive - make sure list is in uppercase

        other_list[in]     A list from another server in the form
                           (engine, support) - same output as
                           get_storage_engines()

        Returns (list, list)
        """
        # Guard for connect() prerequisite
        assert self.db_conn, "You must call connect before check engine lists."

        def _convert_set_to_list(set_items):
            """Convert a set to list
            """
            if len(set_items) > 0:
                item_list = []
                for item in set_items:
                    item_list.append(item)
            else:
                item_list = None
            return item_list

        # trivial, but guard against misuse
        this_list = self.get_storage_engines()
        if other_list is None:
            return (this_list, None)

        same = set(this_list) & set(other_list)
        master_extra = _convert_set_to_list(set(this_list) - same)
        slave_extra = _convert_set_to_list(set(other_list) - same)

        return (master_extra, slave_extra)

    def has_storage_engine(self, target):
        """Check to see if an engine exists and is supported.

        target[in]     name of engine to find

        Returns bool True - engine exists and is active, false = does not
                     exist or is not supported/not active/disabled
        """
        if len(target) == 0:
            return True  # This says we will use default engine on the server.
        if target is not None:
            engines = self.get_storage_engines()
            for engine in engines:
                if engine[0].upper() == target.upper() and \
                   engine[1].upper() in ['YES', 'DEFAULT']:
                    return True
        return False

    def substitute_engine(self, tbl_name, create_str,
                          new_engine, def_engine, quiet=False):
        """Replace storage engine in CREATE TABLE

        This method will replace the storage engine in the CREATE statement
        under the following conditions:
            - If new_engine is specified and it exists on destination, use it.
            - Else if existing engine does not exist and def_engine is specfied
              and it exists on destination, use it. Also, don't substitute if
              the existing engine will not be changed.

        tbl_name[in]       table name
        create_str[in]     CREATE statement
        new_engine[in]     name of storage engine to substitute (convert to)
        def_engine[in]     name of storage engine to use if existing engines
                           does not exist

        Returns string CREATE string with replacements if found, else return
                       original string
        """
        res = [create_str]
        exist_engine = ''
        is_create_like = False
        replace_msg = "# Replacing ENGINE=%s with ENGINE=%s for table %s."
        add_msg = "# Adding missing ENGINE=%s clause for table %s."
        if new_engine is not None or def_engine is not None:
            i = create_str.find("ENGINE=")
            if i > 0:
                j = create_str.find(" ", i)
                exist_engine = create_str[i + 7:j]
            else:
                # Check if it is a CREATE TABLE LIKE statement
                is_create_like = (create_str.find("CREATE TABLE {0} LIKE"
                                                  "".format(tbl_name)) == 0)

        # Set default engine
        #
        # If a default engine is specified and is not the same as the
        # engine specified in the table CREATE statement (existing engine) if
        # specified, and both engines exist on the server, replace the existing
        # engine with the default engine.
        #
        if def_engine is not None and \
                exist_engine.upper() != def_engine.upper() and \
                self.has_storage_engine(def_engine) and \
                self.has_storage_engine(exist_engine):

            # If no ENGINE= clause present, add it
            if len(exist_engine) == 0:
                if is_create_like:
                    alter_str = "ALTER TABLE {0} ENGINE={1}".format(tbl_name,
                                                                    def_engine)
                    res = [create_str, alter_str]
                else:
                    i = create_str.find(";")
                    i = len(create_str) if i == -1 else i
                    create_str = "{0} ENGINE={1};".format(create_str[0:i],
                                                          def_engine)
                    res = [create_str]
            # replace the existing storage engine
            else:
                create_str.replace("ENGINE=%s" % exist_engine,
                                   "ENGINE=%s" % def_engine)
            if not quiet:
                if len(exist_engine) > 0:
                    print replace_msg % (exist_engine, def_engine, tbl_name)
                else:
                    print add_msg % (def_engine, tbl_name)
            exist_engine = def_engine

        # Use new engine
        if (new_engine is not None and
                exist_engine.upper() != new_engine.upper() and
                self.has_storage_engine(new_engine)):
            if len(exist_engine) == 0:
                if is_create_like:
                    alter_str = "ALTER TABLE {0} ENGINE={1}".format(tbl_name,
                                                                    new_engine)
                    res = [create_str, alter_str]
                else:
                    i = create_str.find(";")
                    i = len(create_str) if i == -1 else i
                    create_str = "{0} ENGINE={1};".format(create_str[0:i],
                                                          new_engine)
                    res = [create_str]
            else:
                create_str = create_str.replace("ENGINE=%s" % exist_engine,
                                                "ENGINE=%s" % new_engine)
                res = [create_str]
            if not quiet:
                if len(exist_engine) > 0:
                    print replace_msg % (exist_engine, new_engine, tbl_name)
                else:
                    print add_msg % (new_engine, tbl_name)
        return res

    def get_innodb_stats(self):
        """Return type of InnoDB engine and its version information.

        This method returns a tuple containing the type of InnoDB storage
        engine (builtin or plugin) and the version number reported.

        Returns (tuple) (type = 'builtin' or 'plugin', version_number,
                         have_innodb = True or False)
        """
        # Guard for connect() prerequisite
        assert self.db_conn, "You must call connect before get innodb stats."

        _BUILTIN = """
            SELECT (support='YES' OR support='DEFAULT' OR support='ENABLED')
            AS `exists` FROM INFORMATION_SCHEMA.ENGINES
            WHERE engine = 'innodb';
        """
        _PLUGIN = """
            SELECT (plugin_library LIKE 'ha_innodb_plugin%') AS `exists`
            FROM INFORMATION_SCHEMA.PLUGINS
            WHERE LOWER(plugin_name) = 'innodb' AND
                  LOWER(plugin_status) = 'active';
        """
        _VERSION = """
            SELECT plugin_version, plugin_type_version
            FROM INFORMATION_SCHEMA.PLUGINS
            WHERE LOWER(plugin_name) = 'innodb';
        """

        inno_type = None
        results = self.exec_query(_BUILTIN)
        if results is not None and results != () and results[0][0] is not None:
            inno_type = "builtin"

        results = self.exec_query(_PLUGIN)
        if results is not None and results != () and \
           results != [] and results[0][0] is not None:
            inno_type = "plugin "

        results = self.exec_query(_VERSION)
        version = []
        if results is not None:
            version.append(results[0][0])
            version.append(results[0][1])
        else:
            version.append(None)
            version.append(None)

        results = self.show_server_variable("have_innodb")
        # pylint: disable=R0102
        if results is not None and results != [] and \
           results[0][1].lower() == "yes":
            have_innodb = True
        else:
            have_innodb = False

        return (inno_type, version[0], version[1], have_innodb)

    def read_and_exec_SQL(self, input_file, verbose=False):
        """Read an input file containing SQL statements and execute them.

        input_file[in]     The full path to the file
        verbose[in]        Print the command read
                           Default = False

        Returns True = success, False = error

        TODO : Make method read multi-line queries.
        """
        f_input = open(input_file)
        res = True
        while True:
            cmd = f_input.readline()
            if not cmd:
                break
            res = None
            if len(cmd) > 1:
                if cmd[0] != '#':
                    if verbose:
                        print cmd
                    query_options = {
                        'fetch': False
                    }
                    res = self.exec_query(cmd, query_options)
        f_input.close()
        return res

    def binlog_enabled(self):
        """Check binary logging status for the client.

        Returns bool - True - binary logging is ON, False = OFF
        """
        res = self.show_server_variable("log_bin")
        if not res:
            raise UtilRplError("Cannot retrieve status of log_bin variable.")
        if res[0][1] in ("OFF", "0"):
            return False
        return True

    def toggle_binlog(self, action="disable"):
        """Enable or disable binary logging for the client.

        Note: user must have SUPER privilege

        action[in]         if 'disable', turn off the binary log
                           elif 'enable' turn binary log on
                           do nothing if action != 'enable' or 'disable'
        """

        if action.lower() == 'disable':
            self.exec_query("SET SQL_LOG_BIN=0")
        elif action.lower() == 'enable':
            self.exec_query("SET SQL_LOG_BIN=1")

    def foreign_key_checks_enabled(self, force=False):
        """Check foreign key status for the connection.
        force[in]       if True, returns the value directly from the server
                        instead of returning the cached fkey value

        Returns bool - True - foreign keys are enabled
        """
        if self.fkeys is None or force:
            res = self.exec_query("SELECT @@GLOBAL.foreign_key_checks")
            self.fkeys = (res is not None) and (res[0][0] == "1")
        return self.fkeys

    def disable_foreign_key_checks(self, disable=True):
        """Enable or disable foreign key checks for the connection.

        disable[in]     if True, turn off foreign key checks
                        elif False turn foreign key checks on.
        """
        if self.fkeys is None:
            self.foreign_key_checks_enabled()

        # Only do something if foreign keys are OFF and shouldn't be disabled
        # or if they are ON and should be disabled
        if self.fkeys == disable:
            val = "OFF" if disable else "ON"
            self.exec_query(_FOREIGN_KEY_SET.format(val),
                            {'fetch': False, 'commit': False})
            self.fkeys = not self.fkeys

    def autocommit_set(self):
        """Check autocommit status for the connection.

        Returns bool - True if autocommit is enabled and False otherwise.
        """
        if self.autocommit is None:
            res = self.show_server_variable('autocommit')
            self.autocommit = (res and res[0][1] == '1')
        return self.autocommit

    def toggle_autocommit(self, enable=None):
        """Enable or disable autocommit for the connection.

        This method switch the autocommit value or enable/disable it according
        to the given parameter.

        enable[in]         if True, turn on autocommit (set to 1)
                           else if False turn autocommit off (set to 0).
        """
        if enable is None:
            # Switch autocommit value.
            if self.autocommit is None:
                # Get autocommit value if unknown
                self.autocommit_set()
            if self.autocommit:
                value = '0'
                self.autocommit = False
            else:
                value = '1'
                self.autocommit = True
        else:
            # Set AUTOCOMMIT according to provided value.
            if enable:
                value = '1'
                self.autocommit = True
            else:
                value = '0'
                self.autocommit = False
        # Change autocommit value.
        self.exec_query(_AUTOCOMMIT_SET.format(value), {'fetch': 'false'})

    def get_server_id(self):
        """Retrieve the server id.

        Returns int - server id.
        """
        try:
            res = self.show_server_variable("server_id")
        except:
            raise UtilRplError("Cannot retrieve server id from "
                               "%s." % self.role)

        return int(res[0][1])

    def get_server_uuid(self):
        """Retrieve the server uuid.

        Returns string - server uuid.
        """
        try:
            res = self.show_server_variable("server_uuid")
            if res is None or res == []:
                return None
        except:
            raise UtilRplError("Cannot retrieve server_uuid from "
                               "%s." % self.role)

        return res[0][1]

    def get_lctn(self):
        """Get lower_case_table_name setting.

        Returns lctn value or None if cannot get value
        """
        res = self.show_server_variable("lower_case_table_names")
        if res != []:
            return res[0][1]
        return None

    def get_binary_logs(self, options=None):
        """Return a list of the binary logs.

        options[in]        query options

        Returns list - binlogs or None if binary logging turned off
        """
        if options is None:
            options = {}
        if self.binlog_enabled():
            return self.exec_query("SHOW BINARY LOGS", options)

        return None

    def set_read_only(self, on=False):
        """Turn read only mode on/off

        on[in]         if True, turn read_only ON
                       Default is False
        """
        # Only turn on|off read only if it were off at connect()
        if on and not self.read_only:
            self.exec_query("SET @@GLOBAL.READ_ONLY = 'ON'")
            self.read_only = True
        elif not on and self.read_only:
            self.read_only = False
            self.exec_query("SET @@GLOBAL.READ_ONLY = 'OFF'")
        return None

    def grant_tables_enabled(self):
        """Check to see if grant tables are enabled

        Returns bool - True = grant tables are enabled, False = disabled
        """
        if self.grants_enabled is None:
            try:
                self.exec_query("SHOW GRANTS FOR 'snuffles'@'host'")
                self.grants_enabled = True
            except UtilError as error:
                if "--skip-grant-tables" in error.errmsg:
                    self.grants_enabled = False
                # Ignore other errors as they are not pertinent to the check
                else:
                    self.grants_enabled = True
        return self.grants_enabled

    def get_server_binlogs_list(self, include_size=False):
        """Find the binlog file names listed on a server.

        Obtains the binlog file names available on the server by using the
        'SHOW BINARY LOGS' query at the given server instance and returns these
        file names as a list.

        include_size[in]  Boolean value to indicate if the returning list shall
                          include the size of the file.

        Returns a list with the binary logs names available on master.
        """
        res = self.exec_query("SHOW BINARY LOGS")

        server_binlogs = []
        for row in res:
            if include_size:
                server_binlogs.append(row)
            else:
                server_binlogs.append(row[0])
        return server_binlogs

    def sql_mode(self, mode, enable):
        """Set the sql_mode

        This method sets the sql_mode passed. If enable is True,
        the method adds the mode, else, it removes the mode.

        mode[in]      The sql_mode you want to set
        enable[in]    If True, set the mode, else remove the mode.

        Returns string - new sql_mode setting or None=not enabled/disabled
        """
        SQL_MODE = 'SET @@GLOBAL.SQL_MODE = "{0}"'
        sql_mode = self.show_server_variable("sql_mode")
        if sql_mode[0]:
            modes = sql_mode[0][1].split(",")
            sql_mode_str = 'mt'
            if enable:
                if mode not in modes:
                    modes.append(mode)
                else:
                    sql_mode_str = None
            else:
                if mode in modes:
                    index = modes.index(mode)
                    modes.pop(index)
                else:
                    sql_mode_str = None
            if sql_mode_str:
                sql_mode_str = SQL_MODE.format(",".join(modes))
                self.exec_query(sql_mode_str)
                return sql_mode_str
        return None


class QueryKillerThread(threading.Thread):
    """Class to run a thread to kill an executing query.

    This class is used to spawn a thread than will kill the execution
    (connection) of a query upon reaching a given timeout.
    """

    def __init__(self, server, query, timeout):
        """Constructor.

        server[in]      Server instance where the target query is executed.
        query[in]       Target query to kill.
        timeout[in]     Timeout value in seconds used to kill the query when
                        reached.
        """
        threading.Thread.__init__(self)
        self._stop_event = threading.Event()
        self._query = query
        self._timeout = timeout
        self._server = server
        self._connection = server.get_connection()
        server.get_version()

    def run(self):
        """Main execution of the query killer thread.
        Stop the thread if instructed as such
        """
        connector_error = None
        # Kill the query connection upon reaching the given execution timeout.
        while not self._stop_event.is_set():
            # Wait during the defined time.
            self._stop_event.wait(self._timeout)
            # If the thread was asked to stop during wait, it does not try to
            # kill the query.
            if not self._stop_event.is_set():
                try:
                    if mysql.connector.__version_info__ < (2, 0):
                        cur = self._connection.cursor(raw=True)
                    else:
                        cur = self._connection.cursor(
                            cursor_class=MySQLUtilsCursorRaw)

                    # Get process information from threads table when available
                    # (for versions > 5.6.1), since it does not require a mutex
                    # and has minimal impact on server performance.
                    if self._server.check_version_compat(5, 6, 1):
                        cur.execute(
                            "SELECT processlist_id "
                            "FROM performance_schema.threads"
                            " WHERE processlist_command='Query'"
                            " AND processlist_info='{0}'".format(self._query))
                    else:
                        cur.execute(
                            "SELECT id FROM information_schema.processlist"
                            " WHERE command='Query'"
                            " AND info='{0}'".format(self._query))
                    result = cur.fetchall()

                    try:
                        process_id = result[0][0]
                    except IndexError:
                        # No rows are returned if the query ended in the
                        # meantime.
                        process_id = None

                    # Kill the connection associated to que process id.
                    # Note: killing the query will not work with
                    # connector-python,since it will hang waiting for the
                    #  query to return.
                    if process_id:
                        cur.execute("KILL {0}".format(process_id))
                except mysql.connector.Error as err:
                    # Hold error to raise at the end.
                    connector_error = err
                finally:
                    # Close cursor if available.
                    if cur:
                        cur.close()
                # Stop this thread.
                self.stop()

        # Close connection.
        try:
            self._connection.disconnect()
        except mysql.connector.Error:
            # Only raise error if no previous error has occurred.
            if not connector_error:
                raise
        finally:
            # Raise any previous error that already occurred.
            if connector_error is not None:
                # pylint: disable=E0702
                raise connector_error

    def stop(self):
        """Stop the thread.

        Set the event flag for the thread to stop as soon as possible.
        """
        self._stop_event.set()
#
# Copyright (c) 2011, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains the methods for building SQL statements for definition
differences.
"""

import re

from mysql.connector.conversion import MySQLConverter


_IGNORE_COLUMN = -1  # Ignore column in comparisons and transformations
_FORCE_COLUMN = -2   # Force column to be included in build phase

# Define column control symbols
_DROP_COL, _ADD_COL, _CHANGE_COL_TYPE, _CHANGE_COL_ORDER = range(0, 4)

# List of database objects for enumeration
_DATABASE, _TABLE, _VIEW, _TRIG, _PROC, _FUNC, _EVENT, _GRANT = "DATABASE", \
    "TABLE", "VIEW", "TRIGGER", "PROCEDURE", "FUNCTION", "EVENT", "GRANT"

# Define database INFORMATION_SCHEMA column numbers
_DB_NAME, _DB_CHARSET, _DB_COLLATION, _DB_SQL_PATH = range(0, 4)

# Define table INFORMATION_SCHEMA column numbers and index values
_COLUMN_ORDINAL_POSITION, _COLUMN_NAME, _COLUMN_TYPE, _COLUMN_IS_NULLABLE, \
    _COLUMN_DEFAULT, _COLUMN_EXTRA, _COLUMN_COMMENT, _COLUMN_KEY = range(0, 8)

_TABLE_DEF, _COLUMN_DEF, _PART_DEF = range(0, 3)
_TABLE_DB, _TABLE_NAME, _TABLE_ENGINE, _TABLE_AUTO_INCREMENT, \
    _TABLE_AVG_ROW_LENGTH, _TABLE_CHECKSUM, _TABLE_COLLATION, _TABLE_COMMENT, \
    _TABLE_ROW_FORMAT, _TABLE_CREATE_OPTIONS = range(0, 10)

# Define view INFORMATION_SCHEMA column numbers
_VIEW_DB, _VIEW_NAME, _VIEW_BODY, _VIEW_CHECK, _VIEW_DEFINER, \
    _VIEW_SECURITY = range(0, 6)

# Define trigger INFORMATION_SCHEMA column numbers
_TRIGGER_DB, _TRIGGER_NAME, _TRIGGER_EVENT, _TRIGGER_TABLE, _TRIGGER_BODY, \
    _TRIGGER_TIME, _TRIGGER_DEFINER = range(0, 7)

# Define routine INFORMATION_SCHEMA column numbers
_ROUTINE_DB, _ROUTINE_NAME, _ROUTINE_BODY, _ROUTINE_SQL_DATA_ACCESS, \
    _ROUTINE_SECURITY_TYPE, _ROUTINE_COMMENT, _ROUTINE_DEFINER, \
    _ROUTINE_PARAMS, _ROUTINE_RETURNS, _ROUTINE_IS_DETERMINISTIC = range(0, 10)

# Define event INFORMATION_SCHEMA column numbers
_EVENT_DB, _EVENT_NAME, _EVENT_DEFINER, _EVENT_BODY, _EVENT_TYPE, \
    _EVENT_INTERVAL_FIELD, _EVENT_INTERVAL_VALUE, _EVENT_STATUS, \
    _EVENT_ON_COMPLETION, _EVENT_STARTS, _EVENT_ENDS = range(0, 11)

# Get the constraints but ignore primary keys
_CONSTRAINT_QUERY = """
  SELECT CONSTRAINT_NAME, CONSTRAINT_TYPE
  FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS
  WHERE TABLE_SCHEMA = '%(db)s' AND TABLE_NAME = '%(name)s'
        and CONSTRAINT_TYPE != 'PRIMARY KEY'
        and CONSTRAINT_TYPE != 'UNIQUE'
"""


def to_sql(obj):
    """Convert a value to a suitable SQL value placing quotes where needed.

    obj[in]           object (value) to convert

    Returns (string) converted value
    """
    to_sql.__dict__.setdefault('converter', MySQLConverter())
    obj = to_sql.converter.escape(obj)  # pylint: disable=E1101
    return str(to_sql.converter.quote(obj))  # pylint: disable=E1101


def quote_with_backticks(identifier, sql_mode=''):
    """Quote the given identifier with backticks, converting backticks (`) in
    the identifier name with the correct escape sequence (``) unless the
    identifier is quoted (") as in sql_mode set to ANSI_QUOTES.

    identifier[in] identifier to quote.

    Returns string with the identifier quoted with backticks.
    """
    if "ANSI_QUOTES" in sql_mode:
        return '"{0}"'.format(identifier.replace('"', '""'))
    else:
        return "`{0}`".format(identifier.replace("`", "``"))


def quote_with_backticks_definer(definer, sql_mode=''):
    """Quote the given definer clause with backticks.

    This functions quotes the given definer clause with backticks, converting
    backticks (`) in the string with the correct escape sequence (``).

    definer[in]     definer clause to quote.

    Returns string with the definer quoted with backticks.
    """
    if not definer:
        return definer
    parts = definer.split('@')
    if len(parts) != 2:
        return definer
    return '@'.join([quote_with_backticks(parts[0], sql_mode),
                     quote_with_backticks(parts[1], sql_mode)])


def remove_backtick_quoting(identifier, sql_mode=''):
    """Remove backtick quoting from the given identifier, reverting the
    escape sequence (``) to a backtick (`) in the identifier name.

    identifier[in] identifier to remove backtick quotes.

    Returns string with the identifier without backtick quotes.
    """
    double_quoting = identifier.startswith('"') and identifier.endswith('"')
    # remove quotes
    identifier = identifier[1:-1]
    if 'ANSI_QUOTES' in sql_mode and double_quoting:
        return identifier.replace('""', '"')
    else:
        # Revert backtick escape sequence
        return identifier.replace("``", "`")


def is_quoted_with_backticks(identifier, sql_mode=''):
    """Check if the given identifier is quoted with backticks.

    identifier[in] identifier to check.

    Returns True if the identifier has backtick quotes, and False otherwise.
    """
    if 'ANSI_QUOTES' in sql_mode:
        return (identifier[0] == "`" and identifier[-1] == "`") or \
            (identifier[0] == '"' and identifier[-1] == '"')
    else:
        return identifier[0] == "`" and identifier[-1] == "`"


def convert_special_characters(str_val):
    """Convert especial characters in the string to respective escape sequence.

    This method converts special characters in the input string to the
    corresponding MySQL escape sequence, according to:
    http://dev.mysql.com/doc/en/string-literals.html#character-escape-sequences

    str_val[in]  string value to be converted.

    Returns the input string with all special characters replaced by its
    respective escape sequence.
    """
    # Check if the input value is a string before performing replacement.
    if str_val and isinstance(str_val, basestring):
        # First replace backslash '\' character, to avoid replacing '\' in
        # further escape sequences. backslash_re matches '|' not followed by %
        # as \% and \_ do not need to be replaced, and when '|' appear at the
        # end of the string to be replaced correctly.
        backslash_re = r'\\(?=[^%_])|\\\Z'
        res = re.sub(backslash_re, r'\\\\', str_val)

        # Replace remaining especial characters
        res = res.replace('\x00', '\\0')  # \0
        res = res.replace("'", "\\'")  # \'
        res = res.replace('"', '\\"')  # \"
        res = res.replace('\b', '\\b')  # \b
        res = res.replace('\n', '\\n')  # \n
        res = res.replace('\r', '\\r')  # \r
        res = res.replace('\t', '\\t')  # \t
        res = res.replace(chr(26), '\\Z')  # \Z

        return res
    else:
        # Not a string, return the input value
        return str_val


def build_pkey_where_clause(table, row):
    """Build the WHERE clause based on the primary keys

    table[in]              instance of Table class for table
    row[in]                row of data

    Returns string - WHERE clause or "" if no keys
    """
    where_str = ""
    pkeys = table.get_primary_index()
    if len(pkeys) > 0:
        col_names = table.get_col_names()
        where_str += "WHERE "
        pkey_cond_lst = []
        for pkey in pkeys:
            key_col = pkey[0]                         # get the column name
            col_data = row[col_names.index(key_col)]  # get column value
            # quote key column with backticks
            q_key_col = quote_with_backticks(key_col, table.sql_mode)
            pkey_cond_lst.append("{0} = {1}".format(q_key_col,
                                                    to_sql(col_data)))
        where_str = "{0}{1}".format(where_str, ' AND '.join(pkey_cond_lst))

    return where_str


def build_set_clauses(table, table_cols, dest_row, src_row):
    """Build the SET clauses for an UPDATE statement

    table[in]              instance of Table class for table
    dest_row[in]           row of data for destination (to be changed)
    src_row[in]            row of data for source (to be changed to)

    Returns string - WHERE clause or "" if no keys
    """
    table.get_column_metadata()
    # do SETs
    set_str = ""
    do_comma = False
    for col_idx in range(0, len(table_cols)):
        if dest_row[col_idx] != src_row[col_idx]:
            # do comma
            if do_comma:
                set_str += ", "
            else:
                set_str = "SET "
                do_comma = True
            # Check for NULL for non-text fields that have no value in new row
            if src_row[col_idx] is None:
                set_str += "%s = %s" % (table_cols[col_idx], "NULL")
            else:
                set_str += "%s = %s" % (table_cols[col_idx],
                                        to_sql(src_row[col_idx]))

    return set_str


def transform_data(destination, source, operation, rows):
    """Transform data for tables.

    This method will generate INSERT, UPDATE, and DELETE statements for
    transforming data found to differ among tables.

    destination[in]    Table class instance of the destination
    source[in]         Table class instance of the source
    operation[in]      specify if INSERT, UPDATE, or DELETE
    rows[in]           rows for transformation as follows:
                       UPDATE - tuple (old, new)
                       DELETE - list to delete
                       INSERT - list to insert

    Returns list - SQL statement(s) for transforming the data or a warning
                   if the columns differ between the tables
    """
    statements = []

    # Get column names quoted with backticks
    dest_cols = destination.get_col_names(quote_backticks=True)
    src_cols = source.get_col_names(quote_backticks=True)

    # We cannot do the data changes if the columns are different in the
    # destination and source!
    if dest_cols != src_cols:
        return ["WARNING: Cannot generate SQL UPDATE commands for "
                "tables whose definitions are different. Check the "
                "table definitions for changes."]
    data_op = operation.upper()
    if data_op == "INSERT":
        for row in rows:
            formatted_row = []
            for col in row:
                formatted_row.append(to_sql(col))
            statements.append("INSERT INTO %s (%s) VALUES(%s);" %
                              (destination.q_table, ', '.join(dest_cols),
                               ', '.join(formatted_row)))
    elif data_op == "UPDATE":
        for i in range(0, len(rows[0])):
            row1 = rows[0][i]
            row2 = rows[1][i]
            sql_str = "UPDATE %s" % destination.q_table
            sql_str += " %s" % build_set_clauses(source, src_cols, row1, row2)
            sql_str += " %s" % build_pkey_where_clause(source, row2)
            statements.append("%s;" % sql_str)
    elif data_op == "DELETE":
        for row in rows:
            sql_str = "DELETE FROM %s " % destination.q_table
            sql_str += build_pkey_where_clause(source, row)
            statements.append("%s;" % sql_str)
    else:
        raise UtilError("Unknown data transformation option: %s." % data_op)

    return statements


class SQLTransformer(object):
    """
    The SQLTransformer class provides a mechanism for generating SQL statments
    for conforming an object to another for a specific database. For example,
    it will generate the ALTER statement(s) for transforming a table definition
    as well as the UPDATE statement(s) for transforming a row in the table.

    Note: This class is designed to work with the output of the Database class
          method get_db_objects with full INFORMATION_SCHEMA columns for the
          object definition.

    This class contains transformation methods for the objects supported.
    Each object's ALTER statement is generated using the following steps.
    Note: tables are a bit different due to their many parts but still follow
    the general layout.

    - a list of dictionaries structure is built to contain the parts of the
      statement where each dictionary has fields for format ('fmt') that
      contains the string format for building the value, column ('col') for
      containing the column number for the value, and value ('val') which
      is for holding the value.
    - any special formatting, conditionals, etc. concerning the fields is
      processed. In some cases this means filling the 'val' for the field.
    - the structure values are filled
    - the statement is build by concatenating those fields where 'val' is
      not empty.

    You can tell the fill values phase to ignore filling the value by using
    _IGNORE_COLUMN as the column number.

    You can tell the build phase to include the field (say after special
    processing has filled the value) by using _FORCE_COLUMN as the column
    number.
    """

    def __init__(self, destination_db, source_db, destination,
                 source, obj_type, verbosity, options=None):
        """Constructor

        destination_db[in] destination Database instance
        source_db[in]      source Database instance
        destination[in]    the original object definition or data
        source[in]         the source object definition or data
        obj_type[in]       type of object
        verbosity[in]      verbosity level
        options[in]        Options dictionary

        """
        self.destination_db = destination_db
        self.source_db = source_db
        self.destination = destination
        self.source = source
        self.obj_type = obj_type.upper()
        self.verbosity = verbosity
        self.dest_tbl = None
        self.src_tbl = None
        if options is None:
            options = {}
        self.skip_table_opts = options.get("skip_table_opts", False)

    def transform_definition(self):
        """Transform an object definition

        This method will transform an object definition to match the source
        configuration. It returns the appropriate SQL statement(s) to
        transform the object or None if no transformation is needed.

        Note: the method will throw an exception if the transformation cannot
              be completed or there is another error during processing

        Returns list - SQL statement(s) for transforming the object
        """
        trans_method = {
            _DATABASE: self._transform_database,
            _TABLE: self._transform_table,
            _VIEW: self._transform_view,
            _TRIG: self._transform_trigger,
            _PROC: self._transform_routine,
            _FUNC: self._transform_routine,
            _EVENT: self._transform_event,
        }
        try:
            return trans_method[self.obj_type]()
        except IndexError:
            raise UtilDBError("Unknown object type '%s' for transformation." %
                              self.obj_type)

    def _transform_database(self):
        """Transform a database definition

        This method will transform a database definition to match the source
        configuration. It returns the ALTER DATABASE SQL statement to
        transform the object or None if no transformation is needed.

        Returns list - ALTER DATABASE statement for transforming the database
        """
        statements = []

        # build a list of the parts
        statement_parts = [
            # preamble
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': "ALTER DATABASE"},
            # object name
            {'fmt': " %s", 'col': _IGNORE_COLUMN,
             'val': self.destination[_DB_NAME]},
            # charset
            {'fmt': " CHARACTER SET %s", 'col': _DB_CHARSET, 'val': ""},
            # collation
            {'fmt': " COLLATE = %s", 'col': _DB_COLLATION, 'val': ""},
        ]

        # if no changes, return None
        if not self._fill_values(statement_parts, False):
            return None

        sql_stmt = "%s;" % self._build_statement(statement_parts)
        statements.append(sql_stmt)

        return statements

    @staticmethod
    def _convert_option_values(option_values):
        """Convert a list of option=value to a list of names and name, value
        pairs.

        This method takes a list like the following where each element is a
        name=value string:

        (a=1, b=3, c=5, d=4)

        turning into a tuple containing a list of names and a list of
        name,value pairs as follows:

        ((a,b,c,d), ((a,1),(b,3),(c,5),(d,4)))

        Value pairs that do not have a value are ignored. For example,
        'a=3, b, c=2' will ignore 'b' but return a and c.

        option_values[in]  list of name=value strings

        Returns tuple - (list of names, list of (name, value))
        """
        names = []
        name_values = []
        for value_pair in option_values:
            name_value = value_pair.split('=')
            # Ignore any value pairs that do not have a value
            if len(name_value[0]) > 0:
                names.append(name_value[0].upper())
                name_values.append(name_value)
        return (names, name_values)

    @staticmethod
    def _find_value(name, name_values):
        """Find a value for a name in a list of tuple (name, value)

        name[in]           name of pair
        name_values[in]    list of tuples

        Returns string - value at index of match or None
        """
        name = name.upper()
        for item in name_values:
            if item[0].upper() == name:
                try:
                    return item[1]
                except IndexError:
                    return None

        return None

    def _parse_table_options(self, destination, source):
        """Parse the table options into a list and compare.

        This method returns a comma-separated list of table options that
        differ from the destination to the source.

        destination[in]    the original object definition or data
        source[in]         the source object definition or data

        Returns string - comma-separated values for table options that differ
                         or None if options are found in the destination that
                         are not in the source. These, we do not know how
                         to remove or turn off without extensive, specialized
                         code.
        """
        from mysql.utilities.common.dbcompare import get_common_lists

        # Here we have a comma-separated list of options in the form
        # name=value. To determine the inclusion/exclusion lists, we
        # must compare on names only so we make a list for each of only
        # the names.
        dest_opts_names = []
        dest_opts = [item.strip() for item in destination.split(',')]
        dest_opts_names, dest_opts_val = self._convert_option_values(dest_opts)
        dest_opts_names.sort()
        src_opts = [item.strip() for item in source.split(',')]
        src_opts_names, src_opts_val = self._convert_option_values(src_opts)
        src_opts_names.sort()
        in_both, in_dest_not_src, in_src_not_dest = \
            get_common_lists(dest_opts_names, src_opts_names)

        # Whoops! There are things set in the destination that aren't in the
        # source so we don't know if these are Ok or if we need to do
        # something special.
        if len(in_dest_not_src) > 0:
            return None

        changes = []
        # Now check for changes for both
        for name in in_both:
            dest_val = self._find_value(name, dest_opts_val)
            src_val = self._find_value(name, src_opts_val)
            if dest_val is not None and dest_val != src_val:
                changes.append("%s=%s" % (name.upper(), src_val))

        # Get values for those not in destination
        for item in in_src_not_dest:
            val = self._find_value(item, src_opts_val)
            if val is not None:
                changes.append("%s=%s" % (item.upper(), val))

        return ', '.join(changes)

    def _get_table_defns(self, destination, source):
        """Get the transform fpr the general options for a table

        This method creates an ALTER TABLE statement for table definitions
        that differ. The items covered include only those options described
        in the reference manual as table_options and include the following:

            engine, auto_increment, avg_row_count, checksum, collation,
            comment, and create options

        destination[in]    the original object definition or data
        source[in]         the source object definition or data

        Returns string - ALTER TABLE clause or None if no transform needed
        """
        changes = self._check_columns([_TABLE_COMMENT], destination, source)

        # build a list of the parts
        statement_parts = [
            # rename
            {'fmt': "RENAME TO %s.%s \n", 'col': _IGNORE_COLUMN, 'val': ""},
            # engine
            {'fmt': "ENGINE=%s", 'col': _TABLE_ENGINE, 'val': ""},
            # auto increment
            {'fmt': "AUTO_INCREMENT=%s", 'col': _TABLE_AUTO_INCREMENT,
             'val': ""},
            # collation
            {'fmt': "COLLATE=%s", 'col': _TABLE_COLLATION, 'val': ""},
            # comment - always include to ensure comments can be removed
            {'fmt': "COMMENT='%s'", 'col': _IGNORE_COLUMN,
             'val': source[_TABLE_COMMENT]},
            # create options - will be completed later
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': ""},
        ]

        dest_create = destination[_TABLE_CREATE_OPTIONS]
        src_create = source[_TABLE_CREATE_OPTIONS]
        if dest_create != src_create:
            create = statement_parts[5]
            opt_val = self._parse_table_options(dest_create, src_create)
            if opt_val is None:
                return ("# WARNING: the destination table contains options "
                        "that are not in the source.\n# Cannot generate ALTER "
                        "statement.")
            else:
                create['val'] = "%s" % opt_val
                changes = True

        # if no changes, return None
        if not changes and not self._fill_values(statement_parts, False,
                                                 destination, source):
            return None

        # We need to check the comment again and include it if source == ''
        if self._check_columns([_TABLE_COMMENT], destination, source) and \
           source[_TABLE_COMMENT] == '':
            statement_parts[4]['col'] = _FORCE_COLUMN

        # Check for rename
        if destination[_TABLE_NAME] != source[_TABLE_NAME]:
            statement_parts[0]['val'] = (source[_DB_NAME], source[_TABLE_NAME])

        # check and set commas
        do_comma = False
        for part in statement_parts:
            if do_comma:
                part['fmt'] = ', ' + part['fmt']
            elif part['col'] == _FORCE_COLUMN or part['val'] != '':
                do_comma = True

        return self._build_statement(statement_parts)

    @staticmethod
    def _get_column_format(col_data):
        """Build the column data type format string

        col_data[in]       the row containing the column definition

        Retuns string - column data type format
        """
        if col_data is None:
            return ""
        col_fmt = "%(type)s%(null)s%(default)s%(extra)s%(comment)s"
        values = {
            'type': col_data[_COLUMN_TYPE],
            'null': "",
            'default': "",
            'extra': "",
            'comment': "",
        }
        if col_data[_COLUMN_IS_NULLABLE].upper() == "NO":
            values['null'] = " NOT NULL"
        else:
            values['null'] = " NULL"
        if col_data[_COLUMN_DEFAULT] is not None:
            def_val = col_data[_COLUMN_DEFAULT]
            # add quotes if needed
            if def_val.upper() != "CURRENT_TIMESTAMP":
                def_val = to_sql(def_val)
            values['default'] = " DEFAULT %s" % def_val
        if len(col_data[_COLUMN_EXTRA]) > 0:
            if col_data[_COLUMN_EXTRA].upper() != "AUTO_INCREMENT":
                values['extra'] = " %s" % col_data[_COLUMN_EXTRA]
        if len(col_data[_COLUMN_COMMENT]) > 0:
            values['comment'] = " COMMENT '%s'" % col_data[_COLUMN_COMMENT]
        return col_fmt % values

    @staticmethod
    def _get_column_position(destination_def, source_def, destination, source,
                             drop_cols, add_cols):
        """Get the column position in the list

        destination_def[in] destination column definition
        source_def[in]      source column definition
        destination[in]     destination column definitions
        source[in]          source column definitions
        drop_cols[in]       list of columns to be dropped - used to
                            calculate position of existing columns by
                            eliminating those cols in destination that will be
                            dropped
        add_cols[in]        list of columns to be added - used to
                            calculate position of existing columns by
                            eliminating those cols in destination that will be
                            dropped

        Returns string - 'BEFORE' or 'AFTER' for column position or "" if
                         position cannot be determined (add or drop column)
        """

        # Converting ordinal position to index positions:
        #
        #    - ordinal positions start counting at 1
        #    - list indexes start at 0
        #
        # So if you want to find the column that is one less than the ordinal
        # position of the current column, you must subtract 1 then subtract 1
        # again to convert it to the list index.

        dest_loc_idx = None
        src_loc_idx = int(source_def[_COLUMN_ORDINAL_POSITION]) - 1
        if destination_def is not None:
            dest_loc_idx = int(destination_def[_COLUMN_ORDINAL_POSITION]) - 1

        # Check to see if previous column has been dropped. If it has,
        # don't include the BEFORE|AFTER - it will be ordered correctly.
        if dest_loc_idx is not None and dest_loc_idx - 1 >= 0 and \
           destination[dest_loc_idx - 1][_COLUMN_NAME] in drop_cols:
            return ""

        # Check to see if previous column has been added. If it has,
        # don't include the BEFORE|AFTER - it will be ordered correctly.
        if (src_loc_idx - 1 >= 0 and
                source[src_loc_idx - 1][_COLUMN_NAME] in add_cols):
            return ""

        # compare ordinal position - if not the same find where it goes
        if dest_loc_idx is None or dest_loc_idx != src_loc_idx:
            if src_loc_idx == 0:
                return " FIRST"
            for col in source:
                if src_loc_idx == int(col[_COLUMN_ORDINAL_POSITION]):
                    return " AFTER %s" % col[_COLUMN_NAME]
        return ""

    @staticmethod
    def _find_column(name, columns):
        """Find a column in a list by name

        name[in]           name of the column
        columns[in]        list of column definitions

        Returns - column definition or None if column not found
        """
        for col_def in columns:
            if name == col_def[_COLUMN_NAME]:
                return col_def
        return None

    def _get_column_change(self, column, destination, source,
                           drop_cols, add_cols):
        """Identify if column differs and return the changes

        column[in]         column name and operation type
        destination[in]    column definitions for destination
        source[in]         column definitions for source
        drop_cols[in]      list of columns to be dropped - used to
                           calculate position of existing columns
        add_cols[in]       list of columns to be added - used to
                           calculate position of existing columns

        Returns string - new changes for column or ""
        """
        operation = column[1]

        # Get column from the origins
        destination_def = self._find_column(column[0], destination)
        source_def = self._find_column(column[0], source)

        # Here we look for columns that are set for checking the order but
        # the extra data (null, etc.) is different. So we change it to
        # a type change instead. Exclude key column in compare.
        if operation == _CHANGE_COL_ORDER and \
           destination_def[:_COLUMN_KEY] != source_def[:_COLUMN_KEY]:
            operation = _CHANGE_COL_TYPE

        # Check for drop column
        if operation == _DROP_COL:
            colstr = "  DROP COLUMN %s" % destination_def[_COLUMN_NAME]
        else:
            # Determine position and get the type format string
            col_pos = self._get_column_position(destination_def, source_def,
                                                destination, source,
                                                drop_cols, add_cols)
            col_fmt = self._get_column_format(source_def)

            # Check for order changes
            if operation == _CHANGE_COL_ORDER:
                if len(col_pos) > 0:
                    colstr = "  CHANGE COLUMN %s %s %s%s" % \
                             (source_def[_COLUMN_NAME],
                              source_def[_COLUMN_NAME],
                              col_fmt, col_pos)
                else:
                    colstr = ""  # No change needed here
            # Add or change column
            elif operation == _ADD_COL:
                colstr = "  ADD COLUMN %s %s%s" % (source_def[_COLUMN_NAME],
                                                   col_fmt, col_pos)
            else:  # must be change
                colstr = "  CHANGE COLUMN %s %s " % \
                         (destination_def[_COLUMN_NAME],
                          destination_def[_COLUMN_NAME])
                colstr += "%s%s" % (col_fmt, col_pos)

        return colstr

    def _get_columns(self, destination, source):
        """Get the column definition changes

        This method loops through the columns and if different builds ALTER
        statments for transforming the columns of the destination table to the
        source table.

        destination[in]    the original object definition or data
        source[in]         the source object definition or data

        Returns string - ALTER statement or None if no column differences.
        """
        from mysql.utilities.common.dbcompare import get_common_lists

        drop_clauses = []
        add_clauses = []

        # Build lists with minimal matching data (column name and type) for
        # destination and source. Then do the compare. Result is as follows:
        #
        #   - those in both (name, type) will need to be checked for order
        #     of cols to generate CHANGE COLUMN x x <type> BEFORE|AFTER x
        #   - those in destination but not source will be dropped unless the
        #     name appears in source but not destination to generate
        #     DROP COULMN x
        #   - those in destination but not source where the name does appear in
        #     source is a change of type to generate CHANGE COLUMN x x <type>
        #   - those in source but not destination that don't match by name in
        #     destination but not source are new columns to generate
        #     ADD COLUMN x <type>
        #   - those columns that match on both name and type need to be
        #     checked for order changes to generate the
        #     CHANGE COLUMN x BEFORE|AFTER
        #   - we need to check those that the column order changes to see
        #     if they are actually extra col def changes

        dest_min = [item[1:3] for item in destination]  # name, type
        src_min = [item[1:3] for item in source]  # name, type

        # find matches by name + type
        # <both_min>, <dest_src_min>, <src_dest_min> = get_common_lists
        (both_min, _, _,) = get_common_lists(dest_min, src_min)
        dest_src_names = [item[0] for item in dest_min]  # only name
        src_dest_names = [item[0] for item in src_min]  # only name

        # find matches by name only
        both_names = [item[0] for item in both_min]   # only name
        both_check, dest_drop, src_new = get_common_lists(dest_src_names,
                                                          src_dest_names)

        # find matches by name but not type
        both_change_type = list(set(both_check) - set(both_names))

        # remove type changes and form list for checking order
        both_change_order = list(set(both_names) - set(both_change_type))

        column_drops = []
        column_changes = []  # a list of tuples in form (col_name, operation)

        # Form drops
        for col in dest_drop:
            column_drops.append((col, _DROP_COL))

        # Build the drop statements
        for col in column_drops:
            change_str = self._get_column_change(col, destination, source,
                                                 dest_drop, src_new)
            if len(change_str) > 0:
                # if first is specified, push to front of list
                if change_str.endswith(" FIRST"):
                    drop_clauses.insert(0, change_str)
                else:
                    drop_clauses.append(change_str)

        # Form change type
        for col in both_change_type:
            column_changes.append((col, _CHANGE_COL_TYPE))

        # Form add column
        for col in src_new:
            column_changes.append((col, _ADD_COL))

        # Form change order
        for col in both_change_order:
            column_changes.append((col, _CHANGE_COL_ORDER))

        # Build the add/change statements
        for col in column_changes:
            change_str = self._get_column_change(col, destination, source,
                                                 dest_drop, src_new)
            if len(change_str) > 0:
                # if first is specified, push to front of list
                if change_str.endswith(" FIRST"):
                    add_clauses.insert(0, change_str)
                else:
                    add_clauses.append(change_str)

        return (drop_clauses, add_clauses)

    def _get_foreign_keys(self, src_db, src_name, dest_db, dest_name):
        """Get the foreign key constraints

        This method returns the table foreign keys via ALTER TABLE clauses
        gathered from the Table class methods.

        src_db[in]         database name for source table
        src_name[in]       table name for source table
        dest_db[in]        database name for destination table
        dest_name[in]      table name for destination table

        Returns tuple - (drop, add/changes)
        """
        from mysql.utilities.common.table import Table
        from mysql.utilities.common.dbcompare import get_common_lists

        # Get the Table instances
        self.dest_tbl = Table(self.destination_db.source, "%s.%s" %
                              (dest_db, dest_name))
        self.src_tbl = Table(self.source_db.source, "%s.%s" %
                             (src_db, src_name))

        drop_constraints = []
        add_constraints = []

        # Now we do foreign keys
        dest_fkeys = self.dest_tbl.get_tbl_foreign_keys()
        src_fkeys = self.src_tbl.get_tbl_foreign_keys()

        # Now we determine the foreign keys we need to add and those to drop
        # <both_min>, <dest_src_min>, <src_dest_min> = get_common_lists
        _, drop_rows, add_rows = get_common_lists(dest_fkeys, src_fkeys)

        # Generate DROP foreign key clauses
        for fkey in drop_rows:
            drop_constraints.append("  DROP FOREIGN KEY %s" % fkey[0])
            # if fkey[0] not in drop_idx_recorded:
            #    constraints.append("  DROP INDEX %s" % fkey[0])

        # Generate Add foreign key clauses
        clause_fmt = "ADD CONSTRAINT %s FOREIGN KEY(%s) REFERENCES " + \
                     "`%s`.`%s`(%s)"
        for fkey in add_rows:
            add_constraints.append(clause_fmt % fkey)

        return (drop_constraints, add_constraints)

    @staticmethod
    def _get_index_sql_clauses(rows, sql_mode=''):
        """Return the ALTER TABLE index clauses for the table.

        This method returns the SQL index clauses for use in ALTER or CREATE
        TABLE commands for defining the indexes for the table.

        rows[in]           result set of index definitions

        Returns list - list of SQL index clause statements or
                       [] if no indexes
        """
        index_clauses = []

        if rows != []:
            pri_key_cols = []
            unique_indexes = []
            unique_key_cols = []
            unique_name = None
            unique_method = None
            unique_setting = None
            for key in rows:
                if key[2] == 'PRIMARY':
                    q_key = quote_with_backticks(key[4], sql_mode)
                    pri_key_cols.append(q_key)
                else:
                    if unique_name is None:
                        unique_name = key[2]
                        unique_method = key[10]
                        unique_setting = key[1]
                        unique_key_cols.append(key[4])
                    elif unique_name == key[2]:
                        unique_key_cols.append(key[4])
                    else:
                        unique_indexes.append((unique_name, unique_method,
                                               unique_setting,
                                               unique_key_cols))
                        unique_key_cols = []
                        unique_name = key[2]
                        unique_method = key[10]
                        unique_setting = key[1]
                        unique_key_cols.append(key[4])

            # add the last one
            if unique_name is not None:
                unique_indexes.append((unique_name, unique_method,
                                       unique_setting,
                                       unique_key_cols))

            # Build SQL statement clause
            if len(pri_key_cols) > 0:
                index_clauses.append("  ADD PRIMARY KEY(%s)" %
                                     ','.join(pri_key_cols))
            if len(unique_indexes) > 0:
                for idx in unique_indexes:
                    create_idx = "  ADD "
                    if int(idx[2]) != 1:
                        create_idx += "UNIQUE "
                    if idx[1] == "FULLTEXT":
                        create_idx += "FULLTEXT "
                    if (idx[1] == "RTREE"):
                        using = " USING %s" % (idx[1])
                    else:
                        using = ""
                    create_idx += "INDEX %s%s (%s)" % \
                                  (idx[0], using,
                                   ','.join(idx[3]))
                    index_clauses.append(create_idx)

        return index_clauses

    def _get_indexes(self, src_db, src_name, dest_db, dest_name):
        """Get the index constraints

        This method returns the table primary keys, and other indexes via
        ALTER TABLE clauses gathered from the Table class methods.

        src_db[in]         database name for source table
        src_name[in]       table name for source table
        dest_db[in]        database name for destination table
        dest_name[in]      table name for destination table

        Returns tuple - (drop, add/changes)
        """
        from mysql.utilities.common.table import Table
        from mysql.utilities.common.dbcompare import get_common_lists

        # Get the Table instances
        self.dest_tbl = Table(self.destination_db.source, "%s.%s" %
                              (dest_db, dest_name))
        self.src_tbl = Table(self.source_db.source, "%s.%s" %
                             (src_db, src_name))

        drop_indexes = []
        add_indexes = []

        # Get the list of indexes
        # Do not compare with the name of the tables
        dest_idx = [('',) + tuple(idx[1:])
                    for idx in self.dest_tbl.get_tbl_indexes()]
        src_idx = [('',) + tuple(idx[1:])
                   for idx in self.src_tbl.get_tbl_indexes()]

        # Now we determine the indexes we need to add and those to drop
        _, drop_idx, add_idx = get_common_lists(dest_idx, src_idx)
        if not drop_idx and not add_idx:
            return ([], [])

        # Generate DROP index clauses
        drop_idx_recorded = []  # used to avoid duplicate index drops
        for index in drop_idx:
            if index[2] == "PRIMARY":
                drop_indexes.append("  DROP PRIMARY KEY")
            elif index[2] not in drop_idx_recorded:
                drop_indexes.append("  DROP INDEX %s" % index[2])
                drop_idx_recorded.append(index[2])

        # Generate ADD index clauses
        if len(add_idx) > 0:
            add_indexes.extend(self._get_index_sql_clauses(
                add_idx,
                self.dest_tbl.sql_mode
            ))

        return (drop_indexes, add_indexes)

    @staticmethod
    def _check_for_partitions(destination_row, source_row):
        """Determine if there are transformations involving partitions

        This method returns TRUE if the destination and source differ in
        partitioning configurations

        destination_row[in] the original object definition or data
        source_row[in]      the source object definition or data

        Returns bool - True = differences found, False = no differences
        """
        #
        # TODO: Complete this operation with a new worklog.
        #       This release does not support transformation of partitions.

        part_changes_found = False
        if len(destination_row) != len(source_row):
            part_changes_found = True
        elif len(destination_row) == 0:
            return None
        elif len(destination_row) == 1:
            if not (destination_row[0][3] is None and
                    source_row[0][3] is None):
                part_changes_found = True
        else:
            part_stop = len(destination_row)
            row_stop = len(destination_row[0])
            for i in range(0, part_stop):
                for j in range(0, row_stop):
                    if destination_row[i][j] != source_row[i][j]:
                        part_changes_found = True
                        break
        return part_changes_found

    def _transform_table(self):
        """Transform a table definition

        This method will transform a table definition to match the source
        configuration. It returns the ALTER TABLE SQL statement to
        transform the object or None if no transformation is needed.

        Note: The incoming lists contain a tuple defined as:
              (table definitions, columns, partitions, constraints)
              for destination and source.

        Returns list - ALTER TABLE statements for transforming the table
        """
        statements = []

        # Collect a list of all of the ALTER clauses. Order is important in
        # building an ALTER TABLE statement. For safety (and correct execution)
        # we must order the clauses as follows:
        #
        #  - drop foreign key constraints
        #  - drop indexes
        #  - drop columns
        #  - add/change columns
        #  - add/change indexes
        #  - add/change foreign keys
        #  - general table changes
        #
        #  Note: partition changes not supported by this release

        src_db_name = self.source[_TABLE_DEF][_TABLE_DB]
        src_tbl_name = self.source[_TABLE_DEF][_TABLE_NAME]
        dest_db_name = self.destination[_TABLE_DEF][_TABLE_DB]
        dest_tbl_name = self.destination[_TABLE_DEF][_TABLE_NAME]

        # Quote identifiers with bacticks
        src_sql_mode = self.source_db.sql_mode
        q_src_db_name = quote_with_backticks(src_db_name, src_sql_mode)
        q_src_tbl_name = quote_with_backticks(src_tbl_name, src_sql_mode)
        dest_sql_mode = self.destination_db.sql_mode
        q_dest_db_name = quote_with_backticks(dest_db_name, dest_sql_mode)
        q_dest_tbl_name = quote_with_backticks(dest_tbl_name, dest_sql_mode)

        # build a list of the parts
        statement_parts = [
            # preamble
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': "ALTER TABLE"},
            # object name
            {'fmt': " %s.%s", 'col': _IGNORE_COLUMN,
             'val': (q_dest_db_name, q_dest_tbl_name)},
            # alter clauses - will be completed later
            {'fmt': " \n%s", 'col': _IGNORE_COLUMN, 'val': ""},
        ]

        # For foreign key changes, we need two collections: drop statements,
        # add and change statements. Method returns tuple of (drop, add).
        fkeys = self._get_foreign_keys(q_src_db_name, q_src_tbl_name,
                                       q_dest_db_name, q_dest_tbl_name)

        # For index changes, we need two collections: drop statements, add and
        # change statements. Method returns tuple of (drop, add).
        indexes = self._get_indexes(q_src_db_name, q_src_tbl_name,
                                    q_dest_db_name, q_dest_tbl_name)

        # For column changes, we need two collections: drop statements, add and
        # change statements. Method returns tuple of (drop, add/change).
        columns = self._get_columns(self.destination[_COLUMN_DEF],
                                    self.source[_COLUMN_DEF])

        # Now add drops then add/changes
        for i in range(0, 2):
            statements.extend(fkeys[i])
            statements.extend(indexes[i])
            statements.extend(columns[i])

        # General definition returns a single string of the option changes
        if not self.skip_table_opts:
            gen_defn = self._get_table_defns(self.destination[_TABLE_DEF],
                                             self.source[_TABLE_DEF])
        else:
            gen_defn = None

        if gen_defn is not None:
            statements.append(gen_defn)

        # Form the SQL command.
        statement_parts[2]['val'] = ', \n'.join(statements)

        sql_stmts = ["%s;" % self._build_statement(statement_parts)]

        # Currently, we check partitions last because this code will
        # generate a warning message. Later once this code it complete,
        # it can be moved where it belongs in the order of creation of
        # the ALTER TABLE statement
        if self._check_for_partitions(self.destination[_PART_DEF],
                                      self.source[_PART_DEF]):
            sql_stmts.append("# WARNING: Partition transformation is not "
                             "supported in this release.\n# Please check "
                             "the table definitions for partition changes.")

        return sql_stmts

    def _transform_view(self):
        """Transform a view definition

        This method will transform a view definition to match the source
        configuration. It returns the CREATE OR ALTER VIEW SQL statement to
        transform the object or None if no transformation is needed.

        Returns list - ALTER VIEW statement for transforming the view
        """
        statements = []

        # check for create
        do_create = self._check_columns([_VIEW_CHECK])

        # build a list of the parts
        statement_parts = [
            # preamble
            {'fmt': "%s", 'col': _IGNORE_COLUMN,
             'val': "CREATE" if do_create else "ALTER"},
            # definer
            {'fmt': " DEFINER=%s", 'col': _VIEW_DEFINER, 'val': ""},
            # security
            {'fmt': " SQL SECURITY %s", 'col': _VIEW_SECURITY, 'val': ""},
            # object type and name
            {'fmt': " VIEW %s.%s", 'col': _IGNORE_COLUMN,
             'val': (self.destination[_VIEW_DB],
                     self.destination[_VIEW_NAME])},
            # definition
            {'fmt': " AS \n  %s", 'col': _VIEW_BODY, 'val': ""},
            # check option (will be updated later)
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': ""}
        ]

        changes = False
        # view check option is special - we have to handle that separately
        if self.destination[_VIEW_CHECK] != self.source[_VIEW_CHECK]:
            if self.source[_VIEW_CHECK].upper() != 'NONE':
                check = statement_parts[5]
                check['val'] = " WITH %s CHECK OPTION" % \
                               self.source[_VIEW_CHECK]
            changes = True

        # if no changes, return None
        if not changes and not self._fill_values(statement_parts, do_create):
            return None

        # check to see if definer or security or check option have changed and
        # if so add definition (always needed if these change)
        if self._check_columns([_VIEW_DEFINER, _VIEW_SECURITY, _VIEW_CHECK]):
            statement_parts[4]['val'] = self.source[_VIEW_BODY]

        # form the drop if we do a create
        if do_create:
            statements.append("DROP VIEW IF EXISTS `%s`.`%s`;" %
                              (self.destination[_VIEW_DB],
                               self.destination[_VIEW_NAME]))

        sql_stmt = "%s;" % self._build_statement(statement_parts)
        statements.append(sql_stmt)

        return statements

    def _transform_trigger(self):
        """Transform a trigger definition

        This method will transform a trigger definition to match the source
        configuration. It returns the appropriate SQL statement(s) to
        transform the object or None if no transformation is needed.

        Returns list - SQL statement(s) for transforming the trigger
        """
        statements = []

        # build a list of the parts
        statement_parts = [
            # preamble
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': "CREATE"},
            # definer
            {'fmt': " DEFINER=%s", 'col': _TRIGGER_DEFINER, 'val': ""},
            # object name
            {'fmt': " TRIGGER %s.%s", 'col': _IGNORE_COLUMN,
             'val': (self.destination[_TRIGGER_DB],
                     self.destination[_TRIGGER_NAME])},
            # trigger timing
            {'fmt': " %s", 'col': _TRIGGER_TIME, 'val': ""},
            # trigger event
            {'fmt': " %s", 'col': _TRIGGER_EVENT, 'val': ""},
            # trigger table
            {'fmt': " ON %s." % self.destination[_TRIGGER_DB] +
                    "%s FOR EACH ROW",
             'col': _TRIGGER_TABLE, 'val': ""},
            # trigger body
            {'fmt': " %s;", 'col': _TRIGGER_BODY, 'val': ""},
        ]

        # Triggers don't have ALTER SQL so we just pass back a drop + create.
        # if no changes, return None
        if not self._fill_values(statement_parts, True):
            return None

        statements.append("DROP TRIGGER IF EXISTS `%s`.`%s`;" %
                          (self.destination[_TRIGGER_DB],
                           self.destination[_TRIGGER_NAME]))

        sql_stmt = self._build_statement(statement_parts)
        statements.append(sql_stmt)

        return statements

    def _transform_routine(self):
        """Transform a routine definition

        This method will transform a routine (FUNCTION or PROCEDURE) definition
        to match the source configuration. It returns the ALTER [FUNCTION |
        PROCEDURE] SQL statement to transform the object or None if no
        transformation is needed.

        Returns list - [CREATE|ALTER] [FUNCTION|PROCEDURE] statement for
                       transforming the routine
        """
        statements = []

        # check for create
        do_create = self._check_columns([_ROUTINE_BODY,
                                         _ROUTINE_DEFINER,
                                         _ROUTINE_PARAMS])

        # Quote destination db and routine names with backticks
        dest_sql_mode = self.destination_db.sql_mode
        q_dest_db = quote_with_backticks(self.destination[_ROUTINE_DB],
                                         dest_sql_mode)
        q_dest_routine = quote_with_backticks(self.destination[_ROUTINE_NAME],
                                              dest_sql_mode)

        # build a list of the parts
        statement_parts = [
            # delimiter
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': "DELIMITER //\n"},
            # preamble
            {'fmt': "%s", 'col': _IGNORE_COLUMN,
             'val': "CREATE" if do_create else "ALTER"},
            # definer
            {'fmt': " DEFINER=%s", 'col': _ROUTINE_DEFINER,
             'val': ""},
            # object type and name
            {'fmt': " %s %s.%s", 'col': _IGNORE_COLUMN,
             'val': (self.obj_type.upper(), q_dest_db, q_dest_routine)},
            # parameters
            {'fmt': " %s", 'col': _IGNORE_COLUMN, 'val': ""},
            # returns (Functions only)
            {'fmt': " RETURNS %s", 'col': _IGNORE_COLUMN, 'val': ""},
            # access method
            {'fmt': " %s", 'col': _ROUTINE_SQL_DATA_ACCESS, 'val': ""},
            # deterministic (Functions only)
            {'fmt': " %s", 'col': _IGNORE_COLUMN, 'val': ""},
            # security
            {'fmt': " SQL SECURITY %s", 'col': _ROUTINE_SECURITY_TYPE,
             'val': ""},
            # comment
            {'fmt': " COMMENT '%s'", 'col': _ROUTINE_COMMENT, 'val': ""},
            # body
            {'fmt': " %s", 'col': _ROUTINE_BODY, 'val': ""},
            # reset delimiter
            {'fmt': "%s", 'col': _IGNORE_COLUMN,
             'val': "//\nDELIMITER ;\n"},
        ]

        # if no changes, return None
        if not self._fill_values(statement_parts, do_create):
            return None

        # Add parameters and DEFINER if CREATE statement.
        if do_create:
            statement_parts[4]['val'] = \
                '({0})'.format(self.source[_ROUTINE_PARAMS])

            # Quote DEFINER with backticks
            statement_parts[2]['val'] = \
                quote_with_backticks_definer(self.source[_ROUTINE_DEFINER],
                                             dest_sql_mode)

        # Add the returns for functions
        # Only when doing create or modifications to the body
        if self.obj_type.upper() == "FUNCTION":
            if (do_create or
                    (self.destination[_ROUTINE_BODY] !=
                     self.source[_ROUTINE_BODY])):
                statement_parts[5]['val'] = self.source[_ROUTINE_RETURNS]
            # Add deterministic
            if do_create:
                if self.source[_ROUTINE_IS_DETERMINISTIC] == "YES":
                    statement_parts[7]['val'] = "DETERMINISTIC"
                else:
                    statement_parts[7]['val'] = "NOT DETERMINISTIC"

        # form the drop if we do a create
        if do_create:
            statements.append(
                "DROP {0} IF EXISTS {1}.{2};".format(
                    self.obj_type.upper(), q_dest_db, q_dest_routine
                )
            )

        statements.append(self._build_statement(statement_parts))

        return statements

    def _transform_event(self):
        """Transform a event definition

        This method will transform a event definition to match the source
        configuration. It returns the ALTER EVENT SQL statement to
        transform the object or None if no transformation is needed.

        Notes:

            The DEFINER does not compare properly for SHOW CREATE EVENT
            comparison.

            The RENAME cannot be processed because it requires a different
            name and mysqldiff compares on like names.

        Returns list - ALTER EVENT statement for transforming the event
        """
        statements = []

        # build a list of the parts
        statement_parts = [
            # preamble
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': "ALTER"},
            # definer
            {'fmt': " DEFINER=%s", 'col': _EVENT_DEFINER, 'val': ""},
            # type
            {'fmt': " %s", 'col': _IGNORE_COLUMN, 'val': "EVENT"},
            # object name
            {'fmt': " %s.%s", 'col': _IGNORE_COLUMN,
             'val': (self.destination[_EVENT_DB],
                     self.destination[_EVENT_NAME])},
            # schedule - will be filled in later
            {'fmt': " %s", 'col': _IGNORE_COLUMN, 'val': ""},
            # complete
            {'fmt': " ON COMPLETION %s", 'col': _EVENT_ON_COMPLETION,
             'val': ""},
            # rename
            {'fmt': " RENAME TO %s", 'col': _EVENT_NAME, 'val': ""},
            # status
            {'fmt': " %s", 'col': _EVENT_STATUS,
             'val': self.source[_EVENT_STATUS]},
            # event body
            {'fmt': " DO %s", 'col': _EVENT_BODY, 'val': ""},
        ]

        # We can only do the columns we know about and must ignore the others
        # like STARTS which may be Ok to differ.
        changes = self._check_columns([_EVENT_ON_COMPLETION, _EVENT_STATUS,
                                       _EVENT_BODY, _EVENT_NAME, _EVENT_ENDS,
                                       _EVENT_INTERVAL_FIELD, _EVENT_STARTS,
                                       _EVENT_INTERVAL_VALUE, _EVENT_TYPE])

        # We do the schedule separately because requires additional checks
        if changes:
            schedule = statement_parts[4]
            schedule['val'] = "ON SCHEDULE"
            if self.source[_EVENT_TYPE].upper() == "RECURRING":
                schedule['val'] += " EVERY %s" % \
                                   self.source[_EVENT_INTERVAL_VALUE]
            schedule['val'] += " %s" % \
                               self.source[_EVENT_INTERVAL_FIELD].upper()
            if self.source[_EVENT_STARTS] is not None:
                schedule['val'] += " STARTS '%s'" % self.source[_EVENT_STARTS]
            if self.source[_EVENT_ENDS] is not None:
                schedule['val'] += " ENDS '%s'" % self.source[_EVENT_ENDS]

        # if no changes, return None
        if not changes:
            return None

        self._fill_values(statement_parts, False)

        # We must fix the status value
        status = statement_parts[7]
        if status['val'].upper() == "DISABLED":
            status['val'] = "DISABLE"
        elif status['val'].upper() == "ENABLED":
            status['val'] = "ENABLE"
        elif status['val'].upper() == "SLAVESIDE_DISABLED":
            status['val'] = "DISABLE ON SLAVE"

        sql_stmt = "%s;" % self._build_statement(statement_parts)
        statements.append(sql_stmt)

        return statements

    def _check_columns(self, col_list, destination=None, source=None):
        """Check for special column changes to trigger a CREATE

        This method checks a specific list of columns to see if the values
        differ from the destination and source. If they do, the method returns
        True else it returns False.

        col_list[in]       a list of column numbers to check
        destination[in]    If not None, use this list for destination
                           (default = None)
        source[in]         If not None, use this list for source
                           (default = None)

        Returns bool - True = there are differences, False = no differences
        """
        if destination is None:
            destination = self.destination
        if source is None:
            source = self.source
        for column_num in col_list:
            if destination[column_num] != source[column_num]:
                return True
        return False

    def _fill_values(self, stmt_parts, create=False,
                     destination=None, source=None):
        """Fill the structure with values

        This method loops through all of the column dictionaries filling in
        the value for any that differ from the destination to the source. If
        create is True, it will also fill in the values from the source to
        permit the completion of a CREATE statement.

        stmt_parts[in]     a list of column dictionaries
        create[in]         if True, fill in all values
                           if False, fill in only those values that differ
                           (default = False)
        destination[in]         If not None, use this list for destination
                           (default = None)
        source[in]         If not None, use this list for source
                           (default = None)

        Returns bool - True if changes found
        """
        if destination is None:
            destination = self.destination
        if source is None:
            source = self.source
        changes_found = False
        for part in stmt_parts:
            col = part['col']
            if col != _IGNORE_COLUMN:
                if source[col] is not None and destination[col] != source[col]:
                    part['val'] = source[col]
                    changes_found = True
                elif create:
                    part['val'] = destination[col]

        return changes_found

    @staticmethod
    def _build_statement(stmt_parts):
        """Build the object definition statement

        This method will build a completed statement based on the list of parts
        provided.

        stmt_parts[in]     a list of column dictionaries
        create[in]         if True, fill in all values
                           if False, fill in only those values that differ
                           (default = False)

        Returns string - the object definition string
        """
        stmt_values = []
        for part in stmt_parts:
            if part['col'] == _FORCE_COLUMN or part['val'] != "":
                stmt_values.append(part['fmt'] % part['val'])

        return ''.join(stmt_values)
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains abstractions of a MySQL table and an index.
"""

import multiprocessing
import sys
from itertools import izip

from mysql.connector.conversion import MySQLConverter

# Constants
_MAXPACKET_SIZE = 1024 * 1024
_MAXBULK_VALUES = 25000
_MAXTHREADS_INSERT = 6
_MAXROWS_PER_THREAD = 100000
_MAXAVERAGE_CALC = 100

_FOREIGN_KEY_QUERY = """
  SELECT CONSTRAINT_NAME, COLUMN_NAME, REFERENCED_TABLE_SCHEMA,
         REFERENCED_TABLE_NAME, REFERENCED_COLUMN_NAME
  FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
  WHERE TABLE_SCHEMA = '%s' AND TABLE_NAME = '%s' AND
        REFERENCED_TABLE_SCHEMA IS NOT NULL
"""


class Index(object):
    """
    The Index class encapsulates an index for a given table as defined by
    the output of SHOW INDEXES FROM. The class has the following
    capabilities:

        - Check for duplicates
        - Create DROP statement for index
        - Print index CREATE statement
    """

    def __init__(self, db, index_tuple, verbose=False, sql_mode=''):
        """Constructor

        db[in]             Name of database
        index_tuple[in]    A tuple from the get_tbl_indexes() result set
        verbose[in]        print extra data during operations (optional)
                           default value = False
        """

        # Initialize and save values
        self.db = db
        self.sql_mode = sql_mode
        self.q_db = quote_with_backticks(db, self.sql_mode)
        self.verbose = verbose
        self.columns = []
        self.table = index_tuple[0]
        self.q_table = quote_with_backticks(index_tuple[0], self.sql_mode)
        self.unique = not int(index_tuple[1])
        self.name = index_tuple[2]
        self.q_name = quote_with_backticks(index_tuple[2], self.sql_mode)
        col = (index_tuple[4], index_tuple[7])
        self.columns.append(col)
        self.accept_nulls = True if index_tuple[9] else False
        self.type = index_tuple[10]
        self.compared = False                    # mark as compared for speed
        self.duplicate_of = None                 # saves duplicate index
        # pylint: disable=R0102
        if index_tuple[7] > 0:
            self.column_subparts = True          # check subparts e.g. a(20)
        else:
            self.column_subparts = False

    @staticmethod
    def __cmp_columns(col_a, col_b):
        """Compare two columns on name and subpart lengths if present

        col_a[in]          First column to compare
        col_b[in]          Second column to compare

        Returns True if col_a has the same name as col_b and if the
        subparts are col_a.sub <= col_b.sub.
        """

        sz_this = col_a[1]
        sz_that = col_b[1]
        # if column has the same name
        if col_a[0] == col_b[0]:
            # if they both have sub_parts, compare them
            if sz_this and sz_that:
                return (sz_this <= sz_that)
            # if this index has a sub_part and the other does
            # not, it is potentially redundant
            elif sz_this and sz_that is None:
                return True
            # if neither have sub_parts, it is a match
            elif sz_this is None and sz_that is None:
                return True
        else:
            return False  # no longer a duplicate

    def __check_column_list(self, index):
        """Compare the column list of this index with another

        index[in]          Instance of Index to compare

        Returns True if column list is a subset of index.
        """

        num_cols_this = len(self.columns)
        num_cols_that = len(index.columns)
        same_size = num_cols_this == num_cols_that
        if self.type == "BTREE":
            indexes = izip(self.columns, index.columns)
            for idx_pair in indexes:
                if not self.__cmp_columns(*idx_pair):
                    return False
            # All index pairs are the same, so return index with smaller number
            # of columns.
            return num_cols_this <= num_cols_that
        else:  # HASH, RTREE, FULLTEXT
            if self.type != "FULLTEXT":
                # For RTREE or HASH type indexes, an index is redundant if
                # it has the exact same columns on the exact same order.
                indexes = izip(self.columns, index.columns)
                return (same_size and
                        all((self.__cmp_columns(*idx_pair)
                             for idx_pair in indexes)))
            else:  # FULLTEXT index
                # A FULLTEXT index A is redundant of FULLTEXT index B if
                # the columns of A are a subset of B's columns, the order
                # does not matter.
                return all(any(self.__cmp_columns(col, icol) for
                               icol in index.columns) for col in self.columns)

    def is_duplicate(self, index):
        """Compare this index with another

        index[in]          Instance of Index to compare

        Returns True if this index is a subset of the Index presented.
        """

        # Don't compare the same index - no two indexes can have the same name
        if self.name == index.name:
            return False
        else:
            return self.__check_column_list(index)

    def contains_columns(self, col_names):
        """Check if the current index contains the columns of the given index.

        Returns True if it contains all the columns of the given index,
        otherwise False.
        """
        if len(self.columns) < len(col_names):
            # If has less columns than given index it does not contain all.
            return False
        else:
            this_col_names = [col[0] for col in self.columns]
            # Check if all index column are included in current one..
            for col_name in col_names:
                if col_name not in this_col_names:
                    return False  # found one column not included.

        # Pass previous verification; contains all the columns of given index.
        return True

    def add_column(self, column, sub_part, accept_null):
        """Add a column to the list of columns for this index

        column[in]         Column to add
        sub_part[in]       Sub part of colunm e.g. a(20)
        accept_null[in]       True to indicate the column accepts nulls
        """

        col = (column, sub_part)
        if sub_part > 0:
            self.column_subparts = True
        if accept_null:
            self.accept_nulls = True
        self.columns.append(col)

    def get_drop_statement(self):
        """Get the drop statement for this index

        Note: Ignores PRIMARY key indexes.

        Returns the DROP statement for this index.
        """
        if self.name == "PRIMARY":
            return None
        query_str = "ALTER TABLE {db}.{table} DROP INDEX {name}".format(
            db=self.q_db, table=self.q_table, name=self.q_name
        )
        return query_str

    def get_remove_columns_statement(self, col_names):
        """Get the ALTER TABLE statement to remove columns for this index.

        col_names[in]   list of columns names to remove from the index.

        Returns the ALTER TABLE statement (DROP/ADD) to remove the given
        columns names from the index.
        """
        # Create the new columns list for the index.
        idx_cols = [col[0] for col in self.columns if col[0] not in col_names]
        if not idx_cols:
            # Return a DROP statement if no columns are left.
            query_str = "ALTER TABLE {db}.{table} DROP INDEX {name}".format(
                db=self.q_db, table=self.q_table, name=self.q_name
            )
        else:
            # Otherwise, return a DROP/ADD statement with remaining columns.
            idx_cols_str = ', '.join(idx_cols)
            query_str = ("ALTER TABLE {db}.{table} DROP INDEX {name}, "
                         "ADD INDEX {name} ({cols})".format(db=self.q_db,
                                                            table=self.q_table,
                                                            name=self.q_name,
                                                            cols=idx_cols_str))
        return query_str

    def __get_column_list(self, backtick_quoting=True):
        """Get the column list for an index

        This method is used to print the CREATE and DROP statements.

        backtick_quoting[in]    Indicates if the columns names are to be quoted
                                with backticks or not. By default: True.

        Returns a string representing the list of columns for a
        column list. e.g. 'a, b(10), c'
        """
        col_list = []
        for col in self.columns:
            name, sub_part = (col[0], col[1])
            if backtick_quoting:
                name = quote_with_backticks(name, self.sql_mode)
            if sub_part > 0:
                col_str = "{0}({1})".format(name, sub_part)
            else:
                col_str = name
            col_list.append(col_str)
        return ', '.join(col_list)

    def print_index_sql(self):
        """Print the CREATE INDEX for indexes and ALTER TABLE for a primary key
        """
        if self.name == "PRIMARY":
            print("ALTER TABLE {db}.{table} ADD PRIMARY KEY ({cols})"
                  "".format(db=self.q_db, table=self.q_table,
                            cols=self.__get_column_list()))
        else:
            create_str = ("CREATE {unique}{fulltext}INDEX {name} ON "
                          "{db}.{table} ({cols}) {using}")
            unique_str = 'UNIQUE ' if self.unique else ''
            fulltext_str = 'FULLTEXT ' if self.type == 'FULLTEXT' else ''
            if (self.type == "BTREE") or (self.type == "RTREE"):
                using_str = 'USING {0}'.format(self.type)
            else:
                using_str = ''
            print(create_str.format(unique=unique_str, fulltext=fulltext_str,
                                    name=self.q_name, db=self.q_db,
                                    table=self.q_table,
                                    cols=self.__get_column_list(),
                                    using=using_str))

    def get_row(self, verbosity=0):
        """Return index information as a list of columns for tabular output.
        """
        cols = self.__get_column_list(backtick_quoting=False)
        if verbosity > 0:
            return (self.db, self.table, self.name, self.type, self.unique,
                    self.accept_nulls, cols)
        return (self.db, self.table, self.name, self.type, cols)


class Table(object):
    """
    The Table class encapsulates a table for a given database. The class
    has the following capabilities:

        - Check to see if the table exists
        - Check indexes for duplicates and redundancies
        - Print list of indexes for the table
        - Extract table data
        - Import table data
        - Copy table data
    """

    def __init__(self, server1, name, options=None):
        """Constructor

        server[in]         A Server object
        name[in]           Name of table in the form (db.table)
        options[in]        options for class: verbose, quiet, get_cols,
            quiet     If True, do not print information messages
            verbose   print extra data during operations (optional)
                      (default is False)
            get_cols  If True, get the column metadata on construction
                      (default is False)
        """
        if options is None:
            options = {}
        self.verbose = options.get('verbose', False)
        self.quiet = options.get('quiet', False)
        self.server = server1

        # Get sql_mode set on server
        self.sql_mode = self.server.select_variable("SQL_MODE")

        # Keep table identifier considering backtick quotes
        if is_quoted_with_backticks(name, self.sql_mode):
            self.q_table = name
            self.q_db_name, self.q_tbl_name = parse_object_name(name,
                                                                self.sql_mode)
            self.db_name = remove_backtick_quoting(self.q_db_name,
                                                   self.sql_mode)
            self.tbl_name = remove_backtick_quoting(self.q_tbl_name,
                                                    self.sql_mode)
            self.table = ".".join([self.db_name, self.tbl_name])
        else:
            self.table = name
            self.db_name, self.tbl_name = parse_object_name(name,
                                                            self.sql_mode)
            self.q_db_name = quote_with_backticks(self.db_name, self.sql_mode)
            self.q_tbl_name = quote_with_backticks(self.tbl_name,
                                                   self.sql_mode)
            self.q_table = ".".join([self.q_db_name, self.q_tbl_name])
        self.obj_type = "TABLE"
        self.pri_idx = None

        # We store each type of index in a separate list to make it easier
        # to manipulate
        self.btree_indexes = []
        self.hash_indexes = []
        self.rtree_indexes = []
        self.fulltext_indexes = []
        self.unique_not_null_indexes = None
        self.text_columns = []
        self.blob_columns = []
        self.bit_columns = []
        self.column_format = None
        self.column_names = []
        self.column_name_type = []
        self.q_column_names = []
        self.indexes_q_names = []
        if options.get('get_cols', False):
            self.get_column_metadata()
        self.dest_vals = None
        self.storage_engine = None

        # Get max allowed packet
        res = self.server.exec_query("SELECT @@session.max_allowed_packet")
        if res:
            self.max_packet_size = res[0][0]
        else:
            self.max_packet_size = _MAXPACKET_SIZE
        # Watch for invalid values
        if self.max_packet_size > _MAXPACKET_SIZE:
            self.max_packet_size = _MAXPACKET_SIZE

        self._insert = "INSERT INTO %s.%s VALUES "
        self.query_options = {  # Used for skipping fetch of rows
            'fetch': False
        }

    def exists(self, tbl_name=None):
        """Check to see if the table exists

        tbl_name[in]       table name (db.table)
                           (optional) If omitted, operation is performed
                           on the class instance table name.

        return True = table exists, False = table does not exist
        """

        db, table = (None, None)
        if tbl_name:
            db, table = parse_object_name(tbl_name, self.sql_mode)
        else:
            db = self.db_name
            table = self.tbl_name
        res = self.server.exec_query("SELECT TABLE_NAME " +
                                     "FROM INFORMATION_SCHEMA.TABLES " +
                                     "WHERE TABLE_SCHEMA = '%s'" % db +
                                     " and TABLE_NAME = '%s'" % table)

        return (res is not None and len(res) >= 1)

    def get_column_metadata(self, columns=None):
        """Get information about the table for the bulk insert operation.

        This method builds lists that describe the metadata of the table. This
        includes lists for:

          column names
          column format for building VALUES clause
          blob fields - for use in generating INSERT/UPDATE for blobs
          text fields - for use in checking for single quotes

        columns[in]        if None, use EXPLAIN else use column list.
        """

        if columns is None:
            columns = self.server.exec_query("explain %s" % self.q_table)
        stop = len(columns)
        self.column_names = []
        self.q_column_names = []
        col_format_values = [''] * stop
        if columns is not None:
            for col in range(0, stop):
                if is_quoted_with_backticks(columns[col][0], self.sql_mode):
                    self.column_names.append(
                        remove_backtick_quoting(columns[col][0],
                                                self.sql_mode))
                    self.q_column_names.append(columns[col][0])
                else:
                    self.column_names.append(columns[col][0])
                    self.q_column_names.append(
                        quote_with_backticks(columns[col][0], self.sql_mode))
                col_type = columns[col][1].lower()
                if ('char' in col_type or 'enum' in col_type or
                        'set' in col_type or 'binary' in col_type):
                    self.text_columns.append(col)
                    col_format_values[col] = "'%s'"
                elif 'blob' in col_type or 'text'in col_type:
                    self.blob_columns.append(col)
                    col_format_values[col] = "%s"
                elif "date" in col_type or "time" in col_type:
                    col_format_values[col] = "'%s'"
                elif "bit" in col_type:
                    self.bit_columns.append(col)
                    col_format_values[col] = "%d"
                else:
                    col_format_values[col] = "%s"
        self.column_format = "%s%s%s" % \
                             (" (", ', '.join(col_format_values), ")")

    def get_col_names(self, quote_backticks=False):
        """Get column names for the export operation.

        quote_backticks[in]    If True the column names will be quoted with
                               backticks. Default is False.

        Return (list) column names
        """

        if self.column_format is None:
            self.column_names = []
            self.q_column_names = []
            rows = self.server.exec_query("explain {0}".format(self.q_table))
            for row in rows:
                self.column_names.append(row[0])
                self.q_column_names.append(quote_with_backticks(row[0],
                                                                self.sql_mode))

        return self.q_column_names if quote_backticks else self.column_names

    def get_col_names_types(self, quote_backticks=False):
        """Get a list of tuples of column name and type.

        quote_backticks[in]    If True the column name will be quoted with
                               backticks. Default is False.

        Return (list) of touple (column name, type)
        """

        self.column_name_type = []
        rows = self.server.exec_query("explain {0}".format(self.q_table))
        for row in rows:
            if quote_backticks:
                self.column_name_type.append(
                    [quote_with_backticks(row[0], self.sql_mode)] +
                    list(row[1:])
                )
            else:
                self.column_name_type.append(row)

        return self.column_name_type

    def has_index(self, index_q_name):
        """A method to determine if this table has a determinate index using
        his name.

        index_q_name[in]    the name of the index (must be quoted).

        returns True if this Table has an index with the given name, otherwise
        false.
        """
        if [idx_q_name for idx_q_name in self.indexes_q_names
                if idx_q_name == index_q_name]:
            return True
        return False

    def get_not_null_unique_indexes(self, refresh=False):
        """get all the unique indexes which columns does not accepts null
        values.
        refresh[in] Boolean value used to force the method to read index
                    information directly from the server, instead of using
                    cached values.

        Returns list of indexes.
        """
        # First check if the instance variable exists.
        if self.unique_not_null_indexes is None or refresh:
            # Get the indexes for the table.
            try:
                self.get_indexes()
            except UtilDBError:
                # Table may not exist yet. Happens on import operations.
                pass
            # Now for each of them, check if they are UNIQUE and NOT NULL.
            no_null_idxes = []
            no_null_idxes.extend(
                [idx for idx in self.btree_indexes if not idx.accept_nulls and
                 idx.unique]
            )
            no_null_idxes.extend(
                [idx for idx in self.hash_indexes if not idx.accept_nulls and
                 idx.unique]
            )
            no_null_idxes.extend(
                [idx for idx in self.rtree_indexes if not idx.accept_nulls and
                 idx.unique]
            )
            no_null_idxes.extend(
                [idx for idx in self.fulltext_indexes
                 if not idx.accept_nulls and idx.unique]
            )
            self.unique_not_null_indexes = no_null_idxes

        return self.unique_not_null_indexes

    def _build_update_blob(self, row, new_db, name):
        """Build an UPDATE statement to update blob fields.

        row[in]            a row to process
        new_db[in]         new database name
        name[in]           name of the table

        Returns UPDATE string
        """
        if self.column_format is None:
            self.get_column_metadata()

        blob_insert = "UPDATE %s.%s SET " % (new_db, name)
        where_values = []
        do_commas = False
        has_data = False
        stop = len(row)
        for col in range(0, stop):
            col_name = self.q_column_names[col]
            if col in self.blob_columns:
                if row[col] is not None and len(row[col]) > 0:
                    if do_commas:
                        blob_insert += ", "
                    blob_insert += "%s = " % col_name + "%s" % \
                                   MySQLConverter().quote(
                                       convert_special_characters(row[col]))
                    has_data = True
                    do_commas = True
            else:
                # Convert None values to NULL (not '' to NULL)
                if row[col] is None:
                    value = 'NULL'
                else:
                    value = "'{0}'".format(row[col])
                where_values.append("{0} = {1}".format(col_name, value))
        if has_data:
            return "{0} WHERE {1};".format(blob_insert,
                                           " AND ".join(where_values))
        return None

    def _build_insert_blob(self, row, new_db, tbl_name):
        """Build an INSERT statement for the given row.

        row[in]                a row to process
        new_db[in]             new database name
        tbl_name[in]           name of the table

        Returns INSERT string.
        """
        if self.column_format is None:
            self.get_column_metadata()

        converter = MySQLConverter()
        row_vals = []
        # Deal with blob, special characters and NULL values.
        for index, column in enumerate(row):
            # pylint: disable=W0212
            if index in self.blob_columns:
                row_vals.append(converter.quote(
                    convert_special_characters(column)))
            elif index in self.text_columns:
                if column is None:
                    row_vals.append("NULL")
                else:
                    row_vals.append(convert_special_characters(column))
            elif index in self.bit_columns:
                if column is None:
                    row_vals.append("NULL")
                else:
                    row_vals.append(converter._BIT_to_python(column))
            else:
                if column is None:
                    row_vals.append("NULL")
                else:
                    row_vals.append(column)

        # Create the insert statement.
        insert_stm = ("INSERT INTO {0}.{1} VALUES {2};"
                      "".format(new_db, tbl_name,
                                self.column_format % tuple(row_vals)))

        # Replace 'NULL' occurrences with NULL values.
        insert_stm = insert_stm.replace("'NULL'", "NULL")

        return insert_stm

    def get_column_string(self, row, new_db, skip_blobs=False):
        """Return a formatted list of column data.

        row[in]            a row to process
        new_db[in]         new database name
        skip_blobs[in]     boolean value, if True, blob columns are skipped

        Returns (string) column list
        """

        if self.column_format is None:
            self.get_column_metadata()

        blob_inserts = []
        values = list(row)
        is_blob_insert = False
        # find if we have some unique column indexes
        unique_indexes = len(self.get_not_null_unique_indexes())
        # If all columns are blobs or there aren't any UNIQUE NOT NULL indexes
        # then rows won't be correctly copied using the update statement,
        # so we must use insert statements instead.
        if not skip_blobs and \
                (len(self.blob_columns) == len(self.column_names) or
                 self.blob_columns and not unique_indexes):
            blob_inserts.append(self._build_insert_blob(row, new_db,
                                                        self.q_tbl_name))
            is_blob_insert = True
        else:
            # Find blobs
            if self.blob_columns:
                # Save blob updates for later...
                blob = self._build_update_blob(row, new_db, self.q_tbl_name)
                if blob is not None:
                    blob_inserts.append(blob)
                for col in self.blob_columns:
                    values[col] = "NULL"

        if not is_blob_insert:
            # Replace single quotes located in the value for a text field with
            # the correct special character escape sequence. This fixes SQL
            # errors related to using single quotes in a string value that is
            # single quoted. For example, 'this' is it' is changed to
            # 'this\' is it'.
            for col in self.text_columns:
                # Check if the value is not None before replacing quotes
                if values[col]:
                    # Apply escape sequences to special characters
                    values[col] = convert_special_characters(values[col])

            for col in self.bit_columns:
                if values[col] is not None:
                    # Convert BIT to INTEGER for dump.
                    # pylint: disable=W0212
                    values[col] = MySQLConverter()._BIT_to_python(values[col])

            # Build string (add quotes to "string" like types)
            val_str = self.column_format % tuple(values)

            # Change 'None' occurrences with "NULL"
            val_str = val_str.replace(", None", ", NULL")
            val_str = val_str.replace("(None", "(NULL")
            val_str = val_str.replace(", 'None'", ", NULL")
            val_str = val_str.replace("('None'", "(NULL")

        else:
            val_str = None

        return val_str, blob_inserts

    def make_bulk_insert(self, rows, new_db, columns_names=None,
                         skip_blobs=False):
        """Create bulk insert statements for the data

        Reads data from a table (rows) and builds group INSERT statements for
        bulk inserts.

        Note: This method does not print any information to stdout.

        rows[in]           a list of rows to process
        new_db[in]         new database name
        skip_blobs[in]     boolean value, if True, blob columns are skipped

        Returns (tuple) - (bulk insert statements, blob data inserts)
        """

        if self.column_format is None:
            self.get_column_metadata()

        data_inserts = []
        blob_inserts = []
        row_count = 0
        data_size = 0
        val_str = None

        for row in rows:
            if row_count == 0:
                if columns_names:
                    insert_str = "INSERT INTO {0}.{1} ({2}) VALUES ".format(
                        new_db, self.q_tbl_name, ", ".join(columns_names)
                    )
                else:
                    insert_str = self._insert % (new_db, self.q_tbl_name)
                if val_str:
                    row_count += 1
                    insert_str += val_str
                data_size = len(insert_str)

            col_data = self.get_column_string(row, new_db, skip_blobs)
            if len(col_data[1]) > 0:
                blob_inserts.extend(col_data[1])
            if col_data[0]:
                val_str = col_data[0]

                row_size = len(val_str)
                next_size = data_size + row_size + 3
                if ((row_count >= _MAXBULK_VALUES) or
                        (next_size > (int(self.max_packet_size) - 512))):
                    # add to buffer
                    data_inserts.append(insert_str)
                    row_count = 0
                else:
                    row_count += 1
                    if row_count > 1:
                        insert_str += ", "
                    insert_str += val_str
                    data_size += row_size + 3

        if row_count > 0:
            data_inserts.append(insert_str)

        return data_inserts, blob_inserts

    def get_storage_engine(self):
        """Get the storage engine (in UPPERCASE) for the table.

        Returns the name in UPPERCASE of the storage engine use for the table
        or None if the information is not found.
        """
        self.server.exec_query("USE {0}".format(self.q_db_name),
                               self.query_options)
        res = self.server.exec_query(
            "SHOW TABLE STATUS WHERE name = '{0}'".format(self.tbl_name)
        )
        try:
            # Return store engine converted to UPPER cases.
            return res[0][1].upper() if res[0][1] else None
        except IndexError:
            # Return None if table status information is not available.
            return None

    def get_segment_size(self, num_conn=1):
        """Get the segment size based on number of connections (threads).

        num_conn[in]       Number of threads(connections) to use
                           Default = 1 (one large segment)

        Returns (int) segment_size

                Note: if num_conn <= 1 - returns number of rows
        """

        # Get number of rows
        num_rows = 0
        try:
            res = self.server.exec_query("USE %s" % self.q_db_name,
                                         self.query_options)
        except:
            pass
        res = self.server.exec_query("SHOW TABLE STATUS LIKE '%s'" %
                                     self.tbl_name)
        if res:
            num_rows = int(res[0][4])

        if num_conn <= 1:
            return num_rows

        # Calculate number of threads and segment size to fetch
        thread_limit = num_conn
        if thread_limit > _MAXTHREADS_INSERT:
            thread_limit = _MAXTHREADS_INSERT
        if num_rows > (_MAXROWS_PER_THREAD * thread_limit):
            max_threads = thread_limit
        else:
            max_threads = int(num_rows / _MAXROWS_PER_THREAD)
        if max_threads == 0:
            max_threads = 1
        if max_threads > 1 and self.verbose:
            print "# Using multi-threaded insert option. Number of " \
                  "threads = %d." % max_threads
        return (num_rows / max_threads) + max_threads

    def _bulk_insert(self, rows, new_db, destination=None):
        """Import data using bulk insert

        Reads data from a table and builds group INSERT statements for writing
        to the destination server specified (new_db.name).

        This method is designed to be used in a thread for parallel inserts.
        As such, it requires its own connection to the destination server.

        Note: This method does not print any information to stdout.

        rows[in]           a list of rows to process
        new_db[in]         new database name
        destination[in]    the destination server
        """
        if self.dest_vals is None:
            self.dest_vals = self.get_dest_values(destination)

        # Spawn a new connection
        server_options = {
            'conn_info': self.dest_vals,
            'role': "thread",
        }
        dest = Server(server_options)
        dest.connect()

        # Test if SQL_MODE is 'NO_BACKSLASH_ESCAPES' in the destination server
        if dest.select_variable("SQL_MODE") == "NO_BACKSLASH_ESCAPES":
            # Change temporarily the SQL_MODE in the destination server
            dest.exec_query("SET @@SESSION.SQL_MODE=''")

        # Issue the write lock
        lock_list = [("%s.%s" % (new_db, self.q_tbl_name), 'WRITE')]
        my_lock = Lock(dest, lock_list, {'locking': 'lock-all', })

        # First, turn off foreign keys if turned on
        dest.disable_foreign_key_checks(True)

        if self.column_format is None:
            self.get_column_metadata()
        data_lists = self.make_bulk_insert(rows, new_db)
        insert_data = data_lists[0]
        blob_data = data_lists[1]

        # Insert the data first
        for data_insert in insert_data:
            try:
                dest.exec_query(data_insert, self.query_options)
            except UtilError, e:
                raise UtilError("Problem inserting data. "
                                "Error = %s" % e.errmsg)

        # Now insert the blob data if there is any
        for blob_insert in blob_data:
            try:
                dest.exec_query(blob_insert, self.query_options)
            except UtilError, e:
                raise UtilError("Problem updating blob field. "
                                "Error = %s" % e.errmsg)

        # Now, turn on foreign keys if they were on at the start
        dest.disable_foreign_key_checks(False)
        my_lock.unlock()
        del dest

    def insert_rows(self, rows, new_db, destination=None, spawn=False):
        """Insert rows in the table using bulk copy.

        This method opens a new connect to the destination server to insert
        the data with a bulk copy. If spawn is True, the method spawns a new
        process and returns it. This allows for using a multi-threaded insert
        which can be faster on some platforms. If spawn is False, the method
        will open a new connection to insert the data.

        num_conn[in]       Number of threads(connections) to use for insert
        rows[in]           List of rows to insert
        new_db[in]         Rename the db to this name
        destination[in]    Destination server
                           Default = None (copy to same server)
        spawn[in]          If True, spawn a new process for the insert
                           Default = False

        Returns If spawn == True, process
                If spawn == False, None
        """

        if self.column_format is None:
            self.get_column_metadata()

        if self.dest_vals is None:
            self.dest_vals = self.get_dest_values(destination)

        proc = None
        if spawn:
            proc = multiprocessing.Process(target=self._bulk_insert,
                                           args=(rows, new_db, destination))
        else:
            self._bulk_insert(rows, new_db, destination)

        return proc

    def _clone_data(self, new_db):
        """Clone table data.

        This method will copy all of the data for a table
        from the old database to the new database on the same server.

        new_db[in]         New database name for the table
        """
        query_str = "INSERT INTO %s.%s SELECT * FROM %s.%s" % \
                    (new_db, self.q_tbl_name, self.q_db_name, self.q_tbl_name)
        if self.verbose and not self.quiet:
            print query_str

        # Disable foreign key checks to allow data to be copied without running
        # into foreign key referential integrity issues
        self.server.disable_foreign_key_checks(True)
        self.server.exec_query(query_str)
        self.server.disable_foreign_key_checks(False)

    def copy_data(self, destination, cloning=False, new_db=None,
                  connections=1):
        """Retrieve data from a table and copy to another server and database.

        Reads data from a table and inserts the correct INSERT statements into
        the file provided.

        Note: if connections < 1 - retrieve the data one row at-a-time

        destination[in]    Destination server
        cloning[in]        If True, we are copying on the same server
        new_db[in]         Rename the db to this name
        connections[in]    Number of threads(connections) to use for insert
        """
        # Get sql_mode from destination
        dest_sql_mode = destination.select_variable("SQL_MODE")
        if new_db is None:
            new_db = self.q_db_name
        else:
            # If need quote new_db identifier with backticks
            if not is_quoted_with_backticks(new_db, dest_sql_mode):
                new_db = quote_with_backticks(new_db, dest_sql_mode)

        num_conn = int(connections)

        if cloning:
            self._clone_data(new_db)
        else:
            # Read and copy the data
            pthreads = []
            # Change the sql_mode if the mode is different on each server
            # and if "ANSI_QUOTES" is set in source, this is for
            # compatibility between the names.
            prev_sql_mode = ''
            if self.sql_mode != dest_sql_mode and \
               "ANSI_QUOTES" in self.sql_mode:
                prev_sql_mode = self.server.select_variable("SQL_MODE")
                self.server.exec_query("SET @@SESSION.SQL_MODE=''")
                self.sql_mode = ''

                self.q_tbl_name = quote_with_backticks(
                    self.tbl_name,
                    self.sql_mode
                )
                self.q_db_name = quote_with_backticks(
                    self.db_name,
                    self.sql_mode
                )
                self.q_table = ".".join([self.q_db_name, self.q_tbl_name])
                self.q_column_names = []
                for column in self.column_names:
                    self.q_column_names.append(
                        quote_with_backticks(column, self.sql_mode)
                    )
            for rows in self.retrieve_rows(num_conn):
                p = self.insert_rows(rows, new_db, destination, num_conn > 1)
                if p is not None:
                    p.start()
                    pthreads.append(p)

            if num_conn > 1:
                # Wait for all threads to finish
                for p in pthreads:
                    p.join()
            # restoring the previous sql_mode, changed if the sql_mode in both
            # servers is different and one is "ANSI_QUOTES"
            if prev_sql_mode:
                self.server.exec_query("SET @@SESSION.SQL_MODE={0}"
                                       "".format(prev_sql_mode))
                self.sql_mode = prev_sql_mode
                self.q_tbl_name = quote_with_backticks(
                    self.tbl_name,
                    self.sql_mode
                )
                self.q_db_name = quote_with_backticks(
                    self.db_name,
                    self.sql_mode
                )
                self.q_table = ".".join([self.q_db_name, self.q_tbl_name])
                for column in self.column_names:
                    self.q_column_names.append(
                        quote_with_backticks(column, self.sql_mode)
                    )

    def retrieve_rows(self, num_conn=1):
        """Retrieve the table data in rows.

        This method can be used to retrieve rows from a table as a generator
        specifying how many rows to retrieve at one time (segment_size is
        calculated based on number of rows / number of connections).

        Note: if num_conn < 1 - retrieve the data one row at-a-time

        num_conn[in]       Number of threads(connections) to use
                           Default = 1 (one large segment)

        Returns (yield) row data
        """

        if num_conn > 1:
            # Only get the segment size when needed.
            segment_size = self.get_segment_size(num_conn)

        # Execute query to get all of the data
        cur = self.server.exec_query("SELECT * FROM {0}".format(self.q_table),
                                     self.query_options)

        while True:
            rows = None
            if num_conn < 1:
                rows = []
                row = cur.fetchone()
                if row is None:
                    raise StopIteration()
                rows.append(row)
            elif num_conn == 1:
                rows = cur.fetchall()
                yield rows
                raise StopIteration()
            else:
                rows = cur.fetchmany(segment_size)
                if not rows:
                    raise StopIteration()
            if rows is None:
                raise StopIteration()
            yield rows

        cur.close()

    def get_dest_values(self, destination=None):
        """Get the destination connection values if not already set.

        destination[in]    Connection values for destination server

        Returns connection values for destination if set or self.server
        """
        # Get connection to database
        if destination is None:
            conn_val = {
                "host": self.server.host,
                "user": self.server.user,
                "passwd": self.server.passwd,
                "unix_socket": self.server.socket,
                "port": self.server.port
            }
        else:
            conn_val = {
                "host": destination.host,
                "user": destination.user,
                "passwd": destination.passwd,
                "unix_socket": destination.socket,
                "port": destination.port
            }
        return conn_val

    def get_tbl_indexes(self):
        """Return a result set containing all indexes for a given table

        Returns result set
        """
        res = self.server.exec_query("SHOW INDEXES FROM %s" % self.q_table)
        # Clear the cardinality column
        if res:
            new_res = []
            for row in res:
                new_row = []
                i = 0
                for item in row:
                    if not i == 6:
                        new_row.append(item)
                    else:
                        new_row.append("0")
                    i = i + 1
                new_res.append(tuple(new_row))
            res = new_res
        return res

    def get_tbl_foreign_keys(self):
        """Return a result set containing all foreign keys for the table

        Returns result set
        """
        res = self.server.exec_query(_FOREIGN_KEY_QUERY % (self.db_name,
                                                           self.tbl_name))
        return res

    @staticmethod
    def __append(indexes, index):
        """Encapsulated append() method to ensure the primary key index
        is placed at the front of the list.
        """

        # Put the primary key first so that it can be compared to all indexes
        if index.name == "PRIMARY":
            indexes.insert(0, index)
        else:
            indexes.append(index)

    @staticmethod
    def __check_index(index, indexes, master_list):
        """Check a single index for duplicate or redundancy against a list
        of other Indexes.

        index[in]          The Index to compare
        indexes[in]        A list of Index instances to compare
        master_list[in]    A list of know duplicate Index instances

        Returns a tuple of whether duplicates are found and if found the
        list of duplicate indexes for this table
        """

        duplicates_found = False
        duplicate_list = []
        if indexes and index:
            for idx in indexes:
                if index == idx:
                    continue
                # Don't compare b == a when a == b has already occurred
                if not index.compared and idx.is_duplicate(index):
                    # make sure we haven't already found this match
                    if not idx.column_subparts:
                        idx.compared = True
                    if idx not in master_list:
                        duplicates_found = True
                        # PRIMARY key can be identified as redundant of an
                        # unique index with more columns, in that case always
                        # mark the other as the duplicate.
                        if idx.name == "PRIMARY":
                            index.duplicate_of = idx
                            duplicate_list.append(index)
                        else:
                            idx.duplicate_of = index
                            duplicate_list.append(idx)
        return (duplicates_found, duplicate_list)

    def __check_index_list(self, indexes):
        """Check a list of Index instances for duplicates.

        indexes[in]        A list of Index instances to compare

        Returns a tuple of whether duplicates are found and if found the
        list of duplicate indexes for this table
        """

        duplicates_found = False
        duplicate_list = []
        # Caller must ensure there are at least 2 elements in the list.
        if len(indexes) < 2:
            return (False, None)
        for index in indexes:
            res = self.__check_index(index, indexes, duplicate_list)
            if res[0]:
                duplicates_found = True
                duplicate_list.extend(res[1])
        return (duplicates_found, duplicate_list)

    def __check_clustered_index_list(self, indexes):
        """ Check for indexes containing the clustered index from the list.

        indexes[in]     list of indexes instances to check.

        Returns the list of indexes that contain the clustered index or
        None (if none found).
        """
        redundant_indexes = []
        if not self.pri_idx:
            self.get_primary_index()
        pri_idx_cols = [col[0] for col in self.pri_idx]
        for index in indexes:
            if index.name == 'PRIMARY':
                # Skip primary key.
                continue
            elif index.contains_columns(pri_idx_cols):
                redundant_indexes.append(index)

        return redundant_indexes if redundant_indexes else []

    def _get_index_list(self):
        """Get the list of indexes for a table.
        Returns list containing indexes.
        """
        rows = self.get_tbl_indexes()
        return rows

    def get_primary_index(self):
        """Retrieve the primary index columns for this table.
        """
        pri_idx = []

        rows = self.server.exec_query("EXPLAIN {0}".format(self.q_table))

        # Return False if no indexes found.
        if not rows:
            return pri_idx

        for row in rows:
            if row[3] == 'PRI':
                pri_idx.append(row)

        self.pri_idx = pri_idx

        return pri_idx

    def get_column_explanation(self, column_name):
        """Retrieve the explain description for the given column.
        """
        column_exp = []

        rows = self.server.exec_query("EXPLAIN {0}".format(self.q_table))

        # Return False if no indexes found.
        if not rows:
            return column_exp

        for row in rows:
            if row[0] == column_name:
                column_exp.append(row)

        return column_exp

    def get_indexes(self):
        """Retrieve the indexes from the server and load them into lists
        based on type.

        Returns True - table has indexes, False - table has no indexes
        """

        self.btree_indexes = []
        self.hash_indexes = []
        self.rtree_indexes = []
        self.fulltext_indexes = []
        self.indexes_q_names = []

        if self.verbose:
            print "# Getting indexes for %s" % (self.table)
        rows = self._get_index_list()

        # Return False if no indexes found.
        if not rows:
            return False
        idx = None
        prev_name = ""
        for row in rows:
            if (row[2] != prev_name) or (prev_name == ""):
                prev_name = row[2]
                idx = Index(self.db_name, row, sql_mode=self.sql_mode)
                if idx.type == "BTREE":
                    self.__append(self.btree_indexes, idx)
                elif idx.type == "HASH":
                    self.__append(self.hash_indexes, idx)
                elif idx.type == "RTREE":
                    self.__append(self.rtree_indexes, idx)
                else:
                    self.__append(self.fulltext_indexes, idx)
            elif idx:
                idx.add_column(row[4], row[7], row[9])
            self.indexes_q_names.append(quote_with_backticks(row[2],
                                                             self.sql_mode))
        return True

    def check_indexes(self, show_drops=False):
        """Check for duplicate or redundant indexes and display all matches

        show_drops[in]     (optional) If True the DROP statements are printed

        Note: You must call get_indexes() prior to calling this method. If
        get_indexes() is not called, no duplicates will be found.
        """

        dupes = []
        res = self.__check_index_list(self.btree_indexes)
        # if there are duplicates, add them to the dupes list
        if res[0]:
            dupes.extend(res[1])
        res = self.__check_index_list(self.hash_indexes)
        # if there are duplicates, add them to the dupes list
        if res[0]:
            dupes.extend(res[1])
        res = self.__check_index_list(self.rtree_indexes)
        # if there are duplicates, add them to the dupes list
        if res[0]:
            dupes.extend(res[1])
        res = self.__check_index_list(self.fulltext_indexes)
        # if there are duplicates, add them to the dupes list
        if res[0]:
            dupes.extend(res[1])

        # Check if secondary keys contains the clustered index (i.e. Primary
        # key). In InnoDB, each record in a secondary index contains the
        # primary key columns. Therefore the use of keys that include the
        # primary key might be redundant.
        redundant_idxs = []
        if not self.storage_engine:
            self.storage_engine = self.get_storage_engine()
        if self.storage_engine == 'INNODB':
            all_indexes = self.btree_indexes
            all_indexes.extend(self.hash_indexes)
            all_indexes.extend(self.rtree_indexes)
            all_indexes.extend(self.fulltext_indexes)
            redundant_idxs = self.__check_clustered_index_list(all_indexes)

        # Print duplicate and redundant keys on composite indexes.
        if len(dupes) > 0:
            plural_1, verb_conj, plural_2 = (
                ('', 'is a', '') if len(dupes) == 1 else ('es', 'are', 's')
            )
            print("# The following index{0} {1} duplicate{2} or redundant "
                  "for table {3}:".format(plural_1, verb_conj, plural_2,
                                          self.table))
            for index in dupes:
                print("#")
                index.print_index_sql()
                print("#     may be redundant or duplicate of:")
                index.duplicate_of.print_index_sql()
            if show_drops:
                print("#\n# DROP statement{0}:\n#".format(plural_2))
                for index in dupes:
                    print("{0};".format(index.get_drop_statement()))
                print("#")

        # Print redundant indexes containing clustered key.
        if redundant_idxs:
            plural, verb_conj, plural_2 = (
                ('', 's', '') if len(redundant_idxs) == 1 else ('es', '', 's')
            )

            print("# The following index{0} for table {1} contain{2} the "
                  "clustered index and might be redundant:".format(plural,
                                                                   self.table,
                                                                   verb_conj))
            for index in redundant_idxs:
                print("#")
                index.print_index_sql()
            if show_drops:
                print("#\n# DROP/ADD statement{0}:\n#".format(plural_2))
                # Get columns from primary key to be removed.
                pri_idx_cols = [col[0] for col in self.pri_idx]
                for index in redundant_idxs:
                    print("{0};".format(
                        index.get_remove_columns_statement(pri_idx_cols)
                    ))
                print("#")

        if not self.quiet and not dupes and not redundant_idxs:
            print("# Table {0} has no duplicate nor redundant "
                  "indexes.".format(self.table))

    def show_special_indexes(self, fmt, limit, best=False):
        """Display a list of the best or worst queries for this table.

        This shows the best (first n) or worst (last n) performing queries
        for a given table.

        fmt[in]            format out output = sql, table, tab, csv
        limit[in]          number to limit the display
        best[in]           (optional) if True, print best performing indexes
                                      if False, print worst performing indexes
        """

        _QUERY = """
            SELECT
                t.TABLE_SCHEMA AS `db`, t.TABLE_NAME AS `table`,
                s.INDEX_NAME AS `index name`, s.COLUMN_NAME AS `field name`,
                s.SEQ_IN_INDEX `seq in index`, s2.max_columns AS `# cols`,
                s.CARDINALITY AS `card`, t.TABLE_ROWS AS `est rows`,
                ROUND(((s.CARDINALITY / IFNULL(
                IF(t.TABLE_ROWS < s.CARDINALITY, s.CARDINALITY, t.TABLE_ROWS),
                0.01)) * 100), 2) AS `sel_percent`
            FROM INFORMATION_SCHEMA.STATISTICS s
                INNER JOIN INFORMATION_SCHEMA.TABLES t
                ON s.TABLE_SCHEMA = t.TABLE_SCHEMA
                AND s.TABLE_NAME = t.TABLE_NAME
            INNER JOIN (
                SELECT TABLE_SCHEMA, TABLE_NAME, INDEX_NAME,
                    MAX(SEQ_IN_INDEX) AS max_columns
                FROM INFORMATION_SCHEMA.STATISTICS
                WHERE TABLE_SCHEMA = %s AND TABLE_NAME = %s
                      AND INDEX_NAME != 'PRIMARY'
                GROUP BY TABLE_SCHEMA, TABLE_NAME, INDEX_NAME
             ) AS s2
             ON s.TABLE_SCHEMA = s2.TABLE_SCHEMA
                AND s.TABLE_NAME = s2.TABLE_NAME
                AND s.INDEX_NAME = s2.INDEX_NAME
            WHERE t.TABLE_SCHEMA != 'mysql'
                AND t.TABLE_ROWS > 10 /* Only tables with some rows */
                AND s.CARDINALITY IS NOT NULL
                AND (s.CARDINALITY / IFNULL(
                IF(t.TABLE_ROWS < s.CARDINALITY, s.CARDINALITY, t.TABLE_ROWS),
                0.01)) <= 1.00
            ORDER BY `sel_percent`
        """
        query_options = {
            'params': (self.db_name, self.tbl_name,)
        }
        rows = []
        idx_type = "best"
        if not best:
            idx_type = "worst"
        if best:
            rows = self.server.exec_query(_QUERY + "DESC LIMIT %s" % limit,
                                          query_options)
        else:
            rows = self.server.exec_query(_QUERY + "LIMIT %s" % limit,
                                          query_options)
        if rows:
            print("#")
            if limit == 1:
                print("# Showing the {0} performing index from "
                      "{1}:".format(idx_type, self.table))
            else:
                print("# Showing the top {0} {1} performing indexes from "
                      "{2}:".format(limit, idx_type, self.table))
            print("#")
            cols = ("database", "table", "name", "column", "sequence",
                    "num columns", "cardinality", "est. rows", "percent")
            print_list(sys.stdout, fmt, cols, rows)
        else:
            print("# WARNING: Not enough data to calculate "
                  "best/worst indexes.")

    @staticmethod
    def __print_index_list(indexes, fmt, no_header=False, verbosity=0):
        """Print the list of indexes

        indexes[in]        list of indexes to print
        fmt[in]            format out output = sql, table, tab, csv
        no_header[in]      (optional) if True, do not print the header
        """
        if fmt == "sql":
            for index in indexes:
                index.print_index_sql()
        else:
            if verbosity > 0:
                cols = ("database", "table", "name", "type", "unique",
                        "accepts nulls", "columns")
            else:
                cols = ("database", "table", "name", "type", "columns")

            rows = []
            for index in indexes:
                rows.append(index.get_row(verbosity))
            print_list(sys.stdout, fmt, cols, rows, no_header)

    def print_indexes(self, fmt, verbosity):
        """Print all indexes for this table

        fmt[in]         format out output = sql, table, tab, csv
        """

        print "# Showing indexes from %s:\n#" % (self.table)
        if fmt == "sql":
            self.__print_index_list(self.btree_indexes, fmt,
                                    verbosity=verbosity)
            self.__print_index_list(self.hash_indexes, fmt, False,
                                    verbosity=verbosity)
            self.__print_index_list(self.rtree_indexes, fmt, False,
                                    verbosity=verbosity)
            self.__print_index_list(self.fulltext_indexes, fmt, False,
                                    verbosity=verbosity)
        else:
            master_indexes = []
            master_indexes.extend(self.btree_indexes)
            master_indexes.extend(self.hash_indexes)
            master_indexes.extend(self.rtree_indexes)
            master_indexes.extend(self.fulltext_indexes)
            self.__print_index_list(master_indexes, fmt,
                                    verbosity=verbosity)
        print "#"

    def has_primary_key(self):
        """Check to see if there is a primary key.
        Returns bool - True - a primary key was found,
                       False - no primary key.
        """
        primary_key = False
        rows = self._get_index_list()
        for row in rows:
            if row[2] == "PRIMARY":
                primary_key = True
        return primary_key

    def has_unique_key(self):
        """Check to see if there is a unique key.
        Returns bool - True - a unique key was found,
                       False - no unique key.
        """
        unique_key = False
        rows = self._get_index_list()
        for row in rows:
            if row[1] == '0':
                unique_key = True
        return unique_key
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains methods for working with mysql server tools.
"""

import inspect
import os
import re
import sys
import shlex
import shutil
import socket
import subprocess
import time

try:
    import ctypes
except ImportError:
    pass



def _add_basedir(search_paths, path_str):
    """Add a basedir and all known sub directories

    This method builds a list of possible paths for a basedir for locating
    special MySQL files like mysqld (mysqld.exe), etc.

    search_paths[inout] List of paths to append
    path_str[in]        The basedir path to append
    """
    search_paths.append(path_str)
    search_paths.append(os.path.join(path_str, "sql"))       # for source trees
    search_paths.append(os.path.join(path_str, "client"))    # for source trees
    search_paths.append(os.path.join(path_str, "share"))
    search_paths.append(os.path.join(path_str, "scripts"))
    search_paths.append(os.path.join(path_str, "bin"))
    search_paths.append(os.path.join(path_str, "libexec"))
    search_paths.append(os.path.join(path_str, "mysql"))


def get_tool_path(basedir, tool, fix_ext=True, required=True,
                  defaults_paths=None, search_PATH=False, quote=False):
    """Search for a MySQL tool and return the full path

    basedir[in]         The initial basedir to search (from mysql server)
    tool[in]            The name of the tool to find
    fix_ext[in]         If True (default is True), add .exe if running on
                        Windows.
    required[in]        If True (default is True), and error will be
                        generated and the utility aborted if the tool is
                        not found.
    defaults_paths[in]  Default list of paths to search for the tool.
                        By default an empty list is assumed, i.e. [].
    search_PATH[in]     Boolean value that indicates if the paths specified by
                        the PATH environment variable will be used to search
                        for the tool. By default the PATH will not be searched,
                        i.e. search_PATH=False.
    quote[in]           If True, the result path is surrounded with the OS
                        quotes.
    Returns (string) full path to tool
    """
    if not defaults_paths:
        defaults_paths = []
    search_paths = []
    if quote:
        if os.name == "posix":
            quote_char = "'"
        else:
            quote_char = '"'
    else:
        quote_char = ''
    if basedir:
        # Add specified basedir path to search paths
        _add_basedir(search_paths, basedir)
    if defaults_paths and len(defaults_paths):
        # Add specified default paths to search paths
        for path in defaults_paths:
            search_paths.append(path)
    else:
        # Add default basedir paths to search paths
        _add_basedir(search_paths, "/usr/local/mysql/")
        _add_basedir(search_paths, "/usr/sbin/")
        _add_basedir(search_paths, "/usr/share/")

    # Search in path from the PATH environment variable
    if search_PATH:
        for path in os.environ['PATH'].split(os.pathsep):
            search_paths.append(path)

    if os.name == "nt" and fix_ext:
        tool = tool + ".exe"
    # Search for the tool
    for path in search_paths:
        norm_path = os.path.normpath(path)
        if os.path.isdir(norm_path):
            toolpath = os.path.join(norm_path, tool)
            if os.path.isfile(toolpath):
                return r"%s%s%s" % (quote_char, toolpath, quote_char)
            else:
                if tool == "mysqld.exe":
                    toolpath = os.path.join(norm_path, "mysqld-nt.exe")
                    if os.path.isfile(toolpath):
                        return r"%s%s%s" % (quote_char, toolpath, quote_char)
    if required:
        raise UtilError("Cannot find location of %s." % tool)

    return None


def delete_directory(path):
    """Remove a directory (folder) and its contents.

    path[in]           target directory
    """
    if os.path.exists(path):
        # It can take up to 10 seconds for Windows to 'release' a directory
        # once a process has terminated. We wait...
        if os.name == "nt":
            stop = 10
            i = 1
            while i < stop and os.path.exists(path):
                shutil.rmtree(path, True)
                time.sleep(1)
                i += 1
        else:
            shutil.rmtree(path, True)


def estimate_free_space(path, unit_multiple=2):
    """Estimated free space for the given path.

    Calculates free space for the given path, returning the value
    on the size given by the unit_multiple.

    path[in]             the path to calculate the free space for.
    unit_multiple[in]    the unit size given as a multiple.
                         Accepts int values > to zero.
                         Size    unit_multiple
                          bytes        0
                          Kilobytes    1
                          Megabytes    2
                          Gigabytes    3
                         and so on...

    Returns folder/drive free space (in bytes)
    """
    unit_size = 1024 ** unit_multiple
    if os.name == 'nt':
        free_bytes = ctypes.c_ulonglong(0)
        ctypes.windll.kernel32.GetDiskFreeSpaceExW(ctypes.c_wchar_p(path),
                                                   None, None,
                                                   ctypes.pointer(free_bytes))
        return free_bytes.value / unit_size
    else:
        st = os.statvfs(path)  # pylint: disable=E1101
        return st.f_bavail * st.f_frsize / unit_size


def execute_script(run_cmd, filename=None, options=None, verbosity=False):
    """Execute a script.

    This method spawns a subprocess to execute a script. If a file is
    specified, it will direct output to that file else it will suppress
    all output from the script.

    run_cmd[in]        command/script to execute
    filename[in]       file path name to file, os.stdout, etc.
                       Default is None (do not log/write output)
    options[in]        arguments for script
                       Default is no arguments ([])
    verbosity[in]      show result of script
                       Default is False

    Returns int - result from process execution
    """
    if options is None:
        options = []
    if verbosity:
        f_out = sys.stdout
    else:
        if not filename:
            filename = os.devnull
        f_out = open(filename, 'w')

    is_posix = (os.name == "posix")
    command = shlex.split(run_cmd, posix=is_posix)

    if options:
        command.extend([str(opt) for opt in options])

    if verbosity:
        print("# SCRIPT EXECUTED: {0}".format(" ".join(command)))

    try:
        proc = subprocess.Popen(command, shell=False,
                                stdout=f_out, stderr=f_out)
    except:
        _, err, _ = sys.exc_info()
        raise UtilError(str(err))

    ret_val = proc.wait()
    if not verbosity:
        f_out.close()
    return ret_val


def ping_host(host, timeout):
    """Execute 'ping' against host to see if it is alive.

    host[in]           hostname or IP to ping
    timeout[in]        timeout in seconds to wait

    returns bool - True = host is reachable via ping
    """
    if sys.platform == "darwin":
        run_cmd = "ping -o -t %s %s" % (timeout, host)
    elif os.name == "posix":
        run_cmd = "ping -w %s %s" % (timeout, host)
    else:  # must be windows
        run_cmd = "ping -n %s %s" % (timeout, host)

    ret_val = execute_script(run_cmd)

    return (ret_val == 0)


def parse_mysqld_version(vers_str):
    """ Parse the MySQL version string.

    vers_str[in]     MySQL Version from client

    Returns string = version string
    """
    pattern = r"mysqld(?:\.exe)?\s+Ver\s+(\d+\.\d+\.\S+)\s"
    match = re.search(pattern, vers_str)
    if not match:
        return None
    version = match.group(1)
    try:
        # get the version digits. If more than 2, we get first 3 parts
        # pylint: disable=W0612
        maj_ver, min_ver, dev = version.split(".", 2)
        rel = dev.split("-", 1)
        return (maj_ver, min_ver, rel[0])
    except:
        return None


def get_mysqld_version(mysqld_path):
    """Return the version number for a mysqld executable.

    mysqld_path[in]    location of the mysqld executable

    Returns tuple - (major, minor, release), or None if error
    """
    out = open("version_check", 'w')
    proc = subprocess.Popen("%s --version" % mysqld_path,
                            stdout=out, stderr=out, shell=True)
    proc.wait()
    out.close()
    out = open("version_check", 'r')
    line = None
    for line in out.readlines():
        if "Ver" in line:
            break
    out.close()

    try:
        os.unlink('version_check')
    except:
        pass

    if line is None:
        return None
    # strip path for long, unusual paths that contain version number
    fixed_str = "{0} {1}".format("mysqld", line.strip(mysqld_path))
    return parse_mysqld_version(fixed_str)


def show_file_statistics(file_name, wild=False, out_format="GRID"):
    """Show file statistics for file name specified

    file_name[in]    target file name and path
    wild[in]         if True, get file statistics for all files with prefix of
                     file_name. Default is False
    out_format[in]   output format to print file statistics. Default is GRID.
    """

    def _get_file_stats(path, file_name):
        """Return file stats
        """
        stats = os.stat(os.path.join(path, file_name))
        return ((file_name, stats.st_size, time.ctime(stats.st_ctime),
                 time.ctime(stats.st_mtime)))

    columns = ["File", "Size", "Created", "Last Modified"]
    rows = []
    path, filename = os.path.split(file_name)
    if wild:
        for _, _, files in os.walk(path):
            for f in files:
                if f.startswith(filename):
                    rows.append(_get_file_stats(path, f))
    else:
        rows.append(_get_file_stats(path, filename))

    # Local import is needed because of Python compability issues
    from mysql.utilities.common.format import print_list
    print_list(sys.stdout, out_format, columns, rows)


def remote_copy(filepath, user, host, local_path, verbosity=0):
    """Copy a file from a remote machine to the localhost.

    filepath[in]       The full path and file name of the file on the remote
                       machine
    user[in]           Remote login
    local_path[in]     The path to where the file is to be copie

    Returns bool - True = succes, False = failure or exception
    """

    if os.name == "posix":  # use scp
        run_cmd = "scp %s@%s:%s %s" % (user, host, filepath, local_path)
        if verbosity > 1:
            print("# Command =%s" % run_cmd)
        print("# Copying file from %s:%s to %s:" %
              (host, filepath, local_path))
        proc = subprocess.Popen(run_cmd, shell=True)
        proc.wait()
    else:
        print("Remote copy not supported. Please use UNC paths and omit "
              "the --remote-login option to use a local copy operation.")
    return True


def check_python_version(min_version=PYTHON_MIN_VERSION,
                         max_version=PYTHON_MAX_VERSION,
                         raise_exception_on_fail=False,
                         name=None, print_on_fail=True,
                         exit_on_fail=True,
                         return_error_msg=False):
    """Check the Python version compatibility.

    By default this method uses constants to define the minimum and maximum
    Python versions required. It's possible to override this by passing new
    values on ``min_version`` and ``max_version`` parameters.
    It will run a ``sys.exit`` or raise a ``UtilError`` if the version of
    Python detected it not compatible.

    min_version[in]               Tuple with the minimum Python version
                                  required (inclusive).
    max_version[in]               Tuple with the maximum Python version
                                  required (exclusive).
    raise_exception_on_fail[in]   Boolean, it will raise a ``UtilError`` if
                                  True and Python detected is not compatible.
    name[in]                      String for a custom name, if not provided
                                  will get the module name from where this
                                  function was called.
    print_on_fail[in]             If True, print error else do not print
                                  error on failure.
    exit_on_fail[in]              If True, issue exit() else do not exit()
                                  on failure.
    return_error_msg[in]          If True, and is not compatible
                                  returns (result, error_msg) tuple.
    """

    # Only use the fields: major, minor and micro
    sys_version = sys.version_info[:3]

    # Test min version compatibility
    is_compat = min_version <= sys_version

    # Test max version compatibility if it's defined
    if is_compat and max_version:
        is_compat = sys_version < max_version

    if not is_compat:
        if not name:
            # Get the utility name by finding the module
            # name from where this function was called
            frm = inspect.stack()[1]
            mod = inspect.getmodule(frm[0])
            mod_name = os.path.splitext(
                os.path.basename(mod.__file__))[0]
            name = '%s utility' % mod_name

        # Build the error message
        if max_version:
            max_version_error_msg = 'or higher and lower than %s' % \
                '.'.join([str(el) for el in max_version])
        else:
            max_version_error_msg = 'or higher'

        error_msg = (
            'The %(name)s requires Python version %(min_version)s '
            '%(max_version_error_msg)s. The version of Python detected was '
            '%(sys_version)s. You may need to install or redirect the '
            'execution of this utility to an environment that includes a '
            'compatible Python version.'
        ) % {
            'name': name,
            'sys_version': '.'.join([str(el) for el in sys_version]),
            'min_version': '.'.join([str(el) for el in min_version]),
            'max_version_error_msg': max_version_error_msg
        }

        if raise_exception_on_fail:
            raise UtilError(error_msg)

        if print_on_fail:
            print('ERROR: %s' % error_msg)

        if exit_on_fail:
            sys.exit(1)

        if return_error_msg:
            return is_compat, error_msg

    return is_compat


def check_port_in_use(host, port):
    """Check to see if port is in use.

    host[in]            Hostname or IP to check
    port[in]            Port number to check

    Returns bool - True = port is available, False is not available
    """
    try:
        sock = socket.create_connection((host, port))
    except socket.error:
        return True
    sock.close()
    return False


def requires_encoding(orig_str):
    r"""Check to see if a string requires encoding

    This method will check to see if a string requires encoding to be used
    as a MySQL file name (r"[\w$]*").

    orig_str[in]        original string

    Returns bool - True = requires encoding, False = does not require encoding
    """
    ok_chars = re.compile(r"[\w$]*")
    parts = ok_chars.findall(orig_str)
    return len(parts) > 2 and parts[1].strip() == ''


def encode(orig_str):
    r"""Encode a string containing non-MySQL observed characters

    This method will take a string containing characters other than those
    recognized by MySQL (r"[\w$]*") and covert them to embedded ascii values.
    For example, "this.has.periods" becomes "this@002ehas@00e2periods"

    orig_str[in]        original string

    Returns string - encoded string or original string
    """
    # First, find the parts that match valid characters
    ok_chars = re.compile(r"[\w$]*")
    parts = ok_chars.findall(orig_str)

    # Now find each part that does not match the list of valid characters
    # Save the good parts
    i = 0
    encode_parts = []
    good_parts = []
    for part in parts:
        if not len(part):
            continue
        good_parts.append(part)
        if i == 0:
            i = len(part)
        else:
            j = orig_str[i:].find(part)
            encode_parts.append(orig_str[i:i + j])
            i += len(part) + j

    # Next, convert the non-valid parts to the form @NNNN (hex)
    encoded_parts = []
    for part in encode_parts:
        new_part = "".join(["@%04x" % ord(c) for c in part])
        encoded_parts.append(new_part)

    # Take the good parts and the encoded parts and reform the string
    i = 0
    new_parts = []
    for part in good_parts[:len(good_parts) - 1]:
        new_parts.append(part)
        new_parts.append(encoded_parts[i])
        i += 1
    new_parts.append(good_parts[len(good_parts) - 1])

    # Return the new string
    return "".join(new_parts)


def requires_decoding(orig_str):
    """Check to if a string required decoding

    This method will check to see if a string requires decoding to be used
    as a filename (has @NNNN entries)

    orig_str[in]        original string

    Returns bool - True = requires decoding, False = does not require decoding
    """
    return '@' in orig_str


def decode(orig_str):
    r"""Decode a string containing @NNNN entries

    This method will take a string containing characters other than those
    recognized by MySQL (r"[\w$]*") and covert them to character values.
    For example, "this@002ehas@00e2periods" becomes "this.has.periods".

    orig_str[in]        original string

    Returns string - decoded string or original string
    """
    parts = orig_str.split('@')
    if len(parts) == 1:
        return orig_str
    new_parts = [parts[0]]
    for part in parts[1:]:
        # take first four positions and convert to ascii
        new_parts.append(chr(int(part[0:4], 16)))
        new_parts.append(part[4:])
    return "".join(new_parts)


def check_connector_python(print_error=True,
                           min_version=CONNECTOR_MIN_VERSION):

    """Check to see if Connector Python is installed and accessible and
    meets minimum required version.

    By default this method uses constants to define the minimum
    C/Python version required. It's possible to override this by passing  a new
    value to ``min_version`` parameter.

    print_error[in]               If True, print error else do not print
                                  error on failure.
    min_version[in]               Tuple with the minimum C/Python version
                                  required (inclusive).

    """
    is_compatible = True
    try:
        import mysql.connector  # pylint: disable=W0612
    except ImportError:
        if print_error:
            print("ERROR: The MySQL Connector/Python module was not found. "
                  "MySQL Utilities requires the connector to be installed. "
                  "Please check your paths or download and install the "
                  "Connector/Python from http://dev.mysql.com.")
        return False
    else:
        try:
            sys_version = mysql.connector.version.VERSION[:3]
        except AttributeError:
            is_compatible = False

    if is_compatible and sys_version >= min_version:
        return True
    else:
        if print_error:
            print("ERROR: The MYSQL Connector/Python module was found "
                  "but it is either not properly installed or it is an "
                  "old version. MySQL Utilities requires Connector/Python "
                  "version > '{0}'. Download and install Connector/Python "
                  "from http://dev.mysql.com.".format(min_version))
        return False


def print_elapsed_time(start_time):
    """Print the elapsed time to stdout (screen)

    start_time[in]      The starting time of the test
    """
    stop_time = time.time()
    display_time = stop_time - start_time
    print("Time: {0:.2f} sec\n".format(display_time))


def join_and_build_str(list_of_strings, sep=', ', last_sep='and'):
    """Buils and returns a string from a list of elems.

    list_of_strings[in]    the list of strings that will be joined into a
                           single string.
    sep[in]                the separator that will be used to group all strings
                           except the last one.
    last_sep[in]           the separator that is used in last place
    """
    if list_of_strings:
        if len(list_of_strings) > 1:
            res_str = "{0} {1} {2}".format(
                sep.join(list_of_strings[:-1]), last_sep, list_of_strings[-1])
        else:  # list has a single elem
            res_str = list_of_strings[0]
    else:  # if list_of_str is empty, return empty string
        res_str = ""
    return res_str
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains abstractions of MySQL replication functionality.
"""

import sys
import logging
import time
import operator
import os

from multiprocessing.pool import ThreadPool



_HEALTH_COLS = ["host", "port", "role", "state", "gtid_mode", "health"]
_HEALTH_DETAIL_COLS = ["version", "master_log_file", "master_log_pos",
                       "IO_Thread", "SQL_Thread", "Secs_Behind",
                       "Remaining_Delay", "IO_Error_Num", "IO_Error",
                       "SQL_Error_Num", "SQL_Error", "Trans_Behind"]

_GTID_EXECUTED = "SELECT @@GLOBAL.GTID_EXECUTED"
_GTID_WAIT = "SELECT WAIT_UNTIL_SQL_THREAD_AFTER_GTIDS('%s', %s)"
_GTID_SUBTRACT_TO_EXECUTED = ("SELECT GTID_SUBTRACT('{0}', "
                              "@@GLOBAL.GTID_EXECUTED)")

_UPDATE_RPL_USER_QUERY = ("UPDATE mysql.user "
                          "SET password = PASSWORD('{passwd}')"
                          "where user ='{user}'")
# Query for server versions >= 5.7.6.
_UPDATE_RPL_USER_QUERY_5_7_6 = (
    "ALTER USER IF EXISTS '{user}'@'{host}' IDENTIFIED BY '{passwd}'")

_SELECT_RPL_USER_PASS_QUERY = ("SELECT user, host, grant_priv, password, "
                               "Repl_slave_priv FROM mysql.user "
                               "WHERE user ='{user}' AND host ='{host}'")
# Query for server versions >= 5.7.6.
_SELECT_RPL_USER_PASS_QUERY_5_7_6 = (
    "SELECT user, host, grant_priv, authentication_string, "
    "Repl_slave_priv FROM mysql.user WHERE user ='{user}' AND host ='{host}'")


def parse_topology_connections(options, parse_candidates=True):
    """Parse the --master, --slaves, and --candidates options

    This method returns a tuple with server connection dictionaries for
    the master, slaves, and candidates lists.

    If no master, will return (None, ...) for master element.
    If no slaves, will return (..., [], ...) for slaves element.
    If no canidates, will return (..., ..., []) for canidates element.

    Will raise error if cannot parse connection options.

    options[in]        options from parser

    Returns tuple - (master, slaves, candidates) dictionaries
    """
    try:
        timeout = options.conn_timeout
    except:
        timeout = None
    if timeout and options.verbosity > 2:
        print("Note: running with --connection-timeout={0}".format(timeout))

    # Create a basic configuration reader, without looking for the tool
    # my_print_defaults to avoid raising exceptions. This is used for
    # optimization purposes, to reuse data and avoid repeating the execution of
    # some methods in the parse_connection method (e.g. searching for
    # my_print_defaults).
    config_reader = MyDefaultsReader(options, False)

    if options.master:
        try:
            master_val = parse_connection(options.master, config_reader,
                                          options)
            # Add connection timeout if present in options
            if timeout:
                master_val['connection_timeout'] = timeout
        except FormatError as err:
            msg = ("Master connection values invalid or cannot be parsed: %s "
                   "(%s)." % (options.master, err))
            raise UtilRplError(msg)
        except UtilError as err:
            msg = ("Master connection values invalid or cannot be parsed: %s "
                   "(using login-path authentication: %s)" % (options.master,
                                                              err.errmsg))
            raise UtilRplError(msg)
    else:
        master_val = None

    slaves_val = []
    if options.slaves:
        slaves = options.slaves.split(",")
        for slave in slaves:
            try:
                s_values = parse_connection(slave, config_reader, options)
                # Add connection timeout if present in options
                if timeout:
                    s_values['connection_timeout'] = timeout
                slaves_val.append(s_values)
            except FormatError as err:
                msg = ("Slave connection values invalid or cannot be parsed: "
                       "%s (%s)" % (slave, err))
                raise UtilRplError(msg)
            except UtilError as err:
                msg = ("Slave connection values invalid or cannot be parsed: "
                       "%s (%s)" % (slave, err.errmsg))
                raise UtilRplError(msg)

    candidates_val = []
    if parse_candidates and options.candidates:
        candidates = options.candidates.split(",")
        for slave in candidates:
            try:
                s_values = parse_connection(slave, config_reader, options)
                # Add connection timeout if present in options
                if timeout:
                    s_values['connection_timeout'] = timeout
                candidates_val.append(s_values)
            except FormatError as err:
                msg = "Candidate connection values invalid or " + \
                      "cannot be parsed: %s (%s)" % (slave, err)
                raise UtilRplError(msg)
            except UtilError as err:
                msg = ("Candidate connection values invalid or cannot be "
                       "parsed: %s (%s)" % (slave, err.errmsg))
                raise UtilRplError(msg)

    return (master_val, slaves_val, candidates_val)


class Topology(Replication):
    """The Topology class supports administrative operations for an existing
    master-to-many slave topology. It has the following capabilities:

        - determine the health of the topology
        - discover slaves connected to the master provided they have
          --report-host and --report-port specified
        - switchover from master to a candidate slave
        - demote the master to a slave in the topology
        - perform best slave election
        - failover to a specific slave or best of slaves available

    Notes:

        - the switchover and demote methods work with versions prior to and
          after 5.6.5.
        - failover and best slave election require version 5.6.5 and later
          and GTID_MODE=ON.

    """

    def __init__(self, master_vals, slave_vals, options=None,
                 skip_conn_err=False):
        """Constructor

        The slaves parameter requires a dictionary in the form:

        master_vals[in]    master server connection dictionary
        slave_vals[in]     list of slave server connection dictionaries
        options[in]        options dictionary
          verbose          print extra data during operations (optional)
                           Default = False
          ping             maximum number of seconds to ping
                           Default = 3
          max_delay        maximum delay in seconds slave and be behind
                           master and still be 'Ok'. Default = 0
          max_position     maximum position slave can be behind master's
                           binlog and still be 'Ok'. Default = 0
        skip_conn_err[in]  if True, do not fail on connection failure
                           Default = True
        """
        super(Topology, self).__init__(master_vals, slave_vals, options or {})
        # Get options needed
        self.options = options or {}
        self.verbosity = options.get("verbosity", 0)
        self.verbose = self.verbosity > 0
        self.quiet = self.options.get("quiet", False)
        self.pingtime = self.options.get("ping", 3)
        self.max_delay = self.options.get("max_delay", 0)
        self.max_pos = self.options.get("max_position", 0)
        self.force = self.options.get("force", False)
        self.pedantic = self.options.get("pedantic", False)
        self.before_script = self.options.get("before", None)
        self.after_script = self.options.get("after", None)
        self.timeout = int(self.options.get("timeout", 300))
        self.logging = self.options.get("logging", False)
        self.rpl_user = self.options.get("rpl_user", None)
        self.script_threshold = self.options.get("script_threshold", None)
        self.master_vals = None

        # Attempt to connect to all servers
        self.master, self.slaves = self._connect_to_servers(master_vals,
                                                            slave_vals,
                                                            self.options,
                                                            skip_conn_err)
        self.discover_slaves(output_log=True)

    def _report(self, message, level=logging.INFO, print_msg=True):
        """Log message if logging is on

        This method will log the message presented if the log is turned on.
        Specifically, if options['log_file'] is not None. It will also
        print the message to stdout.

        message[in]    message to be printed
        level[in]      level of message to log. Default = INFO
        print_msg[in]  if True, print the message to stdout. Default = True
        """
        # First, print the message.
        if print_msg and not self.quiet:
            print message
        # Now log message if logging turned on
        if self.logging:
            logging.log(int(level), message.strip("#").strip(' '))

    def _connect_to_servers(self, master_vals, slave_vals, options,
                            skip_conn_err=True):
        """Connect to the master and one or more slaves

        This method will attempt to connect to the master and slaves provided.
        For slaves, if the --force option is specified, it will skip slaves
        that cannot be reached setting the slave dictionary to None
        in the list instead of a Slave class instance.

        The dictionary of the list of slaves returns is as follows.

        slave_dict = {
          'host'     : # host name for slave
          'port'     : # port for slave
          'instance' : Slave class instance or None if cannot connect
        }

        master_vals[in]    master server connection dictionary
        slave_vals[in]     list of slave server connection dictionaries
        options[in]        options dictionary
          verbose          print extra data during operations (optional)
                           Default = False
          ping             maximum number of seconds to ping
                           Default = 3
          max_delay        maximum delay in seconds slave and be behind
                           master and still be 'Ok'. Default = 0
          max_position     maximum position slave can be behind master's
                           binlog and still be 'Ok'. Default = 0
        skip_conn_err[in]  if True, do not fail on connection failure
                           Default = True

        Returns tuple - master instance, list of dictionary slave instances
        """
        master = None
        slaves = []

        # Set verbose value.
        verbose = self.options.get("verbosity", 0) > 0

        # attempt to connect to the master
        if master_vals:
            master = get_server('master', master_vals, True, verbose=verbose)
            if self.logging:
                log_server_version(master)

        for slave_val in slave_vals:
            host = slave_val['host']
            port = slave_val['port']
            try:
                slave = get_server('slave', slave_val, True, verbose=verbose)
                if self.logging:
                    log_server_version(slave)
            except:
                msg = "Cannot connect to slave %s:%s as user '%s'." % \
                      (host, port, slave_val['user'])
                if skip_conn_err:
                    if self.verbose:
                        self._report("# ERROR: %s" % msg, logging.ERROR)
                    slave = None
                else:
                    raise UtilRplError(msg)
            slave_dict = {
                'host': host,       # host name for slave
                'port': port,       # port for slave
                'instance': slave,  # Slave class instance or None
            }
            slaves.append(slave_dict)

        return (master, slaves)

    def _is_connected(self):
        """Check to see if all servers are connected.

        Method will skip any slaves that do not have an instance (offline)
        but requires the master be instantiated and connected.

        The method will also skip the checks altogether if self.force is
        specified.

        Returns bool - True if all connected or self.force is specified.
        """
        # Skip check if --force specified.
        if self.force:
            return True
        if self.master is None or not self.master.is_alive():
            return False
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            if slave is not None and not slave.is_alive():
                return False

        return True

    def remove_discovered_slaves(self):
        """Reset the slaves list to the original list at instantiation

        This method is used in conjunction with discover_slaves to remove
        any discovered slave from the slaves list. Once this is done,
        a call to discover slaves will rediscover the slaves. This is helpful
        for when failover occurs and a discovered slave is used for the new
        master.
        """
        new_list = []
        for slave_dict in self.slaves:
            if not slave_dict.get("discovered", False):
                new_list.append(slave_dict)
        self.slaves = new_list

    def check_master_info_type(self, repo="TABLE"):
        """Check all slaves for master_info_repository=repo

        repo[in]       value for master info = "TABLE" or "FILE"
                       Default is "TABLE"

        Returns bool - True if master_info_repository == repo
        """
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            if slave is not None:
                res = slave.show_server_variable("master_info_repository")
                if not res or res[0][1].upper() != repo.upper():
                    return False
        return True

    def discover_slaves(self, skip_conn_err=True, output_log=False):
        """Discover slaves connected to the master

        skip_conn_err[in]   Skip connection errors to the slaves (i.e. log the
                            errors but do not raise an exception),
                            by default True.
        output_log[in]      Output the logged information (i.e. print the
                            information of discovered slave to the output),
                            by default False.

        Returns bool - True if new slaves found
        """
        # See if the user wants us to discover slaves.
        discover = self.options.get("discover", None)
        if not discover or not self.master:
            return

        # Get user and password (support login-path)
        try:
            user, password = parse_user_password(discover,
                                                 options=self.options)
        except FormatError:
            raise UtilError(USER_PASSWORD_FORMAT.format("--discover-slaves"))

        # Find discovered slaves
        new_slaves_found = False
        self._report("# Discovering slaves for master at "
                     "{0}:{1}".format(self.master.host, self.master.port))
        discovered_slaves = self.master.get_slaves(user, password)
        # pylint: disable=R0101
        for slave in discovered_slaves:
            if "]" in slave:
                host, port = slave.split("]:")
                host = "{0}]".format(host)
            else:
                host, port = slave.split(":")
            msg = "Discovering slave at {0}:{1}".format(host, port)
            self._report(msg, logging.INFO, False)
            if output_log:
                print("# {0}".format(msg))
            # Skip hosts that are not registered properly
            if host == 'unknown host':
                continue
            # Check to see if the slave is already in the list
            else:
                found = False
                # Eliminate if already a slave
                for slave_dict in self.slaves:
                    if slave_dict['host'] == host and \
                       int(slave_dict['port']) == int(port):
                        found = True
                        break
                if not found:
                    # Now we must attempt to connect to the slave.
                    conn_dict = {
                        'conn_info': {'user': user, 'passwd': password,
                                      'host': host, 'port': port,
                                      'socket': None,
                                      'ssl_ca': self.master.ssl_ca,
                                      'ssl_cert': self.master.ssl_cert,
                                      'ssl_key': self.master.ssl_key,
                                      'ssl': self.master.ssl},
                        'role': slave,
                        'verbose': self.options.get("verbosity", 0) > 0,
                    }
                    slave_conn = Slave(conn_dict)
                    try:
                        slave_conn.connect()
                        # Skip discovered slaves that are not connected
                        # to the master (i.e. IO thread is not running)
                        if slave_conn.is_connected():
                            self.slaves.append({'host': host, 'port': port,
                                                'instance': slave_conn,
                                                'discovered': True})
                            msg = "Found slave: {0}:{1}".format(host, port)
                            self._report(msg, logging.INFO, False)
                            if output_log:
                                print("# {0}".format(msg))
                            if self.logging:
                                log_server_version(slave_conn)
                            new_slaves_found = True
                        else:
                            msg = ("Slave skipped (IO not running): "
                                   "{0}:{1}").format(host, port)
                            self._report(msg, logging.WARN, False)
                            if output_log:
                                print("# {0}".format(msg))
                    except UtilError, e:
                        msg = ("Cannot connect to slave {0}:{1} as user "
                               "'{2}'.").format(host, port, user)
                        if skip_conn_err:
                            msg = "{0} {1}".format(msg, e.errmsg)
                            self._report(msg, logging.WARN, False)
                            if output_log:
                                print("# {0}".format(msg))
                        else:
                            raise UtilRplError(msg)

        return new_slaves_found

    def _get_server_gtid_data(self, server, role):
        """Retrieve the GTID information from the server.

        This method builds a tuple of three lists corresponding to the three
        GTID lists (executed, purged, owned) retrievable via the global
        variables. It generates lists suitable for format and printing.

        role[in]           role of the server (used for report generation)

        Returns tuple - (executed list, purged list, owned list)
        """
        executed = []
        purged = []
        owned = []

        if server.supports_gtid() == "NO":
            return (executed, purged, owned)

        try:
            gtids = server.get_gtid_status()
        except UtilError, e:
            self._report("# ERROR retrieving GTID information: %s" % e.errmsg,
                         logging.ERROR)
            return None
        for gtid in gtids[0]:
            for row in gtid.split("\n"):
                if len(row):
                    executed.append((server.host, server.port, role,
                                     row.strip(",")))
        for gtid in gtids[1]:
            for row in gtid.split("\n"):
                if len(row):
                    purged.append((server.host, server.port, role,
                                   row.strip(",")))
        for gtid in gtids[2]:
            for row in gtid.split("\n"):
                if len(row):
                    owned.append((server.host, server.port, role,
                                  row.strip(",")))

        return (executed, purged, owned)

    def _check_switchover_prerequisites(self, candidate=None):
        """Check prerequisites for performing switchover

        This method checks the prerequisites for performing a switch from a
        master to a candidate slave.

        candidate[in]  if supplied, use this candidate instead of the
                       candidate supplied by the user. Must be instance of
                       Master class.

        Returns bool - True if success, raises error if not
        """
        if candidate is None:
            candidate = self.options.get("candidate", None)

        assert (candidate is not None), "A candidate server is required."
        assert (isinstance(candidate, Master)), \
            "A candidate server must be a Master class instance."

        # If master has GTID=ON, ensure all servers have GTID=ON
        gtid_enabled = self.master.supports_gtid() == "ON"
        if gtid_enabled:
            gtid_ok = True
            for slave_dict in self.slaves:
                slave = slave_dict['instance']
                # skip dead or zombie slaves
                if not slave or not slave.is_alive():
                    continue
                if slave.supports_gtid() != "ON":
                    gtid_ok = False
            if not gtid_ok:
                msg = "GTIDs are enabled on the master but not " + \
                      "on all of the slaves."
                self._report(msg, logging.CRITICAL)
                raise UtilRplError(msg)
            elif self.verbose:
                self._report("# GTID_MODE=ON is set for all servers.")

        # Need Slave class instance to check master and replication user
        slave = self._change_role(candidate)

        # Check eligibility
        candidate_ok = self._check_candidate_eligibility(slave.host,
                                                         slave.port,
                                                         slave)
        if not candidate_ok[0]:
            # Create replication user if --force is specified.
            if self.force and candidate_ok[1] == "RPL_USER":
                user, passwd = slave.get_rpl_user()
                res = candidate.create_rpl_user(slave.host, slave.port,
                                                user, passwd, self.ssl)
                if not res[0]:
                    print("# ERROR: {0}".format(res[1]))
                    self._report(res[1], logging.CRITICAL, False)
            else:
                msg = candidate_ok[2]
                self._report(msg, logging.CRITICAL)
                raise UtilRplError(msg)

        return True

    def _get_rpl_user(self, server):
        """Get the replication user

        This method returns the user and password for the replication user
        as read from the Slave class.

        Returns tuple - user, password
        """
        # Get replication user from server if rpl_user not specified
        if self.rpl_user is None:
            slave = self._change_role(server)
            user, passwd = slave.get_rpl_user()
            return (user, passwd)

        # Get user and password (support login-path)
        try:
            user, passwd = parse_user_password(self.rpl_user,
                                               options=self.options)
        except FormatError:
            raise UtilError(USER_PASSWORD_FORMAT.format("--rpl-user"))
        return (user, passwd)

    def run_script(self, script, quiet, options=None):
        """Run an external script

        This method executes an external script. Result is checked for
        success (res == 0). If the user specified a threshold and the
        threshold is exceeded, an error is raised.

        script[in]     script to execute
        quiet[in]      if True, do not print messages
        options[in]    options for script
                       Default is none (no options)

        Returns bool - True = success
        """
        if options is None:
            options = []
        if script is None:
            return
        self._report("# Spawning external script.")
        res = execute_script(script, None, options, self.verbose)
        if self.script_threshold and res >= int(self.script_threshold):
            raise UtilRplError("External script '{0}' failed. Result = {1}.\n"
                               "Specified threshold exceeded. Operation abort"
                               "ed.\nWARNING: The operation did not complete."
                               " Depending on when the external script was "
                               "called, you should check the topology "
                               "for inconsistencies.".format(script, res))
        if res == 0:
            self._report("# Script completed Ok.")
        elif not quiet:
            self._report("ERROR: %s Script failed. Result = %s" %
                         (script, res), logging.ERROR)

    def _check_filters(self, master, slave):
        """Check filters to ensure they are compatible with the master.

        This method compares the binlog_do_db with the replicate_do_db and
        the binlog_ignore_db with the replicate_ignore_db on the master and
        slave to ensure the candidate slave is not filtering out different
        databases than the master.

        master[in]     the Master class instance of the master
        slave[in]      the Slave class instance of the slave

        Returns bool - True = filters agree
        """
        m_filter = master.get_binlog_exceptions()
        s_filter = slave.get_binlog_exceptions()

        failed = False
        if len(m_filter) != len(s_filter):
            failed = True
        elif len(m_filter) == 0:
            return True
        elif m_filter[0][1] != s_filter[0][1] or \
                m_filter[0][2] != s_filter[0][2]:
            failed = True
        if failed:
            if self.verbose and not self.quiet:
                fmt = self.options.get("format", "GRID")
                rows = []
                if len(m_filter) == 0:
                    rows.append(('MASTER', '', ''))
                else:
                    rows.append(m_filter[0])
                if len(s_filter) == 0:
                    rows.append(('SLAVE', '', ''))
                else:
                    rows.append(s_filter[0])
                cols = ["role", "*_do_db", "*_ignore_db"]
                self._report("# Filter Check Failed.", logging.ERROR)
                print_list(sys.stdout, fmt, cols, rows)
            return False
        return True

    def _check_candidate_eligibility(self, host, port, slave,
                                     check_master=True, quiet=False):
        """Perform sanity checks for slave promotion

        This method checks the slave candidate to ensure it meets the
        requirements as follows.

        Check Name  Description
        ----------- --------------------------------------------------
        CONNECTED   slave is connected to the master
        GTID        slave has GTID_MODE = ON if master has GTID = ON
                    (GTID only)
        BEHIND      slave is not behind master
                    (non-GTID only)
        FILTER      slave's filters match the master
        RPL_USER    slave has rpl user defined
        BINLOG      slave must have binary logging enabled

        host[in]         host name for the slave (used for errors)
        port[in]         port for the slave (used for errors)
        slave[in]        Slave class instance of candidate
        check_master[in] if True, check that slave is connected to the master
        quiet[in]        if True, do not print messages even if verbosity > 0

        Returns tuple (bool, check_name, string) -
            (True, "", "") = candidate is viable,
            (False, check_name, error_message) = candidate is not viable
        """
        assert (slave is not None), "No Slave instance for eligibility check."

        gtid_enabled = slave.supports_gtid() == "ON"

        # Is slave connected to master?
        if self.verbose and not quiet:
            self._report("# Checking eligibility of slave %s:%s for "
                         "candidate." % (host, port))
        if check_master:
            msg = "#   Slave connected to master ... %s"
            if not slave.is_alive():
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "CONNECTED",
                        "Connection to slave server lost.")
            if not slave.is_configured_for_master(self.master):
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "CONNECTED",
                        "Candidate is not connected to the correct master.")
            if self.verbose and not quiet:
                self._report(msg % "Ok")

        # If GTID is active on master, ensure slave is on too.
        if gtid_enabled:
            msg = "#   GTID_MODE=ON ... %s"
            if slave.supports_gtid() != "ON":
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "GTID",
                        "Slave does not have GTID support enabled.")
            if self.verbose and not quiet:
                self._report(msg % "Ok")

        # Check for slave behind master
        if not gtid_enabled and check_master:
            msg = "#   Slave not behind master ... %s"
            rpl = Replication(self.master, slave, self.options)
            errors = rpl.check_slave_delay()
            if errors != []:
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "BEHIND", " ".join(errors))
            if self.verbose and not quiet:
                self._report(msg % "Ok")

        # Check filters unless force is on.
        if not self.force and check_master:
            msg = "#   Logging filters agree ... %s"
            if not self._check_filters(self.master, slave):
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "FILTERS",
                        "Master and slave filters differ.")
            elif self.verbose and not quiet:
                self._report(msg % "Ok")

        # If no GTIDs, we need binary logging enabled on candidate.
        if not gtid_enabled:
            msg = "#   Binary logging turned on ... %s"
            if not slave.binlog_enabled():
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "BINLOG",
                        "Binary logging is not enabled on the candidate.")
            if self.verbose and not quiet:
                self._report(msg % "Ok")

        # Check replication user - must exist with correct privileges
        try:
            user, _ = slave.get_rpl_user()
        except UtilError:
            if not self.rpl_user:
                raise

            # Get user and password (support login-path)
            try:
                user, _ = parse_user_password(self.rpl_user)
            except FormatError:
                raise UtilError(USER_PASSWORD_FORMAT.format("--rpl-user"))

            # Make new master forget was a slave using slave methods
            s_candidate = self._change_role(slave, slave=False)
            res = s_candidate.get_rpl_users()
            l = len(res)
            user, host, _ = res[l - 1]
            # raise

        msg = "#   Replication user exists ... %s"
        if user is None or slave.check_rpl_user(user, slave.host) != []:
            if not self.force:
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "RPL_USER",
                        "Candidate slave is missing replication user.")
            else:
                self._report("Replication user not found but --force used.",
                             logging.WARN)
        elif self.verbose and not quiet:
            self._report(msg % "Ok")

        return (True, "", "")

    def read_all_retrieved_gtids(self, slave):
        """Ensure any GTIDS in relay log are read

        This method iterates over all slaves ensuring any events read from
        the master but not executed (read) from the relay log are read.

        This step is necessary for failover to ensure all transactions are
        applied to all slaves before the new master is selected.

        slave[in]       Server instance of the slave
        """
        # skip dead or zombie slaves
        if slave is None or not slave.is_alive():
            return
        gtids = slave.get_retrieved_gtid_set()
        if gtids:
            if self.verbose and not self.quiet:
                self._report("# Reading events in relay log for slave "
                             "%s:%s" % (slave.host, slave.port))
            try:
                slave.exec_query(_GTID_WAIT % (gtids.strip(','), self.timeout))
            except UtilRplError as err:
                raise UtilRplError("Error executing %s: %s" %
                                   ((_GTID_WAIT % (gtids.strip(','),
                                                   self.timeout)), err.errmsg))

    def _has_missing_transactions(self, candidate, slave):
        """Determine if there are transactions on the slave not on candidate

        This method uses the function gtid_subset() to determine if there are
        GTIDs (transactions) on the slave that are not on the candidate.

        Return code fopr query should be 0 when there are missing
        transactions, 1 if not, and -1 if there is a non-numeric result
        code generated.

        candidate[in]   Server instance of candidate (new master)
        slave[in]       Server instance of slave to check

        Returns boolean - True if there are transactions else False
        """
        slave_exec_gtids = slave.get_executed_gtid_set()
        slave_retrieved_gtids = slave.get_retrieved_gtid_set()
        cand_slave = self._change_role(candidate)
        candidate_exec_gtids = cand_slave.get_executed_gtid_set()
        slave_gtids = ",".join([slave_exec_gtids.strip(","),
                                slave_retrieved_gtids.strip(",")])
        res = slave.exec_query("SELECT gtid_subset('%s', '%s')" %
                               (slave_gtids, candidate_exec_gtids.strip(",")))
        if res and res[0][0].isdigit():
            result_code = int(res[0][0])
        else:
            result_code = -1

        if self.verbose and not self.quiet:
            if result_code != 1:
                self._report("# Missing transactions found on %s:%s. "
                             "SELECT gtid_subset() = %s" %
                             (slave.host, slave.port, result_code))
            else:
                self._report("# No missing transactions found on %s:%s. "
                             "Skipping connection of candidate as slave." %
                             (slave.host, slave.port))

        return result_code != 1

    def _prepare_candidate_for_failover(self, candidate, user, passwd=""):
        """Prepare candidate slave for slave promotion (in failover)

        This method uses the candidate slave specified and connects it to
        each slave in the topology performing a GTID_SUBSET query to wait
        for the candidate (acting as a slave) to catch up. This ensures
        the candidate is now the 'best' or 'most up-to-date' slave in the
        topology.

        Method works only for GTID-enabled candidate servers.

        candidate[in]  Slave class instance of candidate
        user[in]       replication user
        passwd[in]     replication user password

        Returns bool - True if successful,
                       raises exception if failure and forst is False
        """

        assert (candidate is not None), "Candidate must be a Slave instance."

        if candidate.supports_gtid() != "ON":
            msg = "Candidate does not have GTID turned on or " + \
                  "does not support GTIDs."
            self._report(msg, logging.CRITICAL)
            raise UtilRplError(msg)

        lock_options = {
            'locking': 'flush',
            'verbosity': 3 if self.verbose else self.verbosity,
            'silent': self.quiet,
            'rpl_mode': "master",
        }

        hostport = "%s:%s" % (candidate.host, candidate.port)
        for slave_dict in self.slaves:
            s_host = slave_dict['host']
            s_port = slave_dict['port']

            temp_master = slave_dict['instance']

            # skip dead or zombie slaves
            if temp_master is None or not temp_master.is_alive():
                continue

            # Gather retrieved_gtid_set to execute all events on slaves still
            # in the slave's relay log
            self.read_all_retrieved_gtids(temp_master)

            # Sanity check: ensure candidate and slave are not the same.
            if candidate.is_alias(s_host) and \
               int(s_port) == int(candidate.port):
                continue

            # Check for missing transactions. No need to connect to slave if
            # there are no transactions (GTIDs) to retrieve
            if not self._has_missing_transactions(candidate, temp_master):
                continue

            try:
                candidate.stop()
            except UtilError as err:
                if not self.quiet:
                    self._report("Candidate {0} failed to stop. "
                                 "{1}".format(hostport, err.errmsg))

            # Block writes to slave (temp_master)
            lock_ftwrl = Lock(temp_master, [], lock_options)
            temp_master.set_read_only(True)
            if self.verbose and not self.quiet:
                read_only = temp_master.show_server_variable("READ_ONLY")
                self._report("# Read only is {0} for {1}:{2}."
                             "".format(read_only[0][1], temp_master.host,
                                       temp_master.port))

            # Connect candidate to slave as its temp_master
            if self.verbose and not self.quiet:
                self._report("# Connecting candidate to %s:%s as a temporary "
                             "slave to retrieve unprocessed GTIDs." %
                             (s_host, s_port))

            if not candidate.switch_master(temp_master, user, passwd, False,
                                           None, None,
                                           self.verbose and not self.quiet):
                msg = "Cannot switch candidate to slave for " + \
                      "slave promotion process."
                self._report(msg, logging.CRITICAL)
                raise UtilRplError(msg)

            # Unblock writes to slave (temp_master).
            temp_master.set_read_only(False)
            if self.verbose and not self.quiet:
                read_only = temp_master.show_server_variable("READ_ONLY")
                self._report("# Read only is {0} for {1}:{2}."
                             "".format(read_only[0][1], temp_master.host,
                                       temp_master.port))
            lock_ftwrl.unlock()

            try:
                candidate.start()
                candidate.exec_query("COMMIT")
            except UtilError as err:
                if not self.quiet:
                    self._report("Candidate {0} failed to start. "
                                 "{1}".format(hostport, err.errmsg))

            if self.verbose and not self.quiet:
                self._report("# Waiting for candidate to catch up to slave "
                             "%s:%s." % (s_host, s_port))
            temp_master_gtid = temp_master.exec_query(_GTID_EXECUTED)
            candidate.wait_for_slave_gtid(temp_master_gtid, self.timeout,
                                          self.verbose and not self.quiet)

            # Disconnect candidate from slave (temp_master)
            candidate.stop()

        return True

    def _check_slaves_status(self, stop_on_error=False):
        """Check all slaves for error before performing failover.

        This method check the status of all slaves (before the new master catch
        up with them), using SHOW SLAVE STATUS, reporting any error found and
        warning the user if failover might result in an inconsistent
        replication topology. By default the process will not stop, but if
        the --pedantic option is used then failover will stop with an error.

        stop_on_error[in]  Define the default behavior of failover if errors
                           are found. By default: False (not stop on errors).
        """
        for slave_dict in self.slaves:
            s_host = slave_dict['host']
            s_port = slave_dict['port']
            slave = slave_dict['instance']

            # Verify if the slave is alive
            if not slave or not slave.is_alive():
                msg = "Slave '{host}@{port}' is not alive.".format(host=s_host,
                                                                   port=s_port)
                # Print warning or raise an error according to the default
                # failover behavior and defined options.
                if ((stop_on_error and not self.force) or
                        (not stop_on_error and self.pedantic)):
                    print("# ERROR: {0}".format(msg))
                    self._report(msg, logging.CRITICAL, False)
                    if stop_on_error and not self.force:
                        ignore_opt = "with the --force"
                    else:
                        ignore_opt = "without the --pedantic"
                    ignore_tip = ("Note: To ignore this issue use the "
                                  "utility {0} option.").format(ignore_opt)
                    raise UtilRplError("{err} {note}".format(err=msg,
                                                             note=ignore_tip))
                else:
                    print("# WARNING: {0}".format(msg))
                    self._report(msg, logging.WARN, False)
                    continue

            # Check SQL thread and errors (no need to check for IO errors)
            # Note: IO errors are excepted as the master is down
            res = slave.get_sql_error()

            # First, check if server is acting as a slave
            if not res:
                msg = ("Server '{host}@{port}' is not acting as a "
                       "slave.").format(host=s_host, port=s_port)
                # Print warning or raise an error according to the default
                # failover behavior and defined options.
                if ((stop_on_error and not self.force) or
                        (not stop_on_error and self.pedantic)):
                    print("# ERROR: {0}".format(msg))
                    self._report(msg, logging.CRITICAL, False)
                    if stop_on_error and not self.force:
                        ignore_opt = "with the --force"
                    else:
                        ignore_opt = "without the --pedantic"
                    ignore_tip = ("Note: To ignore this issue use the "
                                  "utility {0} option.").format(ignore_opt)
                    raise UtilRplError("{err} {note}".format(err=msg,
                                                             note=ignore_tip))
                else:
                    print("# WARNING: {0}".format(msg))
                    self._report(msg, logging.WARN, False)
                    continue

            # Now, check the SQL thread status
            sql_running = res[0]
            sql_errorno = res[1]
            sql_error = res[2]
            if sql_running == "No" or sql_errorno or sql_error:
                msg = ("Problem detected with SQL thread for slave "
                       "'{host}'@'{port}' that can result in an unstable "
                       "topology.").format(host=s_host, port=s_port)
                msg_thread = " - SQL thread running: {0}".format(sql_running)
                if not sql_errorno and not sql_error:
                    msg_error = " - SQL error: None"
                else:
                    msg_error = (" - SQL error: {errno} - "
                                 "{errmsg}").format(errno=sql_errorno,
                                                    errmsg=sql_error)
                msg_tip = ("Check the slave server log to identify "
                           "the problem and fix it. For more information, "
                           "see: http://dev.mysql.com/doc/refman/5.6/en/"
                           "replication-problems.html")
                # Print warning or raise an error according to the default
                # failover behavior and defined options.
                if ((stop_on_error and not self.force) or
                        (not stop_on_error and self.pedantic)):
                    print("# ERROR: {0}".format(msg))
                    self._report(msg, logging.CRITICAL, False)
                    print("# {0}".format(msg_thread))
                    self._report(msg_thread, logging.CRITICAL, False)
                    print("# {0}".format(msg_error))
                    self._report(msg_error, logging.CRITICAL, False)
                    print("#  Tip: {0}".format(msg_tip))
                    if stop_on_error and not self.force:
                        ignore_opt = "with the --force"
                    else:
                        ignore_opt = "without the --pedantic"
                    ignore_tip = ("Note: To ignore this issue use the "
                                  "utility {0} option.").format(ignore_opt)
                    raise UtilRplError("{err} {note}".format(err=msg,
                                                             note=ignore_tip))
                else:
                    print("# WARNING: {0}".format(msg))
                    self._report(msg, logging.WARN, False)
                    print("# {0}".format(msg_thread))
                    self._report(msg_thread, logging.WARN, False)
                    print("# {0}".format(msg_error))
                    self._report(msg_error, logging.WARN, False)
                    print("#  Tip: {0}".format(msg_tip))

    def find_errant_transactions(self):
        """Check all slaves for the existence of errant transactions.

        In particular, for all slaves it search for executed transactions that
        are not found on the other slaves (only on one slave) and not from the
        current master.

        Returns a list of tuples, each tuple containing the slave host, port
        and set of corresponding errant transactions, i.e.:
        [(host1, port1, set1), ..., (hostn, portn, setn)]. If no errant
        transactions are found an empty list is returned.
        """
        res = []

        # Get master UUID (if master is available otherwise get it from slaves)
        use_master_uuid_from_slave = True
        if self.master:
            master_uuid = self.master.get_uuid()
            use_master_uuid_from_slave = False

        # Check all slaves for executed transactions not in other slaves
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # Skip not defined or dead slaves
            if not slave or not slave.is_alive():
                continue
            tnx_set = slave.get_executed_gtid_set()

            # Get master UUID from slave if master is not available
            if use_master_uuid_from_slave:
                master_uuid = slave.get_master_uuid()

            slave_set = set()
            for others_slave_dic in self.slaves:
                if (slave_dict['host'] != others_slave_dic['host'] or
                        slave_dict['port'] != others_slave_dic['port']):
                    other_slave = others_slave_dic['instance']
                    # Skip not defined or dead slaves
                    if not other_slave or not other_slave.is_alive():
                        continue
                    errant_res = other_slave.exec_query(
                        _GTID_SUBTRACT_TO_EXECUTED.format(tnx_set))

                    # Only consider the transaction as errant if not from the
                    # current master.
                    # Note: server UUID can appear with mixed cases (e.g. for
                    # 5.6.9 servers the server_uuid is lower case and appears
                    # in upper cases in the GTID_EXECUTED set.
                    errant_set = set()
                    for tnx in errant_res:
                        if tnx[0] and not tnx[0].lower().startswith(
                                master_uuid.lower()):
                            errant_set.update(tnx[0].split(',\n'))

                    # Errant transactions exist on only one slave, therefore if
                    # the returned set is empty the loop can be break
                    # (no need to check the remaining slaves).
                    if not errant_set:
                        break

                    slave_set = slave_set.union(errant_set)
            # Store result
            if slave_set:
                res.append((slave_dict['host'], slave_dict['port'], slave_set))

        return res

    def _check_all_slaves(self, new_master):
        """Check all slaves for errors.

        Check each slave's status for errors during replication. If errors are
        found, they are printed as warning statements to stdout.

        new_master[in] the new master in Master class instance
        """
        slave_errors = []
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                continue
            rpl = Replication(new_master, slave, self.options)
            # Use pingtime to check slave status
            iteration = 0
            slave_ok = True
            while iteration < int(self.pingtime):
                res = rpl.check_slave_connection()
                if not res and iteration >= self.pingtime:
                    slave_error = None
                    if self.verbose:
                        res = slave.get_io_error()
                        slave_error = "%s:%s" % (res[1], res[2])
                    slave_errors.append((slave_dict['host'],
                                         slave_dict['port'],
                                         slave_error))
                    slave_ok = False
                    if self.verbose and not self.quiet:
                        self._report("# %s:%s status: FAIL " %
                                     (slave_dict['host'],
                                      slave_dict['port']), logging.WARN)
                elif res:
                    iteration = int(self.pingtime) + 1
                else:
                    time.sleep(1)
                    iteration += 1
            if slave_ok and self.verbose and not self.quiet:
                self._report("# %s:%s status: Ok " % (slave_dict['host'],
                                                      slave_dict['port']))

        if len(slave_errors) > 0:
            self._report("WARNING - The following slaves failed to connect to "
                         "the new master:", logging.WARN)
            for error in slave_errors:
                self._report("  - %s:%s" % (error[0], error[1]), logging.WARN)
                if self.verbose and error[2] is not None:
                    self._report(error[2], logging.WARN)
                else:
                    print
            return False

        return True

    def remove_slave(self, slave):
        """Remove a slave from the slaves dictionary list

        slave[in]      the dictionary for the slave to remove
        """
        for i, slave_dict in enumerate(self.slaves):
            if (slave_dict['instance'] and
                    slave_dict['instance'].is_alias(slave['host']) and
                    int(slave_dict['port']) == int(slave['port'])):
                # Disconnect to satisfy new server restrictions on termination
                self.slaves[i]['instance'].disconnect()
                self.slaves.pop(i)
                break

    def gtid_enabled(self):
        """Check if topology has GTID turned on.

        This method check if GTID mode is turned ON for all servers in the
        replication topology, skipping the check for not available servers.

        Returns bool - True = GTID_MODE=ON for all available servers (master
        and slaves) in the replication topology..
        """
        if self.master and self.master.supports_gtid() != "ON":
            return False  # GTID disabled or not supported.
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                continue
            if slave.supports_gtid() != "ON":
                return False  # GTID disabled or not supported.
        # GTID enabled for all topology (excluding not available servers).
        return True

    def get_servers_with_gtid_not_on(self):
        """Get the list of servers from the topology with GTID turned off.

        Note: not connected slaves will be ignored

        Returns a list of tuples identifying the slaves (host, port, gtid_mode)
                with GTID_MODE=OFF or GTID_MODE=NO (i.e., not available).
        """
        res = []
        # Check master GTID_MODE
        if self.master:
            gtid_mode = self.master.supports_gtid()
            if gtid_mode != "ON":
                res.append((self.master.host, self.master.port, gtid_mode))

        # Check slaves GTID_MODE
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # skip not available or not alive slaves
            if not slave or not slave.is_alive():
                continue
            gtid_mode = slave.supports_gtid()
            if gtid_mode != "ON":
                res.append((slave_dict['host'], slave_dict['port'], gtid_mode))

        return res

    def get_health(self):
        """Retrieve the replication health for the master and slaves.

        This method will retrieve the replication health of the topology. This
        includes the following for each server.

          - host       : host name
          - port       : connection port
          - role       : "MASTER" or "SLAVE"
          - state      : UP = connected, WARN = cannot connect but can ping,
                         DOWN = cannot connect nor ping
          - gtid       : ON = gtid supported and turned on, OFF = supported
                         but not enabled, NO = not supported
          - rpl_health : (master) binlog enabled,
                         (slave) IO tread is running, SQL thread is running,
                         no errors, slave delay < max_delay,
                         read log pos + max_position < master's log position
                         Note: Will show 'ERROR' if there are multiple
                         errors encountered otherwise will display the
                         health check that failed.

        If verbosity is set, it will show the following additional information.

          (master)
            - server version, binary log file, position

          (slaves)
            - server version, master's binary log file, master's log position,
              IO_Thread, SQL_Thread, Secs_Behind, Remaining_Delay,
              IO_Error_Num, IO_Error

        Note: The method will return health for the master and slaves or just
              the slaves if no master is specified. In which case, the master
              status shall display "no master specified" instead of a status
              for the connection.

        Returns tuple - (columns, rows)
        """
        rows = []
        columns = []
        columns.extend(_HEALTH_COLS)
        if self.verbosity > 0:
            columns.extend(_HEALTH_DETAIL_COLS)
        if self.master:
            # Get master health
            rpl_health = self.master.check_rpl_health()
            self._report("# Getting health for master: %s:%s." %
                         (self.master.host, self.master.port), logging.INFO,
                         False)
            have_gtid = self.master.supports_gtid()
            master_data = [
                self.master.host,
                self.master.port,
                "MASTER",
                get_server_state(self.master, self.master.host, self.pingtime,
                                 self.verbosity > 0),
                have_gtid,
                "OK" if rpl_health[0] else ", ".join(rpl_health[1]),
            ]

            m_status = self.master.get_status()
            if len(m_status):
                master_log, master_log_pos = m_status[0][0:2]
            else:
                master_log = None
                master_log_pos = 0

            # Show additional details if verbosity turned on
            if self.verbosity > 0:
                master_data.extend([self.master.get_version(), master_log,
                                    master_log_pos, "", "", "", "", "", "",
                                    "", "", ""])

            rows.append(master_data)
            if have_gtid == "ON":
                master_gtids = self.master.exec_query(_GTID_EXECUTED)
        else:
            # No master makes these impossible to determine.
            have_gtid = "OFF"
            master_log = ""
            master_log_pos = ""  # pylint: disable=R0204

        # Get the health of the slaves
        slave_rows = []
        for slave_dict in self.slaves:
            host = slave_dict['host']
            port = slave_dict['port']
            slave = slave_dict['instance']
            # Get correct port from slave
            if slave and port != slave.port:
                port = slave.port
            if slave is None:
                rpl_health = (False, ["Cannot connect to slave."])
            elif not slave.is_alive():
                # Attempt to reconnect to the database server.
                try:
                    slave.connect()
                    # Connection succeeded.
                    if not slave.is_configured_for_master(self.master):
                        rpl_health = (False,
                                      ["Slave is not connected to master."])
                        slave = None
                except UtilError:
                    # Connection failed.
                    rpl_health = (False, ["Slave is not alive."])
                    slave = None
            elif not self.master:
                rpl_health = (False, ["No master specified."])
            elif not slave.is_configured_for_master(self.master):
                rpl_health = (False, ["Slave is not connected to master."])
                slave = None

            if self.master and slave is not None:
                rpl_health = slave.check_rpl_health(self.master,
                                                    master_log, master_log_pos,
                                                    self.max_delay,
                                                    self.max_pos,
                                                    self.verbosity)

                # Now, see if filters are in compliance
                if not self._check_filters(self.master, slave):
                    if rpl_health[0]:
                        errors = rpl_health[1]
                        errors.append("Binary log and Relay log filters "
                                      "differ.")
                        rpl_health = (False, errors)

            slave_data = [
                host,
                port,
                "SLAVE",
                get_server_state(slave, host, self.pingtime,
                                 self.verbosity > 0),
                " " if slave is None else slave.supports_gtid(),
                "OK" if rpl_health[0] else ", ".join(rpl_health[1]),
            ]

            # Show additional details if verbosity turned on
            if self.verbosity > 0:
                if slave is None:
                    slave_data.extend([""] * 13)
                else:
                    slave_data.append(slave.get_version())
                    res = slave.get_rpl_details()
                    if res is not None:
                        slave_data.extend(res)
                        if have_gtid == "ON":
                            gtid_behind = slave.num_gtid_behind(master_gtids)
                            slave_data.extend([gtid_behind])
                        else:
                            slave_data.extend([""])
                    else:
                        slave_data.extend([""] * 13)

            slave_rows.append(slave_data)

        # order the slaves
        slave_rows.sort(key=operator.itemgetter(0, 1))
        rows.extend(slave_rows)

        return (columns, rows)

    def get_server_uuids(self):
        """Return a list of the server's uuids.

        Returns list of tuples = (host, port, role, uuid)
        """
        # Get the master's uuid
        uuids = []
        uuids.append((self.master.host, self.master.port, "MASTER",
                      self.master.get_uuid()))
        for slave_dict in self.slaves:
            uuids.append((slave_dict['host'], slave_dict['port'], "SLAVE",
                          slave_dict['instance'].get_uuid()))
        return uuids

    def get_gtid_data(self):
        """Get the GTID information from the topology

        This method retrieves the executed, purged, and owned GTID lists from
        the servers in the topology. It arranges them into three lists and
        includes the host name, port, and role of each server.

        Returns tuple - lists for GTID data
        """
        executed = []
        purged = []
        owned = []

        gtid_data = self._get_server_gtid_data(self.master, "MASTER")
        if gtid_data is not None:
            executed.extend(gtid_data[0])
            purged.extend(gtid_data[1])
            owned.extend(gtid_data[2])

        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            if slave is not None:
                gtid_data = self._get_server_gtid_data(slave, "SLAVE")
                if gtid_data is not None:
                    executed.extend(gtid_data[0])
                    purged.extend(gtid_data[1])
                    owned.extend(gtid_data[2])

        return (executed, purged, owned)

    def get_slaves_dict(self, skip_not_connected=True):
        """Get a dictionary representation of the slaves in the topology.

        This function converts the list of slaves in the topology to a
        dictionary with all elements in the list, using 'host@port' as the
        key for each element.

        skip_not_connected[in]  Boolean value indicating if not available or
                                not connected slaves should be skipped.
                                By default 'True' (not available slaves are
                                skipped).

        Return a dictionary representation of the slaves in the
        topology. Each element has a key with the format 'host@port' and
        a dictionary value with the corresponding slave's data.
        """
        res = {}
        for slave_dic in self.slaves:
            slave = slave_dic['instance']
            if skip_not_connected:
                if slave and slave.is_alive():
                    key = '{0}@{1}'.format(slave_dic['host'],
                                           slave_dic['port'])
                    res[key] = slave_dic
            else:
                key = '{0}@{1}'.format(slave_dic['host'], slave_dic['port'])
                res[key] = slave_dic
        return res

    def slaves_gtid_subtract_executed(self, gtid_set, multithreading=False):
        """Subtract GTID_EXECUTED from the given GTID set on all slaves.

        Compute the difference between the given GTID set and the GTID_EXECUTED
        set for each slave, providing the sets with the missing GTIDs from the
        GTID_EXECUTED set that belong to the input GTID set.

        gtid_set[in]        Input GTID set to find the missing element from
                            the GTID_EXECUTED for all slaves.
        multithreading[in]  Flag indicating if multithreading will be used,
                            meaning that the operation will be performed
                            concurrently on all slaves.
                            By default True (concurrent execution).

        Return a list of tuples with the result for each slave. Each tuple
        contains the identification of the server (host and port) and a string
        representing the set of GTIDs from the given set not in the
        GTID_EXECUTED set of the corresponding slave.
        """
        if multithreading:
            # Create a pool of threads to execute the method for each slave.
            pool = ThreadPool(processes=len(self.slaves))
            res_lst = []
            for slave_dict in self.slaves:
                slave = slave_dict['instance']
                if slave:  # Skip non existing (not connected) slaves.
                    thread_res = pool.apply_async(slave.gtid_subtract_executed,
                                                  (gtid_set, ))
                    res_lst.append((slave.host, slave.port, thread_res))
            pool.close()
            # Wait for all threads to finish here to avoid RuntimeErrors when
            # waiting for the result of a thread that is already dead.
            pool.join()
            # Get the result from each slave and return the results.
            res = []
            for host, port, thread_res in res_lst:
                res.append((host, port, thread_res.get()))
            return res
        else:
            res = []
            # Subtract gtid set on all slaves.
            for slave_dict in self.slaves:
                slave = slave_dict['instance']
                if slave:  # Skip non existing (not connected) slaves.
                    not_in_set = slave.gtid_subtract_executed(gtid_set)
                    res.append((slave.host, slave.port, not_in_set))
            return res

    def check_privileges(self, failover=False, skip_master=False):
        """Check privileges for the master and all known servers

        failover[in]        if True, check permissions for switchover and
                            failover commands. Default is False.
        skip_master[in]     Skip the check for the master.

        Returns list - [(user, host)] if not enough permissions,
                       [] if no errors
        """
        servers = []
        errors = []

        # Collect all users first.
        if skip_master:
            for slave_conn in self.slaves:
                slave = slave_conn['instance']
                # A slave instance is None if the connection failed during the
                # creation of the topology. In this case ignore the slave.
                if slave is not None:
                    servers.append(slave)
        else:
            if self.master is not None:
                servers.append(self.master)
                for slave_conn in self.slaves:
                    slave = slave_conn['instance']
                    # A slave instance is None if the connection failed during
                    # the creation of the topology. In this case ignore the
                    # slave.
                    if slave is not None:
                        servers.append(slave)

        # If candidates were specified, check those too.
        candidates = self.options.get("candidates", None)
        candidate_slaves = []
        if candidates:
            self._report("# Checking privileges on candidates.")
            for candidate in candidates:
                slave_dict = self.connect_candidate(candidate, False)
                slave = slave_dict['instance']
                if slave is not None:
                    servers.append(slave)
                    candidate_slaves.append(slave)

        for server in servers:
            user_inst = User(server, "{0}@{1}".format(server.user,
                                                      server.host))
            if not failover:
                if not user_inst.has_privilege("*", "*", "SUPER"):
                    errors.append((server.user, server.host, server.port,
                                   'SUPER'))
            else:
                if (not user_inst.has_privilege("*", "*", "SUPER") or
                        not user_inst.has_privilege("*", "*",
                                                    "GRANT OPTION") or
                        not user_inst.has_privilege("*", "*", "SELECT") or
                        not user_inst.has_privilege("*", "*", "RELOAD") or
                        not user_inst.has_privilege("*", "*", "DROP") or
                        not user_inst.has_privilege("*", "*", "CREATE") or
                        not user_inst.has_privilege("*", "*", "INSERT") or
                        not user_inst.has_privilege("*", "*",
                                                    "REPLICATION SLAVE")):
                    errors.append((server.user, server.host, server.port,
                                   'SUPER, GRANT OPTION, REPLICATION SLAVE, '
                                   'SELECT, RELOAD, DROP, CREATE, INSERT'))

        # Disconnect if we connected to any candidates
        for slave in candidate_slaves:
            slave.disconnect()

        return errors

    def run_cmd_on_slaves(self, command, quiet=False):
        """Run a command on a list of slaves.

        This method will run one of the following slave commands.

          start - START SLAVE;
          stop  - STOP SLAVE;
          reset - STOP SLAVE; RESET SLAVE;

        command[in]        command to execute
        quiet[in]          If True, do not print messages
                           Default is False
        :param command:
        :param quiet:
        """

        assert (self.slaves is not None), \
            "No slaves specified or connections failed."

        self._report("# Performing %s on all slaves." %
                     command.upper())

        for slave_dict in self.slaves:
            hostport = "%s:%s" % (slave_dict['host'], slave_dict['port'])
            msg = "#   Executing %s on slave %s " % (command, hostport)
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if not slave or not slave.is_alive():
                message = "{0}WARN - cannot connect to slave".format(msg)
                self._report(message, logging.WARN)
            elif command == 'reset':
                if (self.master and
                        not slave.is_configured_for_master(self.master) and
                        not quiet):
                    message = ("{0}WARN - slave is not configured with this "
                               "master").format(msg)
                    self._report(message, logging.WARN)
                try:
                    slave.reset()
                except UtilError:
                    if not quiet:
                        message = "{0}WARN - slave failed to reset".format(msg)
                        self._report(message, logging.WARN)
                else:
                    if not quiet:
                        self._report("{0}Ok".format(msg))
            elif command == 'start':
                if (self.master and
                        not slave.is_configured_for_master(self.master) and
                        not quiet):
                    message = ("{0}WARN - slave is not configured with this "
                               "master").format(msg)
                    self._report(message, logging.WARN)
                try:
                    slave.start()
                except UtilError:
                    if not quiet:
                        message = "{0}WARN - slave failed to start".format(msg)
                        self._report(message, logging.WARN)
                else:
                    if not quiet:
                        self._report("{0}Ok".format(msg))
            elif command == 'stop':
                if (self.master and
                        not slave.is_configured_for_master(self.master) and
                        not quiet):
                    message = ("{0}WARN - slave is not configured with this "
                               "master").format(msg)
                    self._report(message, logging.WARN)
                elif not slave.is_connected() and not quiet:
                    message = ("{0}WARN - slave is not connected to "
                               "master").format(msg)
                    self._report(message, logging.WARN)
                try:
                    slave.stop()
                except UtilError:
                    if not quiet:
                        message = "{0}WARN - slave failed to stop".format(msg)
                        self._report(message, logging.WARN)
                else:
                    if not quiet:
                        self._report("{0}Ok".format(msg))

    def connect_candidate(self, candidate, master=True):
        """Parse and connect to the candidate

        This method parses the candidate string and returns a slave dictionary
        if master=False else returns a Master class instance.

        candidate[in]  candidate connection string
        master[in]     if True, make Master class instance

        Returns slave_dict or Master class instance
        """
        # Need instance of Master class for operation
        conn_dict = {
            'conn_info': candidate,
            'quiet': True,
            'verbose': self.verbose,
        }
        if master:
            m_candidate = Master(conn_dict)
            m_candidate.connect()
            return m_candidate
        else:
            s_candidate = Slave(conn_dict)
            s_candidate.connect()
            slave_dict = {
                'host': s_candidate.host,
                'port': s_candidate.port,
                'instance': s_candidate,
            }
            return slave_dict

    def switchover(self, candidate):
        """Perform switchover from master to candidate slave.

        This method switches the role of master to a candidate slave. The
        candidate is checked for viability before the switch is made.

        If the user specified --demote-master, the method will make the old
        master a slave of the candidate.

        candidate[in]  the connection information for the --candidate option

        Return bool - True = success, raises exception on error
        """

        # Need instance of Master class for operation
        m_candidate = self.connect_candidate(candidate)

        # Switchover needs to succeed and prerequisites must be met else abort.
        self._report("# Checking candidate slave prerequisites.")
        try:
            self._check_switchover_prerequisites(m_candidate)
        except UtilError, e:
            self._report("ERROR: %s" % e.errmsg, logging.ERROR)
            if not self.force:
                return

        # Check if the slaves are configured for the specified master
        self._report("# Checking slaves configuration to master.")
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # Skip not defined or alive slaves (Warning displayed elsewhere)
            if not slave or not slave.is_alive():
                continue

            if not slave.is_configured_for_master(self.master):
                # Slave not configured for master (i.e. not in topology)
                msg = ("Slave {0}:{1} is not configured with master {2}:{3}"
                       ".").format(slave_dict['host'], slave_dict['port'],
                                   self.master.host, self.master.port)
                print("# ERROR: {0}".format(msg))
                self._report(msg, logging.ERROR, False)
                if not self.force:
                    raise UtilRplError("{0} Note: If you want to ignore this "
                                       "issue, please use the utility with "
                                       "the --force option.".format(msg))

        # Check rpl-user definitions
        if self.verbose and self.rpl_user:
            if self.check_master_info_type("TABLE"):
                msg = ("# When the master_info_repository variable is set to"
                       " TABLE, the --rpl-user option is ignored and the"
                       " existing replication user values are retained.")
                self._report(msg, logging.INFO)
                self.rpl_user = None
            else:
                msg = ("# When the master_info_repository variable is set to"
                       " FILE, the --rpl-user option may be used only if the"
                       " user specified matches what is shown in the SLAVE"
                       " STATUS output unless the --force option is used.")
                self._report(msg, logging.INFO)

        user, passwd = self._get_rpl_user(m_candidate)
        if not passwd:
            passwd = ''

        if not self.check_master_info_type("TABLE"):
            slave_candidate = self._change_role(m_candidate, slave=True)
            rpl_master_user = slave_candidate.get_rpl_master_user()

            if not self.force:
                if (user != rpl_master_user):
                    msg = ("The replication user specified with --rpl-user "
                           "does not match the existing replication user.\n"
                           "Use the --force option to use the "
                           "replication user specified with --rpl-user.")
                    self._report("ERROR: %s" % msg, logging.ERROR)
                    return

                # Can't get rpl pass from remote master_repo=file
                # but it can get the current used hashed to be compared.
                slave_qry = slave_candidate.exec_query
                # Use the correct query for server version (changed for 5.7.6)
                if slave_candidate.check_version_compat(5, 7, 6):
                    query = _SELECT_RPL_USER_PASS_QUERY_5_7_6
                else:
                    query = _SELECT_RPL_USER_PASS_QUERY
                passwd_hash = slave_qry(query.format(user=user,
                                                     host=m_candidate.host))
                # if user does not exist passwd_hash will be an empty query.
                if passwd_hash:
                    passwd_hash = passwd_hash[0][3]
                else:
                    passwd_hash = ""
                if passwd == '':
                    msg = ("The specified replication user is using a "
                           "password (but none was specified).\n"
                           "Use the --force option to force the use of "
                           "the user specified with  --rpl-user and no "
                           "password.")
                else:
                    msg = ("The specified replication user is using a "
                           "different password that the one specified.\n"
                           "Use the --force option to force the use of "
                           "the user specified with  --rpl-user and new "
                           "password.")
                # If 5.7.6+, check by trying to connect
                if self.master.check_version_compat(5, 7, 6):
                    config = {
                        'user': user,
                        'passwd': passwd,
                        'host': m_candidate.host,
                        'port': m_candidate.port,
                    }
                    s_conn = Server({'conn_info': config})
                    try:
                        s_conn.connect()
                    except:
                        self._report("ERROR: %s" % msg, logging.ERROR)
                        return
                    else:
                        s_conn.disconnect()
                # else compare the hash fom --rpl-user.
                else:
                    rpl_master_pass = slave_qry("SELECT PASSWORD('%s');" %
                                                passwd)
                    rpl_master_pass = rpl_master_pass[0][0]
                    if rpl_master_pass != passwd_hash:
                        self._report("ERROR: %s" % msg, logging.ERROR)
                        return
            # Use the correct query for server (changed for 5.7.6).
            self.master.toggle_binlog("DISABLE")
            if self.master.check_version_compat(5, 7, 6):
                query = _UPDATE_RPL_USER_QUERY_5_7_6
                self.master.exec_query(query.format(user=user,
                                                    host=m_candidate.host,
                                                    passwd=passwd))
            else:
                query = _UPDATE_RPL_USER_QUERY
                self.master.exec_query(query.format(user=user, passwd=passwd))
            self.master.toggle_binlog("ENABLE")

        if self.verbose:
            self._report("# Creating replication user if it does not exist.")
        self.master.toggle_binlog("DISABLE")
        res = m_candidate.create_rpl_user(m_candidate.host,
                                          m_candidate.port,
                                          user, passwd, ssl=self.ssl)
        self.master.toggle_binlog("ENABLE")
        if not res[0]:
            print("# ERROR: {0}".format(res[1]))
            self._report(res[1], logging.CRITICAL, False)

        # Call exec_before script - display output if verbose on
        try:
            self.run_script(self.before_script, False,
                            [self.master.host, self.master.port,
                             m_candidate.host, m_candidate.port])
        except Exception as err:  # pylint: disable=W0703
            self._report("# Before script failed! {0}".format(err),
                         level=logging.ERROR)

        if self.verbose:
            self._report("# Blocking writes on master.")
        lock_options = {
            'locking': 'flush',
            'verbosity': 3 if self.verbose else self.verbosity,
            'silent': self.quiet,
            'rpl_mode': "master",
        }
        lock_ftwrl = Lock(self.master, [], lock_options)
        self.master.set_read_only(True)
        if self.verbose and not self.quiet:
            read_only = self.master.show_server_variable("READ_ONLY")
            self._report("# Read only is {0} for {1}:{2}."
                         "".format(read_only[0][1], self.master.host,
                                   self.master.port))

        # Wait for all slaves to catch up.
        gtid_enabled = self.master.supports_gtid() == "ON"
        if gtid_enabled:
            master_gtid = self.master.exec_query(_GTID_EXECUTED)
        self._report("# Waiting for slaves to catch up to old master.")
        for slave_dict in self.slaves:
            master_info = self.master.get_status()[0]
            slave = slave_dict['instance']
            # skip dead or zombie slaves, and print warning
            if not slave or not slave.is_alive():
                if self.verbose:
                    msg = ("Slave {0}:{1} skipped (not "
                           "reachable)").format(slave_dict['host'],
                                                slave_dict['port'])
                    print("# WARNING: {0}".format(msg))
                    self._report(msg, logging.WARNING, False)
                continue
            if gtid_enabled:
                print_query = self.verbose and not self.quiet
                res = slave.wait_for_slave_gtid(master_gtid, self.timeout,
                                                print_query)
            else:
                res = slave.wait_for_slave(master_info[0], master_info[1],
                                           self.timeout)
            if not res:
                msg = "Slave %s:%s did not catch up to the master." % \
                      (slave_dict['host'], slave_dict['port'])
                if not self.force:
                    self._report(msg, logging.CRITICAL)
                    raise UtilRplError(msg)
                else:
                    self._report("# %s" % msg)

        # Stop all slaves
        self._report("# Stopping slaves.")
        self.run_cmd_on_slaves("stop", not self.verbose)

        # Unblock master
        self.master.set_read_only(False)
        if self.verbose and not self.quiet:
            read_only = self.master.show_server_variable("READ_ONLY")
            self._report("# Read only is {0} for {1}:{2}."
                         "".format(read_only[0][1], self.master.host,
                                   self.master.port))
        lock_ftwrl.unlock()

        # Make master a slave (if specified)
        if self.options.get("demote", False):
            self._report("# Demoting old master to be a slave to the "
                         "new master.")

            slave = self._change_role(self.master)
            slave.stop()

            slave_dict = {
                'host': self.master.host,  # host name for slave
                'port': self.master.port,  # port for slave
                'instance': slave,         # Slave class instance
            }
            self.slaves.append(slave_dict)

        # Move candidate slave to master position in lists
        self.master_vals = m_candidate.get_connection_values()
        self.master = m_candidate

        # Remove slave from list of slaves
        self.remove_slave({'host': m_candidate.host,
                           'port': m_candidate.port,
                           'instance': m_candidate})

        # Make new master forget was an slave using slave methods
        s_candidate = self._change_role(m_candidate)
        s_candidate.reset_all()

        # Switch all slaves to new master
        self._report("# Switching slaves to new master.")
        new_master_info = m_candidate.get_status()[0]
        master_values = {
            'Master_Host': m_candidate.host,
            'Master_Port': m_candidate.port,
            'Master_User': user,
            'Master_Password': passwd,
            'Master_Log_File': new_master_info[0],
            'Read_Master_Log_Pos': new_master_info[1],
        }

        # Use the options SSL certificates if defined,
        # else use the master SSL certificates if defined.
        if self.ssl:
            master_values['Master_SSL_Allowed'] = 1
            if self.ssl_ca:
                master_values['Master_SSL_CA_File'] = self.ssl_ca
            if self.ssl_cert:
                master_values['Master_SSL_Cert'] = self.ssl_cert
            if self.ssl_key:
                master_values['Master_SSL_Key'] = self.ssl_key

        elif m_candidate.has_ssl:
            master_values['Master_SSL_Allowed'] = 1
            master_values['Master_SSL_CA_File'] = m_candidate.ssl_ca
            master_values['Master_SSL_Cert'] = m_candidate.ssl_cert
            master_values['Master_SSL_Key'] = m_candidate.ssl_key

        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                if self.verbose:
                    self._report("# Skipping CHANGE MASTER for {0}:{1} (not "
                                 "connected).".format(slave_dict['host'],
                                                      slave_dict['port']))
                continue
            if self.verbose:
                self._report("# Executing CHANGE MASTER on {0}:{1}"
                             ".".format(slave_dict['host'],
                                        slave_dict['port']))
            change_master = slave.make_change_master(False, master_values)
            if self.verbose:
                self._report("# {0}".format(change_master))
            slave.exec_query(change_master)

        # Start all slaves
        self._report("# Starting all slaves.")
        self.run_cmd_on_slaves("start", not self.verbose)

        # Call exec_after script - display output if verbose on
        try:
            self.run_script(self.after_script, False,
                            [self.master.host, self.master.port])
        except Exception as err:  # pylint: disable=W0703
            self._report("# After script failed! {0}".format(err),
                         level=logging.ERROR)

        # Check all slaves for status, errors
        self._report("# Checking slaves for errors.")
        if not self._check_all_slaves(self.master):
            return False

        self._report("# Switchover complete.")

        return True

    def _change_role(self, server, slave=True):
        """Reverse role of Master and Slave classes

        This method can be used to get a Slave instance from a Master instance
        or a Master instance from a Slave instance.

        server[in]     Server class instance
        slave[in]      if True, create Slave class instance
                       Default is True

        Return Slave or Master instance
        """
        conn_dict = {
            'conn_info': get_connection_dictionary(server),
            'verbose': self.verbose,
        }
        if slave and not isinstance(server, Slave):
            slave_conn = Slave(conn_dict)
            slave_conn.connect()
            return slave_conn
        if not slave and not isinstance(server, Master):
            master_conn = Master(conn_dict)
            master_conn.connect()
            return master_conn
        return server

    def find_best_slave(self, candidates=None, check_master=True,
                        strict=False):
        """Find the best slave

        This method checks each slave in the topology to determine if
        it is a viable slave for promotion. It returns the first slave
        that is determined to be eligible for promotion.

        The method uses the order of the slaves in the topology as
        specified by the slaves list to search for a best slave. If a
        candidate slave is provided, it is checked first.

        candidates[in]   list of candidate connection dictionaries
        check_master[in] if True, check that slave is connected to the master
                         Default is True
        strict[in]       if True, use only the candidate list for slave
                         election and fail if no candidates are viable.
                         Default = False

        Returns dictionary = (host, port, instance) for 'best' slave,
                             None = no candidate slaves found
        """
        msg = "None of the candidates was the best slave."
        for candidate in candidates:
            slave_dict = self.connect_candidate(candidate, False)
            slave = slave_dict['instance']
            # Ignore dead or offline slaves
            if slave is None or not slave.is_alive():
                continue
            slave_ok = self._check_candidate_eligibility(slave.host,
                                                         slave.port,
                                                         slave,
                                                         check_master)
            if slave_ok is not None and slave_ok[0]:
                return slave_dict
            else:
                self._report("# Candidate %s:%s does not meet the "
                             "requirements." % (slave.host, slave.port),
                             logging.WARN)

        # If strict is on and we have found no viable candidates, return None
        if strict:
            self._report("ERROR: %s" % msg, logging.ERROR)
            return None

        if candidates is not None and len(candidates) > 0:
            self._report("WARNING: %s" % msg, logging.WARN)

        for slave_dict in self.slaves:
            s_host = slave_dict['host']
            s_port = slave_dict['port']
            slave = slave_dict['instance']
            # Fix port
            if slave:
                if os.name == "posix" and slave.socket:
                    slave_dict['port'] = slave.port
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                continue
            # Check eligibility
            try:
                slave_ok = self._check_candidate_eligibility(s_host, s_port,
                                                             slave,
                                                             check_master)
                if slave_ok is not None and slave_ok[0]:
                    return slave_dict
            except UtilError, e:
                self._report("# Slave eliminated due to error: %s" % e.errmsg,
                             logging.WARN)
                # Slave gone away, skip it.

        return None

    def failover(self, candidates, strict=False, stop_on_error=False):
        """Perform failover to best slave in a GTID-enabled topology.

        This method performs a failover to one of the candidates specified. If
        no candidates are specified, the method will use the list of slaves to
        choose a candidate. In either case, priority is given to the server
        listed first that meets the prerequisites - a sanity check to ensure if
        the candidate's GTID_MODE matches the other slaves.

        In the event the candidates list is exhausted, it will use the slaves
        list to find a candidate. If no servers are viable, the method aborts.

        If the strict parameter is True, the search is limited to the
        candidates list.

        Once a candidate is selected, the candidate is prepared to become the
        new master by collecting any missing GTIDs by being made a slave to
        each of the other slaves.

        Once prepared, the before script is run to trigger applications,
        then all slaves are connected to the new master. Once complete,
        all slaves are started, the after script is run to trigger
        applications, and the slaves are checked for errors.

        candidates[in]     list of slave connection dictionary of candidate
        strict[in]         if True, use only the candidate list for slave
                           election and fail if no candidates are viable.
                           Default = False
        stop_on_error[in]  Define the default behavior of failover if errors
                           are found. By default: False (not stop on errors).

        Returns bool - True if successful,
                       raises exception if failure and forst is False
        """
        # Get best slave from list of candidates
        new_master_dict = self.find_best_slave(candidates, False, strict)
        if new_master_dict is None:
            msg = "No candidate found for failover."
            self._report(msg, logging.CRITICAL)
            raise UtilRplError(msg)

        new_master = new_master_dict['instance']
        # All servers must have GTIDs match candidate
        gtid_mode = new_master.supports_gtid()
        if gtid_mode != "ON":
            msg = "Failover requires all servers support " + \
                "global transaction ids and have GTID_MODE=ON"
            self._report(msg, logging.CRITICAL)
            raise UtilRplError(msg)

        for slave_dict in self.slaves:
            # Ignore dead or offline slaves
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                continue
            if slave.supports_gtid() != gtid_mode:
                msg = "Cannot perform failover unless all " + \
                      "slaves support GTIDs and GTID_MODE=ON"
                self._report(msg, logging.CRITICAL)
                raise UtilRplError(msg)

        # We must also ensure the new master and all remaining slaves
        # have the latest GTID support.
        new_master.check_gtid_version()
        for slave_dict in self.slaves:
            # Ignore dead or offline slaves
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                continue
            slave.check_gtid_version()

        host = new_master_dict['host']
        port = new_master_dict['port']
        # Use try block in case master class has gone away.
        try:
            old_host = self.master.host
            old_port = self.master.port
        except:
            old_host = "UNKNOWN"
            old_port = "UNKNOWN"

        self._report("# Candidate slave %s:%s will become the new master." %
                     (host, port))

        user, passwd = self._get_rpl_user(self._change_role(new_master))

        # Check slaves for errors that might result in an unstable topology
        self._report("# Checking slaves status (before failover).")
        self._check_slaves_status(stop_on_error)

        # Prepare candidate
        self._report("# Preparing candidate for failover.")
        self._prepare_candidate_for_failover(new_master, user, passwd)

        # Create replication user on candidate.
        self._report("# Creating replication user if it does not exist.")

        # Need Master class instance to check master and replication user
        self.master = self._change_role(new_master, False)
        res = self.master.create_rpl_user(host, port, user, passwd,
                                          ssl=self.ssl)
        if not res[0]:
            print("# ERROR: {0}".format(res[1]))
            self._report(res[1], logging.CRITICAL, False)

        # Call exec_before script - display output if verbose on
        try:
            self.run_script(self.before_script, False,
                            [old_host, old_port, host, port])
        except Exception as err:  # pylint: disable=W0703
            self._report("# Before script failed! {0}".format(err),
                         level=logging.ERROR)

        # Stop all slaves
        self._report("# Stopping slaves.")
        self.run_cmd_on_slaves("stop", not self.verbose)

        # Take the new master out of the slaves list.
        self.remove_slave(new_master_dict)

        self._report("# Switching slaves to new master.")
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                continue
            slave.switch_master(self.master, user, passwd, False, None, None,
                                self.verbose and not self.quiet)

        # Clean previous replication settings on the new master.
        self._report("# Disconnecting new master as slave.")
        # Make sure the new master is not acting as a slave (STOP SLAVE).
        self.master.exec_query("STOP SLAVE")
        # Execute RESET SLAVE ALL on the new master.
        if self.verbose and not self.quiet:
            self._report("# Execute on {0}:{1}: "
                         "RESET SLAVE ALL".format(self.master.host,
                                                  self.master.port))
        self.master.exec_query("RESET SLAVE ALL")

        # Starting all slaves
        self._report("# Starting slaves.")
        self.run_cmd_on_slaves("start", not self.verbose)

        # Call exec_after script - display output if verbose on
        try:
            self.run_script(self.after_script, False,
                            [old_host, old_port, host, port])
        except Exception as err:  # pylint: disable=W0703
            self._report("# After script failed! {0}".format(err),
                         level=logging.ERROR)

        # Check slaves for errors
        self._report("# Checking slaves for errors.")
        if not self._check_all_slaves(self.master):
            return False

        self._report("# Failover complete.")

        return True

    def get_servers_with_different_sql_mode(self, look_for):
        """Returns a tuple of two list with all the server instances in the
        Topology. The first list is the group of server that have the sql_mode
        given in look_for, the second list is the group of server that does not
        have this sql_mode.

        look_for[in]    The sql_mode to search for.

        Returns tuple of Lists - the group of servers instances that have the
            SQL mode given in look_for, and a group which sql_mode
            differs from the look_for or an empty list.
        """
        # Fill a dict with keys from the SQL modes names and as items the
        # servers with the same sql_mode.
        look_for_list = []
        inconsistent_list = []

        # Get Master sql_mode if given and clasify it.
        if self.master is not None:
            master_sql_mode = self.master.select_variable("SQL_MODE")
            if look_for in master_sql_mode:
                look_for_list.append(self.master)
            else:
                inconsistent_list.append(self.master)

        # Fill the lists with the slaves deppending of his sql_mode.
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            slave_sql_mode = slave.select_variable("SQL_MODE")
            if look_for in slave_sql_mode:
                look_for_list.append(slave)
            else:
                inconsistent_list.append(slave)

        return look_for_list, inconsistent_list
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains an abstraction of a topolgy map object used to discover
slaves and down-stream replicants for mapping topologies.
"""

import getpass
import os


_START_PORT = 3306


class TopologyMap(object):
    """The TopologyMap class can be used to connect to a running MySQL server
    and discover its slaves. Setting the option "recurse" permits the
    class to discover a replication topology by finding the slaves for each
    slave for the first master requested.

    To generate a topology map, the caller must call the
    generate_topology_map() method to build the topology. This is left as a
    separate state because it can be a lengthy process thereby too long for a
    constructor method.

    The class also includes methods for printing a graph of the topology
    as well as returning a list of master, slave tuples reporting the
    host name and port for each.
    """

    def __init__(self, seed_server, options=None):
        """Constructor

        seed_server[in]    Master (seed) server connection dictionary
        options[in]        options for controlling behavior:
          recurse          If True, check each slave found for add'l slaves
                           Default = False
          prompt_user      If True, prompt user if slave connection fails with
                           master connection parameters
                           Default = False
          quiet            if True, print only the data
                           Default = False
          width            width of report
                           Default = 75
          num_retries      Number of times to retry a failed connection attempt
                           Default = 0
        """
        if options is None:
            options = {}
        self.recurse = options.get("recurse", False)
        self.quiet = options.get("quiet", False)
        self.prompt_user = options.get("prompt", False)
        self.num_retries = options.get("num_retries", 0)
        self.socket_path = options.get("socket_path", None)
        self.verbose = options.get('verbosity', 0) > 0
        self.seed_server = seed_server
        self.topology = []
        self.options = options

    def _connect(self, conn):
        """Find the attached slaves for a list of server connections.

        This method connects to each server in the list and retrieves its
        slaves.
        It can be called recursively if the recurse parameter is True.

        conn[in]           Connection dictionary used to connect to server

        Returns tuple - master Server class instance, master:host string
        """
        conn_options = {
            'quiet': self.quiet,
            'src_name': "master",
            'dest_name': None,
            'version': "5.0.0",
            'unique': True,
            'verbose': self.verbose,
        }

        certs_paths = {}
        if 'ssl_ca' in dir(conn) and conn.ssl_ca is not None:
            certs_paths['ssl_ca'] = conn.ssl_ca
        if 'ssl_cert' in dir(conn) and conn.ssl_cert is not None:
            certs_paths['ssl_cert'] = conn.ssl_cert
        if 'ssl_key' in dir(conn) and conn.ssl_key is not None:
            certs_paths['ssl_key'] = conn.ssl_key

        conn_options.update(certs_paths)

        master_info = "{0}:{1}".format(conn['host'], conn['port'])
        master = None

        # Increment num_retries if not set when --prompt is used
        if self.prompt_user and self.num_retries == 0:
            self.num_retries += 1

        # Attempt to connect to the server given the retry limit
        for i in range(0, self.num_retries + 1):
            try:
                servers = connect_servers(conn, None, conn_options)
                master = servers[0]
                break
            except UtilError, e:
                print "FAILED.\n"
                if i < self.num_retries and self.prompt_user:
                    print "Connection to %s has failed.\n" % master_info + \
                        "Please enter the following information " + \
                        "to connect to this server."
                    conn['user'] = raw_input("User name: ")
                    conn['passwd'] = getpass.getpass("Password: ")
                else:
                    # retries expired - re-raise error if still failing
                    raise UtilError(e.errmsg)

        # Correct port for socket connections
        if os.name == 'posix' and master.socket:
            master_info = "{0}:{1}".format(conn['host'], master.port)

        return (master, master_info)

    @staticmethod
    def _check_permissions(server, priv):
        """Check to see if user has permissions to execute.

        server[in]     Server class instance
        priv[in]       privilege to check

        Returns True if permissions available, raises exception if not
        """
        # Check user permissions
        user_pass_host = server.user
        if server.passwd is not None and len(server.passwd) > 0:
            user_pass_host += ":" + server.passwd
        user_pass_host += "@" + server.host
        user = User(server, user_pass_host, False)
        if not user.has_privilege("*", "*", priv):
            raise UtilError("Not enough permissions. The user must have the "
                            "%s privilege." % priv)

    def _get_slaves(self, max_depth, seed_conn=None, masters_found=None):
        """Find the attached slaves for a list of server connections.

        This method connects to each server in the list and retrieves its
        slaves. It can be called recursively if the recurse option is True.

        max_depth[in]       Maximum depth of recursive search
        seed_conn[in]       Current master connection dictionary. Initially,
                            this is the seed server (original master defined
                            in constructor)
        masters_found[in]   a list of all servers in master roles - used to
                            detect a circular replication topology. Initially,
                            this is an empty list as the master detection must
                            occur as the topology is traversed.

        Returns list - list of slaves connected to each server in list
        """
        if not masters_found:
            masters_found = []
        topology = []
        if seed_conn is None:
            seed_conn = self.seed_server

        master, master_info = self._connect(seed_conn)
        if master is None:
            return []

        # Check user permissions
        self._check_permissions(master, "REPLICATION SLAVE")

        # Save the master for circular replication identification
        masters_found.append(master_info)

        if not self.quiet:
            print "# Finding slaves for master: %s" % master_info

        # See if the user wants us to discover slaves.
        discover = self.options.get("discover", None)
        if discover is None:
            return

        # Get user and password (supports login-paths)
        try:
            user, password = parse_user_password(discover,
                                                 options=self.options)
        except FormatError:
            raise UtilError(USER_PASSWORD_FORMAT.format("--discover-slaves"))

        # Get replication topology
        slaves = master.get_slaves(user, password)
        slave_list = []
        depth = 0
        if len(slaves) > 0:
            for slave in slaves:
                if slave.find(":") > 0:
                    host, port = slave.split(":", 1)
                else:
                    host = slave
                    port = _START_PORT  # Use the default
                slave_conn = self.seed_server.copy()
                slave_conn['host'] = host
                slave_conn['port'] = port

                io_sql_running = None
                # If verbose then get slave threads (IO and SQL) status
                if self.verbose:
                    # Create slave instance
                    conn_dict = {
                        'conn_info': {'user': user, 'passwd': password,
                                      'host': host, 'port': port,
                                      'socket': None},
                        'role': slave,
                        'verbose': self.verbose
                    }
                    slave_obj = Slave(conn_dict)
                    # Get IO and SQL status
                    try:
                        slave_obj.connect()
                        thread_status = slave_obj.get_thread_status()
                        if thread_status:
                            io_sql_running = (thread_status[1],
                                              thread_status[2])
                    except UtilError:
                        # Connection error
                        io_sql_running = ('ERROR', 'ERROR')

                # Now check for circular replication topology - do not recurse
                # if slave is also a master.
                if self.recurse and slave not in masters_found and \
                   ((max_depth is None) or (depth < max_depth)):
                    new_list = self._get_slaves(max_depth, slave_conn,
                                                masters_found)
                    if new_list == []:
                        slave_list.append((slave, [], io_sql_running))
                    else:
                        # Add IO and SQL state to slave from recursion
                        if io_sql_running:
                            new_list = [(new_list[0][0], new_list[0][1],
                                         io_sql_running)]
                        slave_list.append(new_list)
                    depth += 1
                else:
                    slave_list.append((slave, [], io_sql_running))
        topology.append((master_info, slave_list))

        return topology

    def generate_topology_map(self, max_depth):
        """Find the attached slaves for a list of server connections.

        This method generates the topology for the seed server specified at
        instantiation.

        max_depth[in]       Maximum depth of recursive search
        """
        self.topology = self._get_slaves(max_depth)

    def depth(self):
        """Return depth of the topology tree.

        Returns int - depth of topology tree.
        """
        return len(self.topology)

    def slaves_found(self):
        """Check to see if any slaves were found.

        Returns bool - True if slaves found, False if no slaves.
        """
        return not (len(self.topology) and self.topology[0][1] == [])

    def print_graph(self, topology_list=None, masters_found=None,
                    level=0, preamble=""):
        """Prints a graph of the topology map to standard output.

        This method traverses a list of the topology and prints a graph. The
        method is designed to be recursive traversing the list to print the
        slaves for each master in the topology. It will also detect a circular
        replication segment and indicate it on the graph.

        topology_list[in]   a list in the form (master, slave) of server
        masters_found[in]   a list of all servers in master roles - used to
                            detect a circular replication topology. Initially,
                            this is an empty list as the master detection must
                            occur as the topology is traversed.
        level[in]           the level of indentation - increases with each
                            set of slaves found in topology
        preamble[in]        prefix calculated during recursion to indent text
        """
        if not topology_list:
            topology_list = []
        if not masters_found:
            masters_found = []
        # if first iteration, use the topology list generated earlier
        if topology_list == []:
            if self.topology == []:
                # topology not generated yet
                raise UtilError("You must first generate the topology.")
            topology_list = self.topology

        # Detect if we are looking at a sublist or not. Get sublist.
        if len(topology_list) == 1:
            topology_list = topology_list[0]
        master = topology_list[0]

        # Save the master for circular replication identification
        masters_found.append(master)

        # For each slave, print the graph link
        slaves = topology_list[1]
        stop = len(slaves)
        if stop > 0:
            # Level 0 is always the first master in the topology.
            if level == 0:
                print("{0} (MASTER)".format(master))
            for i in range(0, stop):
                if len(slaves[i]) == 1:
                    slave = slaves[i][0]
                else:
                    slave = slaves[i]
                new_preamble = "{0}   ".format(preamble)
                print("{0}|".format(new_preamble))
                role = "(SLAVE"
                if slave[1] != [] or slave[0] in masters_found:
                    role = "{0} + MASTER".format(role)
                role = "{0})".format(role)

                # Print threads (IO and SQL) status if verbose
                t_status = ''
                if self.verbose:
                    try:
                        t_status = " [IO: {0}, SQL: {1}]".format(slave[2][0],
                                                                 slave[2][1])
                    except IndexError:
                        # This should never happened... (done to avoid crash)
                        t_status = " [IO: ??, SQL: ??]"

                print "{0}+--- {1}{2}".format(new_preamble, slave[0],
                                              t_status),

                if (slave[0] in masters_found):
                    print "<-->",
                else:
                    print "-",
                print role

                if slave[1] != []:
                    if i < stop - 1:
                        new_preamble = "{0}|".format(new_preamble)
                    else:
                        new_preamble = "{0} ".format(new_preamble)
                    self.print_graph(slave, masters_found,
                                     level + 1, new_preamble)

    def _get_row(self, topology_list):
        """Get a row (master, slave) for the topology map.

        topology_list[in]  The topology list

        Returns tuple - a row (master, slave)
        """
        new_row = []
        if len(topology_list) == 1:
            topology_list = topology_list[0]
        master = topology_list[0]
        slaves = topology_list[1]
        for slave in slaves:
            if len(slave) == 1:
                new_slave = slave[0]
            else:
                new_slave = slave
            new_row.append((master, new_slave[0]))
            new_row.extend(self._get_row(new_slave))
        return new_row

    def get_topology_map(self):
        """Get a list of the topology map suitable for export

        Returns list - a list of masters and their slaves in two columns
        """
        # Get a row for the list
        # make a list from the topology
        master_slaves = [self._get_row(row) for row in self.topology]
        return master_slaves[0]
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains and abstraction of a MySQL user object.
"""

import re

from collections import namedtuple, defaultdict


def change_user_privileges(server, user_name, user_passwd, host,
                           grant_list=None, revoke_list=None,
                           disable_binlog=False, create_user=False):
    """ Change the privileges of a new or existing user.

    This method GRANT or REVOKE privileges to a new user (creating it) or
    existing user.

    server[in]          MySQL server instances to apply changes
                        (from mysql.utilities.common.server.Server).
    user_name[in]       user name to apply changes.
    user_passwd[in]     user's password.
    host[in]            host name associated to the user account.
    grant_list[in]      List of privileges to GRANT.
    revoke_list[in]     List of privileges to REVOKE.
    disable_binlog[in]  Boolean value to determine if the binary logging
                        will be disabled to perform this operation (and
                        re-enabled at the end). By default: False (do not
                        disable binary logging).
    create_user[in]     Boolean value to determine if the user will be
                        created before changing its privileges. By default:
                        False (do no create user).
    """
    if disable_binlog:
        server.exec_query("SET SQL_LOG_BIN=0")
    if create_user:
        server.exec_query("CREATE USER '{0}'@'{1}' IDENTIFIED BY "
                          "'{2}'".format(user_name, host, user_passwd))
    if grant_list:
        grants_str = ", ".join(grant_list)
        server.exec_query("GRANT {0} ON *.* TO '{1}'@'{2}' IDENTIFIED BY "
                          "'{3}'".format(grants_str, user_name, host,
                                         user_passwd))
    if revoke_list:
        revoke_str = ", ".join(revoke_list)
        server.exec_query("REVOKE {0} ON *.* FROM '{1}'@'{2}'"
                          "".format(revoke_str, user_name, host))
    if disable_binlog:
        server.exec_query("SET SQL_LOG_BIN=1")


def parse_user_host(user_name):
    """Parse user, passwd, host, port from user:passwd@host

    user_name[in]      MySQL user string (user:passwd@host)

    returns - tuple - user, passwd, host
    """
    # Check for anonymous user. If not, continue.
    if user_name == "''@'%'":
        return ('', None, '%')
    no_ticks = user_name.replace("'", "")
    try:
        conn_values = parse_connection(no_ticks)
    except FormatError:
        raise UtilError("Cannot parse user:pass@host : %s." %
                        no_ticks)
    return (conn_values['user'], conn_values['passwd'], conn_values['host'])


def grant_proxy_ssl_privileges(server, user, passw, at='localhost',
                               privs="ALL PRIVILEGES", grant_opt=True,
                               ssl=True, grant_proxy=True, proxy_user='root',
                               proxy_host='localhost'):
    """Grant privileges to an user in a server with GRANT OPTION or/and
    REQUIRE SSL if required.

    server[in]         Server to execute the grant query at.
    user_name[in]      New user name.
    passw[in]          password of the new user.
    at[in]             Used in GRANT "TO '{0}'@'{1}'".format(user, at),
                       (default localhost)
    grant_opt[in]      if True, it will grant with GRANT OPTION (default True).
    ssl[in]            if True, it will set REQUIRE SSL (default True).
    grant_proxy[in]    if True, it will grant GRANT PROXY (default True).
    proxy_user[in]     username for the proxied account (default: root)
    proxy_host[in]     hostname for the proxied account (default: localhost)

    Note: Raises UtilError on any Error.
    """

    grant_parts = [
        "GRANT", privs,
        "ON *.*",
        "TO '{0}'@'{1}'".format(user, at),
        "IDENTIFIED BY '{0}'".format(passw) if passw else "",
        "REQUIRE SSL" if ssl else "",
        "WITH GRANT OPTION" if grant_opt else ""
    ]

    try:
        server.exec_query(" ".join(grant_parts))
    except UtilDBError as err:
        raise UtilError("Cannot create new user {0} at {1}:{2} reason:"
                        "{3}".format(user, server.host, server.port,
                                     err.errmsg))

    if grant_proxy:
        grant = ("GRANT PROXY ON '{0}'@'{1}' "
                 "TO '{2}'@'{3}' "
                 "WITH GRANT OPTION").format(proxy_user, proxy_host, user, at)
        try:
            server.exec_query(grant)
        except UtilDBError as err:
            raise UtilError("Cannot grant proxy to user {0} at {1}:{2} "
                            "reason:{3}".format(user, server.host,
                                                server.port, err.errmsg))


def check_privileges(server, operation, privileges, description,
                     verbosity=0, reporter=None):
    """Check required privileges.

    This method check if the used user possess the required privileges to
    execute a statement or operation.
    An exception is thrown if the user doesn't have enough privileges.

    server[in]        Server instance to check.
    operation[in]     The name of tha task that requires the privileges,
                      used in the error message if an exception is thrown.
    privileges[in]    List of the required privileges.
    description[in]   Description of the operation requiring the User's
                      privileges, used in the message if verbosity if given.
    verbosity[in]     Verbosity.
    reporter[in]      A method to invoke with messages and warnings
                      (by default print).
    """
    # print message with the given reporter.
    if reporter is None and verbosity > 0:
        print("# Checking user permission to {0}...\n"
              "#".format(description))
    elif reporter is not None and verbosity > 0:
        reporter("# Checking user permission to {0}...\n"
                 "#".format(description))

    # Check privileges
    user_obj = User(server, "{0}@{1}".format(server.user, server.host))
    need_privileges = []
    for privilege in privileges:
        if not user_obj.has_privilege('*', '*', privilege):
            need_privileges.append(privilege)

    if len(need_privileges) > 0:
        if len(need_privileges) > 1:
            privileges_needed = "{0} and {1}".format(
                ", ".join(need_privileges[:-1]),
                need_privileges[-1]
            )
        else:
            privileges_needed = need_privileges[0]
        raise UtilError(ERROR_USER_WITHOUT_PRIVILEGES.format(
            user=server.user, host=server.host, port=server.port,
            operation=operation, req_privileges=privileges_needed
        ))


class User(object):
    """
    The User class can be used to clone the user and its grants to another
    user with the following utilities:

        - Parsing user@host:passwd strings
        - Create, Drop user
        - Check to see if user exists
        - Retrieving and printing grants for user
    """

    def __init__(self, server1, user, verbosity=0):
        """Constructor

        server1[in]        Server class
        user[in]           MySQL user credentials string (user@host:passwd)
        verbose[in]        print extra data during operations (optional)
                           default value = False
        """

        self.server1 = server1
        if server1.db_conn:
            self.sql_mode = self.server1.select_variable("SQL_MODE")
        else:
            self.sql_mode = ""
        self.user, self.passwd, self.host = parse_user_host(user)
        self.verbosity = verbosity
        self.current_user = None
        self.grant_dict = None
        self.global_grant_dict = None
        self.grant_list = None
        self.global_grant_list = None
        self.query_options = {
            'fetch': False
        }

    def create(self, new_user=None, authentication=None):
        """Create the user

        Attempts to create the user. If the operation fails, an error is
        generated and printed.

        new_user[in]       MySQL user string (user@host:passwd)
                           (optional) If omitted, operation is performed
                           on the class instance user name.
        authentication[in] Special authentication clause for non-native
                           authentication plugins
        """
        auth_str = "SELECT * FROM INFORMATION_SCHEMA.PLUGINS WHERE " \
                   "PLUGIN_NAME = '{0}' AND PLUGIN_STATUS = 'ACTIVE';"
        query_str = "CREATE USER "
        user, passwd, host = None, None, None
        if new_user:
            user, passwd, host = parse_user_host(new_user)
            user_host_str = "'{0}'@'{1}' ".format(user, host)
        else:
            user_host_str = "'{0}'@'{1}' ".format(self.user, self.host)
            passwd = self.passwd
        query_str += user_host_str

        if passwd and authentication:
            print("WARNING: using a password and an authentication plugin is "
                  "not permited. The password will be used instead of the "
                  "authentication plugin.")
        if passwd:
            query_str += "IDENTIFIED BY '{0}'".format(passwd)
        elif authentication:
            # need to validate authentication plugin
            res = self.server1.exec_query(auth_str.format(authentication))
            if (res is None) or (res == []):
                raise UtilDBError("Plugin {0} not loaded or not active. "
                                  "Cannot create user.".format(authentication))
            query_str += "IDENTIFIED WITH '{0}'".format(authentication)
        if self.verbosity > 0:
            print query_str

        self.server1.exec_query(query_str, self.query_options)

    def drop(self, new_user=None):
        """Drop user from the server

        Attempts to drop the user. If the operation fails, an error is
        generated and printed.

        new_user[in]       MySQL user string (user@host:passwd)
                           (optional) If omitted, operation is performed
                           on the class instance user name.
        """
        query_str = "DROP USER "
        if new_user:
            user, _, host = parse_user_host(new_user)
            query_str += "'%s'@'%s' " % (user, host)
        else:
            query_str += "'%s'@'%s' " % (self.user, self.host)

        if self.verbosity > 0:
            print query_str

        try:
            self.server1.exec_query(query_str, self.query_options)
        except UtilError:
            return False
        return True

    def exists(self, user_name=None):
        """Check to see if the user exists

        user_name[in]      MySQL user string (user@host:passwd)
                           (optional) If omitted, operation is performed
                           on the class instance user name.

        return True = user exists, False = user does not exist
        """

        user, host, _ = self.user, self.host, self.passwd
        if user_name:
            user, _, host = parse_user_host(user_name)

        res = self.server1.exec_query("SELECT * FROM mysql.user "
                                      "WHERE user = %s and host = %s",
                                      {'params': (user, host)})

        return (res is not None and len(res) >= 1)

    @staticmethod
    def _get_grants_as_dict(grant_list, verbosity=0, sql_mode=''):
        """Transforms list of grant string statements into a dictionary.

        grant_list[in]    List of grant strings as returned from the server

        Returns a default_dict with the grant information
        """
        grant_dict = defaultdict(lambda: defaultdict(set))
        for grant in grant_list:
            grant_tpl = User._parse_grant_statement(grant[0], sql_mode)
            # Ignore PROXY privilege, it is not yet supported
            if verbosity > 0:
                if 'PROXY' in grant_tpl:
                    print("#WARNING: PROXY privilege will be ignored.")
            grant_tpl.privileges.discard('PROXY')
            if grant_tpl.privileges:
                grant_dict[grant_tpl.db][grant_tpl.object].update(
                    grant_tpl.privileges)
        return grant_dict

    def get_grants(self, globals_privs=False, as_dict=False, refresh=False):
        """Retrieve the grants for the current user

        globals_privs[in]     Include global privileges in clone (i.e. user@%)
        as_dict[in]           If True, instead of a list of plain grant
                              strings, return a dictionary with the grants.
        refresh[in]           If True, reads grant privileges directly from the
                              server and updates cached values, otherwise uses
                              the cached values.

        returns result set or None if no grants defined
        """

        # only read values from server if needed
        if refresh or not self.grant_list or not self.global_grant_list:
            # Get the users' connection user@host if not retrieved
            if self.current_user is None:
                res = self.server1.exec_query("SELECT CURRENT_USER()")
                parts = res[0][0].split('@')
                # If we're connected as some other user, use the user@host
                # defined at instantiation
                if parts[0] != self.user:
                    host = clean_IPv6(self.host)
                    self.current_user = "'%s'@'%s'" % (self.user, host)
                else:
                    self.current_user = "'%s'@'%s'" % (parts[0], parts[1])
            grants = []
            try:
                res = self.server1.exec_query("SHOW GRANTS FOR "
                                              "{0}".format(self.current_user))
                for grant in res:
                    grants.append(grant)
            except UtilDBError:
                pass  # Error here is ok - no grants found.

            # Cache user grants
            self.grant_list = grants[:]
            self.grant_dict = User._get_grants_as_dict(self.grant_list,
                                                       self.verbosity,
                                                       self.sql_mode)
            # If current user is already using global host wildcard '%', there
            # is no need to run the show grants again.
            if globals_privs:
                if self.host != '%':
                    try:
                        res = self.server1.exec_query(
                            "SHOW GRANTS FOR '{0}'{1}".format(self.user,
                                                              "@'%'"))
                        for grant in res:
                            grants.append(grant)
                        self.global_grant_list = grants[:]
                        self.global_grant_dict = User._get_grants_as_dict(
                            self.global_grant_list, self.verbosity)
                    except UtilDBError:
                        # User has no global privs, return the just the ones
                        # for current host
                        self.global_grant_list = self.grant_list
                        self.global_grant_dict = self.grant_dict
                else:
                    # if host is % then we already have the global privs
                    self.global_grant_list = self.grant_list
                    self.global_grant_dict = self.grant_dict

        if globals_privs:
            if as_dict:
                return self.global_grant_dict
            else:
                return self.global_grant_list
        else:
            if as_dict:
                return self.grant_dict
            else:
                return self.grant_list

    def get_grants_for_object(self, qualified_obj_name, obj_type_str,
                              global_privs=False):
        """ Retrieves the list of grants that the current user has that that
         have effect over a given object.

        qualified_obj_name[in]   String with the qualified name of the object.
        obj_type_str[in]         String with the type of the object that we are
                                 working with, must be one of 'ROUTINE',
                                 'TABLE' or 'DATABASE'.
        global_privs[in]         If True, the wildcard'%' host privileges are
                                 also taken into account


        This method takes the MySQL privilege hierarchy into account, e.g,
        if the qualified object is a table, it returns all the grant
        statements for this user regarding that table, as well as the grant
        statements for this user regarding the db where the table is at and
        finally any global grants that the user might have.

        Returns a list of strings with the grant statements.
        """

        grant_stm_lst = self.get_grants(global_privs)
        m_objs = parse_object_name(qualified_obj_name, self.sql_mode)
        grants = []
        if not m_objs:
            raise UtilError("Cannot parse the specified qualified name "
                            "'{0}'".format(qualified_obj_name))
        else:
            db_name, obj_name = m_objs
            # Quote database and object name if necessary
            if not is_quoted_with_backticks(db_name, self.sql_mode):
                db_name = quote_with_backticks(db_name, self.sql_mode)
            if obj_name and obj_name != '*':
                if not is_quoted_with_backticks(obj_name, self.sql_mode):
                    obj_name = quote_with_backticks(obj_name, self.sql_mode)

            # For each grant statement look for the ones that apply to this
            # user and object
            for grant_stm in grant_stm_lst:
                grant_tpl = self._parse_grant_statement(grant_stm[0],
                                                        self.sql_mode)
                if grant_tpl:
                    # Check if any of the privileges applies to this object
                    # and if it does then check if it inherited from this
                    # statement
                    if filter_grants(grant_tpl.privileges, obj_type_str):
                        # Add global grants
                        if grant_tpl.db == '*':
                            grants.append(grant_stm[0])
                            continue
                        # Add database level grants
                        if grant_tpl.db == db_name and grant_tpl.object == '*':
                            grants.append(grant_stm[0])
                            continue
                        # If it is an object, add existing object level grants
                        # as well.
                        if obj_name:
                            if (grant_tpl.db == db_name and
                                    grant_tpl.object == obj_name):
                                grants.append(grant_stm[0])

        return grants

    def has_privilege(self, db, obj, access, allow_skip_grant_tables=True,
                      globals_privs=True):
        """Check to see user has a specific access to a db.object.

        db[in]             Name of database
        obj[in]            Name of object
        access[in]         MySQL privilege to check (e.g. SELECT, SUPER, DROP)
        allow_skip_grant_tables[in]  If True, allow silent failure for
                           cases where the server is started with
                           --skip-grant-tables. Default=True
        globals_privs[in]  Include global privileges in clone (i.e. user@%)
                           Default is True

        Returns True if user has access, False if not
        """
        grants_enabled = self.server1.grant_tables_enabled()
        # If grants are disabled and it is Ok to allow skipped grant tables,
        # return True - privileges disabled so user can do anything.
        if allow_skip_grant_tables and not grants_enabled:
            return True
        # Convert privilege to upper cases.
        access = access.upper()

        # Get grant dictionary
        grant_dict = self.get_grants(globals_privs=globals_privs, as_dict=True)

        # If self has all privileges for all databases, no need to check,
        # simply return True
        if ("ALL PRIVILEGES" in grant_dict['*']['*'] and
                "GRANT OPTION" in grant_dict['*']['*']):
            return True

        # Quote db and obj with backticks if necessary
        if not is_quoted_with_backticks(db, self.sql_mode) and db != '*':
            db = quote_with_backticks(db, self.sql_mode)

        if not is_quoted_with_backticks(obj, self.sql_mode) and obj != '*':
            obj = quote_with_backticks(obj, self.sql_mode)

        # USAGE privilege is the same as no privileges,
        # so everyone has it.
        if access == "USAGE":
            return True
        # Even if we have ALL PRIVILEGES grant, we might not have WITH GRANT
        # OPTION privilege.
        # Check server wide grants.
        elif (access in grant_dict['*']['*'] or
              "ALL PRIVILEGES" in grant_dict['*']['*'] and
              access != "GRANT OPTION"):
            return True
        # Check database level grants.
        elif (access in grant_dict[db]['*'] or
              "ALL PRIVILEGES" in grant_dict[db]['*'] and
              access != "GRANT OPTION"):
            return True
        # Check object level grants.
        elif (access in grant_dict[db][obj] or
              "ALL PRIVILEGES" in grant_dict[db][obj] and
              access != "GRANT OPTION"):
            return True
        else:
            return False

    def contains_user_privileges(self, user, plus_grant_option=False):
        """Checks if privileges of given user are a subset of self's privileges

        user[in]               instance of the user class
        plus_grant_option[in]  if True, checks if besides the all the other
                               privileges, self has also the GRANT OPTION
                               in all of the bd, tables in which the user
                               passed as argument has privileges. Required for
                               instance if we will be using self to clone the
                               user.
        return_missing[in]     if True, return a set with the missing grants
                               instead of simply a boolean value.

        Returns True if the grants of the user passed as argument
        are a subset of the grants of self, otherwise returns False.
        """
        user_grants = user.get_grants(as_dict=True)

        # If we are cloning User1, using User2, then User2 needs
        # the GRANT OPTION privilege in each of the db,table where
        # User1 has privileges.
        if plus_grant_option:
            for db in user_grants:
                for table in user_grants[db]:
                    priv_set = user_grants[db][table]
                    # Ignore empty grant sets that might exist as a
                    # consequence of consulting the defaultdict.
                    if priv_set:
                        # Ignore USAGE grant as it means no privileges.
                        if (len(priv_set) == 1 and
                                "USAGE" in priv_set):
                            continue
                        else:
                            priv_set.add('GRANT OPTION')

        for db in user_grants:
            for table in user_grants[db]:
                priv_set = user_grants[db][table]
                for priv in priv_set:
                    if self.has_privilege(db, table, priv):
                        continue
                    else:
                        return False
        return True

    def missing_user_privileges(self, user, plus_grant_option=False):
        """Checks if privileges of given user are a subset of self's privileges

        user[in]               instance of the user class
        plus_grant_option[in]  if True, checks if besides the all the other
                               privileges, self has also the GRANT OPTION
                               in all of the bd, tables in which the user
                               passed as argument has privileges. Required for
                               instance if we will be using self to clone the
                               user.
        return_missing[in]     if True, return a set with the missing grants
                               instead of simply a boolean value.

        Returns empty set if the grants of the user passed as argument
        are a subset of the grants of self, otherwise a set with the missing
        privileges from self.
        """
        user_grants = user.get_grants(as_dict=True)
        missing_grants = set()

        # If we are cloning User1, using User2, then User2 needs
        # the GRANT OPTION privilege in each of the db,table where
        # User1 has privileges.
        if plus_grant_option:
            for db in user_grants:
                for table in user_grants[db]:
                    priv_set = user_grants[db][table]
                    # Ignore empty grant sets that might exist as a
                    # consequence of consulting the defaultdict.
                    if priv_set:
                        # Ignore USAGE grant as it means no privileges.
                        if (len(priv_set) == 1 and
                                "USAGE" in priv_set):
                            continue
                        else:
                            priv_set.add('GRANT OPTION')

        for db in user_grants:
            for table in user_grants[db]:
                priv_set = user_grants[db][table]
                for priv in priv_set:
                    if self.has_privilege(db, table, priv):
                        continue
                    else:
                        missing_grants.add((priv, db, table))

        return missing_grants

    def print_grants(self):
        """Display grants for the current user"""

        res = self.get_grants(True)
        for grant_tuple in res:
            print grant_tuple[0]

    def _get_authentication(self):
        """ Return authentication string """
        res = self.server1.exec_query("SELECT plugin FROM mysql.user "
                                      "WHERE user='{0}' and host='{1}'"
                                      "".format(self.user, self.host))
        if res == [] or res[0][0] == 'mysql_native_password':
            return None
        return res[0][0]

    def clone(self, new_user, destination=None, globals_privs=False):
        """Clone the current user to the new user

        Operation will create the new user account copying all of the
        grants for the current user to the new user. If operation fails,
        an error message is generated and the process halts.

        new_name[in]       MySQL user string (user@host:passwd)
        destination[in]    A connection to a new server to clone the user
                           (default is None)
        globals_privs[in]  Include global privileges in clone (i.e. user@%)

        Note: Caller must ensure the new user account does not exist.
        """

        res = self.get_grants(globals_privs)
        server = self.server1
        if destination is not None:
            server = destination
        for row in res:
            # Create an instance of the user class.
            user = User(server, new_user, self.verbosity)
            if not user.exists():
                # Get authentication plugin if different from native plugin
                auth = self._get_authentication()
                # Add authentication if available
                user.create(authentication=auth)

            if globals_privs and '%' in row[0]:
                base_user_ticks = "'" + self.user + "'@'" + '%' + "'"
            else:
                base_user_ticks = "'" + self.user + "'@'" + self.host + "'"
            user, _, host = parse_user_host(new_user)
            new_user_ticks = "'" + user + "'@'" + host + "'"
            grant = row[0].replace(base_user_ticks, new_user_ticks, 1)

            # Need to remove the IDENTIFIED BY clause for the base user.
            search_str = "IDENTIFIED BY PASSWORD"
            try:
                start = grant.index(search_str)
            except:
                start = 0

            if start > 0:
                end = grant.index("'", start + len(search_str) + 2) + 2
                grant = grant[0:start] + grant[end:]

            if self.verbosity > 0:
                print grant

            res = server.exec_query(grant, self.query_options)

    @staticmethod
    def _parse_grant_statement(statement, sql_mode=''):
        """ Returns a namedtuple with the parsed GRANT information.

        statement[in] Grant string in the sql format returned by the server.

        Returns named tuple with GRANT information or None.
        """

        grant_parse_re = re.compile(r"""
            GRANT\s(.+)?\sON\s # grant or list of grants
            (?:(?:PROCEDURE\s)|(?:FUNCTION\s))? # optional for routines only
            (?:(?:(\*|`?[^']+`?)\.(\*|`?[^']+`?)) # object where grant applies
            | ('[^']*'@'[^']*')) # For proxy grants user/host
            \sTO\s([^@]+@[\S]+) # grantee
            (?:\sIDENTIFIED\sBY\sPASSWORD
             (?:(?:\s<secret>)|(?:\s\'[^\']+\')?))? # optional pwd
            (?:\sREQUIRE\sSSL)? # optional SSL
            (\sWITH\sGRANT\sOPTION)? # optional grant option
            $ # End of grant statement
            """, re.VERBOSE)

        grant_tpl_factory = namedtuple("grant_info", "privileges proxy_user "
                                                     "db object user")
        match = re.match(grant_parse_re, statement)

        if match:
            # quote database name and object name with backticks
            if match.group(1).upper() != 'PROXY':
                db = match.group(2)
                if not is_quoted_with_backticks(db, sql_mode) and db != '*':
                    db = quote_with_backticks(db, sql_mode)
                obj = match.group(3)
                if not is_quoted_with_backticks(obj, sql_mode) and obj != '*':
                    obj = quote_with_backticks(obj, sql_mode)
            else:  # if it is not a proxy grant
                db = obj = None
            grants = grant_tpl_factory(
                # privileges
                set([priv.strip() for priv in match.group(1).split(",")]),
                match.group(4),  # proxied user
                db,  # database
                obj,  # object
                match.group(5),  # user
            )
            # If user has grant option, add it to the list of privileges
            if match.group(6) is not None:
                grants.privileges.add("GRANT OPTION")
        else:
            raise UtilError("Unable to parse grant statement "
                            "{0}".format(statement))

        return grants
#
# Copyright (c) 2011, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains classes and functions used to determine what MySQL
utilities are installed, their options, and usage. This module can be
used to allow a client to provide auto type and option completion.
"""

import glob
import os
import sys
import re
import subprocess


_MAX_WIDTH = 78

# These utilities should not be used with the console
_EXCLUDE_UTILS = ['mysqluc', ]

RE_USAGE = (
    r"(?P<Version>.*?)"
    r"(?P<Usage>Usage:\s.*?)\w+\s\-\s"  # This match first
    # section <Usage> matching all till find a " - "
    r"(?P<Description>.*?)"  # Description is the text next
    # to " - " and till next match.
    r"(?P<O>\w*):"  # This is beginning of Options section
    r"(?P<Options>.*(?=^Introduction.\-{12})|.*$)"
    # match Options till end or till find Introduction -.
    r"(?:^Introduction.\-{12}){0,1}"  # not catching group
    r"(?P<Introduction>.*(?=^Helpful\sHints.\-{13})|.*$)"
    # captures Introduction (optional)
    # it will match Introduction till end or till Hints -
    r"(?:^Helpful\sHints.\-{13}){0,1}"  # Not catching group
    r"(?P<Helpful_Hints>.*)"
    # captures Helpful Hints (optional)
)

RE_OPTIONS = (
    r"^(?P<Alias>\s\s\-.*?)\s{2,}"  # Option Alias
    # followed by 2 o more spaces is his description
    r"(?P<Desc>.*?)(?=^\s\s\-)"  # description is all
    # text till not found other alias in the form
    # <-|--Alias> at the begin of the line.
)

RE_OPTION = r"\s+\-\-(.*?)\s"  # match Alias of the form <--Alias>

RE_ALIAS = r"\s+\-(\w+)\s*"  # match Alias of the form <-Alias>

WARNING_FAIL_TO_READ_OPTIONS = ("WARNING: {0} failed to read options."
                                " This utility will not be shown in 'help "
                                "utilities' and cannot be accessed from the "
                                "console.")


def get_util_path(default_path=''):
    """Find the path to the MySQL utilities

    This method will attempt to

    default_path[in]   provides known location of utilities
                       if provided, method will search this location first
                       before searching PYTHONPATH

    Returns string - path to utilities or None if not found
    """
    def _search_paths(needles, paths):
        """Search and return normalized path
        """
        for path in paths:
            norm_path = os.path.normpath(path)
            hay_stack = [os.path.join(norm_path, n) for n in needles]
            for needle in hay_stack:
                if os.path.isfile(needle):
                    return norm_path

        return None

    needle_name = 'mysqlreplicate'
    needles = [needle_name + ".py"]
    if os.name == "nt":
        needles.append(needle_name + ".exe")
    else:
        needles.append(needle_name)

    # Try the default by itself
    path_found = _search_paths(needles, [default_path])
    if path_found:
        return path_found

    # Try the pythonpath environment variable
    pythonpath = os.getenv("PYTHONPATH")
    if pythonpath:
        # This is needed on windows without a python setup, cause needs to
        # find the executable scripts.
        path = _search_paths(needles, [os.path.join(n, "../")
                                       for n in pythonpath.split(";", 1)])
        if path:
            return path
        path = _search_paths(needles, pythonpath.split(";", 1))
        if path:
            return path

    # Try the system paths
    path_found = _search_paths(needles, sys.path)
    if path_found:
        return path_found

    return None


class Utilities(object):
    """The utilities class can be used to discover what utilities are installed
    on the system as well as the usage and options for each utility.

    The list of utilities are read at initialization.

    This class is designed to support the following operations:

        get_util_matches()    - find all utilities that match a prefix
        get_option_matches()  - find all options that match a prefix for a
                                given utility
        get_usage()           - return the usage statement for a given utility
        show_utilities()      - display a 2-column list of utilities and their
                                descriptions
        show_options()        - display a 2-column list of the options for a
                                given utility including the name and
                                description of each option
    """

    def __init__(self, options=None):
        """Constructor
        """
        if options is None:
            options = {}
        self.util_list = []
        self.width = options.get('width', _MAX_WIDTH)
        self.util_path = get_util_path(options.get('utildir', ''))
        self.extra_utilities = options.get('add_util', {})
        self.hide_utils = options.get('hide_util', False)

        self.program_usage = re.compile(RE_USAGE, re.S | re.M)
        self.program_options = re.compile(RE_OPTIONS, re.S | re.M)
        self.program_option = re.compile(RE_OPTION)
        self.program_name = re.compile(RE_ALIAS)

        self.util_cmd_dict = {}
        self.posible_utilities = {}
        self.posible_utilities.update(AVAILABLE_UTILITIES)
        if self.extra_utilities and self.hide_utils:
            self.posible_utilities = self.extra_utilities
        else:
            self.posible_utilities.update(self.extra_utilities)
        self.available_utilities = self.posible_utilities
        for util_name, ver_compatibility in self.posible_utilities.iteritems():
            name_utility = "{0} utility".format(util_name)
            if ver_compatibility:
                min_v, max_v = ver_compatibility
                res = check_python_version(min_version=min_v,
                                           max_version=max_v,
                                           name=name_utility,
                                           print_on_fail=False,
                                           exit_on_fail=False,
                                           return_error_msg=True)
            else:
                res = check_python_version(name=name_utility,
                                           print_on_fail=False,
                                           exit_on_fail=False,
                                           return_error_msg=True)
            if isinstance(res, tuple):
                is_compat, error_msg = res
                if not is_compat:
                    self.available_utilities.remove(util_name)
                    print(WARNING_FAIL_TO_READ_OPTIONS.format(util_name))
                    print("ERROR: {0}\n".format(error_msg))
                    continue
            self._find_utility_cmd(util_name)

    @staticmethod
    def find_executable(util_name):
        """Search the system path for an executable matching the utility

        util_name[in]  Name of utility

        Returns string - name of executable (util_name or util_name.exe) or
                         original name if not found on the system path
        """
        paths = os.getenv("PATH").split(os.pathsep)
        for path in paths:
            new_path = os.path.join(path, util_name + "*")
            if os.name == "nt":
                new_path = '"{0}"'.format(new_path)
            found_path = glob.glob(new_path)
            if found_path:
                return os.path.split(found_path[0])[1]
        return util_name

    def _find_utility_cmd(self, utility_name):
        """ Locate the utility scripts

        util_name[in]   utility to find

        This method builds a dict of commands for invoke the utilities.
        """
        util_path = self.find_executable(os.path.join(self.util_path,
                                                      utility_name))
        util_path_parts = os.path.split(util_path)
        parts = os.path.splitext(util_path_parts[len(util_path_parts) - 1])
        # filter extensions
        exts = ['.py', '.exe', '', 'pyc']
        if (parts[0] not in _EXCLUDE_UTILS and
                (len(parts) == 1 or (len(parts) == 2 and parts[1] in exts))):
            util_name = str(parts[0])
            file_ext = parts[1]
            command = "{0}{1}".format(util_name, file_ext)

            util_path = self.util_path
            utility_path = command
            if not os.path.exists(command):
                utility_path = os.path.join(util_path, utility_name)

            # Now try the extensions
            if not os.path.exists(utility_path):
                if file_ext:
                    utility_path = "{0}{1}".format(utility_path, file_ext)
                else:
                    for ext in exts:
                        try_path = "{0}{1}".format(utility_path, ext)
                        if os.path.exists(try_path):
                            utility_path = try_path

            if not os.path.exists(utility_path):
                print("WARNING: Unable to locate utility {0}."
                      "".format(utility_name))
                print(WARNING_FAIL_TO_READ_OPTIONS.format(util_name))
                return

            # Check for running against .exe
            if utility_path.endswith(".exe"):
                cmd = []
            # Not using .exe
            else:
                cmd = [sys.executable]

            cmd.extend([utility_path])
            self.util_cmd_dict[utility_name] = tuple(cmd)

    def find_utilities(self, this_utils=None):
        """ Locate the utility scripts
        this_utils[in]   list of utilities to find, default None to find all.

        This method builds a list of utilities.
        """

        if not this_utils:
            # Not utilities name to find was passed, find help for all those
            # utilities not previously found in a previos call.
            utils = self.available_utilities
            working_utils = [util['name'] for util in self.util_list]
            if len(working_utils) >= len(self.util_list):
                utils = [name for name in utils if name not in working_utils]
            if len(utils) < 1:
                return
        else:
            # utilities name given to find for, find help for all these which
            # was not previously found in a previos call.
            working_utils = [util['name'] for util in self.util_list]
            utils = [util for util in this_utils if util not in working_utils]
            if len(utils) < 1:
                return

        # Execute the utility command using get_util_info()
        # that returns --help partially parsed.
        for util_name in utils:
            if util_name in self.util_cmd_dict:
                cmd = self.util_cmd_dict.pop(util_name)
                util_info = self.get_util_info(list(cmd), util_name)
                if util_info and util_info["usage"]:
                    util_info["cmd"] = tuple(cmd)
                    self.util_list.append(util_info)
                    working_utils.append(util_name)

        self.util_list.sort(key=lambda util_list: util_list['name'])

    def get_util_info(self, cmd, util_name):
        """Get information about utility

        cmd[in]        a list with the elements that conform the command
                       to invoke the utility
        util_name[in]  name of utility to get information

        Returns dictionary - name, description, usage, options
        """
        cmd.extend(["--help"])
        # rmv print('executing ==> {0}'.format(cmd))
        try:
            proc = subprocess.Popen(cmd, shell=False,
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            stdout_temp, stderr_temp = proc.communicate()
            returncode = proc.returncode
        except OSError:
            # always OS error if not found.
            # No such file or directory
            stdout_temp = ""
            returncode = 0

        # Parse the help output and save the information found
        usage = None
        description = None

        if stderr_temp or returncode:
            print(WARNING_FAIL_TO_READ_OPTIONS.format(util_name))
            if stderr_temp:
                print("The execution of the command returned: {0}"
                      "".format(stderr_temp))
            else:
                print("UNKNOWN. To diagnose, exit mysqluc and attempt the "
                      "command: {0} --help".format(util_name))
            return None

        res = self.program_usage.match(stdout_temp.replace("\r", ""))
        if not res:
            print(WARNING_FAIL_TO_READ_OPTIONS.format(util_name))
            print("An error occurred while trying to parse the options "
                  "from the utility")
            return None
        else:
            usage = res.group("Usage").replace("\n", "")
            desc_clean = res.group("Description").replace("\n", " ").split()
            description = (" ".join(desc_clean)) + " "
            # standardize string.
            Options = res.group("Options") + "\n  -"

        # Create dictionary for the information
        utility_data = {
            'name': util_name,
            'description': description,
            'usage': usage,
            'options': Options
        }
        return utility_data

    def parse_all_options(self, utility):
        """ Parses all options for the given utility.

        utility[inout]   that contains the options info to parse
        """
        options_info = utility['options']
        if isinstance(options_info, list):
            # nothing to do if it is a list.
            return

        options = []
        res = self.program_options.findall(options_info)

        for opt in res:
            option = {}
            name = self.program_option.search(opt[0] + " ")
            if name:
                option['name'] = str(name.group(1))
            alias = self.program_name.search(opt[0] + " ")
            if alias:
                option['alias'] = str(alias.group(1))
            else:
                option['alias'] = None

            desc_clean = opt[1].replace("\n", " ").split()
            option['description'] = " ".join(desc_clean)
            option['long_name'] = option['name']
            parts = option['name'].split('=')
            option['req_value'] = len(parts) == 2
            if option['req_value']:
                option['name'] = parts[0]
            if option:
                options.append(option)

        utility['options'] = options

    def get_util_matches(self, util_prefix):
        """Get list of utilities that match a prefix

        util_prefix[in] prefix for name of utility

        Returns dictionary entry for utility based on matching first n chars
        """
        matches = []
        if not util_prefix.lower().startswith('mysql'):
            util_prefix = 'mysql' + util_prefix
        for util in self.available_utilities:
            if util[0:len(util_prefix)].lower() == util_prefix:
                matches.append(util)
        # make sure the utilities description has been found for the matches.
        self.find_utilities(matches)
        matches = [util for util in self.util_list if util['name'] in matches]
        return matches

    def get_option_matches(self, util_info, option_prefix, find_alias=False):
        """Get list of option dictionary entries for options that match
        the prefix.

        util_info[in]     utility information
        option_prefix[in] prefix for option name
        find_alias[in]    if True, match alias (default = False)

        Returns list of dictionary items that match prefix
        """
        # Check type of util_info
        if util_info is None or util_info == {} or \
                not isinstance(util_info, dict):
            raise UtilError("Empty or invalide utility dictionary.")

        matches = []

        stop = len(option_prefix)
        if isinstance(util_info['options'], str):
            self.parse_all_options(util_info)
        for option in util_info['options']:
            if option is None:
                continue
            name = option.get('name', None)
            if name is None:
                continue
            if find_alias:
                if option.get('alias', '') == option_prefix:
                    matches.append(option)
            else:
                if name[0:stop] == option_prefix:
                    matches.append(option)

        return matches

    def show_utilities(self, print_list=None):
        """Show list of utilities as a 2-column list.

        print_list[in]    list of utilities to print - default is None
                          which means print all utilities
        """

        if print_list is None:
            if len(self.util_list) != len(self.available_utilities):
                self.find_utilities()
            list_of_utilities = self.util_list
        else:
            list_of_utilities = print_list
        print
        if len(list_of_utilities) > 0:
            print_dictionary_list(['Utility', 'Description'],
                                  ['name', 'description'],
                                  list_of_utilities, self.width)
        else:
            print
            print "No utilities match the search term."
        print

    def get_options_dictionary(self, utility_options):
        """Retrieve the options dictionary.

        This method builds a new dictionary that contains the options for the
        utilities read.

        utility_options[in]   list of options for utilities or the utility.

        Return dictionary - list of options for all utilities.
        """
        dictionary_list = []

        if isinstance(utility_options, dict):
            if isinstance(utility_options['options'], str):
                # options had not been parsed yet
                self.parse_all_options(utility_options)
            options = utility_options['options']
        else:
            options = utility_options

        for option in options:
            name = option.get('long_name', '')
            if len(name) == 0:
                continue
            name = '--' + name
            alias = option.get('alias', None)
            if alias is not None:
                name = '-' + alias + ", " + name
            item = {
                'long_name': name,
                'description': option.get('description', '')
            }
            dictionary_list.append(item)

        return dictionary_list

    def show_options(self, options):
        """Show list of options for a utility by name.

        options[in]    structure containing the options

        This method displays a list of the options and their descriptions
        for the given utility.
        """
        if len(options) > 0:
            dictionary_list = self.get_options_dictionary(options)
            print
            print
            print_dictionary_list(['Option', 'Description'],
                                  ['long_name', 'description'],
                                  dictionary_list, self.width)
            print

    @staticmethod
    def get_usage(util_info):
        """Get the usage statement for the utility

        util_info[in]  dictionary entry for utility information

        Returns string usage statement
        """
        # Check type of util_info
        if util_info is None or util_info == {} or \
                not isinstance(util_info, dict):
            return False

        return util_info['usage']


def kill_process(pid, force=False, silent=False):
    """This function tries to kill the given subprocess.

    pid [in]    Process id of the subprocess to kill.
    force [in]  Boolean value, if False try to kill process with SIGTERM
                (Posix only) else kill it forcefully.
    silent[in]  If true, do no print message

    Returns True if operation was successful and False otherwise.
    """
    res = True
    if os.name == "posix":
        if force:
            os.kill(pid, subprocess.signal.SIGABRT)
        else:
            os.kill(pid, subprocess.signal.SIGTERM)
    else:
        with open(os.devnull, 'w') as f_out:
            ret_code = subprocess.call("taskkill /F /T /PID {0}".format(pid),
                                       shell=True, stdout=f_out, stdin=f_out)
            if ret_code not in (0, 128):
                res = False
                if not silent:
                    print("Unable to successfully kill process with PID "
                          "{0}".format(pid))
    return res
#
# Copyright (c) 2011, 2013, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains classes and functions used to manage a user-defined
variables.
"""

import re



class Variables(dict):
    """
    The Variables class contains user-defined variables for replacement
    in custom commands.
    """

    def __init__(self, options=None, data=None):
        """Constructor

        options[in]        Width
        data[in]           Data to initialize class
        """
        self.options = options or {}
        self.width = options.get('width', 80)
        super(Variables, self).__init__(data or {})

    def find_variable(self, name):
        """Find a variable

        This method searches for a variable in the list and returns it
        if found.

        name[in]           Name of variable

        Returns dict - variable if found, None if not found.
        """
        if name in self:
            return {name: self[name]}
        return None

    def add_variable(self, name, value):
        """Add variable to the list

        name[in]           Name of variable
        value[in]          Value to store
        """
        self[name] = value

    def get_matches(self, prefix):
        """Get a list of variables that match a prefix

        This method returns a list of the variables that match the first N
        characters specified by var_prefix.

        var_prefix[in]     Prefix for search

        Returns list - matches or [] for no matches
        """
        result = []
        for key, value in self.iteritems():
            if key.startswith(prefix):
                result.append({key: value})
        return result

    def show_variables(self, variables=None):
        """Display variables

        This method displays the variables included in the list passed or all
        variables is list passed is empty.

        variables[in]      List of variables
        """
        if self.options.get("quiet", False):
            return

        var_list = [{'name': key, 'value': value}
                    for key, value in self.iteritems()]

        print "\n"
        if not self:
            print "There are no variables defined.\n"
            return

        print_dictionary_list(['Variable', 'Value'], ['name', 'value'],
                              var_list, self.width)
        print

    def replace_variables(self, cmd_string):
        """Replace all instances of variables with their values.

        This method will search a string for all variables designated by the
        '$' prefix and replace it with values from the list.

        cmd_string[in]     String to search

        Returns string - string with variables replaced
        """
        new_cmd = cmd_string
        finds = re.findall(r'\$(\w+)', cmd_string)
        for variable in finds:
            try:
                new_cmd = new_cmd.replace('$' + variable, str(self[variable]))
            except KeyError:
                # something useful when variable was not found?
                pass
        return new_cmd

    def search_by_key(self, pattern):
        """Find value by key pattern

        pattern[in]    regex pattern

        Returns tuple - key, value
        """
        regex = re.compile(pattern)

        for key, value in self.iteritems():
            if regex.match(key):
                yield key, value
#
# Copyright (c) 2012, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains features to parse an audit log file, including
searching and displaying the results.
"""

import re


class AuditLogParser(AuditLogReader):
    """The AuditLogParser class is used to parse the audit log file, applying
    search criterion and filtering the logged data.
    """

    def __init__(self, options):
        """Constructor

        options[in]       dictionary of options (e.g. log_name and verbosity)
        """
        self.options = options
        AuditLogReader.__init__(self, options)
        self.header_rows = []
        self.connects = []
        self.rows = []
        self.connection_ids = []

        # Compile regexp pattern
        self.regexp_pattern = None
        if self.options['pattern']:
            try:
                self.regexp_pattern = re.compile(self.options['pattern'])
            except:
                raise UtilError("Invalid Pattern: " + self.options['pattern'])

        # Add a space after the query type to reduce false positives.
        # Note: Although not perfect, this simple trick considerably reduce
        # false positives, avoiding the use of complex regex (with lower
        # performance).
        self.match_qtypes = []  # list of matching SQL statement/command types.
        self.regexp_comment = None
        self.regexp_quoted = None
        self.regexp_backtick = None
        if self.options['query_type']:
            # Generate strings to match query types
            for qt in self.options['query_type']:
                if qt == "commit":
                    # COMMIT is an exception (can appear alone without spaces)
                    self.match_qtypes.append(qt)
                else:
                    self.match_qtypes.append("{0} ".format(qt))
            # Compile regexp to match comments (/*...*/) to be ignored/removed.
            self.regexp_comment = re.compile(r'/\*.*?\*/', re.DOTALL)
            # Compile regexp to match single quoted text ('...') to be ignored.
            self.regexp_quoted = re.compile(r"'.*?'", re.DOTALL)
            # Compile regexp to match text between backticks (`) to be ignored.
            self.regexp_backtick = re.compile(r'`.*?`', re.DOTALL)

    def parse_log(self):
        """Parse audit log records, apply search criteria and store results.
        """
        # Find and store records matching search criteria
        for record, line in self.get_next_record():
            name = record.get("NAME")
            name_case = name.upper()
            # The variable matching_record is used to avoid unnecessary
            # executions the match_* function of the remaining search criteria
            # to check, as it suffice that one match fails to not store the
            # records in the results. This implementation technique was applied
            # to avoid the use of too deep nested if-else statements that will
            # make the code more complex and difficult to read and understand,
            # trying to optimize the execution performance.
            matching_record = True
            if name_case == 'AUDIT':
                # Store audit start record
                self.header_rows.append(record)

            # Apply filters and search criteria
            if self.options['users']:
                self._track_new_users_connection_id(record, name_case)
                # Check if record matches users search criteria
                if not self.match_users(record):
                    matching_record = False

            # Check if record matches event type criteria
            if (matching_record and self.options['event_type'] and
                    not self.match_event_type(record,
                                              self.options['event_type'])):
                matching_record = False

            # Check if record matches status criteria
            if (matching_record and self.options['status'] and
                    not self.match_status(record, self.options['status'])):
                matching_record = False

            # Check if record matches datetime range criteria
            if (matching_record and
                    not self.match_datetime_range(record,
                                                  self.options['start_date'],
                                                  self.options['end_date'])):
                matching_record = False

            # Check if record matches query type criteria
            if (matching_record and self.options['query_type'] and
                    not self.match_query_type(record)):
                matching_record = False

            # Search attributes values for matching pattern
            if (matching_record and self.regexp_pattern and
                    not self.match_pattern(record)):
                matching_record = False

            # Store record into resulting rows (i.e., survived defined filters)
            if matching_record:
                if self.options['format'] == 'raw':
                    self.rows.append(line)
                else:
                    self.rows.append(record)

    def retrieve_rows(self):
        """Retrieve the resulting entries from the log parsing process
        """
        return self.rows if self.rows != [] else None

    def _track_new_users_connection_id(self, record, name_upper):
        """Track CONNECT records and store information of users and associated
        connection IDs.
        """
        user = record.get("USER", None)
        priv_user = record.get("PRIV_USER", None)

        # Register new connection_id (and corresponding user)
        if (name_upper.upper() == "CONNECT" and
                (user and (user in self.options['users'])) or
                (priv_user and (priv_user in self.options['users']))):
            self.connection_ids.append((user, priv_user,
                                        record.get("CONNECTION_ID")))

    def match_users(self, record):
        """Match users.

        Check if the given record match the user search criteria.
        Returns True if the record matches one of the specified users.

        record[in] audit log record to check
        """
        for con_id in self.connection_ids:
            if record.get('CONNECTION_ID', None) == con_id[2]:
                # Add user columns
                record['USER'] = con_id[0]
                record['PRIV_USER'] = con_id[1]
                # Add server_id column
                if self.header_rows:
                    record['SERVER_ID'] = self.header_rows[0]['SERVER_ID']
                return True
        return False

    @staticmethod
    def match_datetime_range(record, start_date, end_date):
        """Match date/time range.

        Check if the given record match the datetime range criteria.
        Returns True if the record matches the specified date range.

        record[in] audit log record to check;
        start_date[in] start date/time of the record (inclusive);
        end_date[in] end date/time of the record (inclusive);
        """
        if (start_date and (record.get('TIMESTAMP', None) < start_date)) or \
           (end_date and (end_date < record.get('TIMESTAMP', None))):
            # Not within datetime range
            return False
        else:
            return True

    def match_pattern(self, record):
        """Match REGEXP pattern.

        Check if the given record matches the defined pattern.
        Returns True if one of the record values matches the pattern.

        record[in] audit log record to check;
        """
        for val in record.values():
            if val and self.regexp_pattern.match(val):
                return True
        return False

    def match_query_type(self, record):
        """Match query types.

        Check if the given record matches one of the given query types.
        Returns True if the record possesses a SQL statement/command that
        matches one of the query types from the given list of query types.

        record[in]          audit log record to check;
        """
        sqltext = record.get('SQLTEXT', None)
        if sqltext:
            # Ignore (i.e., remove) comments in query.
            if self.regexp_comment:
                sqltext = re.sub(self.regexp_comment, '', sqltext)
            # Ignore (i.e., remove) quoted text in query.
            if self.regexp_quoted:
                sqltext = re.sub(self.regexp_quoted, '', sqltext)
            # Ignore (i.e., remove) names quoted with backticks in query.
            if self.regexp_backtick:
                sqltext = re.sub(self.regexp_backtick, '', sqltext)
            # Search query types strings inside text.
            sqltext = sqltext.lower()
            for qtype in self.match_qtypes:
                # Handle specific query-types to avoid false positives.
                if (qtype.startswith('set') and
                        ('insert ' in sqltext or 'update ' in sqltext)):
                    # Do not match SET in INSERT or UPDATE queries.
                    continue
                if (qtype.startswith('prepare') and
                        ('drop ' in sqltext or 'deallocate ' in sqltext)):
                    # Do not match PREPARE in DROP or DEALLOCATE queries.
                    continue
                # Check if query type is found.
                if qtype in sqltext:
                    return True
        return False

    @staticmethod
    def match_event_type(record, event_types):
        """Match audit log event/record type.

        Check if the given record matches one of the given event types.
        Returns True if the record type (i.e., logged event) matches one of the
        types from the given list of event types.

        record[in] audit log record to check;
        event_types[in] list of matching record/event types;
        """
        name = record.get('NAME').lower()
        return(name in event_types)

    @staticmethod
    def match_status(record, status_list):
        """Match the record status.

        Check if the given record match the specified status criteria.

        record[in]          audit log record to check;
        status_list[in]     list of status values or intervals (representing
                            MySQL error codes) to match;

        Returns True if the record status matches one of the specified values
        or intervals in the list.
        """
        rec_status = record.get('STATUS', None)
        if rec_status:
            rec_status = int(rec_status)
            for status_val in status_list:
                # Check if the status value is an interval (tuple) or int
                if isinstance(status_val, tuple):
                    # It is an interval; Check if it contains the record
                    # status.
                    if status_val[0] <= rec_status <= status_val[1]:
                        return True
                else:
                    # Directly check if the status match (is equal).
                    if rec_status == status_val:
                        return True
        return False
#
# Copyright (c) 2012, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains the methods for reading the audit log.
"""

import os
import xml.etree.ElementTree as xml


# Import appropriate XML exception to be compatible with python 2.6.
try:
    # Exception only available from python 2.7 (i.e., ElementTree 1.3)
    # pylint: disable=E0611,C0411
    from xml.etree.ElementTree import ParseError
except ImportError:
    # Instead use ExpatError for earlier python versions.
    # pylint: disable=C0411
    from xml.parsers.expat import ExpatError as ParseError


# Fields for the old format.
_MANDATORY_FIELDS = ['NAME', 'TIMESTAMP']
_OPTIONAL_FIELDS = ['CONNECTION_ID', 'DB', 'HOST', 'IP', 'MYSQL_VERSION',
                    'OS_LOGIN', 'OS_VERSION', 'PRIV_USER', 'PROXY_USER',
                    'SERVER_ID', 'SQLTEXT', 'STARTUP_OPTIONS', 'STATUS',
                    'USER', 'VERSION']

# Fields for the new format.
_NEW_MANDATORY_FIELDS = _MANDATORY_FIELDS + ['RECORD_ID']
_NEW_OPTIONAL_FIELDS = _OPTIONAL_FIELDS + ['COMMAND_CLASS', 'STATUS_CODE']


class AuditLogReader(object):
    """The AuditLogReader class is used to read the data stored in the audit
    log file. This class provide methods to open the audit log, get the next
    record, and close the file.
    """

    def __init__(self, options=None):
        """Constructor

        options[in]       dictionary of options (e.g. log_name and verbosity)
        """
        if options is None:
            options = {}
        self.verbosity = options.get('verbosity', 0)
        self.log_name = options.get('log_name', None)
        self.log = None
        self.tree = None
        self.root = None
        self.remote_file = False

    def __del__(self):
        """Destructor
        """
        if self.remote_file:
            os.unlink(self.log_name)

    def open_log(self):
        """Open the audit log file.
        """
        # Get the log from a remote server
        # TODO : check to see if the log is local. If not, attempt
        #        to log into the server via rsh and copy the file locally.
        self.remote_file = False
        if not self.log_name or not os.path.exists(self.log_name):
            raise UtilError("Cannot read log file '%s'." % self.log_name)
        self.log = open(self.log_name)

    def close_log(self):
        """Close the previously opened audit log.
        """
        self.log.close()

    @staticmethod
    def _validXML(line):
        """Check if line is a valid XML element, apart from audit records.
        """
        return (('<?xml ' in line) or
                ('<AUDIT>' in line) or ('</AUDIT>' in line))

    def get_next_record(self):
        """Get the next audit log record.

        Generator function that return the next audit log record.
        More precisely, it returns a tuple with a formatted record dict and
        the original record.
        """
        next_line = ""
        new_format = False
        multiline = False
        for line in self.log:
            if line.lstrip().startswith('<AUDIT_RECORD>'):
                # Found first record line in the new format.
                new_format = True
                multiline = True
                next_line = line
                continue
            elif (line.lstrip().startswith('<AUDIT_RECORD') and
                  not line.endswith('/>\n')):
                # Found (first) record line in the old format.
                next_line = "{0} ".format(line.strip('\n'))
                if not line.endswith('/>\n'):
                    multiline = True
                    continue
            elif multiline:
                if ((new_format and
                     line.strip().endswith('</AUDIT_RECORD>')) or
                        (not new_format and line.endswith('/>\n'))):
                    # Detect end of record in the old and new format and
                    # append last record line.
                    next_line += line
                else:
                    if not line.strip().startswith('<'):
                        # Handle SQL queries broke into multiple lines,
                        # removing newline characters.
                        next_line = '{0}{1}'.format(next_line.strip('\n'),
                                                    line.strip('\n'))
                    else:
                        next_line += line
                    continue
            else:
                next_line += line
            log_entry = next_line
            next_line = ""
            try:
                yield (
                    self._make_record(xml.fromstring(log_entry), new_format),
                    log_entry
                )
            except (ParseError, SyntaxError):
                # SyntaxError is also caught for compatibility reasons with
                # python 2.6. In case an ExpatError which does not inherits
                # from SyntaxError is used as a ParseError.
                if not self._validXML(log_entry):
                    raise UtilError("Malformed XML - Cannot parse log file: "
                                    "'{0}'\nInvalid XML element: "
                                    "{1!r}".format(self.log_name, log_entry))

    @staticmethod
    def _do_replacements(old_str):
        """Replace special masked characters.
        """
        new_str = old_str.replace("&lt;", "<")
        new_str = new_str.replace("&gt;", ">")
        new_str = new_str.replace("&quot;", '"')
        new_str = new_str.replace("&amp;", "&")
        return new_str

    def _make_record(self, node, new_format=False):
        """Make a dictionary record from the node element.

        The given node is converted to a dictionary record, reformatting
        as needed for the special characters.

        node[in]        XML node holding a single audit log record.
        new_format[in]  Flag indicating if the new XML format is used for the
                        audit log record. By default False (old format used).

        Return a dictionary with the data in the given audit log record.
        """
        if new_format:
            # Handle audit record in the new format.
            # Do mandatory fields.
            # Note: Use dict constructor for compatibility with Python 2.6.
            record = dict((field, node.find(field).text)
                          for field in _NEW_MANDATORY_FIELDS)
            # Do optional fields.
            for field in _NEW_OPTIONAL_FIELDS:
                field_node = node.find(field)
                if field_node is not None and field_node.text:
                    record[field] = self._do_replacements(field_node.text)
        else:
            # Handle audit record in the old format.
            # Do mandatory fields.
            # Note: Use dict constructor for compatibility with Python 2.6.
            record = dict((field, node.get(field))
                          for field in _MANDATORY_FIELDS)
            # Do optional fields.
            for field in _OPTIONAL_FIELDS:
                if node.get(field, None):
                    record[field] = self._do_replacements(node.get(field))
        return record
#
# Copyright (c) 2014, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains common features to manage and handle binary log files.
"""
import io
import errno
import os
import shutil
import time

from datetime import datetime


LOG_TYPES = ['bin', 'relay', 'all']
LOG_TYPE_BIN = LOG_TYPES[0]
LOG_TYPE_RELAY = LOG_TYPES[1]
LOG_TYPE_ALL = LOG_TYPES[2]

_DAY_IN_SECONDS = 86400


def is_binary_log_filename(filename, log_type=LOG_TYPE_ALL, basename=None):
    """Check if the filename matches the name format for binary log files.

    This function checks if the given filename corresponds to the filename
    format of known binary log files, according to the specified log_type and
    optional basename. The file extension is a sequence number (.nnnnnn). If
    a basename is given then the filename for the binary log file must have
    the format 'basename.nnnnnn'. Otherwise the default filename is assumed,
    depending on the log_type: '*-bin.nnnnnn' for the 'bin' log type,
    '*-relay-bin.nnnnnn' for the 'relay' log type, and both for the 'all' type.

    filename[in]    Filename to check.
    log_type[in]    Type of the binary log, must be one of the following
                    values: 'bin' for binlog files, 'relay' for relay log
                    files, 'all' for both binary log files. By default = 'all'.
    basename[in]    Basename defined for the binary log file. None by default,
                    meaning that the default server name formats are assumed
                    (according to the given log type).
    """
    # Split file basename and extension.
    f_base, f_ext = os.path.splitext(filename)
    f_ext = f_ext[1:]  # remove starting dot '.'

    # Check file basename.
    if basename:
        if f_base != basename:
            # Defined basename does not match.
            return False
    else:
        # Check default serve basename for the given log_type.
        if log_type == LOG_TYPE_BIN:
            # *-bin.nnnnnn (excluding *-relay-bin.nnnnnn)
            if not f_base.endswith('-bin') or f_base.endswith('-relay-bin'):
                return False
        elif log_type == LOG_TYPE_RELAY:
            # *-relay-bin.nnnnnn
            if not f_base.endswith('-relay-bin'):
                return False
        elif log_type == LOG_TYPE_ALL:
            # *-bin.nnnnnn (including *-relay-bin.nnnnnn)
            if not f_base.endswith('-bin'):
                return False
        else:
            raise UtilError("Unsupported log-type: {0}".format(log_type))

    # Check file extension.
    try:
        int(f_ext)
    except ValueError:
        # Extension is not a sequence number (error converting to integer).
        return False

    # Return true if basename and extension checks passed.
    return True


def get_index_file(source, binary_log_file):
    """ Find the binary log index file.

    Search the index file in the specified source directory for the given
    binary log file and retrieve its location (i.e., full path).

    source[in]              Source directory to search for the index file.
    binary_log_file[in]     Binary log file associated to the index file.

    Return the location (full path) of the binary log index file.
    """
    f_base, _ = os.path.splitext(binary_log_file)
    index_filename = '{0}.index'.format(f_base)
    index_file = os.path.join(source, index_filename)
    if os.path.isfile(index_file):
        return index_file
    else:
        raise UtilError("Unable to find the index file associated to file "
                        "'{0}'.".format(binary_log_file))


def filter_binary_logs_by_sequence(filenames, seq_list):
    """Filter filenames according to the given sequence number list.

    This function filters the given list of filenames according to the given
    sequence number list, excluding the filenames that do not match.

    Note: It is assumed that given filenames are valid binary log files.
    Use is_binary_log_filename() to check each filenames.

    filenames[in]   List of binary log filenames to check.
    seq_list[in]    List of allowed sequence numbers or intervals.
                    For example: 3,5-12,16,21.

    Returns a list of the filenames matching the given sequence number filter.
    """
    res_list = []
    for filename in filenames:
        # Split file basename and extension.
        _, f_ext = os.path.splitext(filename)
        f_ext = int(f_ext[1:])  # remove starting dot '.' and convert to int
        for seq_value in seq_list:
            # Check if the sequence value is an interval (tuple) or int.
            if isinstance(seq_value, tuple):
                # It is an interval; Check if it contains the file sequence
                # number.
                if seq_value[0] <= f_ext <= seq_value[1]:
                    res_list.append(filename)
                    break
            else:
                # Directly check if the sequence numbers match (are equal).
                if f_ext == seq_value:
                    res_list.append(filename)
                    break

    # Retrieve the resulting filename list (filtered by sequence number).
    return res_list


def filter_binary_logs_by_date(filenames, source, max_date):
    """Filter filenames according their last modification date.

    This function filters the given list of files according to their last
    modification date, excluding those with the last change before the given
    max_date.

    Note: It is assumed that given filenames are valid binary log files.
    Use is_binary_log_filename() to check each filename.

    filenames[in]   List of binary log filenames to check.
    source[in]      Source directory where the files are located.
    max_date[in]    Maximum modification date, in the format 'yyyy-mm-dd' or
                    'yyyy-mm-ddThh:mm:ss', or number of days since the last
                    modification.

    Returns a list of the filenames not changed within the given elapsed days
    (i.e., recently changed files will be excluded).
    """
    res_list = []
    # Compute maximum modified date/time, according to supported formats.
    try:
        elapsed_days = int(max_date)
    except ValueError:
        # Max date is not a valid integer (i.e., number of days).
        elapsed_days = None
    if elapsed_days:  # Process the specified number fo days
        if elapsed_days < 1:
            raise UtilError(
                "Invalid number of days (must be an integer greater than "
                "zero): {0}".format(max_date)
            )
        # Get current local time.
        ct_tuple = time.localtime()
        # Set time to 00:00:00.
        ct_list = list(ct_tuple)
        ct_list[3] = 0  # hours
        ct_list[4] = 0  # minutes
        ct_list[5] = 0  # seconds
        ct_tuple_0000 = tuple(ct_list)
        # Get seconds since epoch for the current day at 00:00.
        day_start_time = time.mktime(ct_tuple_0000)
        # Compute max modified date based on elapsed days ignoring time, i.e.,
        # 00:00 is used as reference to count days. Current day count as one.
        max_time = day_start_time - (_DAY_IN_SECONDS * (elapsed_days - 1))
        max_date = time.strftime('%Y-%m-%dT%H:%M:%S', time.localtime(max_time))
    else:  # Process the specified date
        # Check the date format.
        _, _, time_val = max_date.partition('T')
        if time_val:
            try:
                dt_max_date = datetime.strptime(max_date, '%Y-%m-%dT%H:%M:%S')
            except ValueError:
                raise UtilError(
                    "Invalid date/time format (yyyy-mm-ddThh:mm:ss): "
                    "{0}".format(max_date)
                )
        else:
            try:
                dt_max_date = datetime.strptime(max_date, '%Y-%m-%d')
            except ValueError:
                raise UtilError(
                    "Invalid date format (yyyy-mm-dd): {0}".format(max_date)
                )
        max_date = dt_max_date.strftime('%Y-%m-%dT%H:%M:%S')

    # Check modified date for each file.
    for filename in filenames:
        source_file = os.path.join(source, filename)
        modified_time = os.path.getmtime(source_file)
        modified_date = time.strftime('%Y-%m-%dT%H:%M:%S',
                                      time.localtime(modified_time))
        if modified_date < max_date:
            res_list.append(filename)

    # Retrieve the resulting filename list (filtered by modified date).
    return res_list


def move_binary_log(source, destination, filename, log_index,
                    undo_on_error=True):
    """Move a binary log file to a specific destination.

    This method move the given binary log file (filename), located in the
    source directory, to the specified destination directory and updates the
    respective index file accordingly.

    Note: An error is raised if any issue occurs during the process.
    Additionally, if the undo_on_error=True (default) then the file is moved
    back to the source directory if an error occurred while updating the index
    file (keeping the file in the original location and the index file
    unchanged). Otherwise the file might be moved and the index file not
    correctly updated. In either cases an error is issued.

    source[in]          Source directory where the binary log file is located.
    destination[in]     Destination directory to move the binary log.
    filename[in]        Name of the binary log file to move.
    log_index[in]       Location (full path) of the binary log index file.
    undo_on_error[in]   Flag to undo the file move if an error occurs (when
                        updating the index file) or not. By default = True,
                        meaning that the move operation is reverted ().
    """
    def _move_file_back():
        """Try to move the file back to its original source directory.
        Returns a warning message indicating if the file was moved back
        successfully or not.
        """
        try:
            # Move file back to source directory.
            destination_file = os.path.join(destination, filename)
            shutil.move(destination_file, source)
        except (IOError, shutil.Error) as move_err:
            # Warn the user that an error occurred while trying to
            # move the file back.
            return ("\nWARNING: Failed to move file back to source directory: "
                    "{0}").format(move_err)
        else:
            # Notify user that the file was successfully moved back.
            return "\nWARNING: File move aborted."

    # Move file to destination directory.
    source_file = os.path.join(source, filename)
    if os.path.isdir(destination):
        shutil.move(source_file, destination)
    else:
        # Raise an error if the destination dir does not exist.
        # Note: To be consistent with the IOError raised by shutil.move() if
        # the source file does not exist.
        raise IOError(errno.ENOENT, "No such destination directory",
                      destination)

    # Update index file.
    found_pos = None
    try:
        with io.open(log_index, 'r') as index_file:
            # Read all data from index file.
            data = index_file.readlines()
            # Search for the binary log file entry.
            for pos, line in enumerate(data):
                if line.strip().endswith(filename):
                    found_pos = pos
                    break
            if found_pos is not None:
                # Replace binary file entry with absolute destination path.
                data[found_pos] = u'{0}\n'.format(
                    os.path.join(destination, filename)
                )
            else:
                warning = ""  # No warning if undo_on_error = False.
                if undo_on_error:
                    warning = _move_file_back()
                # Raise error (including cause).
                raise UtilError("Entry for file '{0}' not found in index "
                                "file: {1}{2}".format(filename, log_index,
                                                      warning))
            # Create a new temporary index_file with the update entry.
            # Note: original file is safe is something goes wrong during write.
            tmp_file = '{0}.tmp'.format(log_index)
            try:
                with io.open(tmp_file, 'w', newline='\n') as tmp_index_file:
                    tmp_index_file.writelines(data)
            except IOError as err:
                warning = ""  # No warning if undo_on_error = False.
                if undo_on_error:
                    warning = _move_file_back()
                # Raise error (including cause).
                raise UtilError('Unable to write temporary index file: '
                                '{0}{1}'.format(err, warning))
    except IOError as err:
        warning = ""  # No warning if undo_on_error = False.
        if undo_on_error:
            warning = _move_file_back()
        # Raise error (including cause).
        raise UtilError('Failed to update index file: '
                        '{0}{1}'.format(err, warning))
    # Replace the original index file with the new one.
    if os.name == 'posix':
        os.rename(tmp_file, log_index)
    else:
        # On windows, rename does not work if the target file already exists.
        shutil.move(tmp_file, log_index)
#
# Copyright (c) 2014, 2016 Oracle and/or its affiliates. All rights
# reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains the binary log administrative operations purge and rotate
operations.
"""

import os



def get_binlog_info(server, reporter=None, server_name="server", verbosity=0):
    """Get binlog information from the server.

    This method queries the server for binary log information as the binlog
    base name, binlog file name and the active binlog file index.
    Note: An error is raised in case the binlog information can not be retried.

    server[in]       Source instance server to obtain information from.
    reporter[in]     Method to invoke to report messages.
    server_name[in]  Name of server to use when reporting. Default "server".
    verbosity[in]    Level of verbosity for report purposes.

    Returns a tuple with the active binlog base name, file name and index.
    """

    res = server.show_server_variable('log_bin_basename')
    binlog_b_name = None
    if res and res[0][1]:
        binlog_basename_path = res[0][1]
        if reporter is not None and verbosity >= 3:
            reporter("# Binary log basename path: {0}"
                     "".format(binlog_basename_path))
        binlog_b_name = os.path.basename(binlog_basename_path)
        if reporter is not None and verbosity >= 3:
            reporter("# Binary log basename: {0}"
                     "".format(binlog_b_name))

    res = server.exec_query("SHOW MASTER STATUS")

    if not res:
        raise UtilError("Unable to get binlog information from {0} at {1}:{2}"
                        "".format(server_name, server.host, server.port))
    else:
        master_active_binlog_file = res[0][0]

        master_active_binlog_index = int(res[0][0].split('.')[1])

        if binlog_b_name is None:
            binlog_b_name = res[0][0].split('.')[0]
            if reporter is not None and verbosity >= 3:
                reporter("# Binary log basename: {0}"
                         "".format(binlog_b_name))

        if reporter is not None and verbosity > 0:
            reporter("# {server_name} active binlog file: {act_log}"
                     "".format(server_name=server_name.capitalize(),
                               act_log=master_active_binlog_file))

    return (binlog_b_name, master_active_binlog_file,
            master_active_binlog_index)


def determine_purgeable_binlogs(active_binlog_index, slaves, reporter,
                                verbosity=0):
    """Determine the purgeable binary logs.

    This method will look at each slave given and will determinate the lowest
    binary log file that is being in use.

    active_binlog_index[in]    Index of binlog currently in use by the
                               master server or the higher binlog index value
                               it wants to be purged.
    slaves[in]                 Slaves list.
    reporter[in]               Method to call to report.
    verbosity[in]              The verbosity level for reporting information.

    Returns the last index in use by the slaves, that is the newest binlog
    index that has between read by all the slave servers.
    """
    # Determine old no needed binlogs
    master_log_file_in_use = []
    index_last_in_use = active_binlog_index
    # pylint: disable=R0101
    if slaves:
        for slave in slaves:
            if reporter is not None and verbosity >= 1:
                reporter("# Checking slave: {0}@{1}"
                         "".format(slave['host'], slave['port']))

            res = slave['instance'].get_status()

            if res:
                master_log_file = res[0][5]

                if reporter is not None and verbosity >= 1:
                    reporter("# I/O thread is currently reading: {0}"
                             "".format(master_log_file))
                master_log_file_in_use.append(master_log_file)
                reading_index_file = int(master_log_file.split('.')[1])

                if index_last_in_use > reading_index_file:
                    index_last_in_use = reading_index_file

                if reporter is not None and verbosity >= 2:
                    reporter("# File position of the I/O thread: {0}"
                             "".format(res[0][6]))
                    reporter("# Master binlog file with last event executed "
                             "by the SQL thread: {0}".format(res[0][9]))
                    reporter("# I/O thread running: {0}".format(res[0][10]))
                    reporter("# SQL thread running: {0}".format(res[0][11]))
                    if len(res[0]) > 52:
                        if res[0][51]:
                            reporter("# Retrieved GTid_Set: {0}"
                                     "".format(res[0][51]))
                        if res[0][52]:
                            reporter("# Executed GTid_Set: {0}"
                                     "".format(res[0][52]))
        return index_last_in_use
    else:
        raise UtilError("None Slave is connected to master")


def purge(server, purge_to_binlog, server_binlogs_list=None,
          reporter=None, dryrun=False, verbosity=0):
    """Purge the binary log for the given server.

    This method purges all the binary logs from the given server that are older
    than the given binlog file name specified by purge_to_binlog. The method
    can receive a list of the binary logs listed on the server to avoid
    querying the server again for this list. If The given purge_to_binlog is
    not listed on the server_binlogs_list the purge will not occur. For
    reporting capabilities if given the method report will be invoked to
    report messages and the server name that appears on the messages can be
    change with server_name.

    server[in]                server instance where to purge binlogs on
    purge_to_binlog[in]       purge binlog files older than this binlog file
                              name.
    server_binlogs_list[in]   A list of binlog files available on the given
                              server, if not given, the list will be retrieved
                              from the given server (default None).
    server_name[in]           This name will appear when reporting (default
                              'Server').
    reporter[in]              A method to invoke with messages and warnings
                              (default None).
    dryrun[in]                boolean value that indicates if the purge query
                              should be run on the server or reported only
                              (default False).
    verbosity[in]             The verbosity level for report messages.
    """
    if server_binlogs_list is None:
        server_binlogs_list = server.get_server_binlogs_list()

    # The PURGE BINARY LOGS statement deletes all the binary log files listed
    # in the log index file, prior to the specified log file name.
    # Verify purge_to_binlog is listed on server binlog list and if not is the
    # first in the list continue the purge, else there is no binlogs to purge
    if (purge_to_binlog in server_binlogs_list and
            purge_to_binlog != server_binlogs_list[0]):
        purge_query = (
            "PURGE BINARY LOGS TO '{0}'"
        ).format(purge_to_binlog)
        if dryrun:
            reporter("# To manually purge purge the binary logs Execute the "
                     "following query:")
            reporter(purge_query)
        else:
            if verbosity > 1:
                reporter("# Executing query {0}".format(purge_query))
            else:
                reporter("# Purging binary logs prior to '{0}'"
                         "".format(purge_to_binlog))
            try:
                server.exec_query(purge_query)
            except UtilDBError as err:
                raise UtilError("Unable to purge binary log, reason: {0}"
                                "".format(err.errmsg))

    else:
        reporter("# No binlog files can be purged.")


def get_active_binlog_and_size(server):
    """Retrieves the current active binlog file name and his size

    server[in]    server instance to query for the required info.

    Returns a tuple with two values, active binlog file name and his size
    """
    binlogs_list = server.get_server_binlogs_list(include_size=True)
    if binlogs_list:
        active_binlog_and_size = binlogs_list[-1]
        active_binlog = active_binlog_and_size[0]
        binlog_size = int(active_binlog_and_size[1])
        return active_binlog, binlog_size
    return None, None


def rotate(server, min_size=-1, reporter=None):
    """Rotates the binary log on the given server.

    This method rotates the active binary log from the given server, if
    min_size is given the size of the active binlog will be compared with this
    value, and rotation will only occur if the binlog size is greater than the
    given value. This method will execute the FLUSH BINARY LOGS on MySQL
    servers version 5.5.3 and greater and in older ones the FLUSH LOGS command
    to rotate the active binary log.

    server[in]      The source server instance where log rotation will occur
    min_size[in]    An integer value representing the minimum file size that
                    the active binlog must reach before rotate it. (default -1)
    reporter[in]    A method to invoke with messages and warnings.

    Returns True if the rotation command has been executed on the given server.
    """
    # Retrieve current active binlog and his file size.
    active_binlog, binlog_size = get_active_binlog_and_size(server)

    # Compare the active binlog size with the min_size
    # if the active binlog file size is greater than the min_size totate it
    # else show a Warning.
    if binlog_size >= min_size:
        if server.check_version_compat(5, 5, 3):
            type_log = "BINARY"
        else:
            type_log = ""
        server.exec_query("FLUSH {type_log} LOGS".format(type_log=type_log))
        return True
    else:
        if reporter:
            reporter("WARNING: The active binlog file '{0}' was not rotated "
                     "because it's size {1} is lower than the minimum "
                     "specified size: {2}".format(active_binlog, binlog_size,
                                                  min_size))
        return False
#
# Copyright (c) 2013, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains the charset_info class designed to read character set
and collation information from /share/charsets/index.xml.
"""

import sys


_CHARSET_INDEXES = ID, CHARACTER_SET_NAME, COLLATION_NAME, MAXLEN, IS_DEFAULT \
    = range(0, 5)

_CHARSET_QUERY = """
SELECT CL.ID,CL.CHARACTER_SET_NAME,CL.COLLATION_NAME,CS.MAXLEN, CL.IS_DEFAULT
FROM INFORMATION_SCHEMA.CHARACTER_SETS CS, INFORMATION_SCHEMA.COLLATIONS CL
WHERE CS.CHARACTER_SET_NAME=CL.CHARACTER_SET_NAME ORDER BY CHARACTER_SET_NAME
"""


class CharsetInfo(object):
    """
    Read character set information for lookup. Methods include:

      - get_charset_name(id) : get the name for a characterset id
      - get_default_collation(name) : get default collation name
      - get_name_by_collation(name) : given collation, find charset name
      - print_charsets() : print the character set map

    """

    def __init__(self, options=None):
        """Constructor

        options[in]        array of general options
        """
        if options is None:
            options = {}
        self.verbosity = options.get("verbosity", 0)
        self.format = options.get("format", "grid")
        self.server = options.get("server", None)

        self.charset_map = None

        if self.server:
            self.charset_map = self.server.exec_query(_CHARSET_QUERY)

    def print_charsets(self):
        """Print the character set list
        """
        print_list(sys.stdout, self.format,
                   ["id", "character_set_name", "collation_name",
                    "maxlen", "is_default"],
                   self.charset_map)
        print len(self.charset_map), "rows in set."

    def get_name(self, chr_id):
        """Get the character set name for the given id

        chr_id[in]     id for character set (as read from .frm file)

        Returns string - character set name or None if not found.
        """
        for cs in self.charset_map:
            if int(chr_id) == int(cs[ID]):
                return cs[CHARACTER_SET_NAME]
        return None

    def get_collation(self, col_id):
        """Get the collation name for the given id

        col_id[in]     id for collation (as read from .frm file)

        Returns string - collation name or None if not found.
        """
        for cs in self.charset_map:
            if int(col_id) == int(cs[ID]):
                return cs[COLLATION_NAME]
        return None

    def get_name_by_collation(self, colname):
        """Get the character set name for the given collation

        colname[in]    collation name

        Returns string - character set name or None if not found.
        """
        for cs in self.charset_map:
            if cs[COLLATION_NAME] == colname:
                return cs[CHARACTER_SET_NAME]
        return None

    def get_default_collation(self, col_id):
        """Get the default collation for the character set

        col_id[in]     id for collation (as read from .frm file)

        Returns tuple - (default collation id, name) or None if not found.
        """
        # Exception for utf8
        if col_id == 83:
            return "utf8_bin"
        for cs in self.charset_map:
            if int(cs[ID]) == int(col_id) and cs[IS_DEFAULT].upper() == "YES":
                return cs[COLLATION_NAME]
        return None

    def get_maxlen(self, col_id):
        """Get the maximum length for the character set

        col_id[in]     id for collation (as read from .frm file)

        Returns int - max length or 1 if not found.
        """
        for cs in self.charset_map:
            if int(cs[ID]) == int(col_id):
                return int(cs[MAXLEN])
        return int(1)
#
# Copyright (c) 2011, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains classes and functions used to manage a console utility.
"""

import os
import sys
import shlex


_COMMAND_COMPLETE = 0
_OPTION_COMPLETE = 1
_VARIABLE_COMPLETE = 2

# TODO remove this pylint disable regarding duplicate keys
# pylint: disable=W0109
_COMMAND_KEY = {
    '\x7f': 'DELETE_POSIX',
    '\x1b[3~': 'DELETE_MAC',
    '\x0a': 'ENTER_POSIX',
    '\r': 'ENTER_WIN',
    '\x1b': 'ESCAPE',
    '\x1b[A': 'ARROW_UP',
    '\x1b[B': 'ARROW_DN',
    '\x1b[C': 'ARROW_RT',
    '\x1b[D': 'ARROW_LT',
    '\t': 'TAB',
    '\x7f': 'BACKSPACE_POSIX',
    '\xe0': 'SPECIAL_WIN',
    '\x08': 'BACKSPACE_WIN',
    '\x1bOH': 'HOME',
    '\x1bOF': 'END'
}

# Some windows keys are different and require reading two keys.
# The following are the second characters.
_WIN_COMMAND_KEY = {
    'S': 'DELETE_WIN',
    'H': 'ARROW_UP',
    'P': 'ARROW_DN',
    'M': 'ARROW_RT',
    'K': 'ARROW_LT',
    'G': 'HOME',
    'O': 'END'
}

_COMMAND_COMPLETE = 0
_OPTION_COMPLETE = 1
_VARIABLE_COMPLETE = 2

# Base commands for all consoles.
#
# The list includes a tuple for each command that contains the name of the
# command, an alias (if defined) and its help text.
_BASE_COMMANDS = [
    {'name': 'help',
     'alias': 'help commands',
     'text': 'Show this list.'},
    {'name': 'exit',
     'alias': 'quit',
     'text': 'Exit the console.'},
    {'name': 'set <variable>=<value>',
     'alias': '',
     'text': 'Store a variable for recall in commands.'},
    {'name': 'show options',
     'alias': '',
     'text': 'Display list of options specified by the user on launch.'},
    {'name': 'show variables',
     'alias': '',
     'text': 'Display list of variables.'},
    {'name': '<ENTER>',
     'alias': '',
     'text': 'Press ENTER to execute command.'},
    {'name': '<ESCAPE>',
     'alias': '',
     'text': 'Press ESCAPE to clear the command entry.'},
    {'name': '<DOWN>',
     'alias': '',
     'text': 'Press DOWN to retrieve the previous command.'},
    {'name': '<UP>',
     'alias': '',
     'text': 'Press UP to retrieve the next command in history.'},
    {'name': '<TAB>',
     'alias': '',
     'text': 'Press TAB for type completion of utility, '
             'option, or variable names.'},
    {'name': '<TAB><TAB>',
     'alias': '',
     'text': 'Press TAB twice for list of matching type '
             'completion (context sensitive).'}
]


# Try to import the windows getch() if it fails, we're on Posix so define
# a custom getch() method to return keys.
try:
    # Win32
    # pylint: disable=C0413
    from msvcrt import getch  # pylint: disable=F0401
except ImportError:
    # UNIX/Posix
    # pylint: disable=C0411,C0413
    import termios

    def getch():
        """getch function
        """
        fd = sys.stdin.fileno()
        old = termios.tcgetattr(fd)
        new = termios.tcgetattr(fd)
        new[3] = new[3] & ~termios.ICANON & ~termios.ECHO
        new[6][termios.VMIN] = 1
        new[6][termios.VTIME] = 0
        termios.tcsetattr(fd, termios.TCSANOW, new)
        key = None
        try:
            key = os.read(fd, 80)
        finally:
            termios.tcsetattr(fd, termios.TCSAFLUSH, old)
        return key


class _CommandHistory(object):
    """
    The _CommandHistory class encapsulates a list of commands that can be
    retrieved either via the previous or next command in the list. The
    list grows to a circular list of max size as specified at initialization.
    """

    def __init__(self, options=None):
        """Constructor

        options[in]        Options for the class member variables
        """
        if options is None:
            options = {}
        self.position = 0
        self.commands = []
        self.max_size = options.get('max_size', 40)

    def add(self, command):
        """Add a command to the history list

        This method appends the command list if the max size is not met or
        replaces the last entry if the max size has been met.
        """
        if len(self.commands) < self.max_size:
            self.commands.append(command)
            self.position = 0
        else:
            if self.position == 0:
                self.commands[self.max_size - 1] = command
            else:
                self.commands[self.position - 1] = command

    def next(self):
        """Get next command in list.

        Returns string next command
        """
        if len(self.commands) == 0:
            return ''
        if self.position == len(self.commands) - 1:
            self.position = 0
        else:
            self.position += 1
        return self.commands[self.position]

    def previous(self):
        """Get previous command in list.

        Returns string prev command
        """
        if len(self.commands) == 0:
            return ''
        if self.position == 0:
            self.position = len(self.commands) - 1
        else:
            self.position -= 1
        return self.commands[self.position]


class _Command(object):
    """
    The _Command class encapsulates the operations of a console command line.
    """

    def __init__(self, prompt):
        """Constructor

        prompt[in]         The prompt written to the screen after each command
        """
        self.prompt = prompt
        self.position = 0
        self.command = ""
        self.length = 0

    @staticmethod
    def _erase_portion(num):
        """Erase a portion of the command line using backspace and spaces.

        num[in]            Number of spaces to erase starting from cursor left
        """
        i = 0
        while i < num:
            sys.stdout.write('\b')
            sys.stdout.write(' ')
            sys.stdout.write('\b')
            i += 1

    def get_command(self):
        """Return the current command.

        Returns string - the current command
        """
        return self.command

    def get_nearest_option(self):
        """Get the option for tab completion that is closest to the cursor

        This method returns the portion of the command line nearest the cursor
        and to the left until a space is encountered. For example, if the
        cursor was one space to the right of 'b' in some_command --verb --some
        it would return '--verb' or if the cursor is at the end of the command
        it will return the last portion of the command. In the previous
        example it would return '--some'.

        This portion is used for tab completion of options.

        Returns string - most local portion of the command.
        """
        parts = self.command.split(' ')
        # if not at the end of the command line, return the phrase where
        # the cursor is located indicated by self.position
        if self.position < self.length:
            for i in range(self.position - 1, len(parts[0]) - 1, -1):
                if self.command[i] == ' ':
                    return self.command[i + 1:self.position].strip(' ')
            return 'ERROR'
        else:
            return parts[len(parts) - 1]

    def erase_command(self):
        """Erase the command and reprint the prompt.
        """
        sys.stdout.write(' ' * (self.length - self.position))
        self._erase_portion(self.length)
        self.command = ''

    def _erase_inline(self, backspace=True):
        """Adjust command line by removing current char

        backspace[in]      If True, erase to the left (backspace)
                           If False, erase to the right
        """
        if self.position < self.length:
            num_erase = 1 + self.length - self.position
            if backspace:
                sys.stdout.write('\b')
            i = 0
            while i < num_erase:
                sys.stdout.write(' ')
                i += 1
            i = 0
            while i < num_erase:
                sys.stdout.write('\b')
                i += 1
        elif backspace:
            self._erase_portion(1)

    def home_keypress(self):
        """Executes the 'HOME' key press.

        This moves the cursor to the beginning of the command.
        """
        tmp = self.position
        self.position = 0
        sys.stdout.write('\b' * tmp)

    def end_keypress(self):
        """Executes the 'END' key press.

        This moves the cursor to the end of the command.
        """
        sys.stdout.write(self.command[self.position:self.length])
        self.position = self.length

    def delete_keypress(self):
        """Execute the 'DELETE' key press.

        This deletes one character from the right of the cursor.
        """
        if self.position < self.length:
            if self.length == 1:
                self.command = ''
                sys.stdout.write(' ')
                sys.stdout.write('\b')
                self.length = 0
            elif self.length > 0:
                self._erase_inline(False)
                old_command = self.command
                self.command = old_command[0:self.position]
                if self.position < self.length:
                    self.command += old_command[self.position + 1:]
                sys.stdout.write(self.command[self.position:])
                self.length = len(self.command)
                spaces = len(self.command[self.position:])
                # pylint: disable=W0612
                for i in range(0, spaces):
                    sys.stdout.write('\b')
                sys.stdout.flush()

    def backspace_keypress(self):
        """Execute the 'BACKSPACE' key press.

        This deletes one character to the left of the cursor.
        """
        # Here we need to move back one character calculating for in-string
        # edits (self.position < self.length)
        # if position less than length, we're inserting values
        if self.position <= 0:
            return
        if self.position < self.length:
            self._erase_inline(True)
            # build new command
            self.command = self.command[0:self.position - 1] + \
                self.command[self.position:]
            sys.stdout.write(self.command[self.position - 1:])
            i = 0
            while i < (self.length - self.position):
                sys.stdout.write('\b')
                i += 1
        else:
            self._erase_portion(1)
            self.command = self.command[0:self.length - 1]
        self.length -= 1
        self.position -= 1

    def left_arrow_keypress(self):
        """Execute the 'LEFT ARROW' keypress

        This moves the cursor position one place to the left until the
        beginning of the command.
        """
        if self.position > 0:
            self.position -= 1
            sys.stdout.write('\b')

    def right_arrow_keypress(self):
        """Execute the 'RIGHT ARROW' keypress

        This moves the cursor to the right one space until the end of the
        command.
        """
        # Here we need to move to the right one space but we don't have a
        # forward space print character. So we reprint the one character where
        # the position indicator is.
        if self.position < self.length:
            sys.stdout.write(self.command[self.position:self.position + 1])
            self.position += 1

    def replace_command(self, new_cmd):
        """Replace the command with a new command.

        This replaces the command and redisplays the prompt and new command.
        """
        if new_cmd != '':
            self._erase_portion(self.length)
            self.command = new_cmd
            sys.stdout.write(self.command)
            self.position = len(self.command)
            self.length = len(self.command)

    def add(self, key):
        """Add one or more characters to the command

        This method adds the characters specified in key to the command based
        on the location of the cursor. If in-string, the characters will be
        inserted accordingly or if at the end of the command (if cursor at
        the end).
        """
        if key is None:
            return
        # if position less than length, we're inserting values
        if self.position < self.length:
            # erase position forward.
            num_erase = self.length - self.position
            i = 0
            while i < num_erase:
                sys.stdout.write(' ')
                i += 1
            i = 0
            while i < num_erase:
                sys.stdout.write('\b')
                i += 1
            # build new command
            self.command = self.command[0:self.position] + key + \
                self.command[self.position:]
            sys.stdout.write(self.command[self.position:])
            self.position += len(key)
            # move cursor back to location at end of new key
            i = 0
            while i < (self.length - self.position + len(key)):
                sys.stdout.write('\b')
                i += 1
            self.length += len(key)
        else:
            self.command += key
            sys.stdout.write(key)
            self.position += len(key)
            self.length += len(key)

    def display_command(self):
        """Redisplay the command
        """
        sys.stdout.write(self.prompt + self.command)
        sys.stdout.flush()

    def clear(self):
        """Clear the command line - user must get the command first.
        """
        self.command = ''
        self.position = 0
        self.length = 0


class Console(object):
    """Console class
    """
    def __init__(self, new_base_commands, options):
        """Constructor

        new_base_commands  Additions to the base commands
        options[in]        Options for the class member variables
        """
        self.options = options
        self.tab_count = 0
        self.base_commands = []
        self.base_commands.extend(new_base_commands)
        self.base_commands.extend(_BASE_COMMANDS)
        self.type_complete_mode = _COMMAND_COMPLETE
        self.cmd_line = _Command(self.options.get('prompt', '> '))
        self.width = self.options.get('width', 80)
        self.commands = self.options.get("commands", None)
        self.custom_commands = self.options.get("custom", False)
        self.quiet = self.options.get("quiet", False)
        self.variables = Variables(options)
        self.history = _CommandHistory({'max_size': 20})
        self.position = 0
        self.errors = []
        var_list = self.options.get('variables', [])
        for var in var_list:
            self.variables.add_variable(var['name'], var['value'])

    def show_errors(self):
        """Show errors

        Displays the errors captured when executing an utility.
        """
        if self.quiet:
            return
        if not self.errors:
            print
            print("No errors to display.\n")
        for error in self.errors:
            print
            print("{0}\n".format(error))

    def clear_errors(self):
        """Clear errors

        Clears captured errors occurring while executing an utility.
        """
        if self.quiet:
            return
        self.errors = []
        print

    def show_last_error(self):
        """Show errors

        Displays the last error occurred when executing an utility.
        """
        if self.quiet:
            return
        if not self.errors:
            print
            print("No error to display.\n")
        else:
            print
            print("{0}\n".format(self.errors[-1]))

    def show_custom_command_help(self, arg):
        """Display the help for a custom command.

        Note: Override this method for help on custom commands.

        arg[in]            Help command argument
        """
        if self.quiet:
            return
        print "\nNo commands like '%s' exist.\n" % arg

    def do_custom_tab(self, prefix):
        """Do custom tab key processing

        Note: Override this method for tab completion for custom commands.

        prefix[in]        Prefix of the custom command
        """
        pass

    def do_custom_option_tab(self, prefix):
        """Do custom command option tab key processing

        Note: Override this method for tab completion for options for custom
        commands.

        prefix[in]        Prefix of the custom command
        """
        if self.quiet:
            return
        print "\n\nNo custom commands found.\n"

    @staticmethod
    def is_valid_custom_command(command_text):
        """Is command a valid custom command?

        This method evaluates the custom command for validity.

        Note: Override this command to determine if command_text is a valid
        custom command.

        command_text[in]   The complete command as entered by user

        Returns bool - True if valid, False if not recognized
        """
        return False  # return False by default if method not overridden

    def execute_custom_command(self, command, parameters):
        """Execute a custom command.
        Note: Override this method to execute a custom command.
        """
        pass

    def show_custom_options(self):
        """Show custom options

        Note: Override this for 'show options' functionality.
        """
        if self.quiet:
            return

    def do_option_tab(self, prefix):
        """Do tab completion for options

        This method will search for an option using the prefix passed. It
        first searches the console commands defined at instantiation (the
        general commands for the shell) and if not found, checks the
        options for a custom command.

        prefix[in]        Prefix of the option
        """
        full_command = self.cmd_line.get_command()
        matches = self.get_commands(full_command.strip(' '))
        if len(matches) > 0:
            self.do_base_command_tab(full_command, matches)
        elif self.custom_commands:
            # if prefix is 'help', try command complete for custom commands
            if full_command[0:4].lower() == 'help':
                self.do_custom_tab(prefix)
            else:
                self.do_custom_option_tab(prefix)

    def _set_complete_mode(self):
        """Set the tab completion mode

        If the command buffer is only 1 part (command and no options),
        we are in _COMMAND_COMPLETE mode.

        Else if the nearest option contains a $ at the start, we are in
        _VARIABLE_COMPLETE mode.

        Else we are in _OPTION_COMPLETE mode.

        _COMMAND_COMPLETE = tab complete for base and custom commands
        _VARIABLE_COMPLETE = tab complete for user-defined variables
        _OPTION_COMPLETE = tab complete for base or custom command options
        """
        buf = self.cmd_line.get_command()
        parts = buf.split(' ')
        segment = ''
        if (len(buf) > 0 and len(parts) == 1):
            self.type_complete_mode = _COMMAND_COMPLETE
        else:
            segment = self.cmd_line.get_nearest_option()
            if segment.find('$') > 0:
                self.type_complete_mode = _VARIABLE_COMPLETE
            else:
                self.type_complete_mode = _OPTION_COMPLETE
        return segment

    def show_help(self, parameter):
        """Display the help for either all commands or the help for a
        custom command.

        parameter[in]      Any parameter for the help command.
                           For example, 'help commands'
        """
        if self.quiet:
            return
        if not parameter or (parameter and parameter.lower() == 'commands'):
            print
            print_dictionary_list(['Command', 'Description'],
                                  ['name', 'text', 'alias'],
                                  self.base_commands, self.width, True)
            print
        else:
            matches = self.get_commands(parameter)
            if len(matches) > 0:
                self.show_command_help(matches)
            elif self.custom_commands:
                self.show_custom_command_help(parameter)

    def do_variable_tab(self, segment):
        """Do the tab completion for a variable

        This method will attempt to find a variable in the list of user-
        defined variables and complete the name of variable. If the user
        types 'TAB' twice, it will display a list of all possible matches.
        """
        # find the last $
        variable = ''
        start_var = 0
        new_var = ''
        stop = len(segment)
        for i in range(stop - 1, 0, -1):
            if segment[i] == ' ':
                break
            elif segment[i] == '$':
                variable = segment[i + 1:]
                start_var = i

        if start_var == stop:
            # show all of the variables
            matches = self.variables.get_matches({})
        else:
            matches = self.variables.get_matches(variable)

        if self.tab_count == 2:
            if len(matches) > 0:
                self.variables.show_variables(matches)
            else:
                self.variables.show_variables({})
            self.cmd_line.display_command()
            self.tab_count = 0
        else:
            # Do command completion here
            if len(matches) == 1:
                new_var = matches[0].items()[0][0] + ' '
                self.cmd_line.add(new_var[len(variable):])
                self.tab_count = 0

    def do_command_tab(self, command_text):
        """Do the tab completion for a command

        If the command is in the base commands, complete it there. If not,
        attempt to perform tab completion for custom commands (if defined).
        """
        # See if command is in the base command list first
        matches = self.get_commands(command_text)
        if len(matches) > 0:
            self.do_base_command_tab(command_text, matches)
        # Ok, not in command list, now check custom commands
        elif self.custom_commands:
            self.do_custom_tab(command_text)

    def do_base_command_tab(self, command_text, matches):
        """Do the tab completion for a base command.

        This method prints the list of base commands that match the
        command. If the user pressed TAB twice, it displays the list of all
        matches. If a single match is found, it returns the balance of the
        command.

        Note: this method gets its matches from do_command_tab.

        command_text[in]   Command
        matches[in]        Known matches (from do_command_tab)
        """
        if self.tab_count == 2:
            print "\n"
            print_dictionary_list(['Command', 'Description'],
                                  ['name', 'text', 'alias'],
                                  matches, self.width, True)
            print
            self.cmd_line.display_command()
            self.tab_count = 0
        else:
            if len(matches) == 1:
                if matches[0]['name'][:len(command_text)] == command_text:
                    new_cmd = matches[0]['name'] + ' '
                else:
                    new_cmd = matches[0]['alias'] + ' '
                self.tab_count = 0
                self.cmd_line.add(new_cmd[len(command_text):])

    def get_commands(self, cmd_prefix):
        """Get list of commands that match a prefix

        cmd_prefix[in]  prefix for name of command

        Returns dictionary entry for command based on matching first n chars
        """
        matches = []
        stop = len(cmd_prefix)
        start = 0
        for cmd in self.base_commands:
            if cmd['name'][start:stop] == cmd_prefix or \
               cmd['alias'][start:stop] == cmd_prefix:
                matches.append(cmd)

        return matches

    def show_command_help(self, commands):
        """Show the help for a list of commands.

        commands[in]       List of commands
        """
        if self.quiet:
            return
        print
        print_dictionary_list(['Command', 'Description'],
                              ['name', 'text', 'alias'],
                              commands, self.width, True)
        print

    def _do_command(self, command):
        """Execute a command

        This method routes the command to the appropriate methods for
        execution.

        command[in]        Command to execute

        Returns bool True - exit utility, False - do not exit
        """
        # do variable replacement
        command = self._replace_variables(command.strip(' '))
        if self.options.get('verbosity', False):
            print "\nExecuting command:", command
        # process simple commands
        if command.lower().startswith('set '):
            self._add_variable(command[4:])
            if not self.quiet:
                print
        elif command[0:11].lower() == 'show errors':
            self.show_errors()
        elif command[0:12].lower() == 'clear errors':
            self.clear_errors()
        elif command[0:15].lower() == 'show last error':
            self.show_last_error()
        elif command[0:14].lower() == 'show variables':
            self.variables.show_variables()
        elif self.custom_commands and command[0:12].lower() == 'show options':
            self.show_custom_options()
        else:
            cmd, parameters = self._get_util_parameters(command)
            if cmd is None:
                return False
            else:
                if cmd.lower() == 'help':
                    token = parameters[0] if parameters else ''
                    self.show_help(token)
                    self.cmd_line.clear()
                    self.tab_count = 0
                elif cmd == '':
                    print
                elif cmd.lower() in ['exit', 'quit']:
                    print
                    return True
                elif self.custom_commands:
                    if not self.is_valid_custom_command(cmd):
                        print("\n\nUnknown command: {0} {1}\n"
                              "".format(cmd, ' '.join(parameters)))
                    else:
                        try:
                            self.execute_custom_command(cmd, parameters)
                            print
                        except UtilError as err:
                            print err.errmsg

        self.cmd_line.clear()
        self.tab_count = 0
        return False

    def _process_command_keys(self, cmd_key):
        """Do the action associated with a command key.

        This method will act on the recognized command keys and execute the
        effect for each.

        cmd_key[in]        Key pressed
        """
        if cmd_key in ['ESCAPE']:
            self.cmd_line.erase_command()
        elif cmd_key in ['DELETE_POSIX', 'DELETE_WIN', 'DELETE_MAC']:
            self.cmd_line.delete_keypress()
            self.tab_count = 0
        elif cmd_key == 'ARROW_UP':
            self.cmd_line.replace_command(self.history.previous())
        elif cmd_key == 'ARROW_DN':
            self.cmd_line.replace_command(self.history.next())
        elif cmd_key == 'ARROW_LT':
            self.cmd_line.left_arrow_keypress()
        elif cmd_key == 'ARROW_RT':
            self.cmd_line.right_arrow_keypress()
        elif cmd_key in ['BACKSPACE_POSIX', 'BACKSPACE_WIN']:
            self.cmd_line.backspace_keypress()
        elif cmd_key == 'HOME':
            self.cmd_line.home_keypress()
        elif cmd_key == 'END':
            self.cmd_line.end_keypress()
        else:  # 'TAB'
            segment = self._set_complete_mode()
            self.tab_count += 1
            if self.type_complete_mode == _COMMAND_COMPLETE:
                self.do_command_tab(self.cmd_line.get_command())
            elif self.type_complete_mode == _OPTION_COMPLETE:
                self.do_option_tab(segment)
            else:  # _VARIABLE_COMPLETE
                self.do_variable_tab(segment)
        cmd_key = ''

    def _add_variable(self, set_command):
        """Add a variable to the list of variables.

        This method adds the user-defined variable to the internal list.

        set_command[in]    Set command from the user
        """
        if set_command.find('=') <= 0:
            print "\n\nSET command invalid. Syntax: SET <NAME> = <value>"
            return

        # get name and value
        name, value = set_command.split('=')
        name = name.strip().strip('$')
        value = value.strip()
        self.variables.add_variable(name, value)

    def _replace_variables(self, cmd_string):
        """Replace user-defined variables with values from the internal list.

        This method replaces $VARNAME with the value stored when the set
        command was issued.

        cmd_string[in]     Command from the user

        Returns string - command string with replacements
        """
        i = 1
        new_cmd = cmd_string
        while i > 0:
            i = new_cmd.find('$', i)
            if i > 0:
                j = new_cmd.find(' ', i)
                if j == -1:
                    j = len(new_cmd)
                if j > i:
                    var_name = new_cmd[i + 1:j]
                    var = self.variables.find_variable(var_name)
                    if var is not None:
                        new_cmd = new_cmd[0:i] + var[var_name] + new_cmd[j:]
                    else:
                        i = j

        return new_cmd

    @staticmethod
    def _get_util_parameters(cmd_string):
        """Split the command name from the command and return balance as
        parameters.

        cmd_string[in]     Command

        Returns tuple - command, list of parameters
        """
        try:
            tokens = shlex.split(cmd_string)
        except ValueError as err:
            print
            print("WARNING: Unable to execute command, reason: {0}"
                  "".format(str(err)))
            return None, None
        else:
            if len(tokens) > 1:
                return tokens[0], tokens[1:]
        return cmd_string.strip(' '), []

    def get_user_command(self):
        """Get a command from the user.

        This method displays a prompt to the user and returns when one of
        the command keys is pressed.
        """
        self.cmd_line.display_command()
        cmd_string = ''
        cmd_key = None
        self.tab_count = 0
        while cmd_key not in ['ENTER_POSIX', 'ENTER_WIN']:
            key = getch()
            # If a special key, act on it
            if key in _COMMAND_KEY:
                cmd_key = _COMMAND_KEY[key]
                # Windows does things oddly for some keys
                if os.name != 'posix' and cmd_key == 'SPECIAL_WIN':
                    key = getch()
                    cmd_key = _WIN_COMMAND_KEY.get(key)
                    if cmd_key is None:
                        continue
                self._process_command_keys(cmd_key)
                cmd_string = self.cmd_line.get_command()
            # else add key to command buffer
            else:
                cmd_string = self.cmd_line.get_command()
                self.cmd_line.add(key)
                cmd_string = self.cmd_line.get_command()
            sys.stdout.flush()

        self.position = 0
        return cmd_string

    def run_console(self, lines=None):
        """Run the console.

        This method is the main loop for executing commands. For all subclassed
        classes, the user need only call this method to execute an interactive
        shell or execute commands and exit. It can be used in three modes:

        1) it can process commands passed via lines list
        2) it can process commands passed via a pipe to the python exec
        3) it can prompt for commands and execute them as entered

        Modes (1) and (2) execute all commands then exit.

        lines[in]          If not empty, execute the list of commands.
        """
        if not lines:
            lines = []
        # If we have commands issued by the command line, execute and exit.
        if self.commands is not None:
            command_list = self.commands.split(';')
            for command in command_list:
                command = command.strip('\n').strip(' ')
                if os.name == 'nt':
                    command = command.strip('"')
                if self._do_command(command.strip('"')):
                    break

        # If we have piped input, read the input by line and execute
        elif not os.isatty(sys.stdin.fileno()) or len(lines) > 0:
            for command in sys.stdin.readlines():
                command_list = command.split(';')
                for cmd in command_list:
                    cmd = cmd.strip('\n').strip(' ')
                    if os.name == 'nt':
                        cmd = cmd.strip('"')
                    if self._do_command(cmd.strip('"')):
                        break

        # Otherwise, we are in an interactive mode where we get a command
        # from the user and execute
        else:
            cmd = ''
            if not self.quiet:
                print self.options.get('welcome', 'Welcome to the console!\n')
            while cmd.lower() not in ['exit', 'quit']:
                command = self.get_user_command()
                self.history.add(command)
                if self._do_command(command):
                    break
            if not self.quiet:
                print self.options.get('goodbye',
                                       'Thanks for using the console.\n')
#
# Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains a base class that implements a POSIX daemon.
"""

import os
import sys
import time
import atexit
import signal
import logging



class Daemon(object):
    """Posix Daemon.

    This is a base class for implementing a POSIX daemon.
    """
    def __init__(self, pidfile, umask=0o27, chdir="/", stdin=None, stdout=None,
                 stderr=None):
        """Constructor

        pidfile[in]  pid filename.
        umask[in]    posix umask.
        chdir[in]    working directory.
        stdin[in]    standard input object.
        stdout[in]   standard output object.
        stderr[in]   standard error object.
        """
        self.pid = None
        self.pidfile = os.path.realpath(os.path.normpath(pidfile))
        self.umask = umask
        self.chdir = chdir
        self.stdin = stdin
        self.stdout = stdout
        self.stderr = stderr

    def _report(self, message, level=logging.INFO, print_msg=True):
        """Log message if logging is on.

        This method will log the message presented if the log is turned on.
        Specifically, if options['log_file'] is not None. It will also
        print the message to stdout.

        This method should be overridden when subclassing Daemon.

        message[in]    message to be printed.
        level[in]      level of message to log. Default = INFO.
        print_msg[in]  if True, print the message to stdout. Default = True.
        """
        raise NotImplementedError("_report() method is not implemented.")

    def run(self, *args, **kwargs):
        """It will be called after the process has been daemonized by start()
        or restart.

        This method should be overridden when subclassing Daemon.
        """
        raise NotImplementedError("run() method is not implemented.")

    def start(self, detach_process=True):
        """Starts the daemon.

        It will start the daemon if detach_process is True.
        """
        if detach_process:
            # Check for a pidfile presence
            try:
                with open(self.pidfile, "rb") as f:
                    self.pid = int(f.read().strip())
            except IOError:
                self.pid = None
            except SystemExit:
                self.pid = None
            except ValueError:
                self.pid = None

            if self.pid:
                # Daemon already runs
                msg = ("pidfile {0} already exists. The daemon is already "
                       "running?".format(self.pidfile))
                self._report(msg, logging.CRITICAL)
                raise UtilDaemonError(msg)

            # Start the daemon
            self.daemonize()

        # Run automatic failover
        return self.run()

    def cleanup(self):
        """It will be called during the process to stop the daemon.

        This method should be overridden when subclassing Daemon.
        """
        raise NotImplementedError("cleanup() method is not implemented.")

    def stop(self):
        """Stops the daemon.

        It will stop the daemon by sending a signal.SIGTERM to the process.
        """
        # Get the pid from the pidfile
        try:
            with open(self.pidfile, "rb") as f:
                self.pid = int(f.read().strip())
        except IOError:
            self._report("pidfile {0} does not exist.".format(self.pidfile),
                         logging.ERROR)
            return False
        except ValueError:
            self._report("Invalid pid in pidfile {0}.".format(self.pidfile),
                         logging.ERROR)
            return False

        # Kill the daemon process
        try:
            while 1:
                os.kill(self.pid, signal.SIGTERM)
                time.sleep(0.1)
        except OSError as err:
            strerror = err.strerror
            if err.errno == 3:  # No such process
                if os.path.exists(self.pidfile):
                    self.delete_pidfile()
            else:
                msg = "Unable to delete pidfile: {0}".format(strerror)
                self._report(msg, logging.ERROR)
                raise UtilDaemonError(msg)

        return True

    def restart(self):
        """Restarts the daemon.

        It will execute a stop and start on the daemon.
        """
        self.stop()
        return self.start()

    def daemonize(self):
        """Creates the daemon.

        It will fork a child process and then exit parent. By performing a
        double fork, set the current process's user id, change the current
        working directory, set the current numeric umask, redirect standard
        streams and write the pid to a file.
        """
        def redirect_stream(system_stream, target_stream):
            """Redirect a system stream to a specified file.
            """
            if target_stream is None:
                target_f = os.open(os.devnull, os.O_RDWR)
            else:
                target_f = target_stream.fileno()
            os.dup2(target_f, system_stream.fileno())

        def fork_then_exit_parent(error_message):
            """Fork a child process, then exit the parent process.
            """
            try:
                pid = os.fork()
                if pid > 0:
                    os._exit(0)  # pylint: disable=W0212
            except OSError as err:
                msg = "{0}: [{1}] {2}".format(error_message, err.errno,
                                              err.strerror)
                self._report(msg, logging.CRITICAL)
                raise UtilDaemonError(msg)

        # Fork
        fork_then_exit_parent("Failed first fork.")

        try:
            os.setsid()
            os.chdir(self.chdir)
            os.umask(self.umask)
        except Exception as err:
            msg = "Unable to change directory ({0})".format(err)
            self._report(msg, logging.CRITICAL)
            raise UtilDaemonError(msg)

        # Double fork
        fork_then_exit_parent("Failed second fork.")

        # Redirect streams
        redirect_stream(sys.stdin, self.stdin)
        redirect_stream(sys.stdout, self.stdout)
        redirect_stream(sys.stderr, self.stderr)

        # Call a cleanup task to unregister the master.
        atexit.register(self.cleanup)
        # write pidfile
        atexit.register(self.delete_pidfile)
        pid = str(os.getpid())
        try:
            with open(self.pidfile, "w") as f:
                f.write("{0}\n".format(pid))
        except IOError as err:
            msg = "Unable to write pidfile: {0}".format(err.strerror)
            self._report(msg, logging.CRITICAL)
            raise UtilDaemonError(msg)

    def delete_pidfile(self):
        """Deletes pidfile.
        """
        try:
            os.remove(self.pidfile)
        except (OSError, IOError) as err:
            msg = "Unable to delete pidfile: {0}".format(err.strerror)
            self._report(msg, logging.ERROR)
            raise UtilDaemonError(msg)
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains abstractions of a MySQL Database object used by
multiple utilities.
"""

import multiprocessing
import os
import re
import sys

from collections import deque


# List of database objects for enumeration
_DATABASE, _TABLE, _VIEW, _TRIG, _PROC, _FUNC, _EVENT, _GRANT = "DATABASE", \
    "TABLE", "VIEW", "TRIGGER", "PROCEDURE", "FUNCTION", "EVENT", "GRANT"

_OBJTYPE_QUERY = """
    (
       SELECT TABLE_TYPE as object_type
       FROM INFORMATION_SCHEMA.TABLES
       WHERE TABLES.TABLE_SCHEMA = '%(db_name)s' AND
         TABLES.TABLE_NAME = '%(obj_name)s'
    )
    UNION
    (
        SELECT 'TRIGGER' as object_type
        FROM INFORMATION_SCHEMA.TRIGGERS
        WHERE TRIGGER_SCHEMA = '%(db_name)s' AND
          TRIGGER_NAME = '%(obj_name)s'
    )
    UNION
    (
        SELECT TYPE as object_type
        FROM mysql.proc
        WHERE DB = '%(db_name)s' AND NAME = '%(obj_name)s'
    )
    UNION
    (
        SELECT 'EVENT' as object_type
        FROM mysql.event
        WHERE DB = '%(db_name)s' AND NAME = '%(obj_name)s'
    )
"""

_DEFINITION_QUERY = """
  SELECT %(columns)s
  FROM INFORMATION_SCHEMA.%(table_name)s WHERE %(conditions)s
"""

_PARTITION_QUERY = """
  SELECT PARTITION_NAME, SUBPARTITION_NAME, PARTITION_ORDINAL_POSITION,
         SUBPARTITION_ORDINAL_POSITION, PARTITION_METHOD, SUBPARTITION_METHOD,
         PARTITION_EXPRESSION, SUBPARTITION_EXPRESSION, PARTITION_DESCRIPTION
  FROM INFORMATION_SCHEMA.PARTITIONS
  WHERE TABLE_SCHEMA = '%(db)s' AND TABLE_NAME = '%(name)s'
"""

_COLUMN_QUERY = """
  SELECT ORDINAL_POSITION, COLUMN_NAME, COLUMN_TYPE, IS_NULLABLE,
         COLUMN_DEFAULT, EXTRA, COLUMN_COMMENT, COLUMN_KEY
  FROM INFORMATION_SCHEMA.COLUMNS
  WHERE TABLE_SCHEMA = '%(db)s' AND TABLE_NAME = '%(name)s'
"""

_FK_CONSTRAINT_QUERY = """
SELECT TABLE_NAME, CONSTRAINT_NAME, COLUMN_NAME, REFERENCED_TABLE_SCHEMA,
REFERENCED_TABLE_NAME, REFERENCED_COLUMN_NAME, UPDATE_RULE, DELETE_RULE
FROM INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS
JOIN INFORMATION_SCHEMA.KEY_COLUMN_USAGE
USING (CONSTRAINT_SCHEMA, CONSTRAINT_NAME, TABLE_NAME, REFERENCED_TABLE_NAME)
WHERE CONSTRAINT_SCHEMA = '{DATABASE!s}'
AND TABLE_NAME = '{TABLE!s}'
"""

_ALTER_TABLE_ADD_FK_CONSTRAINT = """
ALTER TABLE {DATABASE!s}.{TABLE!s} add CONSTRAINT `{CONSTRAINT_NAME!s}`
FOREIGN KEY (`{COLUMN_NAMES}`)
REFERENCES `{REFERENCED_DATABASE}`.`{REFERENCED_TABLE!s}`
(`{REFERENCED_COLUMNS!s}`)
ON UPDATE {UPDATE_RULE}
ON DELETE {DELETE_RULE}
"""


def _multiprocess_tbl_copy_task(copy_tbl_task):
    """Multiprocess copy table data method.

    This method wraps the copy of the table's data to allow its concurrent
    execution by a pool of processes.

    copy_tbl_task[in]   dictionary of values required by a process to
                        perform the table copy task, namely:
                        'source_srv': <dict with source connections values>,
                        'dest_srv': <dict with destination connections values>,
                        'source_db': <source database name>,
                        'destination_db': <destination database name>,
                        'table': <table to copy>,
                        'options': <dict of options>,
                        'cloning': <cloning flag>,
                        'connections': <number of concurrent connections>,
                        'q_source_db': <quoted source database name>.
    """
    # Get input to execute task.
    source_srv = copy_tbl_task.get('source_srv')
    dest_srv = copy_tbl_task.get('dest_srv')
    source_db = copy_tbl_task.get('source_db')
    target_db = copy_tbl_task.get('target_db')
    table = copy_tbl_task.get('table')
    options = copy_tbl_task.get('options')
    cloning = copy_tbl_task.get('cloning')
    # Execute copy table task.
    # NOTE: Must handle any exception here, because worker processes will not
    # propagate them to the main process.
    try:
        _copy_table_data(source_srv, dest_srv, source_db, target_db, table,
                         options, cloning)
    except UtilError:
        _, err, _ = sys.exc_info()
        print("ERROR copying data for table '{0}': {1}".format(table,
                                                               err.errmsg))


def _copy_table_data(source_srv, destination_srv, db_name, new_db_name,
                     tbl_name, tbl_options, cloning, connections=1):
    """Copy the data of the specified table.

    This method copies/clones all the data from a table to another (new)
    database.

    source_srv[in]      Source server (Server instance or dict. with the
                        connection values).
    destination_srv[in] Destination server (Server instance or dict. with the
                        connection values).
    db_name[in]         Name of the database with the table to copy.
    new_db_name[in]     Name of the destination database to copy the table.
    tbl_name[in]        Name of the table to copy.
    tbl_options[in]     Table options.
    cloning[in]         Cloning flag, in order to use a different method to
                        copy data on the same server
    connections[in]     Specify the use of multiple connections/processes to
                        copy the table data (rows). By default, only 1 used.
                        Note: Multiprocessing option should be preferred.
    """
    # Import table needed here to avoid circular import issues.
    from mysql.utilities.common.table import Table
    # Handle source and destination server instances or connection values.
    # Note: For multiprocessing the use of connection values instead of a
    # server instance is required to avoid internal errors.
    if isinstance(source_srv, Server):
        source = source_srv
    else:
        # Get source server instance from connection values.
        conn_options = {
            'quiet': True,  # Avoid repeating output for multiprocessing.
            'version': "5.1.30",
        }
        servers = connect_servers(source_srv, None, conn_options)
        source = servers[0]
    if isinstance(destination_srv, Server):
        destination = destination_srv
    else:
        # Get source server instance from connection values.
        conn_options = {
            'quiet': True,  # Avoid repeating output for multiprocessing.
            'version': "5.1.30",
        }
        servers = connect_servers(destination_srv, None, conn_options)
        destination = servers[0]

    # Copy table data.
    if not tbl_options.get("quiet", False):
        print("# Copying data for TABLE {0}.{1}".format(db_name,
                                                        tbl_name))
    source_sql_mode = source.select_variable("SQL_MODE")
    q_tbl_name = "{0}.{1}".format(quote_with_backticks(db_name,
                                                       source_sql_mode),
                                  quote_with_backticks(tbl_name,
                                                       source_sql_mode))
    tbl = Table(source, q_tbl_name, tbl_options)
    if tbl is None:
        raise UtilDBError("Cannot create table object before copy.", -1,
                          db_name)
    tbl.copy_data(destination, cloning, new_db_name, connections)


class Database(object):
    """
    The Database class encapsulates a database. The class has the following
    capabilities:

        - Check to see if the database exists
        - Drop the database
        - Create the database
        - Clone the database
        - Print CREATE statements for all objects
    """
    obj_type = _DATABASE

    def __init__(self, source, name, options=None):
        """Constructor

        source[in]         A Server object
        name[in]           Name of database
        verbose[in]        print extra data during operations (optional)
                           default value = False
        options[in]        Array of options for controlling what is included
                           and how operations perform (e.g., verbose)
        """
        if options is None:
            options = {}
        self.source = source
        # Get the SQL_MODE set on the source
        self.sql_mode = self.source.select_variable("SQL_MODE")
        # Keep database identifier considering backtick quotes
        if is_quoted_with_backticks(name, self.sql_mode):
            self.q_db_name = name
            self.db_name = remove_backtick_quoting(self.q_db_name,
                                                   self.sql_mode)
        else:
            self.db_name = name
            self.q_db_name = quote_with_backticks(self.db_name,
                                                  self.sql_mode)
        self.verbose = options.get("verbose", False)
        self.skip_tables = options.get("skip_tables", False)
        self.skip_views = options.get("skip_views", False)
        self.skip_triggers = options.get("skip_triggers", False)
        self.skip_procs = options.get("skip_procs", False)
        self.skip_funcs = options.get("skip_funcs", False)
        self.skip_events = options.get("skip_events", False)
        self.skip_grants = options.get("skip_grants", False)
        self.skip_create = options.get("skip_create", False)
        self.skip_data = options.get("skip_data", False)
        self.exclude_patterns = options.get("exclude_patterns", None)
        self.use_regexp = options.get("use_regexp", False)
        self.skip_table_opts = options.get("skip_table_opts", False)
        self.new_db = None
        self.q_new_db = None
        self.init_called = False
        self.destination = None  # Used for copy mode
        self.cloning = False    # Used for clone mode
        self.query_options = {  # Used for skipping buffered fetch of rows
            'fetch': False,
            'commit': False,  # No COMMIT needed for DDL operations (default).
        }
        # Used to store constraints to execute
        # after table creation, deque is
        # thread-safe
        self.constraints = deque()

        self.objects = []
        self.new_objects = []

    def exists(self, server=None, db_name=None):
        """Check to see if the database exists

        server[in]         A Server object
                           (optional) If omitted, operation is performed
                           using the source server connection.
        db_name[in]        database name
                           (optional) If omitted, operation is performed
                           on the class instance table name.

        return True = database exists, False = database does not exist
        """

        if not server:
            server = self.source
        db = None
        if db_name:
            db = db_name
        else:
            db = self.db_name

        _QUERY = """
            SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA
            WHERE SCHEMA_NAME = '%s'
        """
        res = server.exec_query(_QUERY % db)
        return (res is not None and len(res) >= 1)

    def drop(self, server, quiet, db_name=None):
        """Drop the database

        server[in]         A Server object
        quiet[in]          ignore error on drop
        db_name[in]        database name
                           (optional) If omitted, operation is performed
                           on the class instance table name.

        return True = database successfully dropped, False = error
        """

        db = None
        # Get the SQL_MODE set on the server
        sql_mode = server.select_variable("SQL_MODE")
        if db_name:
            db = db_name if is_quoted_with_backticks(db_name, sql_mode) \
                else quote_with_backticks(db_name, sql_mode)
        else:
            db = self.q_db_name
        op_ok = False
        if quiet:
            try:
                server.exec_query("DROP DATABASE %s" % (db),
                                  self.query_options)
                op_ok = True
            except:
                pass
        else:
            server.exec_query("DROP DATABASE %s" % (db),
                              self.query_options)
            op_ok = True
        return op_ok

    def create(self, server, db_name=None, charset_name=None,
               collation_name=None):
        """Create the database

        server[in]         A Server object
        db_name[in]        database name
                           (optional) If omitted, operation is performed
                           on the class instance table name.

        return True = database successfully created, False = error
        """
        # Get the SQL_MODE set on the server
        sql_mode = server.select_variable("SQL_MODE")
        if db_name:
            db = db_name if is_quoted_with_backticks(db_name, sql_mode) \
                else quote_with_backticks(db_name, sql_mode)
        else:
            db = self.q_db_name

        specification = ""
        if charset_name:
            specification = " DEFAULT CHARACTER SET {0}".format(charset_name)
        if collation_name:
            specification = "{0} DEFAULT COLLATE {1}".format(specification,
                                                             collation_name)
        query_create_db = "CREATE DATABASE {0} {1}".format(db, specification)
        server.exec_query(query_create_db, self.query_options)

        return True

    def __make_create_statement(self, obj_type, obj):
        """Construct a CREATE statement for a database object.

        This method will get the CREATE statement from the method
        get_create_statement() and also replace all occurrances of the
        old database name with the new.

        obj_type[in]       Object type (string) e.g. DATABASE
        obj[in]            A row from the get_db_objects() method
                           that contains the elements of the object

        Note: This does not work for tables.

        Returns the CREATE string
        """

        if not self.new_db:
            self.new_db = self.db_name
            self.q_new_db = self.q_db_name
        create_str = None
        # Tables are not supported
        if obj_type == _TABLE and self.cloning:
            return None
        # Grants are a different animal!
        if obj_type == _GRANT:
            if obj[3]:
                create_str = "GRANT %s ON %s.%s TO %s" % \
                             (obj[1], self.q_new_db, obj[3], obj[0])
            else:
                create_str = "GRANT %s ON %s.* TO %s" % \
                             (obj[1], self.q_new_db, obj[0])
        else:
            create_str = self.get_create_statement(self.db_name,
                                                   obj[0], obj_type)
            if self.new_db != self.db_name:
                # Replace the occurrences of the old database name (quoted with
                # backticks) with the new one when preceded by: a whitespace
                # character, comma or optionally a left parentheses.
                create_str = re.sub(
                    r"(\s|,)(\(?){0}\.".format(self.q_db_name),
                    r"\1\2{0}.".format(self.q_new_db),
                    create_str
                )
                # Replace the occurrences of the old database name (without
                # backticks) with the new one when preceded by: a whitespace
                # character, comma or optionally a left parentheses and
                # surrounded by single or double quotes.
                create_str = re.sub(
                    r"(\s|,)(\(?)(\"|\'?){0}(\"|\'?)\.".format(self.db_name),
                    r"\1\2\3{0}\4.".format(self.new_db),
                    create_str
                )
        return create_str

    def _get_views_sorted_by_dependencies(self, views, columns,
                                          need_backtick=True):
        """Get a list of views sorted by their dependencies.

        views[in]          List of views objects
        columns[in]        Column mode - names (default), brief, or full
        need_backtick[in]  True if view need backticks in the name

        Returns the list of view sorted by their dependencies
        """
        if columns == "names":
            name_idx = 0
        elif columns == "full":
            name_idx = 2
        else:
            name_idx = 1

        def _get_dependent_views(view, v_name_dict):
            """Get a list with all the dependent views for a given view
            view          [in]  current view being analyzed
            v_name_dict   [in]  mapping from short view names to used view_stm
            """
            # Get view name and use backticks if necessary
            v_name = view[name_idx]
            if need_backtick:
                v_name = quote_with_backticks(v_name, self.sql_mode)

            # Get view create statement and for each view in views_to_check
            # see if it is mentioned in the statement
            stmt = self.get_create_statement(self.db_name, v_name, _VIEW)
            base_views = []
            for v in v_name_dict:
                # No looking for itself
                if v != v_name:
                    # split off the from clause
                    # strip WHERE, ORDER BY, and GROUP BY
                    try:
                        from_clause = stmt.rsplit('from', 1)[1]
                        from_clause = from_clause.split('WHERE', 1)[0]
                    except:
                        from_clause = None
                    if from_clause:
                        index = from_clause.find(v)
                    else:
                        index = stmt.find(v)
                    if index >= 0:
                        base_views.append(v_name_dict[v])
            return base_views

        def build_view_deps(view_lst):
            """Get a list of views sorted by their dependencies.

            view_lst   [in]   list with views yet to to be ordered

            Returns the list of view sorted by their dependencies
            """
            # Mapping from view_names to views(brief, name or full)
            v_name_dict = {}
            for view in view_lst:
                key = quote_with_backticks(view[name_idx], self.sql_mode) if \
                    need_backtick else view[name_idx]
                v_name_dict[key] = view

            # Initialize sorted_tpl
            sorted_views = []
            # set with view whose dependencies were/are being analyzed.key
            visited_views = set()

            # set with views that have already been processed
            # (subset of processed_views). Contains the same elements as
            # sorted_views.
            processed_views = set()

            # Init stack
            view_stack = view_lst[:]
            while view_stack:
                curr_view = view_stack[-1]  # look at top of the stack
                if curr_view in visited_views:
                    view_stack.pop()
                    if curr_view not in processed_views:
                        sorted_views.append(curr_view)
                        processed_views.add(curr_view)
                else:
                    visited_views.add(curr_view)
                    children_views = _get_dependent_views(curr_view,
                                                          v_name_dict)
                    if children_views:
                        for child in children_views:
                            # store not yet processed base views the temp stack
                            if child not in processed_views:
                                view_stack.append(child)
            # No more views on the stack, return list of sorted views
            return sorted_views
        # Returns without columns names
        if isinstance(views[0], tuple):
            return build_view_deps(views)

        # Returns the tuple reconstructed with views sorted
        return (views[0], build_view_deps(views[1]),)

    def __add_db_objects(self, obj_type):
        """Get a list of objects from a database based on type.

        This method retrieves the list of objects for a specific object
        type and adds it to the class' master object list.

        obj_type[in]       Object type (string) e.g. DATABASE
        """

        rows = self.get_db_objects(obj_type)
        if rows:
            for row in rows:
                tup = (obj_type, row)
                self.objects.append(tup)

    def init(self):
        """Get all objects for the database based on options set.

        This method initializes the database object with a list of all
        objects except those object types that are excluded. It calls
        the helper method self.__add_db_objects() for each type of
        object.

        NOTE: This method must be called before the copy method. A
              guard is in place to ensure this.
        """
        self.init_called = True
        # Get tables
        if not self.skip_tables:
            self.__add_db_objects(_TABLE)
        # Get functions
        if not self.skip_funcs:
            self.__add_db_objects(_FUNC)
        # Get stored procedures
        if not self.skip_procs:
            self.__add_db_objects(_PROC)
        # Get views
        if not self.skip_views:
            self.__add_db_objects(_VIEW)
        # Get triggers
        if not self.skip_triggers:
            self.__add_db_objects(_TRIG)
        # Get events
        if not self.skip_events:
            self.__add_db_objects(_EVENT)
        # Get grants
        if not self.skip_grants:
            self.__add_db_objects(_GRANT)

    def __drop_object(self, obj_type, name):
        """Drop a database object.

        Attempts a quiet drop of a database object (no errors are
        printed).

        obj_type[in]       Object type (string) e.g. DATABASE
        name[in]           Name of the object
        """

        if self.verbose:
            print "# Dropping new object %s %s.%s" % \
                  (obj_type, self.new_db, name)
        drop_str = "DROP %s %s.%s" % \
                   (obj_type, self.q_new_db, name)
        # Suppress the error on drop
        if self.cloning:
            try:
                self.source.exec_query(drop_str, self.query_options)
            except UtilError:
                if self.verbose:
                    print("# WARNING: Unable to drop {0} from {1} database "
                          "(object may not exist): {2}".format(name,
                                                               "source",
                                                               drop_str))
        else:
            try:
                self.destination.exec_query(drop_str, self.query_options)
            except UtilError:
                if self.verbose:
                    print("# WARNING: Unable to drop {0} from {1} database "
                          "(object may not exist): {2}".format(name,
                                                               "destination",
                                                               drop_str))

    def __create_object(self, obj_type, obj, show_grant_msg,
                        quiet=True, new_engine=None, def_engine=None):
        """Create a database object.

        obj_type[in]       Object type (string) e.g. DATABASE
        obj[in]            A row from the get_db_object_names() method
                           that contains the elements of the object
        show_grant_msg[in] If true, display diagnostic information
        quiet[in]          do not print informational messages
        new_engine[in]     Use this engine if not None for object
        def_engine[in]     If target storage engine doesn't exist, use
                           this engine.

        Note: will handle exception and print error if query fails
        """
        # Use the sql_mode set on destination server
        dest_sql_mode = self.destination.select_variable("SQL_MODE")
        q_new_db = quote_with_backticks(self.new_db, dest_sql_mode)
        q_db_name = quote_with_backticks(self.db_name, dest_sql_mode)
        if obj_type == _TABLE and self.cloning:
            obj_name = quote_with_backticks(obj[0], dest_sql_mode)
            create_list = [
                "CREATE TABLE {0!s}.{1!s} LIKE {2!s}.{1!s}"
                "".format(q_new_db, obj_name, q_db_name)
            ]
        else:
            create_list = [self.__make_create_statement(obj_type, obj)]
        if obj_type == _TABLE:
            may_skip_fk = False  # Check possible issues with FK Constraints
            obj_name = quote_with_backticks(obj[0], dest_sql_mode)
            tbl_name = "%s.%s" % (self.q_new_db, obj_name)
            create_list = self.destination.substitute_engine(tbl_name,
                                                             create_list[0],
                                                             new_engine,
                                                             def_engine,
                                                             quiet)

            # Get storage engines from the source table and destination table
            # If the source table's engine is INNODB and the destination is
            # not we will loose any FK constraints that may exist
            src_eng = self.get_object_definition(self.q_db_name,
                                                 obj[0], obj_type)[0][0][2]
            dest_eng = None

            # Information about the engine is always in the last statement of
            # the list, be it a regular create table statement or a create
            # table; alter table statement.
            i = create_list[-1].find("ENGINE=")
            if i > 0:
                j = create_list[-1].find(" ", i)
                dest_eng = create_list[-1][i + 7:j]
            dest_eng = dest_eng or src_eng

            if src_eng.upper() == 'INNODB' and dest_eng.upper() != 'INNODB':
                may_skip_fk = True

        string = "# Copying"
        if not quiet:
            if obj_type == _GRANT:
                if show_grant_msg:
                    print "%s GRANTS from %s" % (string, self.db_name)
            else:
                print "%s %s %s.%s" % \
                      (string, obj_type, self.db_name, obj[0])
            if self.verbose:
                print("; ".join(create_list))

        try:
            self.destination.exec_query("USE %s" % self.q_new_db,
                                        self.query_options)
        except:
            pass
        for stm in create_list:
            try:
                if obj_type == _GRANT:
                    user = User(self.destination, obj[0])
                    if not user.exists():
                        user.create()
                self.destination.exec_query(stm, self.query_options)
            except UtilDBError as e:
                raise UtilDBError("Cannot operate on {0} object."
                                  " Error: {1}".format(obj_type, e.errmsg),
                                  -1, self.db_name)

        # Look for foreign key constraints
        if obj_type == _TABLE:
            params = {
                'DATABASE': self.db_name,
                'TABLE': obj[0],
            }
            try:
                query = _FK_CONSTRAINT_QUERY.format(**params)
                fkey_constr = self.source.exec_query(query)
            except UtilDBError as e:
                raise UtilDBError("Unable to obtain Foreign Key constraint "
                                  "information for table {0}.{1}. "
                                  "Error: {2}".format(self.db_name, obj[0],
                                                      e.errmsg), -1,
                                  self.db_name)

            # Get information about the foreign keys of the table being
            # copied/cloned.
            if fkey_constr and not may_skip_fk:

                # Create a constraint dictionary with the constraint
                # name as key
                constr_dict = {}

                # This list is used to ensure the same constraints are applied
                # in the same order, because iterating the dictionary doesn't
                # offer any guarantees regarding order, and Python 2.6 has
                # no ordered_dict
                constr_lst = []

                for fkey in fkey_constr:
                    params = constr_dict.get(fkey[1])
                    # in case the constraint entry already exists, it means it
                    # is composite, just update the columns names and
                    # referenced column fields
                    if params:
                        params['COLUMN_NAMES'].append(fkey[2])
                        params['REFERENCED_COLUMNS'].append(fkey[5])
                    else:  # else create a new entry
                        constr_lst.append(fkey[1])
                        constr_dict[fkey[1]] = {
                            'DATABASE': self.new_db,
                            'TABLE': fkey[0],
                            'CONSTRAINT_NAME': fkey[1],
                            'COLUMN_NAMES': [fkey[2]],
                            'REFERENCED_DATABASE': fkey[3],
                            'REFERENCED_TABLE': fkey[4],
                            'REFERENCED_COLUMNS': [fkey[5]],
                            'UPDATE_RULE': fkey[6],
                            'DELETE_RULE': fkey[7],
                        }
                # Iterate all the constraints and get the necessary parameters
                # to create the query
                for constr in constr_lst:
                    params = constr_dict[constr]
                    if self.cloning:  # if it is a cloning table operation

                        # In case the foreign key is composite we need to join
                        # the columns to use in in alter table query. Only
                        # useful when cloning
                        params['COLUMN_NAMES'] = '`,`'.join(
                            params['COLUMN_NAMES'])
                        params['REFERENCED_COLUMNS'] = '`,`'.join(
                            params['REFERENCED_COLUMNS'])

                        # If the foreign key points to a table under the
                        # database being cloned, change the referenced database
                        #  name to the new cloned database
                        if params['REFERENCED_DATABASE'] == self.db_name:
                            params['REFERENCED_DATABASE'] = self.new_db
                        else:
                            print("# WARNING: The database being cloned has "
                                  "external Foreign Key constraint "
                                  "dependencies, {0}.{1} depends on {2}."
                                  "{3}".format(params['DATABASE'],
                                               params['TABLE'],
                                               params['REFERENCED_DATABASE'],
                                               params['REFERENCED_TABLE']))
                        query = _ALTER_TABLE_ADD_FK_CONSTRAINT.format(**params)

                        # Store constraint query for later execution
                        self.constraints.append(query)
                        if self.verbose:
                            print(query)
                    else:  # if we are copying
                        if params['REFERENCED_DATABASE'] != self.db_name:
                            # if the table being copied has dependencies
                            # to external databases
                            print("# WARNING: The database being copied has "
                                  "external Foreign Key constraint "
                                  "dependencies, {0}.{1} depends on {2}."
                                  "{3}".format(params['DATABASE'],
                                               params['TABLE'],
                                               params['REFERENCED_DATABASE'],
                                               params['REFERENCED_TABLE']))
            elif fkey_constr and may_skip_fk:
                print("# WARNING: FOREIGN KEY constraints for table {0}.{1} "
                      "are missing because the new storage engine for "
                      "the table is not InnoDB".format(self.new_db, obj[0]))

    def __apply_constraints(self):
        """This method applies to the database the constraints stored in the
        self.constraints instance variable
        """

        # Enable Foreign Key Checks to prevent the swapping of
        # RESTRICT referential actions with NO ACTION
        query_opts = {'fetch': False, 'commit': False}
        self.destination.exec_query("SET FOREIGN_KEY_CHECKS=1", query_opts)

        # while constraint queue is not empty
        while self.constraints:
            try:
                query = self.constraints.pop()
            except IndexError:
                # queue is empty, exit while statement
                break
            if self.verbose:
                print(query)
            try:
                self.destination.exec_query(query, query_opts)
            except UtilDBError as err:
                raise UtilDBError("Unable to execute constraint query "
                                  "{0}. Error: {1}".format(query, err.errmsg),
                                  -1, self.new_db)

        # Turn Foreign Key Checks off again
        self.destination.exec_query("SET FOREIGN_KEY_CHECKS=0", query_opts)

    def copy_objects(self, new_db, options, new_server=None,
                     connections=1, check_exists=True):
        """Copy the database objects.

        This method will copy a database and all of its objects and data
        to another, new database. Options set at instantiation will determine
        if there are objects that are excluded from the copy. Likewise,
        the method will also skip data if that option was set and process
        an input file with INSERT statements if that option was set.

        The method can also be used to copy a database to another server
        by providing the new server object (new_server). Copy to the same
        name by setting new_db = old_db or as a new database.

        new_db[in]         Name of the new database
        options[in]        Options for copy e.g. do_drop, etc.
        new_server[in]     Connection to another server for copying the db
                           Default is None (copy to same server - clone)
        connections[in]    Number of threads(connections) to use for insert
        check_exists[in]   If True, check for database existence before copy
                           Default is True
        """

        # Must call init() first!
        # Guard for init() prerequisite
        assert self.init_called, "You must call db.init() before " + \
                                 "db.copy_objects()."

        grant_msg_displayed = False

        # Get sql_mode in new_server
        sql_mode = new_server.select_variable("SQL_MODE")

        if new_db:
            # Assign new database identifier considering backtick quotes.
            if is_quoted_with_backticks(new_db, sql_mode):
                self.q_new_db = new_db
                self.new_db = remove_backtick_quoting(new_db, sql_mode)
            else:
                self.new_db = new_db
                self.q_new_db = quote_with_backticks(new_db, sql_mode)
        else:
            # If new_db is not defined use the same as source database.
            self.new_db = self.db_name
            self.q_new_db = self.q_db_name

        self.destination = new_server

        # We know we're cloning if there is no new connection.
        self.cloning = (new_server == self.source)

        if self.cloning:
            self.destination = self.source

        # Check to see if database exists
        if check_exists:
            if self.cloning:
                exists = self.exists(self.source, new_db)
                drop_server = self.source
            else:
                exists = self.exists(self.destination, new_db)
                drop_server = self.destination
            if exists:
                if options.get("do_drop", False):
                    self.drop(drop_server, True, new_db)
                elif not self.skip_create:
                    raise UtilDBError("destination database exists. Use "
                                      "--drop-first to overwrite existing "
                                      "database.", -1, new_db)

        db_name = self.db_name
        definition = self.get_object_definition(db_name, db_name, _DATABASE)
        _, character_set, collation, _ = definition[0]
        # Create new database first
        if not self.skip_create:
            if self.cloning:
                self.create(self.source, new_db, character_set,
                            collation)
            else:
                self.create(self.destination, new_db, character_set,
                            collation)

        # Get sql_mode set on destination server
        dest_sql_mode = self.destination.select_variable("SQL_MODE")

        # Create the objects in the new database
        # Save any views that fail due to dependencies
        dependent_views = []
        for obj in self.objects:
            # Drop object if --drop-first specified and database not dropped
            # Grants do not need to be dropped for overwriting
            if options.get("do_drop", False) and obj[0] != _GRANT:
                obj_name = quote_with_backticks(obj[1][0], dest_sql_mode)
                self.__drop_object(obj[0], obj_name)

            # Attempt to create the object.
            try:
                # Create the object
                self.__create_object(obj[0], obj[1], not grant_msg_displayed,
                                     options.get("quiet", False),
                                     options.get("new_engine", None),
                                     options.get("def_engine", None))
            except UtilDBError as err:
                # If this is a view and it fails dependency checking, save
                # it and retry the view later, but only if we're not skipping
                # tables.
                if (obj[0] == _VIEW and "doesn't exist" in err.errmsg and
                        not self.skip_tables):
                    dependent_views.append(obj)
                else:
                    raise err

            if obj[0] == _GRANT and not grant_msg_displayed:
                grant_msg_displayed = True

        # Now retry the views
        if self.verbose and len(dependent_views) > 0:
            print("# Attempting to create views that failed dependency "
                  "checks on first pass.")
        for obj in dependent_views:
            # Drop object if --drop-first specified and database not dropped
            if self.verbose:
                print("#  Retrying view {0}".format(obj[1]))
            if options.get("do_drop", False):
                obj_name = quote_with_backticks(obj[1][0], dest_sql_mode)
                self.__drop_object(obj[0], obj_name)

            # Create the object
            self.__create_object(obj[0], obj[1], not grant_msg_displayed,
                                 options.get("quiet", False),
                                 options.get("new_engine", None),
                                 options.get("def_engine", None))

        # After object creation, add the constraints
        if self.constraints:
            self.__apply_constraints()

    def copy_data(self, new_db, options, new_server=None, connections=1,
                  src_con_val=None, dest_con_val=None):
        """Copy the data for the tables.

        This method will copy the data for all of the tables to another, new
        database. The method will process an input file with INSERT statements
        if the option was selected by the caller.

        new_db[in]          Name of the new database
        options[in]         Options for copy e.g. do_drop, etc.
        new_server[in]      Connection to another server for copying the db
                            Default is None (copy to same server - clone)
        connections[in]     Number of threads(connections) to use for insert
        src_con_val[in]     Dict. with the connection values of the source
                            server (required for multiprocessing).
        dest_con_val[in]    Dict. with the connection values of the
                            destination server (required for multiprocessing).
        """

        # Must call init() first!
        # Guard for init() prerequisite
        assert self.init_called, "You must call db.init() before " + \
                                 "db.copy_data()."

        if self.skip_data:
            return

        self.destination = new_server

        # We know we're cloning if there is no new connection.
        self.cloning = (new_server == self.source)

        if self.cloning:
            self.destination = self.source

        quiet = options.get("quiet", False)

        tbl_options = {
            'verbose': self.verbose,
            'get_cols': True,
            'quiet': quiet
        }

        copy_tbl_tasks = []
        table_names = [obj[0] for obj in self.get_db_objects(_TABLE)]
        for tblname in table_names:
            # Check multiprocess table copy (only on POSIX systems).
            if options['multiprocess'] > 1 and os.name == 'posix':
                # Create copy task.
                copy_task = {
                    'source_srv': src_con_val,
                    'dest_srv': dest_con_val,
                    'source_db': self.db_name,
                    'target_db': new_db,
                    'table': tblname,
                    'options': tbl_options,
                    'cloning': self.cloning,
                }
                copy_tbl_tasks.append(copy_task)
            else:
                # Copy data from a table (no multiprocessing).
                _copy_table_data(self.source, self.destination, self.db_name,
                                 new_db, tblname, tbl_options, self.cloning)

        # Copy tables concurrently.
        if copy_tbl_tasks:
            # Create process pool.
            workers_pool = multiprocessing.Pool(
                processes=options['multiprocess']
            )
            # Concurrently export tables.
            workers_pool.map_async(_multiprocess_tbl_copy_task, copy_tbl_tasks)
            workers_pool.close()
            # Wait for all task to be completed by workers.
            workers_pool.join()

    def get_create_statement(self, db, name, obj_type):
        """Return the create statement for the object

        db[in]             Database name
        name[in]           Name of the object
        obj_type[in]       Object type (string) e.g. DATABASE
                           Note: this is used to form the correct SHOW command

        Returns create statement
        """
        # Save current sql_mode and switch it to '' momentarily as this
        # prevents issues when copying blobs and destination server is
        # set with SQL_MODE='NO_BACKSLASH_ESCAPES'
        prev_sql_mode = ''
        if (self.destination is not None and 'ANSI_QUOTES' in self.sql_mode and
                'ANSI_QUOTES' not in
                self.destination.select_variable("SQL_MODE")):
            prev_sql_mode = self.source.select_variable("SQL_MODE")
            self.source.exec_query("SET @@SESSION.SQL_MODE=''")
            self.sql_mode = ""
            # Quote with current sql_mode
            name = (name if not is_quoted_with_backticks(name, prev_sql_mode)
                    else remove_backtick_quoting(name, prev_sql_mode))
            db = (db if not is_quoted_with_backticks(db, prev_sql_mode)
                  else remove_backtick_quoting(db, prev_sql_mode))
        # Quote database and object name with backticks.
        q_name = (name if is_quoted_with_backticks(name, self.sql_mode)
                  else quote_with_backticks(name, self.sql_mode))
        if obj_type == _DATABASE:
            name_str = q_name
        else:
            q_db = (db if is_quoted_with_backticks(db, self.sql_mode)
                    else quote_with_backticks(db, self.sql_mode))

            # Switch the default database to execute the
            # SHOW CREATE statement without needing to specify the database
            # This is for 5.1 compatibility reasons:
            try:
                self.source.exec_query("USE {0}".format(q_db),
                                       self.query_options)
            except UtilError as err:
                raise UtilDBError("ERROR: Couldn't change "
                                  "default database: {0}".format(err.errmsg))
        name_str = q_name

        # Retrieve the CREATE statement.
        row = self.source.exec_query(
            "SHOW CREATE {0} {1}".format(obj_type, name_str)
        )

        # Restore previews sql_mode
        if prev_sql_mode:
            self.source.exec_query("SET @@SESSION.SQL_MODE={0}"
                                   "".format(prev_sql_mode))
            self.sql_mode = prev_sql_mode

        create_statement = None
        if row:
            if obj_type == _TABLE or obj_type == _VIEW or \
               obj_type == _DATABASE:
                create_statement = row[0][1]
            elif obj_type == _EVENT:
                create_statement = row[0][3]
            else:
                create_statement = row[0][2]

        # Remove all table options from the CREATE statement (if requested).
        if self.skip_table_opts and obj_type == _TABLE:
            # First, get partition options.
            create_tbl, sep, part_opts = create_statement.rpartition('\n/*')
            # Handle situation where no partition options are found.
            if not create_tbl:
                create_tbl = part_opts
                part_opts = ''
            else:
                part_opts = "{0}{1}".format(sep, part_opts)
            # Then, separate table definitions from table options.
            create_tbl, sep, _ = create_tbl.rpartition(') ')
            # Reconstruct CREATE statement without table options.
            create_statement = "{0}{1}{2}".format(create_tbl, sep, part_opts)

        return create_statement

    def get_create_table(self, db, table):
        """Return the create table statement for the given table.

        This method returns the CREATE TABLE statement for the given table with
        or without the table options, according to the Database object
        property 'skip_table_opts'.

        db[in]             Database name.
        table[in]          Table name.

        Returns a tuple with the CREATE TABLE statement and table options
        (or None). If skip_table_opts=True the CREATE statement does not
        include the table options that are returned separately, otherwise the
        table options are included in the CREATE statement and None is returned
        as the second tuple element.
        """
        # Quote database and table name with backticks.
        q_table = (table if is_quoted_with_backticks(table, self.sql_mode)
                   else quote_with_backticks(table, self.sql_mode))
        q_db = db if is_quoted_with_backticks(db, self.sql_mode) else \
            quote_with_backticks(db, self.sql_mode)

        # Retrieve CREATE TABLE.
        try:
            row = self.source.exec_query(
                "SHOW CREATE TABLE {0}.{1}".format(q_db, q_table)
            )
            create_tbl = row[0][1]
        except UtilError as err:
            raise UtilDBError("Error retrieving CREATE TABLE for {0}.{1}: "
                              "{2}".format(q_db, q_table, err.errmsg))

        # Separate table options from table definition.
        tbl_opts = None
        if self.skip_table_opts:
            # First, get partition options.
            create_tbl, sep, part_opts = create_tbl.rpartition('\n/*')
            # Handle situation where no partition options are found.
            if not create_tbl:
                create_tbl = part_opts
                part_opts = ''
            else:
                part_opts = "{0}{1}".format(sep, part_opts)
            # Then, separate table definitions from table options.
            create_tbl, sep, tbl_opts = create_tbl.rpartition(') ')
            # Reconstruct CREATE TABLE without table options.
            create_tbl = "{0}{1}{2}".format(create_tbl, sep, part_opts)

        return create_tbl, tbl_opts

    def get_table_options(self, db, table):
        """Return the table options.

        This method returns the list of used table options (from the CREATE
        TABLE statement).

        db[in]             Database name.
        table[in]          Table name.

        Returns a list of table options.
        For example: ['AUTO_INCREMENT=5','ENGINE=InnoDB']
        """
        # Quote database and table name with backticks.
        q_table = (table if is_quoted_with_backticks(table, self.sql_mode)
                   else quote_with_backticks(table, self.sql_mode))
        q_db = db if is_quoted_with_backticks(db, self.sql_mode) else \
            quote_with_backticks(db, self.sql_mode)

        # Retrieve CREATE TABLE statement.
        try:
            row = self.source.exec_query(
                "SHOW CREATE TABLE {0}.{1}".format(q_db, q_table)
            )
            create_tbl = row[0][1]
        except UtilError as err:
            raise UtilDBError("Error retrieving CREATE TABLE for {0}.{1}: "
                              "{2}".format(q_db, q_table, err.errmsg))

        # First, separate partition options.
        create_tbl, _, part_opts = create_tbl.rpartition('\n/*')
        # Handle situation where no partition options are found.
        create_tbl = part_opts if not create_tbl else create_tbl
        # Then, separate table options from table definition.
        create_tbl, _, tbl_opts = create_tbl.rpartition(') ')
        table_options = tbl_opts.split()

        return table_options

    def get_object_definition(self, db, name, obj_type):
        """Return a list of the object's creation metadata.

        This method queries the INFORMATION_SCHEMA or MYSQL database for the
        row-based (list) description of the object. This is similar to the
        output EXPLAIN <object>.

        db[in]             Database name
        name[in]           Name of the object
        obj_type[in]       Object type (string) e.g. DATABASE
                           Note: this is used to form the correct SHOW command

        Returns list - object definition, None if db.object does not exist
        """
        definition = []
        from_name = None
        condition = None

        # Remove objects backticks if needed
        db = remove_backtick_quoting(db, self.sql_mode) \
            if is_quoted_with_backticks(db, self.sql_mode) else db
        name = remove_backtick_quoting(name, self.sql_mode) \
            if is_quoted_with_backticks(name, self.sql_mode) else name

        if obj_type == _DATABASE:
            columns = 'SCHEMA_NAME, DEFAULT_CHARACTER_SET_NAME, ' + \
                      'DEFAULT_COLLATION_NAME, SQL_PATH'
            from_name = 'SCHEMATA'
            condition = "SCHEMA_NAME = '%s'" % name
        elif obj_type == _TABLE:
            columns = 'TABLE_SCHEMA, TABLE_NAME, ENGINE, AUTO_INCREMENT, ' + \
                      'AVG_ROW_LENGTH, CHECKSUM, TABLE_COLLATION, ' + \
                      'TABLE_COMMENT, ROW_FORMAT, CREATE_OPTIONS'
            from_name = 'TABLES'
            condition = "TABLE_SCHEMA = '%s' AND TABLE_NAME = '%s'" % \
                        (db, name)
        elif obj_type == _VIEW:
            columns = 'TABLE_SCHEMA, TABLE_NAME, VIEW_DEFINITION, ' + \
                      'CHECK_OPTION, DEFINER, SECURITY_TYPE'
            from_name = 'VIEWS'
            condition = "TABLE_SCHEMA = '%s' AND TABLE_NAME = '%s'" % \
                        (db, name)
        elif obj_type == _TRIG:
            columns = 'TRIGGER_SCHEMA, TRIGGER_NAME, EVENT_MANIPULATION, ' + \
                      'EVENT_OBJECT_TABLE, ACTION_STATEMENT, ' + \
                      'ACTION_TIMING, DEFINER'
            from_name = 'TRIGGERS'
            condition = "TRIGGER_SCHEMA = '%s' AND TRIGGER_NAME = '%s'" % \
                        (db, name)
        elif obj_type == _PROC or obj_type == _FUNC:
            columns = 'ROUTINE_SCHEMA, ROUTINE_NAME, ROUTINE_DEFINITION, ' + \
                      'ROUTINES.SQL_DATA_ACCESS, ROUTINES.SECURITY_TYPE, ' + \
                      'ROUTINE_COMMENT, ROUTINES.DEFINER, param_list, ' + \
                      'DTD_IDENTIFIER, ROUTINES.IS_DETERMINISTIC'
            from_name = 'ROUTINES JOIN mysql.proc ON ' + \
                        'ROUTINES.ROUTINE_SCHEMA = proc.db AND ' + \
                        'ROUTINES.ROUTINE_NAME = proc.name AND ' + \
                        'ROUTINES.ROUTINE_TYPE = proc.type '
            condition = "ROUTINE_SCHEMA = '%s' AND ROUTINE_NAME = '%s'" % \
                        (db, name)
            if obj_type == _PROC:
                typ = 'PROCEDURE'
            else:
                typ = 'FUNCTION'
            condition += " AND ROUTINE_TYPE = '%s'" % typ
        elif obj_type == _EVENT:
            columns = ('EVENT_SCHEMA, EVENT_NAME, DEFINER, EVENT_DEFINITION, '
                       'EVENT_TYPE, INTERVAL_FIELD, INTERVAL_VALUE, STATUS, '
                       'ON_COMPLETION, STARTS, ENDS')
            from_name = 'EVENTS'
            condition = "EVENT_SCHEMA = '%s' AND EVENT_NAME = '%s'" % \
                        (db, name)

        if from_name is None:
            raise UtilError('Attempting to get definition from unknown object '
                            'type = %s.' % obj_type)

        values = {
            'columns': columns,
            'table_name': from_name,
            'conditions': condition,
        }
        rows = self.source.exec_query(_DEFINITION_QUERY % values)
        if rows != []:
            # If this is a table, we need three types of information:
            # basic info, column info, and partitions info
            if obj_type == _TABLE:
                values['name'] = name
                values['db'] = db
                basic_def = rows[0]
                col_def = self.source.exec_query(_COLUMN_QUERY % values)
                part_def = self.source.exec_query(_PARTITION_QUERY % values)
                definition.append((basic_def, col_def, part_def))
            else:
                definition.append(rows[0])

        return definition

    def get_next_object(self):
        """Retrieve the next object in the database list.

        This method is an iterator for retrieving the objects in the database
        as specified in the init() method. You must call this method first.

        Returns next object in list or throws exception at EOL.
        """

        # Must call init() first!
        # Guard for init() prerequisite
        assert self.init_called, "You must call db.init() before db.copy()."

        for obj in self.objects:
            yield obj

    def __build_exclude_patterns(self, exclude_param):
        """Return a string to add to where clause to exclude objects.

        This method will add the conditions to exclude objects based on
        name if there is a dot notation or by a search pattern as specified
        by the options.

        exclude_param[in]  Name of column to check.

        Returns (string) String to add to where clause or ""
        """
        oper = 'NOT REGEXP' if self.use_regexp else 'NOT LIKE'
        string = ""
        for pattern in self.exclude_patterns:
            # Check use of qualified object names (with backtick support).
            if pattern.find(".") > 0:
                use_backtick = is_quoted_with_backticks(pattern, self.sql_mode)
                db, name = parse_object_name(pattern, self.sql_mode, True)
                if use_backtick:
                    # Remove backtick quotes.
                    db = remove_backtick_quoting(db, self.sql_mode)
                    name = remove_backtick_quoting(name, self.sql_mode)
                if db == self.db_name:  # Check if database name matches.
                    value = name  # Only use the object name to exclude.
                else:
                    value = pattern
            # Otherwise directly use the specified pattern.
            else:
                value = pattern
            if value:
                # Append exclude condition to previous one(s).
                string = "{0} AND {1} {2} {3}".format(string, exclude_param,
                                                      oper, obj2sql(value))

        return string

    def get_object_type(self, object_name):
        """Return the object type of an object

        This method attempts to locate the object name among the objects
        in the database. It returns the object type if found or None
        if not found.
        Note: different types of objects with the same name might exist in the
        database.

        object_name[in]    Name of the object to find

        Returns (list of strings) with the object types or None if not found
        """
        object_types = None

        # Remove object backticks if needed
        obj_name = remove_backtick_quoting(object_name, self.sql_mode) \
            if is_quoted_with_backticks(object_name, self.sql_mode) else \
            object_name

        res = self.source.exec_query(_OBJTYPE_QUERY %
                                     {'db_name': self.db_name,
                                      'obj_name': obj_name})

        if res:
            object_types = ['TABLE' if row[0] == 'BASE TABLE' else row[0]
                            for row in res]

        return object_types

    def get_db_objects(self, obj_type, columns='names', get_columns=False,
                       need_backtick=False):
        """Return a result set containing a list of objects for a given
        database based on type.

        This method returns either a list of names for the object type
        specified, a brief list of minimal columns for creating the
        objects, or the full list of columns from INFORMATION_SCHEMA. It can
        also provide the list of column names if desired.

        obj_type[in]       Type of object to retrieve
        columns[in]        Column mode - names (default), brief, or full
                           Note: not valid for GRANT objects.
        get_columns[in]    If True, return column names as first element
                           and result set as second element. If False,
                           return only the result set.
        need_backtick[in]  If True, it returns any identifiers, e.g. table and
                           column names, quoted with backticks.
                           By default, False.

        TODO: Change implementation to return classes instead of a result set.

        Returns mysql.connector result set
        """

        exclude_param = ""
        if obj_type == _TABLE:
            _NAMES = """
            SELECT DISTINCT TABLES.TABLE_NAME
            """
            names_pos_to_quote = (0,)
            _FULL = """
            SELECT TABLES.TABLE_CATALOG, TABLES.TABLE_SCHEMA,
                TABLES.TABLE_NAME, TABLES.TABLE_TYPE,
                TABLES.ENGINE, TABLES.VERSION, TABLES.ROW_FORMAT,
                TABLES.TABLE_ROWS, TABLES.AVG_ROW_LENGTH, TABLES.DATA_LENGTH,
                TABLES.MAX_DATA_LENGTH, TABLES.INDEX_LENGTH, TABLES.DATA_FREE,
                TABLES.AUTO_INCREMENT, TABLES.CREATE_TIME, TABLES.UPDATE_TIME,
                TABLES.CHECK_TIME, TABLES.TABLE_COLLATION, TABLES.CHECKSUM,
                TABLES.CREATE_OPTIONS, TABLES.TABLE_COMMENT,
                COLUMNS.ORDINAL_POSITION, COLUMNS.COLUMN_NAME,
                COLUMNS.COLUMN_TYPE, COLUMNS.IS_NULLABLE,
                COLUMNS.COLUMN_DEFAULT, COLUMNS.COLUMN_KEY,
                REFERENTIAL_CONSTRAINTS.CONSTRAINT_NAME,
                REFERENTIAL_CONSTRAINTS.REFERENCED_TABLE_NAME,
                REFERENTIAL_CONSTRAINTS.UNIQUE_CONSTRAINT_NAME,
                REFERENTIAL_CONSTRAINTS.UNIQUE_CONSTRAINT_SCHEMA,
                REFERENTIAL_CONSTRAINTS.UPDATE_RULE,
                REFERENTIAL_CONSTRAINTS.DELETE_RULE,
                KEY_COLUMN_USAGE.CONSTRAINT_NAME AS KEY_CONSTRAINT_NAME,
                KEY_COLUMN_USAGE.COLUMN_NAME AS COL_NAME,
                KEY_COLUMN_USAGE.REFERENCED_TABLE_SCHEMA,
                KEY_COLUMN_USAGE.REFERENCED_COLUMN_NAME
            """
            full_pos_to_quote = (1, 2, 22, 27, 28, 29, 30, 33, 34, 35, 36)
            full_pos_split_quote = (34, 36)
            _MINIMAL = """
            SELECT TABLES.TABLE_SCHEMA, TABLES.TABLE_NAME, TABLES.ENGINE,
                COLUMNS.ORDINAL_POSITION, COLUMNS.COLUMN_NAME,
                COLUMNS.COLUMN_TYPE, COLUMNS.IS_NULLABLE,
                COLUMNS.COLUMN_DEFAULT, COLUMNS.COLUMN_KEY,
                TABLES.TABLE_COLLATION,
                TABLES.CREATE_OPTIONS,
                REFERENTIAL_CONSTRAINTS.CONSTRAINT_NAME,
                REFERENTIAL_CONSTRAINTS.REFERENCED_TABLE_NAME,
                REFERENTIAL_CONSTRAINTS.UNIQUE_CONSTRAINT_NAME,
                REFERENTIAL_CONSTRAINTS.UPDATE_RULE,
                REFERENTIAL_CONSTRAINTS.DELETE_RULE,
                KEY_COLUMN_USAGE.CONSTRAINT_NAME AS KEY_CONSTRAINT_NAME,
                KEY_COLUMN_USAGE.COLUMN_NAME AS COL_NAME,
                KEY_COLUMN_USAGE.REFERENCED_TABLE_SCHEMA,
                KEY_COLUMN_USAGE.REFERENCED_COLUMN_NAME
            """
            minimal_pos_to_quote = (0, 1, 4, 11, 12, 13, 16, 17, 18, 19)
            minimal_pos_split_quote = (17, 19)
            _OBJECT_QUERY = """
            FROM INFORMATION_SCHEMA.TABLES JOIN INFORMATION_SCHEMA.COLUMNS ON
                TABLES.TABLE_SCHEMA = COLUMNS.TABLE_SCHEMA AND
                TABLES.TABLE_NAME = COLUMNS.TABLE_NAME
            LEFT JOIN INFORMATION_SCHEMA.REFERENTIAL_CONSTRAINTS ON
                TABLES.TABLE_SCHEMA = REFERENTIAL_CONSTRAINTS.CONSTRAINT_SCHEMA
                AND
                TABLES.TABLE_NAME = REFERENTIAL_CONSTRAINTS.TABLE_NAME
            LEFT JOIN (
                  SELECT CONSTRAINT_SCHEMA, TABLE_NAME, CONSTRAINT_NAME,
                         GROUP_CONCAT(COLUMN_NAME ORDER BY ORDINAL_POSITION)
                         AS COLUMN_NAME, REFERENCED_TABLE_SCHEMA,
                         GROUP_CONCAT(REFERENCED_COLUMN_NAME ORDER BY
                         ORDINAL_POSITION) AS REFERENCED_COLUMN_NAME
                  FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
                  GROUP BY CONSTRAINT_SCHEMA, TABLE_NAME, CONSTRAINT_NAME,
                           REFERENCED_TABLE_SCHEMA
            ) AS KEY_COLUMN_USAGE ON
                TABLES.TABLE_SCHEMA = KEY_COLUMN_USAGE.CONSTRAINT_SCHEMA
                AND
                TABLES.TABLE_NAME = KEY_COLUMN_USAGE.TABLE_NAME
            WHERE TABLES.TABLE_SCHEMA = '%s' AND TABLE_TYPE <> 'VIEW' %s
            """
            _ORDER_BY_DEFAULT = """
            ORDER BY TABLES.TABLE_SCHEMA, TABLES.TABLE_NAME,
                     COLUMNS.ORDINAL_POSITION
            """
            _ORDER_BY_NAME = """
            ORDER BY TABLES.TABLE_NAME
            """
            exclude_param = "TABLES.TABLE_NAME"

        elif obj_type == _VIEW:
            _NAMES = """
            SELECT TABLE_NAME
            """
            names_pos_to_quote = (0,)
            _FULL = """
            SELECT TABLE_CATALOG, TABLE_SCHEMA, TABLE_NAME, VIEW_DEFINITION,
                   CHECK_OPTION, IS_UPDATABLE, DEFINER, SECURITY_TYPE,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION
            """
            full_pos_to_quote = (1, 2)
            full_pos_split_quote = ()
            _MINIMAL = """
            SELECT TABLE_SCHEMA, TABLE_NAME, DEFINER, SECURITY_TYPE,
                   VIEW_DEFINITION, CHECK_OPTION, IS_UPDATABLE,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION
            """
            minimal_pos_to_quote = (0, 1)
            minimal_pos_split_quote = ()
            _OBJECT_QUERY = """
            FROM INFORMATION_SCHEMA.VIEWS
            WHERE TABLE_SCHEMA = '%s' %s
            """
            _ORDER_BY_DEFAULT = ""
            _ORDER_BY_NAME = ""
            exclude_param = "VIEWS.TABLE_NAME"
        elif obj_type == _TRIG:
            _NAMES = """
            SELECT TRIGGER_NAME
            """
            names_pos_to_quote = (0,)
            _FULL = """
            SELECT TRIGGER_CATALOG, TRIGGER_SCHEMA, TRIGGER_NAME,
                   EVENT_MANIPULATION, EVENT_OBJECT_CATALOG,
                   EVENT_OBJECT_SCHEMA, EVENT_OBJECT_TABLE, ACTION_ORDER,
                   ACTION_CONDITION, ACTION_STATEMENT, ACTION_ORIENTATION,
                   ACTION_TIMING, ACTION_REFERENCE_OLD_TABLE,
                   ACTION_REFERENCE_NEW_TABLE, ACTION_REFERENCE_OLD_ROW,
                   ACTION_REFERENCE_NEW_ROW, CREATED, SQL_MODE, DEFINER,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION,
                   DATABASE_COLLATION
            """
            full_pos_to_quote = (1, 2, 5, 6)  # 9 ?
            full_pos_split_quote = ()
            _MINIMAL = """
            SELECT TRIGGER_NAME, DEFINER, EVENT_MANIPULATION,
                   EVENT_OBJECT_SCHEMA, EVENT_OBJECT_TABLE,
                   ACTION_ORIENTATION, ACTION_TIMING,
                   ACTION_STATEMENT, SQL_MODE,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION,
                   DATABASE_COLLATION
            """
            # Note: 7 (ACTION_STATEMENT) might require special handling
            minimal_pos_to_quote = (0, 3, 4)
            minimal_pos_split_quote = ()
            _OBJECT_QUERY = """
            FROM INFORMATION_SCHEMA.TRIGGERS
            WHERE TRIGGER_SCHEMA = '%s' %s
            """
            _ORDER_BY_DEFAULT = ""
            _ORDER_BY_NAME = ""
            exclude_param = "TRIGGERS.TRIGGER_NAME"
        elif obj_type == _PROC:
            _NAMES = """
            SELECT NAME
            """
            names_pos_to_quote = (0,)
            _FULL = """
            SELECT DB, NAME, TYPE, SPECIFIC_NAME, LANGUAGE, SQL_DATA_ACCESS,
                   IS_DETERMINISTIC, SECURITY_TYPE, PARAM_LIST, RETURNS, BODY,
                   DEFINER, CREATED, MODIFIED, SQL_MODE, COMMENT,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION, DB_COLLATION,
                   BODY_UTF8
            """
            full_pos_to_quote = (0, 1, 3)
            full_pos_split_quote = ()
            _MINIMAL = """
            SELECT NAME, LANGUAGE, SQL_DATA_ACCESS, IS_DETERMINISTIC,
                   SECURITY_TYPE, DEFINER, PARAM_LIST, RETURNS,
                   BODY, SQL_MODE,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION,
                   DB_COLLATION
            """
            minimal_pos_to_quote = (0,)
            minimal_pos_split_quote = ()
            _OBJECT_QUERY = """
            FROM mysql.proc
            WHERE DB = '%s' AND TYPE = 'PROCEDURE' %s
            """
            _ORDER_BY_DEFAULT = ""
            _ORDER_BY_NAME = ""
            exclude_param = "NAME"
        elif obj_type == _FUNC:
            _NAMES = """
            SELECT NAME
            """
            names_pos_to_quote = (0,)
            _FULL = """
            SELECT DB, NAME, TYPE, SPECIFIC_NAME, LANGUAGE, SQL_DATA_ACCESS,
                   IS_DETERMINISTIC, SECURITY_TYPE, PARAM_LIST, RETURNS, BODY,
                   DEFINER, CREATED, MODIFIED, SQL_MODE, COMMENT,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION, DB_COLLATION,
                   BODY_UTF8
            """
            full_pos_to_quote = (0, 1, 3)
            full_pos_split_quote = ()
            _MINIMAL = """
            SELECT NAME, LANGUAGE, SQL_DATA_ACCESS, IS_DETERMINISTIC,
                   SECURITY_TYPE, DEFINER, PARAM_LIST, RETURNS,
                   BODY, SQL_MODE,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION,
                   DB_COLLATION
            """
            minimal_pos_to_quote = (0,)
            minimal_pos_split_quote = ()
            _OBJECT_QUERY = """
            FROM mysql.proc
            WHERE DB = '%s' AND TYPE = 'FUNCTION' %s
            """
            _ORDER_BY_DEFAULT = ""
            _ORDER_BY_NAME = ""
            exclude_param = "NAME"
        elif obj_type == _EVENT:
            _NAMES = """
            SELECT NAME
            """
            names_pos_to_quote = (0,)
            _FULL = """
            SELECT DB, NAME, BODY, DEFINER, EXECUTE_AT, INTERVAL_VALUE,
                   INTERVAL_FIELD, CREATED, MODIFIED, LAST_EXECUTED, STARTS,
                   ENDS, STATUS, ON_COMPLETION, SQL_MODE, COMMENT, ORIGINATOR,
                   TIME_ZONE, CHARACTER_SET_CLIENT, COLLATION_CONNECTION,
                   DB_COLLATION, BODY_UTF8
            """
            full_pos_to_quote = (0, 1)
            full_pos_split_quote = ()
            _MINIMAL = """
            SELECT NAME, DEFINER, BODY, STATUS,
                   EXECUTE_AT, INTERVAL_VALUE, INTERVAL_FIELD, SQL_MODE,
                   STARTS, ENDS, STATUS, ON_COMPLETION, ORIGINATOR,
                   CHARACTER_SET_CLIENT, COLLATION_CONNECTION,
                   DB_COLLATION
            """
            minimal_pos_to_quote = (0,)
            minimal_pos_split_quote = ()
            _OBJECT_QUERY = """
            FROM mysql.event
            WHERE DB = '%s' %s
            """
            _ORDER_BY_DEFAULT = ""
            _ORDER_BY_NAME = ""
            exclude_param = "NAME"
        elif obj_type == _GRANT:
            _OBJECT_QUERY = """
            (
                SELECT GRANTEE, PRIVILEGE_TYPE, TABLE_SCHEMA,
                       NULL as TABLE_NAME, NULL AS COLUMN_NAME,
                       NULL AS ROUTINE_NAME
                FROM INFORMATION_SCHEMA.SCHEMA_PRIVILEGES
                WHERE table_schema = '%s'
            ) UNION (
                SELECT grantee, privilege_type, table_schema, table_name,
                       NULL, NULL
                FROM INFORMATION_SCHEMA.TABLE_PRIVILEGES
                WHERE table_schema = '%s'
            ) UNION (
                SELECT grantee, privilege_type, table_schema, table_name,
                       column_name, NULL
                FROM INFORMATION_SCHEMA.COLUMN_PRIVILEGES
                WHERE table_schema = '%s'
            ) UNION (
                SELECT CONCAT('''', User, '''@''', Host, ''''),  Proc_priv, Db,
                       Routine_name, NULL, Routine_type
                FROM mysql.procs_priv WHERE Db = '%s'
            ) ORDER BY GRANTEE ASC, PRIVILEGE_TYPE ASC, TABLE_SCHEMA ASC,
                       TABLE_NAME ASC, COLUMN_NAME ASC, ROUTINE_NAME ASC
            """
        else:
            return None

        col_options = {
            'columns': get_columns
        }
        pos_to_quote = ()
        pos_split_quote = ()
        # pylint: disable=R0101
        if obj_type == _GRANT:
            query = _OBJECT_QUERY % (self.db_name, self.db_name,
                                     self.db_name, self.db_name)
            return self.source.exec_query(query, col_options)
        else:
            if columns == "names":
                prefix = _NAMES
                if need_backtick:
                    pos_to_quote = names_pos_to_quote
                sufix = _ORDER_BY_NAME
            elif columns == "full":
                prefix = _FULL
                if need_backtick:
                    pos_to_quote = full_pos_to_quote
                    pos_split_quote = full_pos_split_quote
                sufix = _ORDER_BY_DEFAULT
            else:
                prefix = _MINIMAL
                if need_backtick:
                    pos_to_quote = minimal_pos_to_quote
                    pos_split_quote = minimal_pos_split_quote
                sufix = _ORDER_BY_DEFAULT
            # Form exclusion string
            exclude_str = ""
            if self.exclude_patterns:
                exclude_str = self.__build_exclude_patterns(exclude_param)
            query = (prefix + _OBJECT_QUERY + sufix) % (self.db_name,
                                                        exclude_str)
            res = self.source.exec_query(query, col_options)

            # Quote required identifiers with backticks
            if need_backtick:
                new_rows = []
                for row in res[1]:
                    # Recreate row tuple quoting needed elements with backticks
                    # Note: handle elements that can hold multiple values
                    # quoting them separately (e.g., multiple column names).
                    r = []
                    for i, data in enumerate(row):
                        if data and i in pos_to_quote:
                            if i in pos_split_quote:
                                cols = data.split(',')
                                data = ','.join(
                                    [quote_with_backticks(col, self.sql_mode)
                                     for col in cols]
                                )
                                r.append(data)
                            else:
                                r.append(quote_with_backticks(data,
                                                              self.sql_mode))
                        else:
                            r.append(data)
                    new_rows.append(tuple(r))

                # set new result with with required data quoted with backticks
                res = (res[0], new_rows)

            if res and obj_type == _VIEW:
                res = self._get_views_sorted_by_dependencies(res, columns,
                                                             not need_backtick)

            return res

    def _check_user_permissions(self, uname, host, access):
        """Check user permissions for a given privilege

        uname[in]          user name to check
        host[in]           host name of connection
        access[in]         privilege to check (e.g. "SELECT")

        Returns True if user has permission, False if not
        """
        user = User(self.source, uname + '@' + host)
        result = user.has_privilege(access[0], '*', access[1])
        return result

    def check_read_access(self, user, host, options):
        """Check access levels for reading database objects

        This method will check the user's permission levels for copying a
        database from this server.

        It will also skip specific checks if certain objects are not being
        copied (i.e., views, procs, funcs, grants).

        user[in]           user name to check
        host[in]           host name to check
        options[in]        dictionary of values to include:
            skip_views     True = no views processed
            skip_proc      True = no procedures processed
            skip_func      True = no functions processed
            skip_grants    True = no grants processed
            skip_events    True = no events processed

        Returns True if user has permissions and raises a UtilDBError if the
                     user does not have permission with a message that includes
                     the server context.
        """

        # Build minimal list of privileges for source access
        source_privs = []
        priv_tuple = (self.db_name, "SELECT")
        source_privs.append(priv_tuple)
        # if views are included, we need SHOW VIEW
        if not options.get('skip_views', False):
            priv_tuple = (self.db_name, "SHOW VIEW")
            source_privs.append(priv_tuple)
        # if procs, funcs, events or grants are included, we need read on
        # mysql db
        if not options.get('skip_procs', False) or \
           not options.get('skip_funcs', False) or \
           not options.get('skip_events', False) or \
           not options.get('skip_grants', False):
            priv_tuple = ("mysql", "SELECT")
            source_privs.append(priv_tuple)
        # if events, we need event
        if not options.get('skip_events', False):
            priv_tuple = (self.db_name, "EVENT")
            source_privs.append(priv_tuple)
        # if triggers, we need trigger
        if not options.get('skip_triggers', False):
            priv_tuple = (self.db_name, "TRIGGER")
            source_privs.append(priv_tuple)

        # Check permissions on source
        for priv in source_privs:
            if not self._check_user_permissions(user, host, priv):
                raise UtilDBError("User %s on the %s server does not have "
                                  "permissions to read all objects in %s. " %
                                  (user, self.source.role, self.db_name) +
                                  "User needs %s privilege on %s." %
                                  (priv[1], priv[0]), -1, priv[0])

        return True

    def check_write_access(self, user, host, options, source_objects=None,
                           do_drop=False):
        """Check access levels for creating and writing database objects

        This method will check the user's permission levels for copying a
        database to this server.

        It will also skip specific checks if certain objects are not being
        copied (i.e., views, procs, funcs, grants).

        user[in]           user name to check
        host[in]           host name to check
        options[in]        dictionary of values to include:
            skip_views     True = no views processed
            skip_proc      True = no procedures processed
            skip_func      True = no functions processed
            skip_grants    True = no grants processed
            skip_events    True = no events processed
        source_objects[in] Dictionary containing the list of objects from
                           source database
        do_drop[in]        True if the user is using --drop-first option

        Returns True if user has permissions and raises a UtilDBError if the
                     user does not have permission with a message that includes
                     the server context.
        """
        if source_objects is None:
            source_objects = {}

        dest_privs = [(self.db_name, "CREATE"),
                      (self.db_name, "ALTER"),
                      (self.db_name, "SELECT"),
                      (self.db_name, "INSERT"),
                      (self.db_name, "UPDATE"),
                      (self.db_name, "LOCK TABLES")]

        # Check for the --drop-first
        if do_drop:
            dest_privs.append((self.db_name, "DROP"))

        extra_privs = []
        super_needed = False

        try:
            res = self.source.exec_query("SELECT CURRENT_USER()")
            dest_user = res[0][0]
        except UtilError as err:
            raise UtilError("Unable to execute SELECT current_user(). Error: "
                            "{0}".format(err.errmsg))

        # CREATE VIEW is needed for views
        if not options.get("skip_views", False):
            views = source_objects.get("views", None)
            if views:
                extra_privs.append("CREATE VIEW")
                for item in views:
                    # Test if DEFINER is equal to the current user
                    if item[6] != dest_user:
                        super_needed = True
                        break

        # CREATE ROUTINE and EXECUTE are needed for procedures
        if not options.get("skip_procs", False):
            procs = source_objects.get("procs", None)
            if procs:
                extra_privs.append("CREATE ROUTINE")
                extra_privs.append("EXECUTE")
                if not super_needed:
                    for item in procs:
                        # Test if DEFINER is equal to the current user
                        if item[11] != dest_user:
                            super_needed = True
                            break

        # CREATE ROUTINE and EXECUTE are needed for functions
        # pylint: disable=R0101
        if not options.get("skip_funcs", False):
            funcs = source_objects.get("funcs", None)
            if funcs:
                if "CREATE ROUTINE" not in extra_privs:
                    extra_privs.append("CREATE ROUTINE")
                if "EXECUTE" not in extra_privs:
                    extra_privs.append("EXECUTE")
                if not super_needed:
                    trust_function_creators = False
                    try:
                        res = self.source.show_server_variable(
                            "log_bin_trust_function_creators"
                        )
                        if res and isinstance(res, list) and \
                                res[0][1] in ("ON", "1"):
                            trust_function_creators = True
                        # If binary log is enabled and
                        # log_bin_trust_function_creators is 0, we need
                        # SUPER privilege
                        super_needed = self.source.binlog_enabled() and \
                            not trust_function_creators
                    except UtilError as err:
                        raise UtilDBError("ERROR: {0}".format(err.errmsg))

                    if not super_needed:
                        for item in funcs:
                            # Test if DEFINER is equal to the current user
                            if item[11] != dest_user:
                                super_needed = True
                                break

        # EVENT is needed for events
        if not options.get("skip_events", False):
            events = source_objects.get("events", None)
            if events:
                extra_privs.append("EVENT")
                if not super_needed:
                    for item in events:
                        # Test if DEFINER is equal to the current user
                        if item[3] != dest_user:
                            super_needed = True
                            break

        # TRIGGER is needed for events
        if not options.get("skip_triggers", False):
            triggers = source_objects.get("triggers", None)
            if triggers:
                extra_privs.append("TRIGGER")
                if not super_needed:
                    for item in triggers:
                        # Test if DEFINER is equal to the current user
                        if item[18] != dest_user:
                            super_needed = True
                            break

        # Add SUPER privilege if needed
        if super_needed:
            dest_privs.append(("*", "SUPER"))

        # Add extra privileges needed
        for priv in extra_privs:
            dest_privs.append((self.db_name, priv))

        if not options.get('skip_grants', False):
            priv_tuple = (self.db_name, "GRANT OPTION")
            dest_privs.append(priv_tuple)

        # Check privileges on destination
        for priv in dest_privs:
            if not self._check_user_permissions(user, host, priv):
                raise UtilDBError("User %s on the %s server does not "
                                  "have permissions to create all objects "
                                  "in %s. User needs %s privilege on %s." %
                                  (user, self.source.role, priv[0], priv[1],
                                   priv[0]), -1, priv[0])

        return True

    def check_auto_increment(self, tbl=None):
        """Check for any tables in the database with auto_increment values
        of 0. This will require a special sql_mode to copy or export. The
        method returns True if any table has an auto_increment value of 0.
        If tbl provided, use that table in the query otherwise check all
        tables.

        tbl[in]      If provided, use this table name

        Returns True if any table has 0 in auto_increment, False if not
        """
        FIND_AUTO_INC_COLS = """
            SELECT table_name, column_name FROM INFORMATION_SCHEMA.COLUMNS
            WHERE table_schema = '{0}' AND extra LIKE '%auto_increment%'
        """
        AUTO_INC_ZERO = "SELECT * FROM {0}.`{1}` WHERE {2} < 1;"
        # Watchout for weird tick marks in the name
        if self.db_name.count("`") > 0:
            query = FIND_AUTO_INC_COLS.format(self.q_db_name)
        else:
            query = FIND_AUTO_INC_COLS.format(self.db_name)
        if tbl:
            query = "{0} AND table_name = '{1}'".format(query, tbl)
        res = self.source.exec_query(query)
        for row in res:
            # Watchout for weird tick marks.
            column = row[1]
            # pylint: disable=W0125
            if (i in row[1] for i in ('`', '"', "'")):
                column = "`{0}`".format(row[1])
            query = AUTO_INC_ZERO.format(self.q_db_name, row[0], column)
            res = self.source.exec_query(query)
            if res:
                return True
        return False
#
# Copyright (c) 2011, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains the methods for checking consistency among two databases.
"""

import re
import tempfile
import difflib


# The following are the queries needed to perform table data consistency
# checking.

_COMPARE_TABLE_NAME = 'compare_{tbl}'

_COMPARE_TABLE_DROP = """
    DROP TABLE {db}.{compare_tbl};
"""

# The Length of key for the span index has been increased from 4 to 8 allow
# more accurate hits. This may slow the algorithm for big dbs, for future
# the key length could be calculated by the number of rows.
DEFAULT_SPAN_KEY_SIZE = 8

# Max allowed size for the span_key. Must be smaller or equal than the size of
# the key hash because it is a substring of it. Note: 32 = binary(16).
MAX_SPAN_KEY_SIZE = 32

# Note: Use a composed index (span, pk_hash) instead of only for column "span"
# due to the "ORDER BY pk_hash" in the _COMPARE_DIFF query.
_COMPARE_TABLE = """
    CREATE TEMPORARY TABLE {db}.{compare_tbl} (
        compare_sign binary(16) NOT NULL,
        pk_hash binary(16) NOT NULL,
        {pkdef}
        span binary({span_key_size}) NOT NULL,
        INDEX span_key (span, pk_hash)) ENGINE=MyISAM
"""

_COMPARE_INSERT = """
    INSERT INTO {db}.{compare_tbl}
        (compare_sign, pk_hash, {pkstr}, span)
    SELECT
        UNHEX(MD5(CONCAT_WS('/', {colstr}))),
        UNHEX(MD5(CONCAT_WS('/', {pkstr}))),
        {pkstr},
        UNHEX(LEFT(MD5(CONCAT_WS('/', {pkstr})), {span_key_size}))
    FROM {db}.{table}
"""

_COMPARE_SUM = """
    SELECT HEX(span), COUNT(*) as cnt,
        CONCAT(SUM(CONV(SUBSTRING(HEX(compare_sign),1,8),16,10)),
        SUM(CONV(SUBSTRING(HEX(compare_sign),9,8),16,10)),
        SUM(CONV(SUBSTRING(HEX(compare_sign),17,8),16,10)),
        SUM(CONV(SUBSTRING(HEX(compare_sign),25,8),16,10))) as sig
    FROM {db}.{compare_tbl}
    GROUP BY span
"""

# ORDER BY is used to ensure determinism for the order in which rows are
# returned between compared tables, otherwise rows might be returned in a
# different for server without the binlog enable (--log-bin option) leading to
# incorrect SQL diff statements (UPDATES).
_COMPARE_DIFF = """
    SELECT * FROM {db}.{compare_tbl}
    WHERE span = UNHEX('{span}') ORDER BY pk_hash
"""

_COMPARE_SPAN_QUERY = """
    SELECT * FROM {db}.{table} WHERE {where}
"""

_ERROR_NO_PRI_KEY = ("The table {tb} does not have an usable Index or "
                     "primary key.")

_WARNING_INDEX_NOT_USABLE = ("# Warning: Specified index {idx} for table {tb}"
                             " cannot be used. It contains at least one "
                             "column that accepts null values.")

_RE_EMPTY_ALTER_TABLE = "^ALTER TABLE {0};$"

_RE_DASHES_DIG = re.compile(r"^\-{3}\s\d+")

_RE_ASTERISK_DIG = re.compile(r"^\*{3}\s\d+")

_RE_ASTERISKS = re.compile(r"^\*{15}.{0,2}$")


def _get_objects(server, database, options):
    """Get all objects from the database (except grants)

    server[in]        connected server object
    database[in]      database names
    options[in]       global options

    Returns list - objects in database
    """
    options["skip_grants"] = True   # Tell db class to skip grants

    db_obj = Database(server, database, options)
    if not db_obj.exists():
        raise UtilDBError("The database does not exist: {0}".format(database))
    db_obj.init()
    db_objects = db_obj.objects
    db_objects.sort()

    return db_objects


def get_create_object(server, object_name, options, object_type):
    """Get the object's create statement.

    This method retrieves the object create statement from the database.

    server[in]        server connection
    object_name[in]   name of object in the form db.objectname
    options[in]       options: verbosity, quiet
    object_type[in]   type of the specified object (e.g, TABLE, PROCEDURE,
                      etc.).

    Returns string : create statement or raise error if object or db not exist
    """

    verbosity = options.get("verbosity", 0)
    quiet = options.get("quiet", False)

    # Get the sql_mode set on server
    sql_mode = server.select_variable("SQL_MODE")

    db_name, obj_name = parse_object_name(object_name, sql_mode)
    obj = [db_name]

    if db_name is None:
        raise UtilError(PARSE_ERR_OBJ_NAME_FORMAT.format(
            obj_name=object_name, option=object_type.lower()))
    db = Database(server, obj[0], options)

    # Error if database does not exist
    if not db.exists():
        raise UtilDBError("The database does not exist: {0}".format(obj[0]))

    if not obj_name or object_type == 'DATABASE':
        obj.append(db_name)
    else:
        obj.append(obj_name)

    create_stmt = db.get_create_statement(obj[0], obj[1], object_type)

    if verbosity > 0 and not quiet:
        if obj_name:
            print("\n# Definition for object {0}.{1}:"
                  "".format(remove_backtick_quoting(db_name, sql_mode),
                            remove_backtick_quoting(obj_name, sql_mode)))
        else:
            print("\n# Definition for object {0}:"
                  "".format(remove_backtick_quoting(db_name, sql_mode)))
        print create_stmt

    return create_stmt


def print_missing_list(item_list, first, second):
    """Print the list of items in the list.

    This method is used to display the list of objects that are missing
    from one of the databases in the compare.

    item_list[in]     list of items to print
    first[in]         name of first database
    second[in]        name of second database

    Returns bool True if items in the list, False if list is empty
    """
    if len(item_list) == 0:
        return False
    print "# WARNING: Objects in {0} but not in {1}:".format(first, second)
    for item in item_list:
        print "# {0:>12}: {1}".format(item[0], item[1][0])
    return True


def server_connect(server1_val, server2_val, object1, object2, options):
    """Connect to the servers

    This method connects to the servers and checks to see if the objects
    are different: db1.obj1 != db2.obj2 by name match.

    server1_val[in]    a dictionary containing connection information for the
                       first server including:
                       (user, password, host, port, socket)
    server2_val[in]    a dictionary containing connection information for the
                       second server including:
                       (user, password, host, port, socket)
    object1[in]        the first object in the compare
    object2[in]        the second object in the compare
    options[in]        a dictionary containing the options for the operation:
                       (quiet, verbosity)

    Returns tuple of Server objects (server1, server2)
    """
    quiet = options.get("quiet", False)
    charset = options.get("charset", None)

    conn_options = {
        'quiet': quiet,
        'src_name': "server1",
        'dest_name': "server2",
        'version': "5.1.30",
        'charset': charset,
    }
    servers = connect_servers(server1_val, server2_val, conn_options)
    server1 = servers[0]
    server2 = servers[1]
    if server2 is None:
        server2 = server1

    # Check if the specified objects and servers are the same.
    if object1 == object2 and server1.port == server2.port and \
       server1.is_alias(server2.host):
        raise UtilError("Comparing the same object on the same server.")

    return (server1, server2)


def get_common_lists(list1, list2):
    """Compare the items in two lists

    This method compares the items in two lists returning those items that
    appear in both lists as well as two lists that contain those unique items
    from the original lists.

    For example, given {s,b,c,d,e,f} and {a,b,c,d,e,z}, the lists returned are
        both = {b,c,d,e}
        in list1 not list2 = {s,f}
        in list2 not list1 = {a.z]

    list1[in]         first list
    list2[in]         second list

    Returns three lists
    """
    s1 = set(list1)
    s2 = set(list2)
    both = s1 & s2
    return(list(both), list(s1 - both), list(s2 - both))


def get_common_objects(server1, server2, db1, db2,
                       print_list=True, options=None):
    """Get a list of the common objects among two databases.

    server1[in]        first server connection
    server2[in]        second server connection
    object1[in]        the first object in the compare in the form: (db.name)
    object2[in]        the second object in the compare in the form: (db.name)
    print_list[in]     if True, print list of missing items
    options[in]        global options

    Returns (tuple) lists containing: items in both,
                                      items in db1 and not in db2,
                                      items in db2 not in db1
    """

    if options is None:
        options = {}
    db1_objects = _get_objects(server1, db1, options)
    db2_objects = _get_objects(server2, db2, options)

    in_both, in_db1_not_db2, in_db2_not_db1 = get_common_lists(db1_objects,
                                                               db2_objects)
    if print_list:
        server1_str = "server1." + db1
        if server1 == server2:
            server2_str = "server1." + db2
        else:
            server2_str = "server2." + db2
        print_missing_list(in_db1_not_db2, server1_str, server2_str)
        print_missing_list(in_db2_not_db1, server2_str, server1_str)

    return (in_both, in_db1_not_db2, in_db2_not_db1)


def _get_diff(list1, list2, object1, object2, difftype, compact=False):
    """Get the difference among two lists.

    This method finds the difference of two lists using either unified,
    context, or differ-style output.

    Note: We must strip not only \n but also trailing blanks due to change in
          Python 2.7.1 handling of difflib methods.

    list1[in]         The base list
    list2[in]         The list used for compare
    object1[in]       The 'from' or source
    object2[in]       The 'to' or difference destination
    difftype[in]      Difference type
    compact[in]       IF True, the resulting diff it will not contain all
                      the control lines, resulting in a fewer lines.

    Returns list - differences or []
    """
    diff_str = []

    # Generate unified is SQL is specified for use in reporting errors
    if difftype in ['unified', 'sql']:
        for line in difflib.unified_diff(list1, list2,
                                         fromfile=object1, tofile=object2):
            if compact:
                if not line.startswith("@@ "):
                    diff_str.append(line.strip('\n').rstrip(' '))
            else:
                diff_str.append(line.strip('\n').rstrip(' '))
    elif difftype == 'context':
        for line in difflib.context_diff(list1, list2,
                                         fromfile=object1, tofile=object2):
            if compact:
                if _RE_DASHES_DIG.match(line):
                    diff_str.append("---")
                elif _RE_ASTERISK_DIG.match(line):
                    diff_str.append("***")
                # Asterisks are used as row separators too
                elif not _RE_ASTERISKS.match(line):
                    diff_str.append(line.strip('\n').rstrip(' '))
            else:
                diff_str.append(line.strip('\n').rstrip(' '))
    else:
        has_diff = False
        for line in difflib.ndiff(list1, list2):
            diff_str.append(line.strip('\n').rstrip(' '))
            if line[0] in ['-', '+', '?']:
                has_diff = True

        if not has_diff:
            diff_str = []

    if compact and difftype != 'differ' and difftype != 'context':
        return diff_str[2:]
    # If objects names are the same, avoid print them
    elif (compact and difftype == 'context' and len(diff_str) > 0 and
          diff_str[0].endswith(diff_str[0][3:])):
        return diff_str[2:]

    return diff_str


def _get_transform(server1, server2, object1, object2, options,
                   object_type):
    """Get the transformation SQL statements

    This method generates the SQL statements to transform the destination
    object based on direction of the compare.

    server1[in]        first server connection
    server2[in]        second server connection
    object1            the first object in the compare in the form: (db.name)
    object2            the second object in the compare in the form: (db.name)
    options[in]        a dictionary containing the options for the operation:
                       (quiet, etc.)
    object_type[in]    type of the objects to be compared (e.g., TABLE,
                       PROCEDURE, etc.).

    Returns tuple - (bool - same db name?, list of transformation statements)
    """

    try:
        db1, name1 = parse_object_name(object1,
                                       server1.select_variable("SQL_MODE"))

        db2, name2 = parse_object_name(object2,
                                       server2.select_variable("SQL_MODE"))
    except:
        raise UtilError("Invalid object name arguments for _get_transform"
                        "(): %s, %s." % (object1, object2))
    # If the second part of the object qualified name is None, then the format
    # is not 'db_name.obj_name' for object1 and therefore must treat it as a
    # database name. (supports backticks and the use of '.' (dots) in names.)
    if not name1 or object_type == 'DATABASE':

        # We are working with databases so db and name need to be set
        # to the database name to tell the get_object_definition() method
        # to retrieve the database information.
        name1 = db1
        name2 = db2

    db_1 = Database(server1, db1, options)
    db_2 = Database(server2, db2, options)

    obj1 = db_1.get_object_definition(db1, name1, object_type)
    obj2 = db_2.get_object_definition(db2, name2, object_type)

    # Get the transformation based on direction.
    transform_str = []
    xform = SQLTransformer(db_1, db_2, obj1[0], obj2[0], object_type,
                           options.get('verbosity', 0), options)

    differences = xform.transform_definition()
    if differences and len(differences) > 0:
        transform_str.extend(differences)

    return transform_str


def _check_tables_structure(server1, server2, object1, object2, options,
                            diff_type):
    """Check if the tables have the same structure.

    This method compares the tables structure ignoring the order of the
    columns and retrieves the differences between the table options.

    server1[in]        first server connection.
    server2[in]        second server connection.
    object1            the first object in the compare in the form: (db.name).
    object2            the second object in the compare in the form: (db.name).
    options[in]        a dictionary containing the options for the operation:
                       (quiet, verbosity, difftype, width, suppress_sql).
    diff_type[in]      difference type.

    Returns a tuple (bool, list, bool) - The first tuple value is a boolean
    that indicates if both tables have the same structure (i.e. column
    definitions). The second returns the table options differences. Finally,
    the third is a boolean indicating if the partition options are the same.
    """
    try:
        db1, name1 = parse_object_name(object1,
                                       server1.select_variable("SQL_MODE"))

        db2, name2 = parse_object_name(object2,
                                       server2.select_variable("SQL_MODE"))
    except:
        raise UtilError("Invalid object name arguments for diff_objects(): "
                        "{0}, {1}.".format(object1, object2))

    compact_diff = options.get("compact", False)

    # If the second part of the object qualified name is None, then the format
    # is not 'db_name.obj_name' for object1 and therefore must treat it as a
    # database name.
    if not name1:
        return None, None, None

    db_1 = Database(server1, db1, options)
    db_2 = Database(server2, db2, options)

    # Get tables definitions.
    table_1 = db_1.get_object_definition(db1, name1, 'TABLE')[0]
    table_2 = db_2.get_object_definition(db2, name2, 'TABLE')[0]

    # Check table options.
    table1_opts = db_1.get_table_options(db1, name1)
    table2_opts = db_2.get_table_options(db2, name2)
    diff = _get_diff(table1_opts, table2_opts, object1, object2, diff_type,
                     compact=compact_diff)

    # Check if both tables have the same columns definition.
    # Discard column order.
    table_1_cols = [col[1:] for col in table_1[1]]
    table_2_cols = [col[1:] for col in table_2[1]]
    same_cols_def = set(table_1_cols) == set(table_2_cols)

    # Check if both tables have the same partition options.
    # Discard partition name.
    table_1_part = [part[1:] for part in table_1[2]]
    table_2_part = [part[1:] for part in table_2[2]]
    same_partition_opts = set(table_1_part) == set(table_2_part)

    # Return tables check results.
    return same_cols_def, diff, same_partition_opts


def build_diff_list(diff1, diff2, transform1, transform2,
                    first, second, options):
    """Build the list of differences

    This method builds a list of difference statements based on whether
    the lists are the result of an SQL statement generation, object definition
    differences, or data differences.

    Note: to specify a non-SQL difference for data, set
          options['data_diff'] = True

    diff1[in]              definition diff for first server
    diff2[in]              definition diff for second server
    transform1[in]         transformation for first server
    transform2[in]         transformation for second server
    first[in]              name of first server (e.g. server1)
    second[in]             name of second server (e.g. server2)
    options[in]            options for building the list

    Returns list = list of differences or transformations
    """
    # Don't build the list if there were no differences.
    if len(diff1) == 0:
        return []

    reverse = options.get('reverse', False)
    diff_list = []
    if options.get('difftype') == 'sql':
        if len(transform1) == 0:
            diff_list.append("\n# WARNING: Cannot generate SQL statements "
                             "for these objects.")
            diff_list.append("# Check the difference output for other "
                             "discrepencies.")
            diff_list.extend(diff1)
        else:
            diff_list.append("# Transformation for --changes-for=%s:\n#\n" %
                             first)
            diff_list.extend(transform1)
            diff_list.append("")
            if reverse and len(transform2) > 0:
                diff_list.append("#\n# Transformation for reverse changes "
                                 "(--changes-for=%s):\n#" % second)
                for row in transform2:
                    sub_rows = row.split('\n')
                    for sub_row in sub_rows:
                        diff_list.append("# %s" % sub_row)
                diff_list.append("#\n")
    else:
        # Don't print messages for a data difference (non-SQL)
        if not options.get('data_diff', False):
            diff_list.append("# Object definitions differ. "
                             "(--changes-for=%s)\n#\n" % first)
        diff_list.extend(diff1)
        if reverse and len(diff2) > 0:
            diff_list.append("")
            if not options.get('data_diff', False):
                diff_list.append("#\n# Definition diff for reverse changes "
                                 "(--changes-for=%s):\n#" % second)
            for row in diff2:
                diff_list.append("# %s" % row)
            diff_list.append("#\n")

    return diff_list


def diff_objects(server1, server2, object1, object2, options, object_type):
    """diff the definition (CREATE statement) of two objects

    Produce a diff in the form unified, context, or ndiff of two objects.
    Note: objects must exist else exception is thrown.

    With the transform option, the method will generate the transformation
    SQL statements in addition to the differences found in the CREATE
    statements.

    When the --difftype == 'sql', the method will print the sql statements
    to stdout. To suppress this, use options: quiet=True, suppress_sql=True.

    server1[in]        first server connection
    server2[in]        second server connection
    object1[in]        the first object in the compare in the form: (db.name)
    object2[in]        the second object in the compare in the form: (db.name)
    options[in]        a dictionary containing the options for the operation:
                       (quiet, verbosity, difftype, width, suppress_sql)
    object_type[in]    type of the objects to be compared (e.g., TABLE,
                       PROCEDURE, etc.).

    Returns None = objects are the same, diff[] = objects differ
    """
    quiet = options.get("quiet", False)
    difftype = options.get("difftype", "unified")
    width = options.get("width", 75)
    direction = options.get("changes-for", None)
    reverse = options.get("reverse", False)
    skip_table_opts = options.get("skip_table_opts", False)
    compact_diff = options.get("compact", False)

    # Get object CREATE statement.
    # Note: Table options are discarded if option skip_table_opts=True.
    object1_create = get_create_object(server1, object1, options, object_type)
    object2_create = get_create_object(server2, object2, options, object_type)

    # Only target CREATE DATABASE difference if decorations differ,
    # not just the database names. So we isolate the CREATE statement
    # without the names or +/- and compare. If different, print the
    # difference report otherwise, ignore it.
    if (object_type == "DATABASE") and (object1 != object2):
        quotes = ["'", '"', "`"]
        db1 = object1.translate(None, "".join(quotes))
        db2 = object2.translate(None, "".join(quotes))
        first = object1_create.replace(db1, "")[1::]
        second = object2_create.replace(db2, "")[1::]
        if first == second:
            object1_create = ""
            object2_create = ""

    if not quiet:
        msg = "# Comparing {0} to {1} ".format(object1, object2)
        print msg,
        linelen = width - (len(msg) + 10)
        print ' ' * linelen,

    object1_create_list = object1_create.split('\n')
    object2_create_list = object2_create.split('\n')

    diff_server1 = []
    diff_server2 = []
    transform_server1 = []
    transform_server2 = []

    # Get the difference based on direction.
    if direction == 'server1' or direction is None or reverse:
        diff_server1 = _get_diff(object1_create_list,
                                 object2_create_list,
                                 object1, object2, difftype,
                                 compact=compact_diff)
        # If there is a difference. Check for SQL output
        if difftype == 'sql' and len(diff_server1) > 0:
            transform_server1 = _get_transform(server1, server2,
                                               object1, object2, options,
                                               object_type)

    if direction == 'server2' or reverse:
        diff_server2 = _get_diff(object2_create_list,
                                 object1_create_list,
                                 object2, object1, difftype,
                                 compact=compact_diff)
        # If there is a difference. Check for SQL output
        if difftype == 'sql' and len(diff_server2) > 0:
            transform_server2 = _get_transform(server2, server1,
                                               object2, object1, options,
                                               object_type)

    # Build diff list
    if direction == 'server1' or direction is None:
        diff_list = build_diff_list(diff_server1, diff_server2,
                                    transform_server1, transform_server2,
                                    'server1', 'server2', options)
    else:
        diff_list = build_diff_list(diff_server2, diff_server1,
                                    transform_server2, transform_server1,
                                    'server2', 'server1', options)

    # Note: table structure check ignores columns order.
    same_tbl_def = None
    tbl_opts_diff = None
    same_part_def = None
    if object_type == 'TABLE':
        same_tbl_def, tbl_opts_diff, same_part_def = _check_tables_structure(
            server1, server2, object1, object2, options, difftype
        )

    # Check if ALTER TABLE statement have changes. If not, it is probably
    # because there are differences but they have no influence on the create
    # table, such as different order on indexes.
    if "ANSI_QUOTES" in server1.select_variable("SQL_MODE"):
        regex_pattern = REGEXP_QUALIFIED_OBJ_NAME_AQ
    else:
        regex_pattern = _RE_EMPTY_ALTER_TABLE.format(REGEXP_QUALIFIED_OBJ_NAME)
    if diff_list and same_tbl_def and same_part_def and \
       re.match(regex_pattern, diff_list[1]):
        print("[PASS]")
        return None

    if diff_list and direction is None and same_tbl_def and not tbl_opts_diff:
        if not quiet:
            print("[PASS]")
            print("# WARNING: The tables structure is the same, but the "
                  "columns order is different. Use --change-for to take the "
                  "order into account.")
        return None

    # Check for failure to generate SQL statements
    if (difftype == 'sql') and \
       ((direction == 'server1' and transform_server1 == [] and
         diff_server1 != []) or
        (direction == 'server2' and transform_server2 == [] and
         diff_server2 != [])):

        # Here we found no transformations. So either the change is nothing
        # more than the database name or we missed something. Send a
        # warning to the user.

        if not quiet:
            print "[FAIL]"

        for line in diff_list:
            print line

        print("# WARNING: Could not generate SQL statements for differences "
              "between {0} and {1}. No changes required or not supported "
              "difference.".format(object1, object2))

        return diff_list

    if len(diff_list) > 0:
        if not quiet:
            print "[FAIL]"

        if not quiet or \
           (not options.get("suppress_sql", False) and difftype == 'sql'):
            for line in diff_list:
                print line

            # Full ALTER TABLE for partition difference cannot be generated
            # (not supported). Notify the user.
            if same_part_def is False:
                print("# WARNING: Partition changes were not generated "
                      "(not supported).")

        return diff_list

    if not quiet:
        print("[PASS]")
        if skip_table_opts and tbl_opts_diff:
            print("# WARNING: Table options are ignored and differences were "
                  "found:")
            for diff in tbl_opts_diff:
                print("# {0}".format(diff))

    return None


def _drop_compare_object(server, db_name, tbl_name):
    """Drop the compare object table

    server[in]             Server instance
    db_name[in]            database name
    tbl_name[in]           table name
    """
    # Quote compare table appropriately with backticks
    sql_mode = server.select_variable("SQL_MODE")
    q_db_name = db_name if is_quoted_with_backticks(db_name, sql_mode) \
        else quote_with_backticks(db_name, sql_mode)
    if is_quoted_with_backticks(tbl_name, sql_mode):
        q_tbl_name = remove_backtick_quoting(tbl_name, sql_mode)
    else:
        q_tbl_name = tbl_name
    q_tbl_name = quote_with_backticks(
        _COMPARE_TABLE_NAME.format(tbl=q_tbl_name), sql_mode)

    try:
        # set autocommit=1 if it is 0, because CREATE TEMPORARY TABLE and
        # DROP TEMPORARY TABLE can be executed in a non-transactional context
        # only, and require that AUTOCOMMIT = 1.
        toggle_server = not server.autocommit_set()
        if toggle_server:
            server.toggle_autocommit(enable=True)
        server.exec_query(_COMPARE_TABLE_DROP.format(db=q_db_name,
                                                     compare_tbl=q_tbl_name))
        if toggle_server:
            server.toggle_autocommit(enable=False)
    except:
        pass


def _get_compare_objects(index_cols, table1,
                         span_key_size=DEFAULT_SPAN_KEY_SIZE):
    """Build the compare table and identify the primary index

    This method creates the compare table for use in forming the MD5 hash
    of rows and a hash of the primary key. It also forms the primary key
    list of columns.

    index_cols[in]    a list of columns that form the primary key in the form
                      (column_name, type)
    table1[in]        a Table instance of the original table

    span_key_size[in] the size of key used for the hash.

    Returns tuple (table create statement, concatenated string of the
                   primary index columns)
    """
    table = None

    # build primary key col definition
    index_str = ''.join("{0}, ".format(quote_with_backticks(col[0],
                                                            table1.sql_mode))
                        for col in index_cols)
    index_defn = ''.join("{0} {1}, ".
                         format(quote_with_backticks(col[0], table1.sql_mode),
                                col[1])
                         for col in index_cols)
    if index_defn == "":
        raise UtilError("Cannot generate index definition")
    else:
        # Quote compare table appropriately with backticks
        q_tbl_name = quote_with_backticks(
            _COMPARE_TABLE_NAME.format(tbl=table1.tbl_name), table1.sql_mode)

        table = _COMPARE_TABLE.format(db=table1.q_db_name,
                                      compare_tbl=q_tbl_name,
                                      pkdef=index_defn,
                                      span_key_size=span_key_size / 2)

    return (table, index_str)


def _setup_compare(table1, table2, span_key_size, use_indexes=None):
    """Create and populate the compare summary tables

    This method creates the condensed hash table used to compare groups
    (span) of records. It also creates the Table instance for each table
    and populates values in the table information dictionary for use
    in other methods.

    The method also checks to ensure the tables have primary keys and that
    the keys are the same (have the same columns). An error is raised if
    neither of these are met.

    table1[in]            table1 Table instance
    table2[in]            table2 Table instance
    span_key_size[in]     the size of key used for the hash.
    use_indexes[in]       a tuple of the indexes names that can be used as
                          an unique key, (for_table_1, for_table_2), they will
                          be tested for columns that not accept null.
    diag_msgs[out]       a list of debug and warning messages.

    Returns four-tuple - string representations of the primary index columns,
    the index_columns, the index name used and diagnostic messages.
    """

    def get_column_names_types_for_index(index, table):
        """Useful method to get the columns name and type used by an index
        """
        tb_columns = table.get_col_names_types()
        table_idx = [col_row for column in index.columns
                     for col_row in tb_columns if column[0] == col_row[0]]
        return table_idx

    def find_candidate_indexes(candidate_idexes, no_null_idxes_tb,
                               table_name, diag_msgs):
        """This method search the user's candidate indexes in the given list
        of unique indexes with no null columns. Table name is user to
        create the warning message if the candidate index has a column that
        accepts null values.
        """
        indexs_found = []
        for cte_index in candidate_idexes:
            for no_null_idx in no_null_idxes_tb:
                if no_null_idx.q_name == cte_index:
                    indexs_found.append((no_null_idx.q_name, no_null_idx))
                    break
            else:
                diag_msgs.append(
                    _WARNING_INDEX_NOT_USABLE.format(
                        idx=cte_index, tb=table_name)
                )
        return indexs_found

    diag_msgs = []
    server1 = table1.server
    server2 = table2.server

    # get not nullable indexes for tables
    table1.get_indexes()
    table2.get_indexes()
    no_null_idxes_tb1 = table1.get_not_null_unique_indexes()
    no_null_idxes_tb2 = table2.get_not_null_unique_indexes()

    # if table does not have non nullable unique keys, do not continue.
    if not no_null_idxes_tb1 or not no_null_idxes_tb2:
        raise UtilError(_ERROR_NO_PRI_KEY.format(tb=table1.tbl_name))

    table1_idx = []
    table2_idx = []
    # If user specified indexes with --use-indexes
    if use_indexes:
        # pylint: disable=W0633
        candidate_idxs_tb1, candidate_idxs_tb2 = use_indexes
        # Check if indexes exist,
        for cte_idx in candidate_idxs_tb1:
            if not table1.has_index(cte_idx):
                raise UtilError("The specified index {0} was not found in "
                                "table {1}".format(cte_idx, table1.table))
        for cte_idx in candidate_idxs_tb2:
            if not table2.has_index(cte_idx):
                raise UtilError("The specified index {0} was not found in "
                                "table {1}".format(cte_idx, table2.table))

        # Find the user index specified with --use-indexes
        unique_indexes_tb1 = find_candidate_indexes(
            candidate_idxs_tb1, no_null_idxes_tb1, table1.table, diag_msgs)

        unique_indexes_tb2 = find_candidate_indexes(
            candidate_idxs_tb2, no_null_idxes_tb2, table1.table, diag_msgs)

        if unique_indexes_tb1:
            table1_idx_name = unique_indexes_tb1[0][0]
            table1_idx = get_column_names_types_for_index(
                unique_indexes_tb1[0][1],
                table1
            )
        if unique_indexes_tb2:
            table2_idx = get_column_names_types_for_index(
                unique_indexes_tb2[0][1],
                table2
            )

    # If no user defined index or accepts nulls, use first unique not nullable
    if not table1_idx:
        table1_idx_name = no_null_idxes_tb1[0].name
        table1_idx = get_column_names_types_for_index(no_null_idxes_tb1[0],
                                                      table1)
    if not table2_idx:
        table2_idx = get_column_names_types_for_index(no_null_idxes_tb2[0],
                                                      table2)

    if len(table1_idx) != len(table2_idx):
        raise UtilError("Indexes are not the same.")

    # drop the temporary tables
    _drop_compare_object(server1, table1.db_name, table1.tbl_name)
    _drop_compare_object(server2, table2.db_name, table2.tbl_name)

    # Build the primary key hash if needed
    tbl1_table, pri_idx1 = _get_compare_objects(table1_idx, table1,
                                                span_key_size)
    tbl2_table, pri_idx2 = _get_compare_objects(table2_idx, table2,
                                                span_key_size)

    if tbl1_table is None or tbl2_table is None:
        raise UtilError("Cannot create compare table.")

    # Create the compare tables

    # set autocommit=1 if it is 0, because CREATE TEMPORARY TABLE and DROP
    # TEMPORARY TABLE can be executed in a non-transactional context only, and
    # require that AUTOCOMMIT = 1.

    # Check if server1 and server2 have autocommit=0, and if so set the flag
    #  to 1 and execute the create temporary table query.
    must_toggle_s1 = not server1.autocommit_set()
    if must_toggle_s1:
        server1.toggle_autocommit(enable=True)
    server1.exec_query(tbl1_table)

    must_toggle_s2 = not server2.autocommit_set()
    if must_toggle_s2:
        server2.toggle_autocommit(enable=True)
    server2.exec_query(tbl2_table)

    # if the autocommit flag was toggled, return it to its previous value.
    if must_toggle_s1:
        server1.toggle_autocommit(enable=False)
    if must_toggle_s2:
        server2.toggle_autocommit(enable=False)

    return (pri_idx1, pri_idx2, table1_idx, table1_idx_name, diag_msgs)


def _make_sum_rows(table, idx_str, span_key_size=8):
    """Populate the summary table

    This method inserts rows into the compare table from the original table
    then forms the summary table by combining a prefix of the primary key
    hash (group by).

    table[in]         Table instance
    idx_str[in]       string representation of primary key columns

    Returns result from
    """
    col_str = ", ".join(table.get_col_names(True))

    # Lock table first
    tbl_lock_list = [
        (table.table, 'READ'),
        ("%s.compare_%s" % (table.db_name, table.tbl_name), 'WRITE')
    ]
    my_lock = Lock(table.server, tbl_lock_list)

    # Quote compare table appropriately with backticks
    q_tbl_name = quote_with_backticks(
        _COMPARE_TABLE_NAME.format(tbl=table.tbl_name),
        table.sql_mode
    )

    table.server.exec_query(
        _COMPARE_INSERT.format(db=table.q_db_name, compare_tbl=q_tbl_name,
                               colstr=col_str.strip(", "),
                               pkstr=idx_str.strip(", "),
                               table=table.q_tbl_name,
                               span_key_size=span_key_size))

    res = table.server.exec_query(
        _COMPARE_SUM.format(db=table.q_db_name, compare_tbl=q_tbl_name))

    # Unlock table
    my_lock.unlock()

    return res


def _get_rows_span(table, span, index):
    """Get the rows corresponding to a list of span values

    This method returns the rows from the original table that match the
    span value presented.

    TODO: This may need refactoring to make it more efficient.
          For example, use a WHERE clause such as:
          WHERE some_col IN ('a','b')

    table[in]         Table instance
    span[in]          span value

    Returns rows from original table
    """
    server = table.server
    rows = []
    ukeys = [col[0] for col in index]
    # build WHERE clause
    for row in span:
        # Quote compare table appropriately with backticks
        q_tbl_name = quote_with_backticks(
            _COMPARE_TABLE_NAME.format(tbl=table.tbl_name),
            table.sql_mode
        )

        span_rows = server.exec_query(
            _COMPARE_DIFF.format(db=table.q_db_name, compare_tbl=q_tbl_name,
                                 span=row))
        # Loop through multiple rows with the same span value.
        for res_row in span_rows:
            pk = res_row[2:-1]
            where_clause = ' AND '.join("{0} = '{1}'".
                                        format(key, col)
                                        for key, col in zip(ukeys, pk))
            orig_rows = server.exec_query(
                _COMPARE_SPAN_QUERY.format(db=table.q_db_name,
                                           table=table.q_tbl_name,
                                           where=where_clause))
            rows.append(orig_rows[0])

    return rows


def _get_changed_rows_span(table1, table2, span, index):
    """Get the original changed rows corresponding to a list of span values.

    This method returns the changed rows from the original tables that match
    the given list of span keys. Several rows might be associated to each span,
    including unchanged, changed, missing or extra rows. This method takes all
    these situations into account, ignoring unchanged rows and separating
    changed rows from missing/extra rows for each table when retrieving the
    original data. This separation is required in order to generate the
    appropriate SQL diff statement (UPDATE, INSERT, DELETE) later.

    table1[in]      First table instance.
    table2[in]      Second table instance.
    span[in]        List of span keys.
    index[in]       Used table index (unique key).

    Returns the changed rows from original tables, i.e., a tuple with two
    elements containing the changes for each table. At its turn, the element
    for each table is another tuple where the first element contains the list
    of changed rows and the second the list of extra rows (compared to the
    other table).
    """
    # Get all span rows for table 1.
    server1 = table1.server
    full_span_data_1 = []
    for row in span:
        # Quote compare table appropriately with backticks
        q_tbl_name = quote_with_backticks(
            _COMPARE_TABLE_NAME.format(tbl=table1.tbl_name),
            table1.sql_mode
        )

        span_rows = server1.exec_query(
            _COMPARE_DIFF.format(db=table1.q_db_name, compare_tbl=q_tbl_name,
                                 span=row))
        # Auxiliary set with (compare_sign, pk_hash) tuples for table.
        cmp_signs = set([(row[0], row[1]) for row in span_rows])
        # Keep span rows and auxiliary data for table 1.
        full_span_data_1.append((span_rows, cmp_signs))

    # Get all span rows for table 2.
    server2 = table2.server
    full_span_data_2 = []
    for row in span:
        # Quote compare table appropriately with backticks
        q_tbl_name = quote_with_backticks(
            _COMPARE_TABLE_NAME.format(tbl=table2.tbl_name),
            table2.sql_mode
        )

        span_rows = server2.exec_query(
            _COMPARE_DIFF.format(db=table2.q_db_name, compare_tbl=q_tbl_name,
                                 span=row))
        # Auxiliary set with (compare_sign, pk_hash) tuples for table.
        cmp_signs = set([(row[0], row[1]) for row in span_rows])
        # Keep span rows and auxiliary data for table 1.
        full_span_data_2.append((span_rows, cmp_signs))

    # List of key columns
    ukeys = [col[0] for col in index]

    # Get the original diff rows for tables 1 and 2.
    changed_in1 = []
    extra_in1 = []
    changed_in2 = []
    extra_in2 = []
    for pos, span_data1 in enumerate(full_span_data_1):
        # Also get span data for table 2.
        # Note: specific span data is at the same position for both tables.
        span_data2 = full_span_data_2[pos]

        # Determine different rows for tables 1 and 2 (exclude unchanged rows).
        diff_rows_sign1 = span_data1[1] - span_data2[1]
        diff_rows_sign2 = span_data2[1] - span_data1[1]
        diff_pk_hash1 = set(cmp_sign[1] for cmp_sign in diff_rows_sign1)
        diff_pk_hash2 = set(cmp_sign[1] for cmp_sign in diff_rows_sign2)

        # Get the original diff rows for tables 1.
        for res_row in span_data1[0]:
            # Skip row if not in previously identified changed rows set.
            if (res_row[0], res_row[1]) in diff_rows_sign1:
                # Execute query to get the original row.
                pk = res_row[2:-1]
                where_clause = ' AND '.join("{0} = '{1}'".
                                            format(key, col)
                                            for key, col in zip(ukeys, pk))
                res = server1.exec_query(
                    _COMPARE_SPAN_QUERY.format(db=table1.q_db_name,
                                               table=table1.q_tbl_name,
                                               where=where_clause))

                # Determine if it is a changed or extra row.
                # Check if the same pk_hash is found in table 2.
                if res_row[1] in diff_pk_hash2:
                    # Store original changed row (to UPDATE).
                    changed_in1.append(res[0])
                else:
                    # Store original extra row (to DELETE).
                    extra_in1.append(res[0])

        # Get the original diff rows for table 2.
        for res_row in span_data2[0]:
            # Skip row if not in previously identified changed rows set.
            if (res_row[0], res_row[1]) in diff_rows_sign2:
                # Execute query to get the original row.
                pk = res_row[2:-1]
                where_clause = ' AND '.join("{0} = '{1}'".
                                            format(key, col)
                                            for key, col in zip(ukeys, pk))
                res = server2.exec_query(
                    _COMPARE_SPAN_QUERY.format(db=table2.q_db_name,
                                               table=table2.q_tbl_name,
                                               where=where_clause))

                # Determine if it is a changed or extra row.
                # Check if the same pk_hash is found in table 1.
                if res_row[1] in diff_pk_hash1:
                    # Store original changed row (to UPDATE).
                    changed_in2.append(res[0])
                else:
                    # Store original extra row (to ADD).
                    extra_in2.append(res[0])

    # Return a tuple with a tuple for each table, containing the changed and
    # extra original row for each table.
    return (changed_in1, extra_in1), (changed_in2, extra_in2)


def _get_formatted_rows(rows, table, fmt='GRID', col_widths=None):
    """Get a printable representation of the data rows

    This method generates a formatted view of the rows from a table. The output
    format can be in one of GRID, CSV, TAB, or VERTICAL. This output is
    returned as a list of strings for use in storing the output for later
    presentation.

    rows[in]          missing rows
    table[in]         a Table instance of the table
    obj1_str[in]      full table name for base table
    obj2_str[in]      full table name for other table
    fmt[in]           format to print
    col_widths[in]    column widths to use instead of actual col

    Returns list of formatted rows
    """
    result_rows = []
    if not col_widths:
        col_widths = []
    outfile = tempfile.TemporaryFile()
    to_sql = False
    if fmt.upper() == 'CSV':
        to_sql = True
    print_list(outfile, fmt, table.get_col_names(), rows, to_sql=to_sql,
               col_widths=col_widths)
    outfile.seek(0)
    for line in outfile.readlines():
        result_rows.append(line.strip('\n'))

    return result_rows


def _generate_data_diff_output(diff_data, table1, table2, used_index, options):
    """Generates the data difference output.

    This function generates the output data for the found data differences
    between two tables, according to the provided options (difftype and
    format).

    diff_data[in]   Tuple with three elements containing the data differences
                    between two tables. The first element contains the rows on
                    both tables but with different values, the second contains
                    the rows found in table1 but not in table2, and the third
                    contains the rows found in table2 but not in table1.
    table1[in]      First compared table (source).
    table2[in]      Second compared table (target).
    used_index[in]  Index (key) used to identify rows.
    options[in]     Dictionary of option (format, difftype, compact, etc.).

    Return a list of difference (strings) generated according to the
    specified options.
    """
    difftype = options.get('difftype', 'unified')
    fmt = options.get('format', 'grid')
    compact_diff = options.get("compact", False)
    table1_name = table1.q_table
    table2_name = table2.q_table
    changed_rows, extra1, extra2 = diff_data
    data_diffs = []

    def get_max_cols(tbl1_rows, tbl2_rows):
        """Get maximum columns for each set of rows

        Find maximum column widths for each column for a pair of tables.

        tbl1_rows[in]  first table rows
        tbl2_rows[in]  second table rows

        Return a list of the columns and the max width for each
        """
        # We need to turn the list of tuples to list of lists
        row_list = []
        for r in tbl1_rows:
            for i in r:
                row_list.append(list(i))
        t1_cols = get_col_widths(table1.get_col_names(), row_list)
        row_list = []
        for r in tbl2_rows:
            for i in r:
                row_list.append(list(i))
        t2_cols = get_col_widths(table2.get_col_names(), row_list)

        # Get max of each
        max_cols = []
        for i in range(0, len(t1_cols)):
            if t1_cols[i] > t2_cols[i]:
                max_cols.append(t1_cols[i])
            elif t1_cols[i] <= t2_cols[i]:
                max_cols.append(t2_cols[i])
        return max_cols

    if len(changed_rows) > 0:
        data_diffs.append("# Data differences found among rows:")
        # Get original changed/extra rows for each table within the given
        # span set 'changed_rows' (excluding unchanged rows).
        # Note: each span can refer to multiple rows.
        tbl1_rows, tbl2_rows = _get_changed_rows_span(table1, table2,
                                                      changed_rows,
                                                      used_index)

        if difftype == 'sql':
            # Compute SQL diff for changed rows.
            data_diffs.extend(transform_data(table1, table2, "UPDATE",
                                             (tbl1_rows[0], tbl2_rows[0])))
            # Compute SQL diff for extra rows in table 1.
            if tbl1_rows[1]:
                data_diffs.extend(transform_data(table1, table2,
                                                 "DELETE", tbl1_rows[1]))
            # Compute SQL diff for extra rows in table 2.
            if tbl2_rows[1]:
                data_diffs.extend(transform_data(table1, table2,
                                                 "INSERT", tbl2_rows[1]))
        else:
            # Ok, to make the comparison more uniform, we need to get the
            # max column widths for each table and use the higher of the
            # two to format the rows in the output.
            max_cols = get_max_cols(tbl1_rows, tbl2_rows)

            # Join changed and extra rows for table 1.
            tbl1_rows = tbl1_rows[0] + tbl1_rows[1]
            rows1 = _get_formatted_rows(tbl1_rows, table1, fmt, max_cols)
            # Join changed and extra rows for table 2.
            tbl2_rows = tbl2_rows[0] + tbl2_rows[1]
            rows2 = _get_formatted_rows(tbl2_rows, table2, fmt, max_cols)
            # Compute diff for all changes between table 1 and 2.
            diff_str = _get_diff(rows1, rows2, table1_name, table2_name,
                                 difftype, compact=compact_diff)
            if len(diff_str) > 0:
                data_diffs.extend(diff_str)

    if len(extra1) > 0:
        # Compute diff for extra rows in table 1.
        rows = _get_rows_span(table1, extra1, used_index)
        if difftype == 'sql':
            data_diffs.extend(transform_data(table1, table2,
                                             "DELETE", rows))
        else:
            data_diffs.append("\n# Rows in {0} not in {1}"
                              "".format(table1_name, table2_name))
            res = _get_formatted_rows(rows, table1, fmt)
            data_diffs.extend(res)

    if len(extra2) > 0:
        # Compute diff for extra rows in table 2.
        rows = _get_rows_span(table2, extra2, used_index)
        if difftype == 'sql':
            data_diffs.extend(transform_data(table1, table2,
                                             "INSERT", rows))
        else:
            data_diffs.append("\n# Rows in {0} not in {1}"
                              "".format(table2_name, table1_name))
            res = _get_formatted_rows(rows, table2, fmt)
            data_diffs.extend(res)

    return data_diffs


def check_consistency(server1, server2, table1_name, table2_name,
                      options=None, diag_msgs=None, reporter=None):
    """Check the data consistency of two tables

    This method performs a comparison of the data in two tables.

    Algorithm:

    This procedure uses a separate temporary compare table that
    contains an MD5 hash of the concatenated values of a row along with a
    MD5 hash of the concatenation of the primary key, the primary key columns,
    and a grouping column named span. By default, before executing this
    procedure the result of CHECKSUM TABLE is compared (which is faster when
    no differences are expected). The remaining algorithm to find row
    differences is only executed if this checksum table test fails or if it is
    skipped by the user.

    The process to calculate differences in table data is as follows:

    0. Compare the result of CHECKSUM TABLE for both tables. If the checksums
       match None is returned and the algorithm ends, otherwise the next steps
       to find row differences are executed.

       Note: The following steps are only executed if the table checksums are
             different or if this preliminary step is skipped by the user.

    1. If binary log on for the client (sql_log_bin = 1), turn it off.

    2. Create the temporary compare table for each table to compare
       (db1.table1, db2.table2)

    3. For each table, populate the compare table using an INSERT statement
       that calculates the MD5 hash for the row.

    4. For each table, a summary result is formed by summing the MD5 hash
       values broken into four parts. The MD5 hash is converted to decimal for
       a numerical sum. This summary query also groups the rows in the compare
       table by the span column which is formed from the first 4 positions of
       the primary key hash.

    5. The summary tables are compared using set methods to find rows (spans)
       that appear in both tables, those only in table1, and those only in
       table2. A set operation that does not match the rows means the summed
       hash is different therefore meaning one or more rows in the span have
       either a missing row in the other table or the data is different. If no
       differences found, skip to (9).

    6. The span values from the sets that contain rows that are different are
       then compared again using set operations. Those spans that are in both
       sets contain rows that have changed while the set of rows in one but not
       the other (and vice-versa) contain rows that are missing.

       Note: it is possible given sufficient density of the table for the
             changed rows span to contain missing rows. This is Ok because the
             output of the difference will still present the data as missing.

    7. The output of (6) that contain the same spans (changed rows) is then
       used to form a difference and this is saved for presentation to the
       user.

    8. The output of (7) that contain missing spans (missing rows) is then
       used to form a formatted list of the results for presentation to the
       user.

       Note: The differences output is generated considering the specified
       changes directions (for server1, server2, or both).

    9. The compare databases are destroyed and differences (if any) are
       returned according to the specified change direction in the options.
       A return value of (None, None) indicates the data is consistent.

    10. Turn binary logging on if turned off in step (1).

    Exceptions:

    server1[in]       first server Server instance
    server2[in]       second server Server instance
    table1_name[in]   name of first table in form 'db.name'
    table2_name[in]   name of second table in form 'db.name'
    options[in]       dictionary of options for the operation containing
                        'format'    : format for output of missing rows
                        'difftype'  : type of difference to show
                        'unique_key': column name for pseudo-key
    diag_msgs[out]    a list of diagnostic and warning messages.
    reporter[in]      Instance of the database compare reporter class.

    Returns tuple - string representations of the primary index columns

    Returns a tuple with the list of differences for server1 and/or server2
            according to the specified direction. If the data is consistent
            then the tuple (None, None) is returned.
    """
    if options is None:
        options = {}
    span_key_size = options.get('span_key_size', DEFAULT_SPAN_KEY_SIZE)
    use_indexes = options.get('use_indexes', None)
    direction = options.get('changes-for', 'server1')
    reverse = options.get('reverse', False)

    table1 = Table(server1, table1_name)
    table2 = Table(server2, table2_name)

    # First, check full table checksum if step is not skipped.
    if reporter:
        reporter.report_object("", "- Compare table checksum")
        reporter.report_state("")
        reporter.report_state("")
    if not options['no_checksum_table']:
        checksum1, err1 = server1.checksum_table(table1.q_table)
        checksum2, err2 = server2.checksum_table(table2.q_table)
        if err1 or err2:
            err_data = (server1.host, server1.port, err1) if err1 \
                else (server2.host, server2.port, err2)
            raise UtilError("Error executing CHECKSUM TABLE on '{0}@{1}': "
                            "{2}".format(*err_data))
        if checksum1 == checksum2:
            if reporter:
                reporter.report_state("pass")
            return None, None  # No data diffs (in any direction)
        else:
            if reporter:
                reporter.report_state("FAIL")
    else:
        if reporter:
            reporter.report_state("SKIP")

    # remove quotations to indexes
    unq_use_indexes = []
    if use_indexes:
        for tbl, index in use_indexes:
            unq_use_indexes.append((
                remove_backtick_quoting(tbl, table1.sql_mode),
                remove_backtick_quoting(index, table1.sql_mode)
            ))
            if table1.sql_mode != table2.sql_mode:
                unq_use_indexes.append((
                    remove_backtick_quoting(tbl, table2.sql_mode),
                    remove_backtick_quoting(index, table2.sql_mode)
                ))

    # if given get the unique_key for table_name
    table1_use_indexes = []
    if use_indexes:
        table1_use_indexes.extend(
            [quote_with_backticks(u_key, table1.sql_mode) for tb_name, u_key
             in unq_use_indexes if table1.tbl_name == tb_name]
        )
    table2_use_indexes = []
    if use_indexes:
        table2_use_indexes.extend(
            [quote_with_backticks(u_key, table2.sql_mode) for tb_name, u_key
             in unq_use_indexes if table2.tbl_name == tb_name]
        )

    if options.get('toggle_binlog', 'False'):
        binlog_server1 = server1.binlog_enabled()
        if binlog_server1:
            # Commit to avoid error setting sql_log_bin inside a transaction.
            server1.rollback()
            server1.toggle_binlog("DISABLE")
        binlog_server2 = server2.binlog_enabled()
        if binlog_server2:
            # Commit to avoid error setting sql_log_bin inside a transaction.
            server2.commit()
            server2.toggle_binlog("DISABLE")
    else:  # set to false to skip after actions to turn binlog back on
        binlog_server1 = False
        binlog_server2 = False

    data_diffs1 = None
    data_diffs2 = None

    # Now, execute algorithm to find row differences.
    if reporter:
        reporter.report_object("", "- Find row differences")
        reporter.report_state("")
        reporter.report_state("")
    # Setup the comparative tables and calculate the hashes
    pri_idx_str1, pri_idx_str2, used_index, used_index_name, msgs = (
        _setup_compare(table1, table2,
                       span_key_size,
                       use_indexes=(table1_use_indexes, table2_use_indexes))
    )

    # Add warnings to print them later.
    if diag_msgs is not None and isinstance(diag_msgs, list):
        diag_msgs.extend(msgs)
        diag_msgs.append("# INFO: for table {0} the index {1} is used to "
                         "compare.".format(table1.tbl_name, used_index_name))

    # Populate the compare tables and retrieve rows from each table
    tbl1_hash = _make_sum_rows(table1, pri_idx_str1, span_key_size)
    tbl2_hash = _make_sum_rows(table2, pri_idx_str2, span_key_size)

    # Compare results (between spans).
    _, in1_not2, in2_not1 = get_common_lists(tbl1_hash, tbl2_hash)

    # If mismatch found, go back to compare table and retrieve grouping.
    if len(in1_not2) != 0 or len(in2_not1) != 0:
        table1_diffs = []
        table2_diffs = []

        # Get keys for diffs on table1
        for row in in1_not2:
            table1_diffs.append(row[0])

        # Get keys for diffs on table2
        for row in in2_not1:
            table2_diffs.append(row[0])

        # Find changed and missing rows
        changed_rows, extra1, extra2 = get_common_lists(table1_diffs,
                                                        table2_diffs)

        # Generate data differences output according to direction.
        if direction == 'server1' or reverse:
            data_diffs1 = _generate_data_diff_output(
                (changed_rows, extra1, extra2), table1, table2, used_index,
                options
            )
        if direction == 'server2' or reverse:
            data_diffs2 = _generate_data_diff_output(
                (changed_rows, extra2, extra1), table2, table1, used_index,
                options
            )

    if binlog_server1:
        # Commit to avoid error setting sql_log_bin inside a transaction.
        server1.commit()
        server1.toggle_binlog("ENABLE")
    if binlog_server2:
        # Commit to avoid error setting sql_log_bin inside a transaction.
        server2.commit()
        server2.toggle_binlog("ENABLE")

    if reporter:
        if data_diffs1 or data_diffs2:
            reporter.report_state('FAIL')
        else:
            reporter.report_state('pass')
    return data_diffs1, data_diffs2
#
# Copyright (c) 2010, 2016 Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains helper methods for formatting output.

METHODS
    format_tabular_list - Format and write row data as a separated-value list
                          or as a grid layout like mysql client query results
                          Writes to a file specified (e.g. sys.stdout)
"""

import codecs
import csv
import os
import textwrap

try:
    import cStringIO as StringIO
except ImportError:
    import StringIO



_MAX_WIDTH = 78
_TWO_COLUMN_DISPLAY = "{0:{1}}  {2:{3}}"


class UnicodeWriter(object):
    """A CSV writer which will write rows to CSV file `f_out`,
    which is encoded in the given encoding.
    """

    def __init__(self, f_out, dialect="excel", encoding="utf-8", **kwds):
        """Contructor

        f_out[in]        file to print to (e.g. sys.stdout)
        dialect[in]      description of the dialect in use by the writer
        encoding[in]     encoding
        """
        # Redirect output to a queue
        self.queue = StringIO.StringIO()
        self.writer = csv.writer(self.queue, dialect=dialect, **kwds)
        self.stream = f_out
        self.encoder = codecs.getincrementalencoder(encoding)()

    def writerow(self, row):
        """Write the row parameter to the writer's file object.

        row[in]     sequence of strings or numbers
        """
        self.writer.writerow([val.encode("utf-8") if isinstance(val, unicode)
                              else val for val in row])
        data = self.queue.getvalue()
        data = data.decode("utf-8")  # pylint: disable=R0204
        data = self.encoder.encode(data)
        self.stream.write(data)
        self.queue.truncate(0)

    def writerows(self, rows):
        """Write all rows parameter to the writer's file object.

        rows[in]     list of row objects
        """
        for row in rows:
            self.writerow(row)


def _format_col_separator(f_out, columns, col_widths, quiet=False):
    """Format a row of the header with column separators

    f_out[in]          file to print to (e.g. sys.stdout)
    columns[in]        list of column names
    col_widths[in]     width of each column
    quiet[in]          if True, do not print
    """
    if quiet:
        return
    stop = len(columns)
    for i in range(0, stop):
        width = int(col_widths[i] + 2)
        f_out.write('{0}{1:{1}<{2}}'.format("+", "-", width))
    f_out.write("+\n")


def _format_row_separator(f_out, columns, col_widths, row, quiet=False):
    """Format a row of data with column separators.

    f_out[in]          file to print to (e.g. sys.stdout)
    columns[in]        list of column names
    col_widths[in]     width of each column
    rows[in]           data to print
    quiet[in]          if True, do not print
    """
    i = 0
    if len(columns) == 1 and row != columns:
        row = [row]
    for i, _ in enumerate(columns):
        if not quiet:
            f_out.write("| ")
        val = row[i].encode("utf-8") if isinstance(row[i], unicode) \
            else row[i]
        if isinstance(val, str):
            val = u"{0:<{1}}".format(val.decode("utf-8"), col_widths[i] + 1)
            f_out.write(val.encode("utf-8"))
        else:
            f_out.write("{0:<{1}} ".format("%s" % val, col_widths[i]))

    if not quiet:
        f_out.write("|")
    f_out.write("\n")


def get_col_widths(columns, rows):
    """
    This function gets the maximum column width for a list of rows

    Returns: list - max column widths
    """
    # Calculate column width for each column
    col_widths = []
    for col in columns:
        size = len(col.decode("utf-8") if isinstance(col, str) else col)
        col_widths.append(size + 1)

    stop = len(columns)
    for row in rows:
        row = [val.encode("utf-8") if isinstance(val, unicode)
               else val for val in row]
        # if there is one column, just use row.
        if stop == 1:
            col_size = len(row[0].decode("utf-8")
                           if isinstance(row[0], str) else str(row[0]))
            col_size += 1
            if col_size > col_widths[0]:
                col_widths[0] = col_size
        else:
            for i in range(0, stop):
                col_size = len(row[i].decode("utf-8")
                               if isinstance(row[i], str) else str(row[i]))
                col_size += 1
                if col_size > col_widths[i]:
                    col_widths[i] = col_size
    return col_widths


def format_tabular_list(f_out, columns, rows, options=None):
    """Format a list in a pretty grid format.

    This method will format and write a list of rows in a grid or CSV list.

    f_out[in]          file to print to (e.g. sys.stdout)
    columns[in]        list of column names
    rows[in]           list of rows to print
    options[in]        options controlling list:
        print_header   if False, do not print header
        separator      if set, use the char specified for a CSV output
        quiet          if True, do not print the grid text (no borders)
        print_footer   if False, do not print footer
        none_to_null   if True converts None values to NULL
    """
    if options is None:
        options = {}
    print_header = options.get("print_header", True)
    separator = options.get("separator", None)
    quiet = options.get("quiet", False)
    print_footer = options.get("print_footer", True)
    none_to_null = options.get("none_to_null", False)
    convert_to_sql = options.get('to_sql', False)

    # do nothing if no rows.
    if len(rows) == 0:
        return
    if separator is not None:
        if os.name == "posix":
            # Use \n as line terminator in POSIX (non-Windows) systems.
            csv_writer = UnicodeWriter(f_out, delimiter=separator,
                                       lineterminator='\n')
        else:
            # Use the default line terminator '\r\n' on Windows.
            csv_writer = UnicodeWriter(f_out, delimiter=separator)
        if print_header:
            csv_writer.writerow(columns)
        for row in rows:
            row = [val.encode("utf-8") if isinstance(val, unicode)
                   else val for val in row]
            if convert_to_sql:
                # Convert value to SQL (i.e. add quotes if needed).
                row = ['NULL' if col is None else to_sql(col) for col in row]
            if none_to_null:
                # Convert None values to 'NULL'
                row = ['NULL' if val is None else val for val in row]
            csv_writer.writerow(row)
    else:
        # Calculate column width for each column
        col_widths = options.get('col_widths', None)
        if not col_widths:
            col_widths = get_col_widths(columns, rows)

        # print header
        if print_header:
            _format_col_separator(f_out, columns, col_widths, quiet)
            _format_row_separator(f_out, columns, col_widths, columns, quiet)
        _format_col_separator(f_out, columns, col_widths, quiet)
        for row in rows:
            # Note: lists need to be converted to tuple as expected by
            # next method (to handle single column rows correctly)
            if convert_to_sql:
                # Convert value to SQL (i.e. add quotes if needed).
                row = tuple(('NULL' if col is None else to_sql(col)
                             for col in row))
            if none_to_null:
                # Convert None values to 'NULL'
                row = tuple(('NULL' if val is None else val for val in row))
            _format_row_separator(f_out, columns, col_widths, row, quiet)
        if print_footer:
            _format_col_separator(f_out, columns, col_widths, quiet)


def format_vertical_list(f_out, columns, rows, options=None):
    r"""Format a list in a vertical format.

    This method will format and write a list of rows in a vertical format
    similar to the \G format in the mysql monitor.

    f_out[in]          file to print to (e.g. sys.stdout)
    columns[in]        list of column names
    rows[in]           list of rows to print
    options[in]        options controlling list:
        none_to_null   if True converts None values to NULL
    """
    if options is None:
        options = {}
    none_to_null = options.get("none_to_null", False)

    # do nothing if no rows.
    if len(rows) == 0:
        return

    max_colwidth = 0
    # Calculate maximum column width for all columns
    for col in columns:
        if len(col) + 1 > max_colwidth:
            max_colwidth = len(col) + 1

    stop = len(columns)
    row_num = 0
    for row in rows:
        row_num += 1
        f_out.write('{0:{0}<{1}}{2:{3}>{4}}. row {0:{0}<{1}}\n'.format("*", 25,
                                                                       row_num,
                                                                       ' ', 8))
        if none_to_null:
            # Convert None values to 'NULL'
            row = ['NULL' if not val else val for val in row]
        for i in range(0, stop):
            col = columns[i].decode("utf-8") \
                if isinstance(columns[i], str) else columns[i]
            val = row[i].decode("utf-8") \
                if isinstance(row[i], str) else row[i]
            out = u"{0:>{1}}: {2}\n".format(col, max_colwidth, val)
            f_out.write(out.encode("utf-8"))

    if row_num > 0:
        row_str = 'rows' if row_num > 1 else 'row'
        f_out.write("{0} {1}.\n".format(row_num, row_str))


def print_list(f_out, fmt, columns, rows, no_headers=False, sort=False,
               to_sql=False, col_widths=None):
    """Print a list< based on format.

    Prints a list of rows in the format chosen. Default is GRID.

    f_out[in]         file to print to (e.g. sys.stdout)
    fmt[in]           Format (GRID, CSV, TAB, VERTICAL)
    columns[in]       Column headings
    rows[in]          Rows to print
    no_headers[in]    If True, do not print headings (column names)
    sort[in]          If True, sort list before printing
    to_sql[out]       If True, converts columns to SQL format before
                      printing them to the output.
    col_widths[in]    col widths to use instead of actual col
    """

    if not col_widths:
        col_widths = []
    if sort:
        rows.sort()
    list_options = {
        'print_header': not no_headers,
        'to_sql': to_sql,
        'col_widths': col_widths,
    }
    if fmt == "vertical":
        format_vertical_list(f_out, columns, rows)
    elif fmt == "tab":
        list_options['separator'] = '\t'
        format_tabular_list(f_out, columns, rows, list_options)
    elif fmt == "csv":
        list_options['separator'] = ','
        format_tabular_list(f_out, columns, rows, list_options)
    else:  # default to table format
        format_tabular_list(f_out, columns, rows, list_options)


def _get_max_key_dict_list(dictionary_list, key, alias_key=None):
    """Get maximum key length for display calculation

    dictionary_list[in]   Dictionary to print
    key[in]               Name of the key
    use_alias[in]         If not None, add alias to width too

    Returns int - max width of key
    """
    def lcal(x):
        """ calculate string length """
        return len(str(x or ''))
    dl = dictionary_list
    tmp = [(lcal(item[key]), lcal(item.get(alias_key, 0))) for item in dl]
    return max([(x[0] + x[1] + 3) if x[1] else x[0] for x in tmp])


def print_dictionary_list(column_names, keys, dictionary_list,
                          max_width=_MAX_WIDTH, use_alias=True,
                          show_header=True):
    """Print a multiple-column list with text wrapping

    column_names[in]       Column headings
    keys[in]               Keys for dictionary items
    dictionary_list[in]    Dictionary to print (list of)
    max_width[in]          Max width
    use_alias[in]          If True, use keys[2] to print an alias
    """
    # max column size for the name
    max_name = _get_max_key_dict_list(dictionary_list, keys[0])
    if max_name < len(column_names[0]):
        max_name = len(column_names[0])
    min_value = 25  # min column size for the value
    max_value = max_width - 2 - max_name  # max column size for the value
    if max_value < min_value:
        max_value = min_value
        max_name = max_width - 2 - max_value

    if show_header:
        print(_TWO_COLUMN_DISPLAY.format(column_names[0], max_name,
                                         column_names[1], max_value))
        print(_TWO_COLUMN_DISPLAY.format('-' * (max_name), max_name,
                                         '-' * max_value, max_value))
    for item in dictionary_list:
        name = item[keys[0]]
        if len(name) > max_name:
            name = "{0}...".format(name[:(max_name - 3)])
        value = item[keys[1]]
        if isinstance(value, (bool, int)) or value is None:
            description = [str(value)]
        elif not value:
            description = ['']
        else:
            description = textwrap.wrap(value, max_value)

        if use_alias and len(keys) > 2 and len(item[keys[2]]) > 0:
            name += ' | ' + item[keys[2]]
        print(_TWO_COLUMN_DISPLAY.format(name, max_name,
                                         description[0], max_value))
        for i in range(1, len(description)):
            print(_TWO_COLUMN_DISPLAY.format('', max_name, description[i],
                                             max_value))


def convert_dictionary_list(dict_list):
    """Convert a dictionary to separated lists of keys and values.

    Convert the list of items of the given dictionary (i.e. pairs key, value)
    to a set of columns containing the keys and a set of rows containing the
    values.

    dict_list[in]    Dictionary with a list of items to convert

    Returns tuple - (columns, rows)
    """
    cols = []
    rows = []
    # First, get a list of the columns
    for node in dict_list:
        for key in node.keys():
            if key not in cols:
                cols.append(key)

    # Now form the rows replacing missing columns with None
    for node in dict_list:
        row = []
        for col in cols:
            row.append(node.get(col, None))
        rows.append(row)

    return (cols, rows)
#
# Copyright (c) 2013, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains a module to read .frm files and attempt to create a
facsimile of the CREATE TABLE command.
"""

import bisect
import os
import stat
import struct
import time


#
# Definitions and types for interpreting the .frm file values.
#

# Misc. constants
_PORTABLE_SIZEOF_CHAR_PTR = 8
_MY_CHARSET_BIN_NUM = 63
_HA_NOSAME = 1
_DIG2BYTES = [0, 1, 1, 2, 2, 3, 3, 4, 4, 4]
_DIG_PER_DEC1 = 9
_HEADER_LEN = 64
_TABLE_TYPE = 0x01fe   # Magic number for table .frm files
_VIEW_TYPE = 0x5954    # Magic number for view .frm files
_FIELD_NR_MASK = 16383
_HA_USES_COMMENT = 4096

# MySQL data type definitions
_MYSQL_TYPE_DECIMAL = 0
_MYSQL_TYPE_TINY = 1
_MYSQL_TYPE_SHORT = 2
_MYSQL_TYPE_LONG = 3
_MYSQL_TYPE_FLOAT = 4
_MYSQL_TYPE_DOUBLE = 5
_MYSQL_TYPE_NULL = 6
_MYSQL_TYPE_TIMESTAMP = 7
_MYSQL_TYPE_LONGLONG = 8
_MYSQL_TYPE_INT24 = 9
_MYSQL_TYPE_DATE = 10
_MYSQL_TYPE_TIME = 11
_MYSQL_TYPE_DATETIME = 12
_MYSQL_TYPE_YEAR = 13
_MYSQL_TYPE_NEWDATE = 14
_MYSQL_TYPE_VARCHAR = 15
_MYSQL_TYPE_BIT = 16
_MYSQL_TYPE_TIMESTAMP2 = 17
_MYSQL_TYPE_DATETIME2 = 18
_MYSQL_TYPE_TIME2 = 19
_MYSQL_TYPE_NEWDECIMAL = 246
_MYSQL_TYPE_ENUM = 247
_MYSQL_TYPE_SET = 248
_MYSQL_TYPE_TINY_BLOB = 249
_MYSQL_TYPE_MEDIUM_BLOB = 250
_MYSQL_TYPE_LONG_BLOB = 251
_MYSQL_TYPE_BLOB = 252
_MYSQL_TYPE_VAR_STRING = 253
_MYSQL_TYPE_STRING = 254
_MYSQL_TYPE_GEOMETRY = 255

# Mapping of field data types to data type names
_col_types = [
    {'value': _MYSQL_TYPE_DECIMAL, 'text': 'decimal', 'size': None},
    {'value': _MYSQL_TYPE_TINY, 'text': 'tinyint', 'size': 1},
    {'value': _MYSQL_TYPE_SHORT, 'text': 'smallint', 'size': 2},
    {'value': _MYSQL_TYPE_LONG, 'text': 'int', 'size': 4},
    {'value': _MYSQL_TYPE_FLOAT, 'text': 'float', 'size': 4},
    {'value': _MYSQL_TYPE_DOUBLE, 'text': 'double', 'size': 8},
    {'value': _MYSQL_TYPE_NULL, 'text': 'NULL', 'size': 0},
    {'value': _MYSQL_TYPE_TIMESTAMP, 'text': 'timestamp', 'size': 4},
    {'value': _MYSQL_TYPE_LONGLONG, 'text': 'bigint', 'size': 8},
    {'value': _MYSQL_TYPE_INT24, 'text': 'mediumint', 'size': 3},
    {'value': _MYSQL_TYPE_DATE, 'text': 'date', 'size': 4},
    {'value': _MYSQL_TYPE_TIME, 'text': 'time', 'size': 3},
    {'value': _MYSQL_TYPE_DATETIME, 'text': 'datetime', 'size': 8},
    {'value': _MYSQL_TYPE_YEAR, 'text': 'year', 'size': 1},
    {'value': _MYSQL_TYPE_NEWDATE, 'text': 'date', 'size': 3},
    # Size must be calculated
    {'value': _MYSQL_TYPE_VARCHAR, 'text': 'varchar', 'size': -1},
    # Size must be calculated
    {'value': _MYSQL_TYPE_BIT, 'text': 'bit', 'size': -2},
    {'value': _MYSQL_TYPE_TIMESTAMP2, 'text': 'timestamp', 'size': 4},
    {'value': _MYSQL_TYPE_DATETIME2, 'text': 'datetime', 'size': 8},
    {'value': _MYSQL_TYPE_TIME2, 'text': 'time', 'size': 3},
    {'value': _MYSQL_TYPE_NEWDECIMAL, 'text': 'decimal', 'size': None},
    {'value': _MYSQL_TYPE_ENUM, 'text': 'enum', 'size': 0},
    {'value': _MYSQL_TYPE_SET, 'text': 'set', 'size': 0},
    {'value': _MYSQL_TYPE_TINY_BLOB, 'text': 'tinyblob',
     'size': 1 + _PORTABLE_SIZEOF_CHAR_PTR},
    {'value': _MYSQL_TYPE_MEDIUM_BLOB, 'text': 'mediumblob',
     'size': 3 + _PORTABLE_SIZEOF_CHAR_PTR},
    {'value': _MYSQL_TYPE_LONG_BLOB, 'text': 'longblob',
     'size': 4 + _PORTABLE_SIZEOF_CHAR_PTR},
    {'value': _MYSQL_TYPE_BLOB, 'text': 'blob',
     'size': 2 + _PORTABLE_SIZEOF_CHAR_PTR},
    # Size must be calculated
    {'value': _MYSQL_TYPE_VAR_STRING, 'text': 'varchar', 'size': -1},
    {'value': _MYSQL_TYPE_STRING, 'text': 'char', 'size': None},
    {'value': _MYSQL_TYPE_GEOMETRY, 'text': 'geometry',
     'size': 4 + _PORTABLE_SIZEOF_CHAR_PTR},
]

_col_keys = [item['value'] for item in _col_types]

# Database/engine type definitions
_DB_TYPE_UNKNOWN = 0
_DB_TYPE_DIAB_ISAM = 1
_DB_TYPE_HASH = 2
_DB_TYPE_MISAM = 3
_DB_TYPE_PISAM = 4
_DB_TYPE_RMS_ISAM = 5
_DB_TYPE_HEAP = 6
_DB_TYPE_ISAM = 7
_DB_TYPE_MRG_ISAM = 8
_DB_TYPE_MYISAM = 9
_DB_TYPE_MRG_MYISAM = 10
_DB_TYPE_BERKELEY_DB = 11
_DB_TYPE_INNODB = 12
_DB_TYPE_GEMINI = 13
_DB_TYPE_NDBCLUSTER = 14
_DB_TYPE_EXAMPLE_DB = 15
_DB_TYPE_ARCHIVE_DB = 16
_DB_TYPE_CSV_DB = 17
_DB_TYPE_FEDERATED_DB = 18
_DB_TYPE_BLACKHOLE_DB = 19
_DB_TYPE_PARTITION_DB = 20
_DB_TYPE_BINLOG = 21
_DB_TYPE_SOLID = 22
_DB_TYPE_PBXT = 23
_DB_TYPE_TABLE_FUNCTION = 24
_DB_TYPE_MEMCACHE = 25
_DB_TYPE_FALCON = 26
_DB_TYPE_MARIA = 27
_DB_TYPE_PERFORMANCE_SCHEMA = 28
_DB_TYPE_FIRST_DYNAMIC = 42
_DB_TYPE_DEFAULT = 127

# Mapping of engine types to engine names
_engine_types = [
    {'value': _DB_TYPE_UNKNOWN, 'text': 'UNKNOWN'},
    {'value': _DB_TYPE_DIAB_ISAM, 'text': 'ISAM'},
    {'value': _DB_TYPE_HASH, 'text': 'HASH'},
    {'value': _DB_TYPE_MISAM, 'text': 'MISAM'},
    {'value': _DB_TYPE_PISAM, 'text': 'PISAM'},
    {'value': _DB_TYPE_RMS_ISAM, 'text': 'RMS_ISAM'},
    {'value': _DB_TYPE_HEAP, 'text': 'HEAP'},
    {'value': _DB_TYPE_ISAM, 'text': 'ISAM'},
    {'value': _DB_TYPE_MRG_ISAM, 'text': 'MERGE'},
    {'value': _DB_TYPE_MYISAM, 'text': 'MYISAM'},
    {'value': _DB_TYPE_MRG_MYISAM, 'text': 'MERGE'},
    {'value': _DB_TYPE_BERKELEY_DB, 'text': 'BDB'},
    {'value': _DB_TYPE_INNODB, 'text': 'INNODB'},
    {'value': _DB_TYPE_GEMINI, 'text': 'GEMINI'},
    {'value': _DB_TYPE_NDBCLUSTER, 'text': 'NDBCLUSTER'},
    {'value': _DB_TYPE_EXAMPLE_DB, 'text': 'EXAMPLE'},
    {'value': _DB_TYPE_ARCHIVE_DB, 'text': 'ARCHIVE'},
    {'value': _DB_TYPE_CSV_DB, 'text': 'CSV'},
    {'value': _DB_TYPE_FEDERATED_DB, 'text': 'FEDERATED'},
    {'value': _DB_TYPE_BLACKHOLE_DB, 'text': 'BLACKHOLE'},
    {'value': _DB_TYPE_PARTITION_DB, 'text': 'PARTITION'},
    {'value': _DB_TYPE_BINLOG, 'text': 'BINLOG'},
    {'value': _DB_TYPE_SOLID, 'text': 'SOLID'},
    {'value': _DB_TYPE_PBXT, 'text': 'PBXT'},
    {'value': _DB_TYPE_TABLE_FUNCTION, 'text': 'FUNCTION'},
    {'value': _DB_TYPE_MEMCACHE, 'text': 'MEMCACHE'},
    {'value': _DB_TYPE_FALCON, 'text': 'FALCON'},
    {'value': _DB_TYPE_MARIA, 'text': 'MARIA'},
    {'value': _DB_TYPE_PERFORMANCE_SCHEMA, 'text': 'PERFORMANCE_SCHEMA'},
    {'value': _DB_TYPE_FIRST_DYNAMIC, 'text': 'DYNAMIC'},
    {'value': _DB_TYPE_DEFAULT, 'text': 'DEFAULT'},
]
_engine_keys = [item['value'] for item in _engine_types]

# Key algorithms
_KEY_ALG = ['UNDEFINED', 'BTREE', 'RTREE', 'HASH', 'FULLTEXT']

# Format definitions
#                            1         2         3
#                  01234567890123456789012345678901
_HEADER_FORMAT = "<BBBBHHIHHIHHHHHBBIBBBBBIIIIBBBHH"
#                        11122222333333444445556666
#                  12346824602468023489012371590124
#                 ***   111111
#             0123456789012345
_COL_DATA = "<BBBBBBBBBBBBBBBH"
#             0123456789111111
#                       012345

# Various flags copied from server source code - some may not be used but
# may find a use as more esoteric table configurations are tested. These
# are derived from fields.h and all may not apply but are included for
# future expansion/features.
_FIELDFLAG_DECIMAL = 1
_FIELDFLAG_BINARY = 1
_FIELDFLAG_NUMBER = 2
_FIELDFLAG_ZEROFILL = 4
_FIELDFLAG_PACK = 120	              # Bits used for packing
_FIELDFLAG_INTERVAL = 256             # mangled with decimals!
_FIELDFLAG_BITFIELD = 512	          # mangled with decimals!
_FIELDFLAG_BLOB = 1024	              # mangled with decimals!
_FIELDFLAG_GEOM = 2048                # mangled with decimals!
_FIELDFLAG_TREAT_BIT_AS_CHAR = 4096   # use Field_bit_as_char
_FIELDFLAG_LEFT_FULLSCREEN = 8192
_FIELDFLAG_RIGHT_FULLSCREEN = 16384
_FIELDFLAG_FORMAT_NUMBER = 16384      # predit: ###,,## in output
_FIELDFLAG_NO_DEFAULT = 16384         # sql
_FIELDFLAG_SUM = 32768                # predit: +#fieldflag
_FIELDFLAG_MAYBE_NULL = 32768         # sql
_FIELDFLAG_HEX_ESCAPE = 0x10000
_FIELDFLAG_PACK_SHIFT = 3
_FIELDFLAG_DEC_SHIFT = 8
_FIELDFLAG_MAX_DEC = 31
_FIELDFLAG_NUM_SCREEN_TYPE = 0x7F01
_FIELDFLAG_ALFA_SCREEN_TYPE = 0x7800

# Additional flags
_NOT_NULL_FLAG = 1             # Field can't be NULL
_PRI_KEY_FLAG = 2              # Field is part of a primary key
_UNIQUE_KEY_FLAG = 4           # Field is part of a unique key
_MULTIPLE_KEY_FLAG = 8         # Field is part of a key
_BLOB_FLAG = 16                # Field is a blob
_UNSIGNED_FLAG = 32            # Field is unsigned
_HA_PACK_RECORD = 1            # Pack record?
_HA_FULLTEXT = 128             # For full-text search
_HA_SPATIAL = 1024             # For spatial search

# Row type definitions
_ROW_TYPE_DEFAULT, _ROW_TYPE_FIXED, _ROW_TYPE_DYNAMIC, _ROW_TYPE_COMPRESSED, \
    _ROW_TYPE_REDUNDANT, _ROW_TYPE_COMPACT, _ROW_TYPE_PAGE = range(0, 7)

# enum utypes from field.h
_NONE, _DATE, _SHIELD, _NOEMPTY, _CASEUP, _PNR, _BGNR, _PGNR, _YES, _NO, \
    _REL, _CHECK, _EMPTY, _UNKNOWN_FIELD, _CASEDN, _NEXT_NUMBER, \
    _INTERVAL_FIELD, _BIT_FIELD, _TIMESTAMP_OLD_FIELD, _CAPITALIZE, \
    _BLOB_FIELD, _TIMESTAMP_DN_FIELD, _TIMESTAMP_UN_FIELD, \
    _TIMESTAMP_DNUN_FIELD = range(0, 24)

# Array of field data types that can be unsigned
_UNSIGNED_FIELDS = ['TINYINT', 'SMALLINT', 'MEDIUMINT', 'INT', 'INTEGER',
                    'BIGINT', 'REAL', 'DOUBLE', 'FLOAT', 'DECIMAL', 'NUMERIC']

# Array of field data types that can have character set options
_CS_ENABLED = ['CHAR', 'VARCHAR', 'TINYBLOB', 'BLOB', 'MEDIUMBLOB', 'LONGBLOB',
               'ENUM', 'SET']

# Array of index (key) types
_KEY_TYPES = ['PRIMARY', 'UNIQUE', 'MULTIPLE', 'FULLTEXT', 'SPATIAL',
              'FOREIGN_KEY']

# Array of field data types that do not require parens for size
_NO_PARENS = ['TIMESTAMP', 'DATETIME', 'DATE', 'TIME',
              'TINYBLOB', 'BLOB', 'MEDIUMBLOB', 'LONGBLOB',
              'TINYTEXT', 'TEXT', 'MEDIUMTEXT', 'LONGTEXT']

# Array of field data types that are real data
_REAL_TYPES = ['REAL', 'DOUBLE', 'FLOAT', 'DECIMAL', 'NUMERIC']

# Array of blob data types
_BLOB_TYPES = [_MYSQL_TYPE_TINY_BLOB, _MYSQL_TYPE_MEDIUM_BLOB,
               _MYSQL_TYPE_LONG_BLOB, _MYSQL_TYPE_BLOB,
               _MYSQL_TYPE_GEOMETRY]

# Array of data types that do not use keysize for indexes
_NO_KEYSIZE = ['BIT', 'ENUM', 'SET', 'DECIMAL', 'NUMERIC',
               'TIMESTAMP', 'TIME', 'DATETIME']


def _is_decimal(col):
    """Check for decimal data types
    Returns bool - True if column is decimal or numeric.
    """
    return col['field_type_name'].upper() in ['DECIMAL', 'NUMERIC']


def _is_cs_enabled(col):
    """Check for data types that accept character set option
    Returns bool - True if column supports character set option.
    """
    return col['field_type_name'].upper() in _CS_ENABLED


def _is_unsigned(col):
    """Check for unsigned data types
    Returns bool - True if column is an unsigned type.
    """
    return col['field_type_name'].upper() in _UNSIGNED_FIELDS


def _is_real(col):
    """Check for real data types
    Returns bool - True if column is a real type.
    """
    return col['field_type_name'].upper() in _REAL_TYPES


def _is_blob(col):
    """Check for blob data types
    Returns bool - True if column is a blob.
    """
    return col['field_type'] in _BLOB_TYPES


def _is_geometry(flags):
    """Check for geometry field types
    Returns bool - True if geometry type.
    """
    print "flags: %0x" % flags
    return (flags & _FIELDFLAG_GEOM) == _FIELDFLAG_GEOM


def _no_keysize(col):
    """Check for data types that do not use keysize
    Returns bool - True if column is to be exluded from keysize.
    """
    return col['field_type_name'].upper() in _NO_KEYSIZE


def _print_default_values(values):
    """Print default values

    The method prints the default values 2 bytes at a time in hexidecimal
    and ASCII representation (similar to hexdump).

    values[in]         Array of default values
    """
    num_bytes = len(values)
    print "# Default values raw data:"
    i = 0
    while (i < num_bytes):
        def_str = ""
        j = 0
        print "#",
        while (j < 8) and (i < num_bytes):
            print "%02x" % ord(values[i]),
            def_str += values[i]
            i += 1
            j += 1
        print "",
        j = 0
        while (j < 8) and (i < num_bytes):
            print "%02x" % ord(values[i]),
            def_str += values[i]
            i += 1
            j += 1
        print " |",
        print def_str


def _get_pack_length(col):
    """Find the pack length for the field

    col[in]        Column data read for the column to operate

    Returns tuple - (pack_length, field_size)
    """
    size = _col_types[bisect.bisect_left(_col_keys,
                                         col['field_type'])]['size']
    if size == -1:
        col_len = col['bytes_in_col']
        return (1 if int(col_len) < 256 else 2), col_len
    if size == -2:
        col_len = col['bytes_in_col']
        return col_len / 8, col_len
    if size is None:
        return size, col['bytes_in_col']  # It's a string of some sort
    return 0, size


def _get_blob_text(col):
    """Form the correct field name string for blobs and text fields

    col[in]        Column data read for the column to operate

    Returns string - field name string
    """
    type_str = ""
    if col['field_type'] == _MYSQL_TYPE_TINY_BLOB:
        type_str = "tiny"
    elif col['field_type'] == _MYSQL_TYPE_MEDIUM_BLOB:
        type_str = "medium"
    elif col['field_type'] == _MYSQL_TYPE_LONG_BLOB:
        type_str = "long"
    if col['charset'] == _MY_CHARSET_BIN_NUM:
        type_str = "".join([type_str, "blob"])
    else:
        type_str = "".join([type_str, "text"])

    return type_str


def _format_default(col, col_flags, length, decimals):
    """Format a defaut value for printing

    col[in]        Column data dictionary
    col_flags[in]  Flags for column
    length[in]     Length of default value or integer part for floats
    decimals[in]   Number of decimal positions for floats

    Returns string - default clause for CREATE statement.
    """
    default = col['default']
    if isinstance(default, str):
        fmt_str = "'%s'"
    # Check for zerofill:
    elif col_flags & _FIELDFLAG_ZEROFILL:
        if _is_real(col):
            if decimals > 0 and decimals < length:
                if col['field_type_name'].upper() == "DECIMAL":
                    length += 1
                fmt_str = "'" + '%0' + "%s" % length + '.' + \
                          "%s" % decimals + 'f' + "'"
            else:
                fmt_str = "'" + '%0' + "%s" % length + '.' + 'f' + "'"
            if float(default) == 0.0:
                fmt_str = "%s"
                default = "NULL"
        else:
            fmt_str = "'" + '%0' + "%s" % length + 'd' + "'"
    else:
        if _is_real(col):
            if decimals > 0 and decimals < length:
                fmt_str = "'" + '%' + "%s" % (length - 1) + '.' + \
                          "%s" % decimals + 'f' + "'"
            elif decimals == 0:
                fmt_str = "'%d'"
                default = divmod(default, 1)[0]
            else:
                i, decm = divmod(default, 1)
                if decm == 0:
                    fmt_str = "'%d'"
                    default = i
                else:
                    fmt_str = "'%f'"
            if float(default) == 0.0:
                fmt_str = "%s"
                default = "NULL"
        else:
            fmt_str = "'%d'"

    return " DEFAULT " + fmt_str % default


class FrmReader(object):
    """
    This class implements an abstract of the .frm file format. It can be used
    to produce a likeness of the CREATE TABLE command. It is not a 100% match
    because some of the components are missing from the .frm file. For
    example, there are no character set or collation definitions stored so
    unless one has access to the server definitions, these cannot be
    determined.

    The class permits the following operations:

    - show_create_table_statement() - read a .frm file and print its CREATE
      statement. Optionally displays statistics for the .frm file.

    """

    def __init__(self, db_name, table, frm_path, options):
        """Constructor

        db[in]             the database (if known)
        table[in]          table name
        frm_path[in]       full path to .frm file
        options[in]        options for controlling behavior:
            verbosity      print extra data during operations (optional)
                           default value = 0
            quiet          suppress output except CREATE statement
                           default False
            server        path to server for server install
                           default None
            new_engine     substitute engine
                           default None
        """
        self.general_data = None
        self.key_data = None
        self.comment_str = None
        self.engine_str = None
        self.partition_str = None
        self.col_metadata = None
        self.column_data = None
        self.num_cols = 0
        self.default_values = None
        self.frm_file = None
        self.verbosity = options.get('verbosity', 0)
        self.quiet = options.get('quiet', False)
        self.server = options.get('server', None)
        self.new_engine = options.get('new_engine', None)
        self.show_stats = options.get("show_stats", False)
        self.db_name = db_name
        self.table = table
        self.frm_path = frm_path
        self.options = options

        if self.server is None:
            self.csi = None
        else:
            self.csi = CharsetInfo(options)

    def _read_header(self):
        """Read the header information from the file
        """
        try:
            # Skip to header position
            if self.verbosity > 1:
                print "# Skipping to header at : %0000x" % 2
            self.frm_file.seek(2, 0)
            data = self.frm_file.read(_HEADER_LEN)
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot read header.")

        # Read header
        header = struct.unpack(_HEADER_FORMAT, data)
        engine_name = _engine_types[bisect.bisect_left(_engine_keys,
                                                       header[1])]['text']
        self.general_data = {
            'frm_version': header[0],
            'legacy_db_type': engine_name,
            'IO_SIZE': header[4],
            'length': header[6],
            'tmp_key_length': header[7],
            'rec_length': header[8],
            'max_rows': header[10],
            'min_rows': header[11],
            'db_create_pack': header[12] >> 8,  # only want 1 byte
            'key_info_length': header[13],
            'create_options': header[14],
            'frm_file_ver': header[16],
            'avg_row_length': header[17],
            'default_charset': header[18],
            'row_type': header[20],
            'charset_low': header[21],
            'table_charset': (header[21] << 8) + header[18],
            'key_length': header[24],
            'MYSQL_VERSION_ID': header[25],
            'extra_size': header[26],
            'default_part_eng': header[29],
            'key_block_size': header[30],
        }
        # Fix storage engine string if partitioning engine specified
        if self.general_data['default_part_eng'] > 0 and \
           self.new_engine is None:
            self.engine_str = _engine_types[bisect.bisect_left(
                _engine_keys, header[29])]['text']

        return True

    def _read_keys(self):
        """Read key fields from the file
        """
        offset = self.general_data['IO_SIZE']
        try:
            # Skip ahead to key section
            if self.verbosity > 1:
                print "# Skipping to key data at : %0000x" % int(offset)
            self.frm_file.seek(offset, 0)
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot locate keys.")

        # Decipher key parts
        num_keys = struct.unpack("<B", self.frm_file.read(1))[0]
        if num_keys & 0x80:
            next_byte = struct.unpack("<B", self.frm_file.read(1))[0]
            num_keys = (next_byte << 7) | (num_keys & 0x7f)
            low = struct.unpack("<B", self.frm_file.read(1))[0]
            high = struct.unpack("<B", self.frm_file.read(1))[0]
            num_key_parts = low + (high << 8)
            self.frm_file.read(2)
        else:
            num_key_parts = struct.unpack("<B", self.frm_file.read(1))[0],
            self.frm_file.read(4)

        self.key_data = {
            'num_keys': num_keys,
            'num_key_parts': num_key_parts,
            'key_names': [],
            'keys': [],
        }

        for i in range(0, self.key_data['num_keys']):
            key_info = {
                'flags': struct.unpack("<H", self.frm_file.read(2))[0],
                'key_length': struct.unpack("<H", self.frm_file.read(2))[0],
                'num_parts': struct.unpack("<B", self.frm_file.read(1))[0],
                'algorithm': struct.unpack("<B", self.frm_file.read(1))[0],
                'block_size': struct.unpack("<H", self.frm_file.read(2))[0],
                'key_parts': [],
                'comment': "",
            }
            for j in range(0, key_info['num_parts']):
                if self.verbosity > 1:
                    print "# Reading key part %s." % j
                key_part_info = {
                    'field_num': struct.unpack(
                        "<H", self.frm_file.read(2))[0] & _FIELD_NR_MASK,
                    'offset': struct.unpack("<H",
                                            self.frm_file.read(2))[0] - 1,
                    'key_type': struct.unpack("<H",
                                              self.frm_file.read(2))[0],
                    'key_part_flag': struct.unpack("<B",
                                                   self.frm_file.read(1))[0],
                    'length': struct.unpack("<H",
                                            self.frm_file.read(2))[0],
                }
                key_info['key_parts'].append(key_part_info)
            self.key_data['keys'].append(key_info)

        terminator = struct.unpack("c", self.frm_file.read(1))[0]
        for i in range(0, self.key_data['num_keys']):
            key_name = ""
            # Read until the next 0xff
            char_read = ""
            while char_read != terminator:
                char_read = struct.unpack("c", self.frm_file.read(1))[0]
                if char_read != terminator:
                    key_name += str(char_read)
            self.key_data['key_names'].append(key_name)

        # Now find the key comments!
        self.frm_file.read(1)
        for i in range(0, self.key_data['num_keys']):
            if (self.key_data['keys'][i]['flags'] & _HA_USES_COMMENT) == \
               _HA_USES_COMMENT:
                k_len = struct.unpack("<H", self.frm_file.read(2))[0]
                com_str = struct.unpack("c" * k_len, self.frm_file.read(k_len))
                self.key_data['keys'][i]['comment'] = "".join(com_str)

        return True

    def _read_comment(self):
        """Read the table comments.
        """
        # Fields can be found 1 IO_SIZE more than what has been read to date
        # plus 46 bytes.
        io_size = self.general_data['IO_SIZE']
        record_offset = io_size + self.general_data['tmp_key_length'] + \
            self.general_data['rec_length']
        offset = (((record_offset / io_size) + 1) * io_size) + 46
        try:
            # Skip to column position
            if self.verbosity > 1:
                print "# Skipping to table comments at : %0000x" % int(offset)
            self.frm_file.seek(offset, 0)
            data = self.frm_file.read(1)
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot read table comment.")

        comment_len = struct.unpack("<B", data)[0]
        com_chars = struct.unpack("c" * comment_len,
                                  self.frm_file.read(comment_len))
        self.comment_str = "".join(com_chars)
        return True

    def _read_default_values(self):
        """Read the default values for all columns
        """
        offset = self.general_data['IO_SIZE'] + \
            self.general_data['tmp_key_length']
        try:
            # Skip ahead to key section
            if self.verbosity > 1:
                print "# Skipping to default data at : %0000x" % \
                    int(offset + 1)
            self.frm_file.seek(offset + 1, 0)
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot find default data.")

        num_bytes = self.general_data['rec_length']
        # allow overflow
        self.default_values = self.frm_file.read(num_bytes + 100)

    def _read_engine_data(self):
        """Read the storage engine data.
        """
        # We must calculate the location of the partition information by
        # locating the storage engine name and if it is 'partition' then read
        # the partition string following that.

        offset = self.general_data['IO_SIZE'] + \
            self.general_data['tmp_key_length'] + \
            self.general_data['rec_length']
        try:
            # Skip ahead to key section
            if self.verbosity > 1:
                print "# Skipping to keys at : %0000x" % int(offset + 2)
            self.frm_file.seek(offset + 2, 0)
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot find engine data.")

        engine_len = struct.unpack("<H", self.frm_file.read(2))[0]

        engine_str = "".join(struct.unpack("c" * engine_len,
                                           self.frm_file.read(engine_len)))

        # Save engine name unless user specified a new engine to use
        if self.engine_str is None:
            if self.new_engine is None:
                self.engine_str = engine_str
            else:
                self.engine_str = self.new_engine

        part_len = struct.unpack("<I", self.frm_file.read(4))[0]
        part_str = "".join(struct.unpack("c" * part_len,
                                         self.frm_file.read(part_len)))
        self.partition_str = " ".join(part_str.split('\n'))

        return True

    def _read_column_names(self, fields_per_screen):
        """Read the table column names.
        """
        # Column names start in 00002152.
        screens_read = 1

        cols = []
        col_in_screen = 0
        for i in range(0, self.num_cols):
            if (col_in_screen == fields_per_screen):
                screens_read += 1
                col_in_screen = 1
                # Do the skips
                self.frm_file.read(8)  # read ahead 8 bytes
                val = '\x20'
                while val == '\x20':  # skip the spaces
                    val = self.frm_file.read(1)
                self.frm_file.read(2)  # read past 2 more bytes
            else:
                col_in_screen += 1
            # get length byte
            col_len = struct.unpack("<B", self.frm_file.read(1))[0]
            col_str = ""
            # Don't copy trailing \x00
            j = 0
            while j < col_len - 1:
                char_found = struct.unpack("c", self.frm_file.read(1))[0]
                col_str += char_found
                j += 1
            # skip trailing \x00 and extra bits except for last col read
            if (i < self.num_cols - 1):
                self.frm_file.read(3)
            cols.append(col_str)
        return (screens_read, cols)

    def _get_decimal_value(self, recpos, col):
        """Get a decimal value from the default column data

        recpos[in]     Position in default row to find data
        col[in]        Column dictionary for the column data

        Returns float - default value retrieved
        """
        # Guard
        if not _is_decimal(col):
            return None
        col_flags = (int(col['flags_extra'] << 8) + col['flags'])
        length = col['bytes_in_col']
        decimals = (col_flags >> _FIELDFLAG_DEC_SHIFT) & _FIELDFLAG_MAX_DEC
        length = length - (1 if decimals else 0) - \
            (1 if (col_flags & _FIELDFLAG_DECIMAL) or (length == 0) else 0)

        # algorithm from bin2decimal()
        # int intg=precision-scale,
        #    intg0=intg/DIG_PER_DEC1, frac0=scale/DIG_PER_DEC1,
        #    intg0x=intg-intg0*DIG_PER_DEC1, frac0x=scale-frac0*DIG_PER_DEC1;
        #
        # return intg0*sizeof(dec1)+dig2bytes[intg0x]+
        #       frac0*sizeof(dec1)+dig2bytes[frac0x];

        intg = length - decimals
        intg0 = intg / _DIG_PER_DEC1
        frac0 = decimals / _DIG_PER_DEC1
        intg0x = intg - (intg0 * _DIG_PER_DEC1)
        frac0x = decimals - (frac0 * _DIG_PER_DEC1)
        int_len = (intg0 * 4 + _DIG2BYTES[intg0x]) - 1  # len of integer part
        frac_len = (frac0 * 4 + _DIG2BYTES[frac0x])   # len of fractional part
        int_val = 0
        shift_num = int_len - 1
        for i in range(0, int_len):
            int_val += ord(self.default_values[recpos + i + 1]) << \
                (shift_num * 8)
            shift_num -= 1
        frac_val = 0
        shift_num = frac_len - 1
        for i in range(0, frac_len):
            frac_val += ord(self.default_values[recpos + int_len + i + 1]) << \
                (shift_num * 8)
            shift_num -= 1
        return float("%s.%s" % (int_val, frac_val))

    def _get_field_defaults(self):
        """Retrieve the default values for the columns.
        """
        max_len = len(self.default_values)
        if self.verbosity > 2:
            _print_default_values(self.default_values)
        for i in range(0, len(self.column_data)):
            col = self.column_data[i]
            recpos = self.column_data[i]['recpos']
            recpos -= 2
            if recpos < 0:
                recpos = 0
            if recpos > max_len:  # safety net
                continue

            # Read default for decimal types
            if _is_decimal(col):
                col['default'] = self._get_decimal_value(recpos, col)
                continue
            len_pos, size = _get_pack_length(col)
            field_cs_num = (col['charset_low'] << 8) + col['charset']
            # Adjust size based on character set maximum length per char
            if _is_cs_enabled(col):
                if self.csi:
                    maxlen = self.csi.get_maxlen(field_cs_num)
                else:
                    maxlen = 1
                size = size / maxlen
            if len_pos is None:
                value = self.default_values[recpos:recpos + size]
            else:
                value = self.default_values[recpos:recpos + len_pos + size]

            # Read default for double type
            if col['field_type'] == _MYSQL_TYPE_DOUBLE:
                col['default'] = struct.unpack('d', value)[0]
                continue

            # Read default for float type
            if col['field_type'] == _MYSQL_TYPE_FLOAT:
                col['default'] = struct.unpack('f', value)[0]
                continue

            # Need to check for column type. Some are binary!
            if len_pos is None:  # Some form of string
                col_str = ""
                for col_def in range(0, len(value)):
                    if value[col_def] != '\x20':
                        col_str += value[col_def]
                col['default'] = '' if len(col_str) == 0 else col_str
            elif len_pos == 0:   # packed numeric
                len_pos = size
            if len_pos == 1:
                col['default'] = struct.unpack("<B", value[0:1])[0]
            elif len_pos == 2:
                col['default'] = struct.unpack("<H", value[0:2])[0]
            elif len_pos == 3:
                col['default'] = struct.unpack("<HB", value[0:3])[0]
            elif len_pos == 4:
                col['default'] = struct.unpack("<I", value[0:4])[0]
            elif len_pos == 8:
                col['default'] = struct.unpack("<Q", value[0:8])[0]

    def _read_column_metadata(self):
        """Read the column metadata (size, flags, etc.).

        Returns dictionary - column definition data
        """
        column_data = []
        # Skip ahead
        try:
            for i in range(0, self.num_cols):
                if self.verbosity > 1:
                    print "# Reading column metadata #%s" % i
                data = struct.unpack(_COL_DATA, self.frm_file.read(17))
                data_type = _col_types[bisect.bisect_left(_col_keys,
                                                          data[13])]
                col_def = {
                    'field_length': data[2],  # 1, +3
                    'bytes_in_col': int(data[3]) + (int(data[4]) << 8),
                    'recpos': (int(data[6]) << 8) +
                              (int(data[5])) + (int(data[4]) << 16),
                    'unireg': data[7],  # 1, +8
                    'flags': data[8],  # 1, +9
                    'flags_extra': data[9],  # 1, +10
                    'unireg_type': data[10],  # 1, +11
                    'charset_low': data[11],  # 1, +12
                    'interval_nr': data[12],  # 1, +13
                    'field_type': data[13],  # 1, +14
                    'field_type_name': data_type['text'],
                    'charset': data[14],  # 1, +15
                    'comment_length': data[15],  # 2, +17
                    'enums': [],
                    'comment': "",
                    'default': None,
                }
                column_data.append(col_def)
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot locate column data")
        return column_data

    def _read_column_data(self):
        """Read the column information from the file.

        This method builds the list of columns including defaults,
        data type, and determines enum and set values.
        """
        # Fields can be found 1 IO_SIZE more than what has been read to date
        # plus 258 bytes.
        io_size = self.general_data['IO_SIZE']
        record_offset = io_size + self.general_data['tmp_key_length'] + \
            self.general_data['rec_length']
        offset = ((((record_offset + self.general_data['key_info_length']) /
                    io_size) + 1) * io_size) + 258
        try:
            # Skip to column position
            if self.verbosity > 1:
                print "# Skipping to column data at : %0000x" % int(offset)
            self.frm_file.seek(offset, 0)
            data = struct.unpack("<HHHHHHHHHHHHH", self.frm_file.read(26))
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot read column header.")
        self.num_cols = data[0]
        self.col_metadata = {
            'num_cols': data[0],
            'pos': data[1],
            'unknown': data[2],
            'n_length': data[3],
            'interval_count': data[4],
            'interval_parts': data[5],
            'int_length': data[6],
            'com_length': data[8],
            'null_fields': data[12],
        }
        if self.verbosity > 1:
            pprint(self.col_metadata)

        # Skip ahead
        try:
            self.frm_file.read(7)
            fields_per_screen = struct.unpack("<B", self.frm_file.read(1))[0]
            if self.verbosity > 1:
                print "# Fields per screen =", fields_per_screen
            self.frm_file.read(46)
            col_names = self._read_column_names(fields_per_screen)[1]
            self.frm_file.read(1)  # skip 1 byte
            self.column_data = self._read_column_metadata()
        except Exception, error:
            if self.verbosity > 1:
                print "EXCEPTION:", error
            raise UtilError("Cannot read column data.")

        # TODO: Add ability to read defaults by modifying _get_field_defaults
        #       method to correctly read the default values. Currently, it
        #       does not read some non-character values correctly. When fixed,
        #       remove this comment and uncomment the following line.
        # self._get_field_defaults()

        # Skip column names
        col_len = 0
        for colname in col_names:
            col_len += len(colname)
        # Skip to enum section
        self.frm_file.read(len(col_names) + col_len + 2)
        intervals = []
        interval_num = 0
        # pylint: disable=R0101
        for i in range(0, len(col_names)):
            self.column_data[i]['name'] = col_names[i]
            # Here we read enums and match them to interval_nr.
            i_num = self.column_data[i]['interval_nr']
            if int(i_num) > 0:
                if interval_num < i_num:
                    interval_num += 1
                    cols = []
                    char_found = 99
                    col_str = ''
                    while char_found != 0:
                        char_found = struct.unpack("B",
                                                   self.frm_file.read(1))[0]
                        if char_found == 255:
                            if len(col_str):
                                cols.append(col_str)
                                col_str = ''
                        else:
                            col_str += chr(char_found)
                    intervals.append(cols)
                self.column_data[i]['enums'].extend(
                    intervals[interval_num - 1])

        # Now read column comments
        for i in range(0, len(col_names)):
            if self.verbosity > 1:
                print "# Column comment:", \
                    self.column_data[i]['comment_length']
            if self.column_data[i]['comment_length'] > 0:
                col_str = ''
                for j in range(0, self.column_data[i]['comment_length']):
                    if self.verbosity > 3:
                        print "# Reading column data %s." % j
                    char_found = struct.unpack("B", self.frm_file.read(1))[0]
                    col_str += chr(char_found)
                self.column_data[i]['comment'] = col_str

        return True

    def _get_charset_collation(self, col):
        """Get the character set and collation for column

        col[in]        Column data dictionary

        Returns list - option strings for charset and collation if needed
        """
        parts = []
        field_cs_num = (col['charset_low'] << 8) + col['charset']
        table_cs_num = self.general_data['table_charset']
        # If no character set information, add unknown tag to prompt user
        if self.csi is None:
            if field_cs_num is not None and table_cs_num is not None and \
               field_cs_num != 'binary' and table_cs_num != field_cs_num:
                parts.append(" CHARACTER SET <UNKNOWN>")
            return parts
        field_cs_name = self.csi.get_name(field_cs_num)
        table_cs_name = self.csi.get_name(table_cs_num)
        if field_cs_name is not None and table_cs_name is not None and \
           field_cs_name != 'binary' and table_cs_name != field_cs_name:
            parts.append(" CHARACTER SET `%s`" % field_cs_name)

        elif (field_cs_name is None or table_cs_name is None) and \
                not self.quiet:
            print "C",
            print "# WARNING: Cannot get character set name for id =", id
            parts.append(" CHARACTER SET <UNKNOWN>")
        else:
            parts.append("")

        # Get collation
        def_field_col = self.csi.get_default_collation(field_cs_num)
        field_col = self.csi.get_collation(field_cs_num)
        if def_field_col is not None and field_col is not None and \
           def_field_col[1] != field_col:
            parts.append(" COLLATE `%s`" % field_col)
        elif def_field_col is None and not self.quiet:
            print "# WARNING: Cannot get default collation for id =", id
        elif field_col is None and not self.quiet:
            print "# WARNING: Cannot get collation for id =", id
        else:
            parts.append("")

        return parts

    def _get_column_definitions(self):
        """Build the column definitions

        This method constructs the column definitions from the column data
        read from the file.

        Returns list of strings - column definitions
        """

        def _is_no_parens(col):
            """Check for column uses parens for size
            Returns bool - True if column needs parens for size.
            """
            # If the server version is 5.7.5 or before, we add YEAR to the
            # no parenthesis list. Otherwise, we print the length: YEAR(4)
            ver_str = str(self.general_data['MYSQL_VERSION_ID'])
            vers = (int(ver_str[0]), int(ver_str[1:3]), int(ver_str[3:]))
            # Check to see if it is in the list
            try:
                index_year = _NO_PARENS.index('YEAR')
            except ValueError:
                index_year = None
            if not ((vers[0] >= 5) and (vers[1] >= 7) and (vers[2] >= 5)):
                if not index_year:
                    _NO_PARENS.append("YEAR")
            elif index_year:
                _NO_PARENS.pop(_NO_PARENS.index('YEAR'))
            return col['field_type_name'].upper() in _NO_PARENS

        columns = []
        stop = len(self.column_data)
        for i in range(0, stop):
            col = self.column_data[i]
            col_flags = (int(col['flags_extra'] << 8) + col['flags'])
            length = int(col['bytes_in_col'])
            # Here we need to check for charset maxlen and adjust accordingly
            field_cs_num = (col['charset_low'] << 8) + col['charset']
            if self.csi:
                maxlen = self.csi.get_maxlen(field_cs_num)
            else:
                maxlen = 1
            # Only convert the length for character type fields
            if _is_cs_enabled(col):
                length = length / maxlen
            decimals = int((col_flags >> _FIELDFLAG_DEC_SHIFT) &
                           _FIELDFLAG_MAX_DEC)
            col_parts = []
            # name, data type, length
            # If enum or set values, put those in definition
            if col['enums']:
                col_str = "  `%s` %s(" % (col['name'], col['field_type_name'])
                col_str += ",".join(["'%s'" % i for i in col['enums']])
                col_str += ")"
                col_parts.append(col_str)
            elif _is_no_parens(col) and not _is_blob(col):
                col_parts.append("  `%s` %s" %
                                 (col['name'],
                                  col['field_type_name'].lower()))
            # for blobs
            elif _is_blob(col):
                col_parts.append("  `%s` %s" % (col['name'],
                                                _get_blob_text(col)))
            # for real types:
            elif _is_real(col):
                length_str = ""
                if _is_decimal(col):
                    length = length - (1 if decimals else 0) - \
                        (1 if (col_flags & _FIELDFLAG_DECIMAL) or
                         (length == 0) else 0)
                if decimals == _FIELDFLAG_MAX_DEC:
                    if col['field_type_name'].upper() not in \
                       ["FLOAT", "DOUBLE"]:
                        length_str = "(%s)" % length
                else:
                    length_str = "(%s,%s)" % (length, decimals)
                col_parts.append("  `%s` %s%s" %
                                 (col['name'],
                                  col['field_type_name'].lower(),
                                  length_str))
            else:
                col_parts.append(
                    "  `%s` %s(%s)" % (col['name'],
                                       col['field_type_name'].lower(),
                                       length)
                )

            # unsigned
            if col_flags & _FIELDFLAG_DECIMAL == 0 and _is_unsigned(col):
                col_parts.append(" unsigned")

            # zerofill
            if col_flags & _FIELDFLAG_ZEROFILL and _is_unsigned(col):
                col_parts.append(" zerofill")

            # character set and collation options
            if _is_cs_enabled(col):
                col_parts.extend(self._get_charset_collation(col))

            # null
            if col_flags & _FIELDFLAG_MAYBE_NULL:
                if not col['default']:
                    col_parts.append(" DEFAULT NULL")
            elif not _is_blob(col):
                col_parts.append(" NOT NULL")

            # default - Check the _FIELDFLAG_NO_DEFAULT flag. If this flag
            #           is set, there is no default.
            default = col['default']
            if col['field_type'] in [_MYSQL_TYPE_TIMESTAMP,
                                     _MYSQL_TYPE_TIMESTAMP2]:
                col_parts.append(" DEFAULT CURRENT_TIMESTAMP "
                                 "ON UPDATE CURRENT_TIMESTAMP")
            elif col_flags & _FIELDFLAG_NO_DEFAULT == 0 and \
                    default is not None:
                col_parts.append(_format_default(col, col_flags,
                                                 length, decimals))

            # auto increment
            if col['unireg_type'] == _NEXT_NUMBER:
                col_parts.append(" AUTO_INCREMENT")

            if len(col['comment']) > 0:
                col_parts.append(" comment '%s'" % col['comment'])

            # if not the last column or if there are keys, append comma
            if i < stop - 1 or self.key_data['num_keys'] > 0:
                col_parts.append(",")
            col_parts.append(" ")
            columns.append("".join(col_parts))

        return columns

    def _get_key_size(self, col, key_info, flags):
        """Get the key size option for column

        col[in]        Column data dictionary
        key_info[in]   Key information
        flags[in]      Key flags

        Returns string - string of (N) for size or None for no size information
        """
        size_info = None
        if _no_keysize(col) or self.csi is None:
            return size_info
        key_len = int(key_info['length'])
        pack_len = _get_pack_length(col)
        if col['field_type_name'].upper() == "VARCHAR":
            field_len = int(col['field_length'])
        elif (_is_real(col) or _is_unsigned(col) or _is_decimal(col)) and \
                pack_len[0]:
            field_len = int(pack_len[0])
        else:
            field_len = int(pack_len[1])
        field_cs_num = (col['charset_low'] << 8) + col['charset']
        if self.csi:
            maxlen = self.csi.get_maxlen(field_cs_num)
        else:
            maxlen = 1

        # Geometry is an exception
        if col['field_type'] == _MYSQL_TYPE_GEOMETRY:
            if self.csi:
                size_info = "(%d)" % key_len
            else:
                size_info = "(UNKNOWN)"

        elif field_len != key_len and \
                not int(flags) & _HA_FULLTEXT and not int(flags) & _HA_SPATIAL:
            if self.csi:
                size_info = "(%d)" % (key_len / maxlen)
            else:
                size_info = "(UNKNOWN)"
        return size_info

    def _get_key_columns(self):
        """Build the key column definitions

        This method constructs the key definitions from the column data
        read from the file.

        Returns list of strings - key column definitions
        """
        keys = []
        key_info = zip(self.key_data['key_names'], self.key_data['keys'])
        num_keys = len(key_info)
        i = 0
        for key, info in key_info:
            if key == "PRIMARY":
                key_prefix = "PRIMARY KEY"
            elif not info['flags'] & _HA_NOSAME:
                key_prefix = "UNIQUE KEY"
            else:
                key_prefix = "KEY"
            key_str = "%s `%s` (%s)"
            key_cols = ""
            for k in range(0, len(info['key_parts'])):
                key_part = info['key_parts'][k]
                col = self.column_data[key_part['field_num'] - 1]
                key_cols += "`%s`" % col['name']
                size_str = self._get_key_size(col, key_part, info['flags'])
                if size_str:
                    key_cols += size_str
                if k < len(info['key_parts']) - 1:
                    key_cols += ","
            algorithm = _KEY_ALG[info['algorithm']]
            if algorithm != 'UNDEFINED':
                key_str += " USING %s" % algorithm
            if i < num_keys - 1:
                key_str += ","
            keys.append(key_str % (key_prefix, key, key_cols))
            i += 1
        return keys

    def _get_table_options(self):
        """Read the general table options from the file.

        Returns string - options string for CREATE statement
        """
        options = []

        gen = self.general_data   # short name to save indent, space

        options.append(") ENGINE=%s" % self.engine_str)

        if self.partition_str is not None and len(self.partition_str):
            options.append("%s" % self.partition_str)

        if gen['avg_row_length'] > 0:
            options.append("AVG_ROW_LENGTH = %s" % gen['avg_row_length'])

        if gen['key_block_size'] > 0:
            options.append("KEY_BLOCK_SIZE = %s" % gen['key_block_size'])

        if gen['max_rows'] > 0:
            options.append("MAX_ROWS = %s" % gen['max_rows'])

        if gen['min_rows'] > 0:
            options.append("MIN_ROWS = %s" % gen['min_rows'])

        if gen['default_charset'] > 0:
            # If no character set information, add unknown tag to prompt user
            if self.csi:
                c_id = int(gen['default_charset'])
                cs_name = self.csi.get_name(c_id)
                if cs_name is not None:
                    options.append("DEFAULT CHARSET=%s" % cs_name)
                elif not self.quiet:
                    print "# WARNING: Cannot find character set by id =", c_id

                # collation
                def_col = self.csi.get_default_collation(c_id)
                col = self.csi.get_collation(c_id)
                if def_col is not None and col is not None and def_col != col:
                    options.append("COLLATE=`%s`" % col)
                elif def_col is None and not self.quiet:
                    print "# WARNING: Cannot find default collation " + \
                          "for table using id =", c_id
                elif col is None and not self.quiet:
                    print "# WARNING: Cannot find collation for table " + \
                        "using id =", c_id

        row_format = ""
        row_type = int(gen['row_type'])
        if row_type == _ROW_TYPE_FIXED:
            row_format = "FIXED"
        elif row_type == _ROW_TYPE_DYNAMIC:
            row_format = "DYNAMIC"
        elif row_type == _ROW_TYPE_COMPRESSED:
            row_format = "COMPRESSED"
        elif row_type == _ROW_TYPE_REDUNDANT:
            row_format = "REDUNDANT"
        elif row_type == _ROW_TYPE_COMPACT:
            row_format = "COMPACT"
        if len(row_format) > 0:
            options.append("ROW_FORMAT = %s" % row_type)

        if self.comment_str is not None and len(self.comment_str):
            options.append("COMMENT '%s'" % self.comment_str)

        if len(options) > 1:
            return options[0] + " " + ", ".join(options[1:]) + ";"
        return options[0] + ";"

    def _build_create_statement(self):
        """Build the create statement for the .frm file.

        This method builds the CREATE TABLE information as read from
        the file.

        Returns string - CREATE TABLE string
        """
        if self.general_data is None:
            raise UtilError("Header information missing.")

        # CREATE statement preamble
        parts = []

        # Create preamble
        preamble = "CREATE TABLE %s`%s` ("
        if self.db_name is not None and len(self.db_name) > 1:
            db_str = "`%s`." % self.db_name
        else:
            db_str = ""
        parts.append(preamble % (db_str, self.table))

        # Get columns
        parts.extend(self._get_column_definitions())

        # Get indexes
        parts.extend(self._get_key_columns())

        # Create postamble and table options
        parts.append(self._get_table_options())

        return "\n".join(parts)

    def get_type(self):
        """Return the file type - TABLE or VIEW
        """
        # Fail if we cannot read the file
        try:
            self.frm_file = open(self.frm_path, "rb")
        except Exception, error:
            raise UtilError("The file %s cannot be read.\n%s" %
                            (self.frm_path, error))

        # Read the file type
        file_type = struct.unpack("<H", self.frm_file.read(2))[0]

        # Close file and exit
        self.frm_file.close()

        # Take action based on file type
        if file_type == _TABLE_TYPE:
            return "TABLE"
        elif file_type == _VIEW_TYPE:
            return "VIEW"
        else:
            return "UNKNOWN"

    def show_statistics(self):
        """Show general file and table statistics
        """

        print "# File Statistics:"
        file_stats = os.stat(self.frm_path)
        file_info = {
            'Size': file_stats[stat.ST_SIZE],
            'Last Modified': time.ctime(file_stats[stat.ST_MTIME]),
            'Last Accessed': time.ctime(file_stats[stat.ST_ATIME]),
            'Creation Time': time.ctime(file_stats[stat.ST_CTIME]),
            'Mode': file_stats[stat.ST_MODE],
        }
        for value, data in file_info.iteritems():
            print "#%22s : %s" % (value, data)
        print

        # Fail if we cannot read the file
        try:
            self.frm_file = open(self.frm_path, "rb")
        except Exception, error:
            raise UtilError("The file %s cannot be read.\n%s" %
                            (self.frm_path, error))

        # Read the file type
        file_type = struct.unpack("<H", self.frm_file.read(2))[0]

        # Take action based on file type
        if file_type != _TABLE_TYPE:
            return

        # Read general information
        self._read_header()

        # Close file and exit
        self.frm_file.close()

        version = str(self.general_data['MYSQL_VERSION_ID'])
        ver_str = "%d.%d.%d" % (int(version[0]), int(version[1:3]),
                                int(version[3:]))
        def_part_eng = 'None'
        if self.general_data['default_part_eng'] > 0:
            def_part_eng = _engine_types[bisect.bisect_left(
                _engine_keys,
                self.general_data['default_part_eng'])]['text']
        print "# Table Statistics:"
        table_info = {
            'MySQL Version': ver_str,
            'frm Version': self.general_data['frm_version'],
            'Engine': self.general_data['legacy_db_type'],
            'IO_SIZE': self.general_data['IO_SIZE'],
            'frm File_Version': self.general_data['frm_file_ver'],
            'Def Partition Engine': def_part_eng,
        }
        for value, data in table_info.iteritems():
            print "#%22s : %s" % (value, data)
        print

    def show_create_table_statement(self):
        """Show the CREATE TABLE statement

        This method reads the .frm file specified in the constructor and
        builds a fascimile CREATE TABLE statement if the .frm file describes
        a table. For views, the method displays the CREATE VIEW statement
        contained in the file.
        """
        if not self.quiet:
            print "# Reading .frm file for %s:" % self.frm_path

        # Fail if we cannot read the file
        try:
            self.frm_file = open(self.frm_path, "rb")
        except Exception, error:
            raise UtilError("The file %s cannot be read.\n%s" %
                            (self.frm_path, error))

        # Read the file type
        file_type = struct.unpack("<H", self.frm_file.read(2))[0]

        # Take action based on file type
        if file_type == _TABLE_TYPE:
            if not self.quiet:
                print "# The .frm file is a TABLE."

            # Read general information
            self._read_header()
            if self.verbosity > 1:
                print "# General Data from .frm file:"
                pprint(self.general_data)

            # Read key information
            self._read_keys()
            if self.verbosity > 1:
                print "# Index (key) Data from .frm file:"
                pprint(self.key_data)

            # Read default field values information
            self._read_default_values()

            # Read partition information
            self._read_engine_data()
            if self.verbosity > 1:
                print "# Engine string:", self.engine_str
                print "# Partition string:", self.partition_str

            # Read column information
            self._read_column_data()
            if self.verbosity > 1:
                print "# Column Data from .frm file:"
                pprint(self.column_data)
                print "# Number of columns:", self.num_cols
                pprint(self.column_data[1:])

            # Read comment
            self._read_comment()
            if self.verbosity > 1:
                print "# Comment:", self.comment_str

            if self.csi is not None and self.verbosity > 2:
                print "# Character sets read from server:"
                self.csi.print_charsets()

            create_table_statement = self._build_create_statement()
            if not self.quiet:
                print "# CREATE TABLE Statement:\n"
            print create_table_statement
            print

        elif file_type == _VIEW_TYPE:
            # Skip heading
            self.frm_file.read(8)
            view_data = {}
            for line in self.frm_file.readlines():
                field, value = line.strip('\n').split("=", 1)
                view_data[field] = value
            if self.verbosity > 1:
                pprint(view_data)
            if not self.quiet:
                print "# CREATE VIEW Statement:\n"
            print view_data['query']
            print
        else:
            raise UtilError("Invalid file type. Magic bytes = %02x" %
                            file_type)

        # Close file and exit
        self.frm_file.close()

    def change_storage_engine(self):
        """Change the storage engine in an .frm file to MEMORY

        This method edits a .frm file to change the storage engine to the
        the MEMORY engine.

        CAUTION: Method will change the contents of the file.

        Returns tuple - (original engine type, original engine name,
                         sever version from the file)
        """
        # Here we must change the code in position 0x03 to the engine code
        # and the engine string in body of the file (Calculated location)
        if self.verbosity > 1 and not self.quiet:
            print "# Changing engine for .frm file %s:" % self.frm_path

        # Fail if we cannot read the file
        try:
            self.frm_file = open(self.frm_path, "r+b")
        except Exception, error:
            raise UtilError("The file %s cannot be read.\n%s" %
                            (self.frm_path, error))

        # Read the file type
        file_type = struct.unpack("<H", self.frm_file.read(2))[0]

        # Do nothing if this is a view.
        if file_type == _VIEW_TYPE:
            return None

        # Abort if not table.
        if file_type != _TABLE_TYPE:
            raise UtilError("Invalid file type. Magic bytes = %02x" %
                            file_type)

        # Replace engine value
        self.frm_file.read(1)  # skip 1 byte
        engine_type = struct.unpack("<B", self.frm_file.read(1))[0]

        # Read general information
        self._read_header()
        if self.verbosity > 1:
            print "# General Data from .frm file:"
            pprint(self.general_data)

        engine_str = ""
        server_version = str(self.general_data['MYSQL_VERSION_ID'])

        offset = self.general_data['IO_SIZE'] + \
            self.general_data['tmp_key_length'] + \
            self.general_data['rec_length']

        self.frm_file.seek(offset + 2, 0)

        engine_len = struct.unpack("<H", self.frm_file.read(2))[0]
        engine_str = "".join(struct.unpack("c" * engine_len,
                                           self.frm_file.read(engine_len)))
        if self.verbosity > 1:
            print "# Engine string:", engine_str

        # If this is a CSV storage engine, don't change the engine type
        # and instead create an empty .CSV file
        if engine_type == _DB_TYPE_CSV_DB:
            new_csv = os.path.splitext(self.frm_path)
            f_out = open(new_csv[0] + ".CSV", "w")
            f_out.close()
        elif engine_type == _DB_TYPE_ARCHIVE_DB:
            new_csv = os.path.splitext(self.frm_path)
            f_out = open(new_csv[0] + ".ARZ", "w")
            f_out.close()
        elif engine_type == _DB_TYPE_MRG_MYISAM:
            new_csv = os.path.splitext(self.frm_path)
            f_out = open(new_csv[0] + ".MRG", "w")
            f_out.close()
        elif engine_type == _DB_TYPE_BLACKHOLE_DB:
            pass  # Nothing to do for black hole storage engine
        else:
            # Write memory type
            self.frm_file.seek(3)
            self.frm_file.write(struct.pack("<B", 6))

            # Write memory name
            self.frm_file.seek(offset + 2, 0)
            self.frm_file.write(struct.pack("<H", 6))
            self.frm_file.write("MEMORY")

        # Close file and exit
        self.frm_file.close()

        return engine_type, engine_str, server_version
#
# Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains features to check which users hold privileges, specific or
not, over a given object/list of objects.
"""

from collections import defaultdict


_TABLE_PRIV_QUERY = ("SELECT GRANTEE, IS_GRANTABLE, "
                     "GROUP_CONCAT(PRIVILEGE_TYPE) "
                     "FROM INFORMATION_SCHEMA.TABLE_PRIVILEGES WHERE "
                     "TABLE_SCHEMA='{0}' AND TABLE_NAME='{1}' "
                     "GROUP BY GRANTEE, IS_GRANTABLE")

_DB_PRIVS_QUERY = ("SELECT GRANTEE, IS_GRANTABLE, "
                   "GROUP_CONCAT(PRIVILEGE_TYPE) "
                   "FROM INFORMATION_SCHEMA.SCHEMA_PRIVILEGES WHERE "
                   "TABLE_SCHEMA='{0}' GROUP BY GRANTEE, IS_GRANTABLE")

_GLOBAL_PRIV_QUERY = ("SELECT grantee, IS_GRANTABLE, "
                      "GROUP_CONCAT(privilege_type) FROM "
                      "information_schema.USER_PRIVILEGES GROUP BY GRANTEE,"
                      " IS_GRANTABLE")

_PROCS_PRIV_QUERY = ("SELECT User, Host, Proc_priv FROM "
                     "mysql.procs_priv WHERE db='{0}' AND "
                     "routine_name='{1}'")

_GLOBAL_ALL_PRIVS = set(['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE',
                         'DROP', 'RELOAD', 'SHUTDOWN', 'PROCESS', 'FILE',
                         'REFERENCES', 'INDEX', 'ALTER', 'SHOW DATABASES',
                         'SUPER', 'CREATE TEMPORARY TABLES', 'LOCK TABLES',
                         'EXECUTE', 'REPLICATION SLAVE', 'REPLICATION CLIENT',
                         'CREATE VIEW', 'SHOW VIEW', 'CREATE ROUTINE',
                         'ALTER ROUTINE', 'CREATE USER', 'EVENT', 'TRIGGER',
                         'CREATE TABLESPACE'])

_TABLE_ALL_PRIVS = set(['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE',
                        'DROP', 'REFERENCES', 'INDEX', 'ALTER', 'CREATE VIEW',
                        'SHOW VIEW', 'TRIGGER'])

_DB_ALL_PRIVS = set(['SELECT', 'INSERT', 'UPDATE', 'DELETE', 'CREATE',
                     'DROP', 'REFERENCES', 'INDEX', 'ALTER',
                     'CREATE TEMPORARY TABLES', 'LOCK TABLES', 'EXECUTE',
                     'CREATE VIEW', 'SHOW VIEW', 'CREATE ROUTINE',
                     'ALTER ROUTINE', 'EVENT', 'TRIGGER'])

_ROUTINE_ALL_PRIVS = set(['EXECUTE', 'ALTER ROUTINE'])

DATABASE_TYPE = 'DATABASE'
TABLE_TYPE = 'TABLE'
PROCEDURE_TYPE = 'PROCEDURE'
ROUTINE_TYPE = 'ROUTINE'
FUNCTION_TYPE = 'FUNCTION'
GLOBAL_TYPE = 'GLOBAL'
GLOBAL_LEVEL = 3
DATABASE_LEVEL = 2
OBJECT_LEVEL = 1

ALL_PRIVS_LOOKUP_DICT = {PROCEDURE_TYPE: _ROUTINE_ALL_PRIVS,
                         ROUTINE_TYPE: _ROUTINE_ALL_PRIVS,
                         FUNCTION_TYPE: _ROUTINE_ALL_PRIVS,
                         TABLE_TYPE: _TABLE_ALL_PRIVS,
                         DATABASE_TYPE: _DB_ALL_PRIVS,
                         GLOBAL_TYPE: _GLOBAL_ALL_PRIVS}


def get_table_privs(server, db_name, table_name):
    """ Get the list of grantees and their privileges for a specific table.

    server[in]          Instance of Server class, where the query will be
                        executed.
    db_name[in]     Name of the database where the table belongs to.
    table_name[in]  Name of the table to check.

    Returns list of tuples (<Grantee>, <SET OF GRANTS>).
    """
    tpl_lst = []
    # Get sql_mode in server
    sql_mode = server.select_variable("SQL_MODE")
    # Remove backticks if necessary
    if is_quoted_with_backticks(db_name, sql_mode):
        db_name = remove_backtick_quoting(db_name, sql_mode)
    if is_quoted_with_backticks(table_name, sql_mode):
        table_name = remove_backtick_quoting(table_name, sql_mode)

    # Build query
    query = _TABLE_PRIV_QUERY.format(db_name, table_name)
    res = server.exec_query(query)
    for grantee, grant_option, grants in res:
        grants = set((grant.upper() for grant in grants.split(',')))
        # remove USAGE privilege since it does nothing.
        grants.discard('USAGE')
        if grants:
            if 'Y' in grant_option.upper():
                grants.add('GRANT OPTION')
            tpl_lst.append((grantee, grants))

    return tpl_lst


def get_db_privs(server, db_name):
    """ Get the list of grantees and their privileges for a database.

    server[in]          Instance of Server class, where the query will be
                        executed.
    db_name[in]  Name of the database to check.

    Returns list of tuples (<Grantee>, <SET OF GRANTS>).
    """
    tpl_lst = []
    # Get sql_mode in server
    sql_mode = server.select_variable("SQL_MODE")
    # remove backticks if necessary
    if is_quoted_with_backticks(db_name, sql_mode):
        db_name = remove_backtick_quoting(db_name, sql_mode)

    # Build query
    query = _DB_PRIVS_QUERY.format(db_name)
    res = server.exec_query(query)
    for grantee, grant_option, grants in res:
        grants = set((grant.upper() for grant in grants.split(',')))
        # remove USAGE privilege since it does nothing.
        grants.discard('USAGE')
        if grants:
            if 'Y' in grant_option.upper():
                grants.add('GRANT OPTION')
            tpl_lst.append((grantee, grants))

    return tpl_lst


def get_global_privs(server):
    """ Get the list of grantees and their list of global privileges.

    server[in]          Instance of Server class, where the query will be
                        executed.

    Returns list of tuples (<Grantee>, <SET OF GRANTS>).
    """
    tpl_lst = []
    query = _GLOBAL_PRIV_QUERY
    res = server.exec_query(query)
    for grantee, grant_option, grants in res:
        grants = set((grant.upper() for grant in grants.split(',')))
        # remove USAGE privilege since it does nothing.
        grants.discard('USAGE')
        if grants:
            if 'Y' in grant_option.upper():
                grants.add('GRANT OPTION')
            tpl_lst.append((grantee, grants))
    return tpl_lst


def get_routine_privs(server, db_name, routine_name):
    """ Get the list of grantees and their privileges for a routine.

    server[in]          Instance of Server class, where the query will be
                        executed.
    db_name[in]         Name of the database where the table belongs to.
    routine_name[in]    Name of the routine to check.

    Returns list of tuples (<GRANTEE>, <SET OF GRANTS>).
    """
    tpl_lst = []
    # Get sql_mode in server
    sql_mode = server.select_variable("SQL_MODE")
    # remove backticks if necesssary
    if is_quoted_with_backticks(db_name, sql_mode):
        db_name = remove_backtick_quoting(db_name, sql_mode)
    if is_quoted_with_backticks(routine_name, sql_mode):
        routine_name = remove_backtick_quoting(routine_name, sql_mode)

    # Build query
    query = _PROCS_PRIV_QUERY.format(db_name, routine_name)
    res = server.exec_query(query)
    for user, host, grants in res:
        grants = set((grant.upper() for grant in grants.split(',')))
        # remove USAGE privilege since it does nothing.
        grants.discard('USAGE')
        if grants:
            tpl_lst.append(("'{0}'@'{1}'".format(user, host), grants))
    return tpl_lst


def simplify_grants(grant_set, obj_type):
    """Replaces set of privileges with ALL PRIVILEGES, if possible

    grant_set[in]  set of privileges.
    obj_type[in]   type of the object to which these privileges apply.

    Returns a set with the simplified version of grant_set.
    """
    # Get set with all the privileges for the specified object type.
    all_privs = ALL_PRIVS_LOOKUP_DICT[obj_type]

    # remove USAGE privilege since it does nothing and is not on the global
    # all privileges set of any type
    grant_set.discard('USAGE')

    # Check if grant_set has grant option and remove if before checking
    # if given set of privileges contains all the privileges for the
    # specified type
    grant_opt_set = set(['GRANT OPTION', 'GRANT'])
    has_grant_opt = bool(grant_opt_set.intersection(grant_set))
    if has_grant_opt:
        # Remove grant option.
        grant_set = grant_set.difference(grant_opt_set)
    # Check if remaining privileges can be replaced with ALL PRIVILEGES.
    if all_privs == grant_set:
        grant_set = set(["ALL PRIVILEGES"])
    if has_grant_opt:
        # Insert GRANT OPTION PRIVILEGE again.
        grant_set.add("GRANT OPTION")
    return grant_set


def filter_grants(grant_set, obj_type_str):
    """This method returns a new set with just the grants that are valid to
    a given object type.

    grant_set[in]          Set of grants we want to 'filter'
    obj_type_str[in]       String with the type of the object that we are
                           working with, must be either 'ROUTINE', 'TABLE' or
                           'DATABASE'.

    Returns a new set with just the grants that apply.
    """
    # Get set with all the privs for obj_type
    all_privs_set = ALL_PRIVS_LOOKUP_DICT[obj_type_str]
    # Besides having all the privs from the obj_type, it can also have
    # 'ALL', 'ALL PRIVILEGES' and 'GRANT OPTION'
    all_privs_set = all_privs_set.union(['ALL', 'ALL PRIVILEGES',
                                         'GRANT OPTION'])

    # By intersecting the grants we have with the object type's valid set of
    # grants we will obtain just the set of valid grants.
    return grant_set.intersection(all_privs_set)


def _build_privilege_dicts(server, obj_type_dict, inherit_level=GLOBAL_LEVEL):
    """Builds TABLE, ROUTINE and DB dictionaries with grantee privileges

    server[in]        Server class instance
    obj_type_dict[in] dictionary with the list of objects to obtain the
                      grantee and respective grant information, organized
                      by object type
    inherit_level[in] Level of inheritance that should be taken into account.
                      It must be one of GLOBAL_LEVEL, DATABASE_LEVEL or
                      OBJECT_LEVEL

    This method builds and returns the 3 dictionaries with grantee
    information taking into account the grant hierarchy from mysql, i.e.
    global grants apply to all objects and database grants apply to all
    the database objects (tables, procedures and functions).
    """
    # Get the global Grants:
    global_grantee_lst = get_global_privs(server)
    # Build the Database level grants dict.
    # {db_name: {grantee: set(privileges)}}
    db_grantee_dict = defaultdict(lambda: defaultdict(set))
    for db_name, _ in obj_type_dict[DATABASE_TYPE]:
        db_privs_lst = get_db_privs(server, db_name)
        for grantee, priv_set in db_privs_lst:
            db_grantee_dict[db_name][grantee] = priv_set
        if inherit_level >= GLOBAL_LEVEL:
            # If global inheritance level is turned on, global privileges
            # also apply to the database level.
            for grantee, priv_set in global_grantee_lst:
                db_grantee_dict[db_name][grantee].update(
                    filter_grants(priv_set, DATABASE_TYPE))

    # Build the table Level grants dict.
    # {db_name: {tbl_name: {grantee: set(privileges)}}}
    table_grantee_dict = defaultdict(
        lambda: defaultdict(lambda: defaultdict(set)))

    for db_name, tbl_name in obj_type_dict[TABLE_TYPE]:
        tbl_privs_lst = get_table_privs(server, db_name, tbl_name)
        for grantee, priv_set in tbl_privs_lst:
            table_grantee_dict[db_name][tbl_name][grantee] = priv_set
        # Existing db and global_grantee level privileges also apply to
        # the table level if inherit level is database level or higher
        if inherit_level >= DATABASE_LEVEL:
            # If we already have the privileges for the database where the
            # table is at, we can use that information.
            if db_grantee_dict[db_name]:
                for grantee, priv_set in db_grantee_dict[db_name].iteritems():
                    table_grantee_dict[db_name][tbl_name][grantee].update(
                        filter_grants(priv_set, TABLE_TYPE))
            else:
                # Get the grant information for the db the table is at and
                # merge it together with database grants.
                db_privs_lst = get_db_privs(server, db_name)
                for grantee, priv_set in db_privs_lst:
                    table_grantee_dict[db_name][tbl_name][grantee].update(
                        filter_grants(priv_set, TABLE_TYPE))
                # Now do the same with global grants
                if inherit_level >= GLOBAL_LEVEL:
                    for grantee, priv_set in global_grantee_lst:
                        table_grantee_dict[db_name][tbl_name][grantee].update(
                            filter_grants(priv_set, TABLE_TYPE))

    # Build the ROUTINE Level grants dict.
    # {db_name: {proc_name: {user: set(privileges)}}}
    proc_grantee_dict = defaultdict(
        lambda: defaultdict(lambda: defaultdict(set)))
    for db_name, proc_name in obj_type_dict[ROUTINE_TYPE]:
        proc_privs_lst = get_routine_privs(server, db_name, proc_name)
        for grantee, priv_set in proc_privs_lst:
            proc_grantee_dict[db_name][proc_name][grantee] = priv_set
        # Existing db and global_grantee level privileges also apply to
        # the routine level if inherit level is database level or higher
        if inherit_level >= DATABASE_LEVEL:
            # If we already have the privileges for the database where the
            # routine is at, we can use that information.
            if db_grantee_dict[db_name]:
                for grantee, priv_set in db_grantee_dict[db_name].iteritems():
                    proc_grantee_dict[db_name][proc_name][grantee].update(
                        filter_grants(priv_set, ROUTINE_TYPE))
            else:
                # Get the grant information for the db the routine belongs to
                #  and merge it together with global grants.
                db_privs_lst = get_db_privs(server, db_name)
                for grantee, priv_set in db_privs_lst:
                    proc_grantee_dict[db_name][proc_name][grantee].update(
                        filter_grants(priv_set, ROUTINE_TYPE))
                # Now do the same with global grants.
                if inherit_level >= GLOBAL_LEVEL:
                    for grantee, priv_set in global_grantee_lst:
                        proc_grantee_dict[db_name][proc_name][grantee].update(
                            filter_grants(priv_set, ROUTINE_TYPE))

    # TODO Refactor the code below to remove code repetition.

    # Simplify sets of privileges for databases.
    for grantee_dict in db_grantee_dict.itervalues():
        for grantee, priv_set in grantee_dict.iteritems():
            grantee_dict[grantee] = simplify_grants(priv_set,
                                                    DATABASE_TYPE)

    # Simplify sets of privileges for tables.
    for tbl_dict in table_grantee_dict.itervalues():
        for grantee_dict in tbl_dict.itervalues():
            for grantee, priv_set in grantee_dict.iteritems():
                grantee_dict[grantee] = simplify_grants(priv_set,
                                                        TABLE_TYPE)

    # Simplify sets of privileges for routines.
    for proc_dict in proc_grantee_dict.itervalues():
        for grantee_dict in proc_dict.itervalues():
            for grantee, priv_set in grantee_dict.iteritems():
                grantee_dict[grantee] = simplify_grants(priv_set,
                                                        ROUTINE_TYPE)

    return db_grantee_dict, table_grantee_dict, proc_grantee_dict


def _has_all_privileges(query_priv_set, grantee_priv_set, obj_type):
    """Determines if a grantee has a certain set of privileges.

    query_priv_set[in]     set of privileges to be tested
    grantee_priv_set[in]   list of the privileges a grantee has over the
                           object
    obj_type[in]           string with the type of the object to be tested

    This method's purpose receives a set of privileges to test
    (query_priv_set), a set of privileges that a given grantee user
    possesses over a certain object(grantee_priv_set) and the type of that
    object. It returns True if the set of privileges that
    the user has over the object is a superset of query_priv_set.

    """
    # If the user has GRANT OPTION and and ALL PRIVILEGES, then we can
    # automatically return True
    if ("GRANT OPTION" in grantee_priv_set and
            ('ALL PRIVILEGES' in grantee_priv_set or
             'ALL' in grantee_priv_set)):
        return True

    # Remove USAGE privilege because it is the same has having nothing
    query_priv_set.discard('USAGE')

    # Also if query_priv_set contains ALL or ALL PRIVILEGES we can simply
    # discard the rest of the privileges on the set except for GRANT OPTION
    if 'ALL' in query_priv_set or 'ALL PRIVILEGES' in query_priv_set:
        query_priv_set = set(['ALL PRIVILEGES']).union(
            query_priv_set & set(['GRANT OPTION'])
        )
    else:
        # Remove privileges that do not apply to the type of object
        query_priv_set = query_priv_set.intersection(
            ALL_PRIVS_LOOKUP_DICT[obj_type].union(["GRANT OPTION"]))

    return query_priv_set.issubset(grantee_priv_set)


def get_grantees(server, valid_obj_type_dict, req_privileges=None,
                 inherit_level=GLOBAL_LEVEL):
    """Get grantees and respective grants for the specified objects.

    server[in]            Server class instance
    valid_obj_type_dict   Dict with list of valid object for server, sorted
                          by object type. We assume that each object exists
                          on the server
    req_privileges[in]    Optional set of required privileges
    inherit_level[in]     Level of inheritance that should be taken into
                          account. It must be one of GLOBAL_LEVEL,
                          DATABASE_LEVEL or OBJECT_LEVEL
    """

    # Build the privilege dicts
    db_dict, table_dict, proc_dict = _build_privilege_dicts(
        server, valid_obj_type_dict, inherit_level)

    # Build final dict with grantee/grant information, taking into account
    # required privileges
    # grantee_dict = {obj_type: {obj_name:{grantee:set_privs}}}
    grantee_dict = defaultdict(
        lambda: defaultdict(lambda: defaultdict(set)))

    # pylint: disable=R0101
    for obj_type in valid_obj_type_dict:
        for db_name, obj_name in valid_obj_type_dict[obj_type]:
            if obj_type == DATABASE_TYPE:
                for grantee, priv_set in db_dict[obj_name].iteritems():
                    if req_privileges is not None:
                        if _has_all_privileges(req_privileges,
                                               priv_set, obj_type):
                            grantee_dict[obj_type][obj_name][grantee] = \
                                priv_set
                    else:  # No need to check if it meets privileges
                        grantee_dict[obj_type][obj_name][grantee] = \
                            priv_set
            else:
                # It is either TABLE or ROUTINE and both have equal
                # structure dicts
                if obj_type == TABLE_TYPE:
                    type_dict = table_dict
                else:
                    type_dict = proc_dict

                for grantee, priv_set in \
                        type_dict[db_name][obj_name].iteritems():
                    # Get the full qualified name for the object
                    f_obj_name = "{0}.{1}".format(db_name, obj_name)
                    if req_privileges is not None:
                        if _has_all_privileges(
                                req_privileges, priv_set, obj_type):
                            grantee_dict[obj_type][f_obj_name][grantee] = \
                                priv_set
                    else:
                        grantee_dict[obj_type][f_obj_name][grantee] = \
                            priv_set

    return grantee_dict
#
# Copyright (c) 2014, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains function to manipulate GTIDs.
"""


def get_last_server_gtid(gtid_set, server_uuid):
    """Get the last GTID of the specified GTID set for the given server UUID.

    This function retrieves the last GTID from the specified set for the
    specified server UUID. In more detail, it returns the GTID with the greater
    sequence value that matches the specified UUID.

    Note: The method assumes that GTID sets are grouped by UUID (separated by
    comma ',') and intervals appear in ascending order (i.e., the last one is
    the greater one).

    gtid_set[in]        GTID set to search and get last (greater) GTID value.
    server_uuid[in]     Server UUID to match, as a GTID set might contain data
                        for different servers (UUIDs).

    Returns a string with the last GTID value in the set for the given server
    UUID in the format 'uuid:n'. If no GTID are found in the set for the
    specified server UUID then None is returned.
    """
    uuid_sets = gtid_set.split(',')
    for uuid_set in uuid_sets:
        uuid_set_elements = uuid_set.strip().split(':')
        # Note: UUID are case insensitive, but can appear with mixed cases for
        # some server versions (e.g., for 5.6.9, lower case in server_id
        # variable and upper case in GTID_EXECUTED set).
        if uuid_set_elements[0].lower() == server_uuid.lower():
            last_interval = uuid_set_elements[-1]
            try:
                _, end_val = last_interval.split('-')
                return '{0}:{1}'.format(server_uuid, end_val)
            except ValueError:
                # Error raised for single values (not an interval).
                return '{0}:{1}'.format(server_uuid, last_interval)
    return None


def gtid_set_cardinality(gtid_set):
    """Determine the cardinality of the specified GTID set.

    This function counts the number of elements in the specified GTID set.

    gtid_set[in]    target set of GTIDs to determine the cardinality.

    Returns the number of elements of the specified GTID set.
    """
    count = 0
    uuid_sets = gtid_set.split(',')
    for uuid_set in uuid_sets:
        intervals = uuid_set.strip().split(':')[1:]
        for interval in intervals:
            try:
                start_val, end_val = interval.split('-')
                count = count + int(end_val) - int(start_val) + 1
            except ValueError:
                # Error raised for single values (not an interval).
                count += 1
    return count


def gtid_set_union(gtid_set_a, gtid_set_b):
    """Perform the union of two GTID sets.

    This method computes the union of two GTID sets and returns the result of
    the operation.

    Note: This method support input GTID sets not in the normalized form,
    i.e., with unordered and repeated UUID sets and intervals, but with
    a valid syntax.

    gtid_set_a[in]      First GTID set (set A).
    gtid_set_b[in]      Second GTID set (set B).

    Returns a string with the result of the set union operation between the
    two given GTID sets.
    """
    def get_gtid_dict(gtid_a, gtid_b):
        """Get a dict representation of the specified GTID sets.

        Combine the given GTID sets into a single dict structure, removing
        duplicated UUIDs and string intervals.

        Return a dictionary (not normalized) with the GTIDs contained in both
        input GTID sets. For example, for the given (not normalized) GTID sets
        'uuid_a:2:5-7,uuid_b:4' and 'uuid_a:2:4-6:2,uuid_b:1-3' the follow dict
        will be returned:
        {'uuid_a': set(['2', '5-7', '4-6']), 'uuid_b': set(['4','1-3'])}
        """
        res_dict = {}
        uuid_sets_a = gtid_a.split(',')
        uuid_sets_b = gtid_b.split(',')
        uuid_sets = uuid_sets_a + uuid_sets_b
        for uuid_set in uuid_sets:
            uuid_set_values = uuid_set.split(':')
            uuid_key = uuid_set_values[0]
            if uuid_key in res_dict:
                res_dict[uuid_key] = \
                    res_dict[uuid_key].union(uuid_set_values[1:])
            else:
                res_dict[uuid_key] = set(uuid_set_values[1:])
        return res_dict

    # Create auxiliary dict representation of both input GTID sets.
    gtid_dict = get_gtid_dict(gtid_set_a, gtid_set_b)

    # Perform the union between the GTID sets.
    union_gtid_list = []
    for uuid in gtid_dict:
        intervals = gtid_dict[uuid]
        # Convert the set of string intervals into a single list of tuples
        # with integers, in order to be handled easily.
        intervals_list = []
        for values in intervals:
            interval = values.split('-')
            intervals_list.append((int(interval[0]), int(interval[-1])))
        # Compute the union of the tuples (intervals).
        union_set = []
        for start, end in sorted(intervals_list):
            # Note: no interval start before the next one (ordered list).
            if union_set and start <= union_set[-1][1] + 1:
                # Current interval intersects or is consecutive to the last
                # one in the results.
                if union_set[-1][1] < end:
                    # If the end of the interval is greater than the last one
                    # then augment it (set the new end), otherwise do nothing
                    # (meaning the interval is fully included in the last one).
                    union_set[-1] = (union_set[-1][0], end)
            else:
                # No interval in the results or the interval does not intersect
                # nor is consecutive to the last one, then add it to the end of
                # the results list.
                union_set.append((start, end))
        # Convert resulting union set to a valid string format.
        union_str = ":".join(
            ["{0}-{1}".format(vals[0], vals[1])
             if vals[0] != vals[1] else str(vals[0]) for vals in union_set]
        )
        # Concatenate UUID and add the to the result list.
        union_gtid_list.append("{0}:{1}".format(uuid, union_str))

    # GTID sets are sorted alphabetically, return the result accordingly.
    return ','.join(sorted(union_gtid_list))


def gtid_set_itemize(gtid_set):
    """Itemize the given GTID set.

    Decompose the given GTID set into a list of individual GTID items grouped
    by UUID.

    gtid_set[in]    GTID set to itemize.

    Return a list of tuples with the UUIDs and transactions number for all
    individual items in the GTID set. For example: 'uuid_a:1-3:5,uuid_b:4' is
    converted into [('uuid_a', [1, 2, 3, 5]), ('uuid_b', [4])].
    """
    gtid_list = []
    uuid_sets = gtid_set.split(',')
    for uuid_set in uuid_sets:
        uuid_set_elements = uuid_set.split(':')
        trx_num_list = []
        for interval in uuid_set_elements[1:]:
            try:
                start_val, end_val = interval.split('-')
                trx_num_list.extend(range(int(start_val), int(end_val) + 1))
            except ValueError:
                # Error raised for single values (not an interval).
                trx_num_list.append(int(interval))
        gtid_list.append((uuid_set_elements[0], trx_num_list))
    return gtid_list
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains the following methods design to support common operations
over the ip address or hostnames among the multiple utilities.

Methods:
  parse_connection()         Parse connection parameters
"""

import re
import os
import logging



log = logging.getLogger('ip_parser')

_BAD_CONN_FORMAT = (u"Connection '{0}' cannot be parsed. Please review the "
                    u"used connection string (accepted formats: "
                    u"<user>[:<password>]@<host>[:<port>][:<socket>] or "
                    u"<login-path>[:<port>][:<socket>])")

_BAD_QUOTED_HOST = u"Connection '{0}' has a malformed quoted host"

_MULTIPLE_CONNECTIONS = (u"It appears you are attempting to specify multiple "
                         u"connections. This option does not permit multiple "
                         u"connections")

_UNPARSED_CONN_FORMAT = ("Connection '{0}' not parsed completely. Parsed "
                         "elements '{1}', unparsed elements '{2}'")

_CONN_USERPASS = re.compile(
    r"(?P<fquote>[\'\"]?)"    # First quote
    r"(?P<user>.+?)"          # User name
    r"(?:(?P=fquote))"        # First quote match
    r"(?:\:"                  # Optional :
    r"(?P<squote>[\'\"]?)"    # Second quote
    r"(?P<passwd>.+)"         # Password
    r"(?P=squote))"           # Second quote match
    r"|(?P<sfquote>[\'\"]?)"  # Quote on single user name
    r"(?P<suser>.+)"          # Single user name
    r"(?:(?P=sfquote))"       # Quote match on single user name
)

_CONN_QUOTEDHOST = re.compile(
    r"((?:^[\'].*[\'])|(?:^[\"].*[\"]))"  # quoted host name
    r"(?:\:(\d+))?"                       # Optional port number
    r"(?:\:([\/\\w+.\w+.\-]+))?"          # Optional path to socket
)

_CONN_LOGINPATH = re.compile(
    r"((?:\\\"|[^:])+|(?:\\\'|[^:])+)"  # login-path
    r"(?:\:(\d+))?"                     # Optional port number
    r"(?:\:([\/\\w+.\w+.\-]+))?"        # Optional path to socket
)

_CONN_CONFIGPATH = re.compile(
    r"([\w\:]+(?:\\\"|[^[])+|(?:\\\'|[^[])+)"  # config-path
    r"(?:\[([^]]+))?",                         # group
    re.U
)

_CONN_ANY_HOST = re.compile(
    r"""([\w\.]*%)
       (?:\:{0,1}(.*))                   # capture all the rest
    """, re.VERBOSE)

_CONN_HOST_NAME = re.compile(
    r"""(
        (?:
           (?:
              (?:
                 (?!-)         # must not start with hyphen '-'
                 (?:[\w\d-])*  # must not end with the hyphen
                 [A-Za-z]      # starts with a character from the alphabet
                 (?:[\w\d-])*
                 (?:
                    (?<!-)     # end capturing if a '-' is followed by '.'
                 )
               ){1,63}         # limited length for segment
            )
         (?:                   # following segments
            (?:\.)?            # the segment separator  the dot '.'
            (?:
               (?!-)
               [\w\d-]{1,63}   # last segment
               (?<!-)          #shuld not end with hyphen
            )
          )*
         )
        )
       (.*)                    # capture all the rest
     """, re.VERBOSE)

_CONN_IPv4_NUM_ONLY = re.compile(
    r"""(
          (?:         # start of the IPv4 1st group
             25[0-5]  # this match numbers 250 to 255
                    | # or
             2[0-4]\d # this match numbers from 200 to 249
                    | # or
             1\d\d    # this match numbers from 100 to 199
                    | # or
             [1-9]{0,1}\d # this match numbers from 0 to 99
           )
          (?:         # start of the 3 next groups
             \.       # the prefix '.' like in '.255'
             (?:
                25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d
                      # same group as before
              )
           )
             {3}      # but it will match 3 times of it and prefixed by '.'
          )
          (?:\:{0,1}(.*))
          """, re.VERBOSE)

_CONN_port_ONLY = re.compile(
    r"""(?:
          \]{0,1}             # the ']' of IPv6 -optional
                 \:{0,1}      # the ':' for port number  -optional
                        (
                         \d*  # matches any sequence of numbers
                         )
         )          # end of port number group
        (?:\:{0,1}(.*))      # all the rest to extract the socket
        """, re.VERBOSE)

_CONN_socket_ONLY = re.compile(
    r"""(?:           # Not capturing group of ':'
           \:{0,1}
             ([      # Capturing '\' or '/' file name.ext
               \/\\w+.\w+.\-
               ]+    # to match a path
              )
        )?
       (.*)          # all the rest to advice the user.
    """, re.VERBOSE)

_CONN_IPv6 = re.compile(
    r"""
    \[{0,1}                   # the optional heading '['
    (
     (?!.*::.*::)              # Only a single whildcard allowed
     (?:(?!:)|:(?=:))          # Colon iff it would be part of a wildcard
     (?:                       # Repeat 6 times:
        [0-9a-f]{0,4}          # A group of at most four hexadecimal digits
        (?:(?<=::)|(?<!::):)   # Colon unless preceded by wildcard
     ){6}                      # expecting 6 groups
     (?:                       # Either
        [0-9a-f]{0,4}          # Another group
        (?:(?<=::)|(?<!::):)   # Colon unless preceded by wildcard
        [0-9a-f]{0,4}          # Last group
        (?:(?<=::)             # Colon iff preceded by exacly one colon
           |(?<!:)
           |(?<=:)(?<!::):
         )
      )
     )
     (?:
        \]{0,1}\:{0,1}(.*)     # optional closing ']' and group for the rest
      )
    """, re.VERBOSE)

# Type of address amd Key names for the dictionary IP_matchers
HN = "hostname"
ipv4 = "IPv4"
ipv6 = "IPv6"
ANY_LIKE = "host like"
# This list is used to set an order to the matchers.
IP_matchers_list = [ipv4, ipv6, ANY_LIKE, HN]
# This dictionary is used to identify the matched type..
IP_matchers = {
    ANY_LIKE: _CONN_ANY_HOST,
    HN: _CONN_HOST_NAME,
    ipv4: _CONN_IPv4_NUM_ONLY,
    ipv6: _CONN_IPv6
}


def hostname_is_ip(hostname):
    """Determine hostname is an IP address.

    Return bool - True = is IP address
    """
    if len(hostname.split(":")) <= 1:  # if fewer colons, must be IPv4
        grp = _CONN_IPv4_NUM_ONLY.match(hostname)
    else:
        grp = _CONN_IPv6.match(hostname)
    if not grp:
        return False
    return True


def handle_config_path(config_path, group=None, use_default=True):
    """Retrieve the data associated to the given group.

    config_path[in]    the path to the configuration file.
    group[in]          The group name to retrieve the data from, if None
                       the 'client' group will be use if found and if
                       use_default is True.
    use_default[in]    Use the default 'client' group name, True by default,
                       used if no group is given.

    Returns a dictionary with the options data.
    """
    # first verify if the configuration file exist on the given config_path
    # check config_path as near file as normalized path, then
    # at the default location file.

    if os.name == 'nt':
        default_loc = os.path.join('c:\\', config_path)
    else:
        default_loc = os.path.join('/etc/mysql/', config_path)

    # default group
    default_group = 'client'
    # first look at the given path, if not found look at the default location
    paths = [os.path.normpath(config_path), os.path.normpath(default_loc)]
    req_group = group
    # if not group is given use default.
    if not req_group and use_default:
        req_group = default_group
    for file_loc in paths:
        if os.path.exists(file_loc) and os.path.isfile(file_loc):
            opt_par = MySQLOptionsParser(file_loc)
            dict_grp = opt_par.get_groups_as_dict(req_group)
            if dict_grp:
                return dict_grp[req_group]
            else:
                if group:
                    raise UtilError("The given group '{0}' was not found on "
                                    "the configuration file '{1}'"
                                    "".format(group, file_loc))
                else:
                    raise UtilError("The default group '{0}' was not found "
                                    "on the configuration file '{1}'"
                                    "".format(req_group, file_loc))

    # No configuration file was found
    if paths[0] != paths[1]:
        raise UtilError("Could not find a configuration file neither in the "
                        "given path '{0}' nor the default path '{1}'."
                        "".format(*paths))
    raise UtilError("Could not find a configuration file in the given "
                    "path '{0}'.".format(paths[0]))


def parse_connection(connection_values, my_defaults_reader=None, options=None):
    """Parse connection values.

    The function parses a connection specification of one of the forms::

      - user[:password]@host[:port][:socket]
      - login-path[:port][:socket]

    A dictionary is returned containing the connection parameters. The
    function is designed so that it shall be possible to use it with a
    ``connect`` call in the following manner::

      options = parse_connection(spec)
      conn = mysql.connector.connect(**options)

    conn_values[in]         Connection values in the form:
                            user:password@host:port:socket
                            or login-path:port:socket
    my_defaults_reader[in]  Instance of MyDefaultsReader to read the
                            information of the login-path from configuration
                            files. By default, the value is None.
    options[in]             Dictionary of options (e.g. basedir), from the used
                            utility. By default, it set with an empty
                            dictionary. Note: also supports options values
                            from optparse.

    Notes:

    This method validates IPv4 addresses and standard IPv6 addresses.

    This method accepts quoted host portion strings. If the host is marked
    with quotes, the code extracts this without validation and assigns it to
    the host variable in the returned tuple. This allows users to specify host
    names and IP addresses that are outside of the supported validation.

    Returns dictionary (user, passwd, host, port, socket)
            or raise an exception if parsing error
    """
    if options is None:
        options = {}

    def _match(pattern, search_str):
        """Returns the groups from string search or raise FormatError if it
        does not match with the pattern.
        """
        grp = pattern.match(search_str)
        if not grp:
            raise FormatError(_BAD_CONN_FORMAT.format(connection_values))
        return grp.groups()

    # SSL options, must not be overwritten with those from options.
    ssl_ca = None
    ssl_cert = None
    ssl_key = None
    ssl = None

    # Split on the '@' to determine the connection string format.
    # The user/password may have the '@' character, split by last occurrence.
    conn_format = connection_values.rsplit('@', 1)

    if len(conn_format) == 1:
        # No '@' so try config-path and login-path

        # The config_path and login-path collide on their first element and
        # only differs on their secondary optional values.
        # 1. Try match config_path and his optional value group. If both
        #    matches and the connection data can be retrieved, return the data.
        #    If errors occurs in this step raise them immediately.
        # 2. If config_path matches but group does not, and connection data
        #    can not be retrieved, do not raise errors and try to math
        #    login_path on step 4.
        # 3. If connection data is retrieved on step 2, then try login_path on
        #    next step to overwrite values from the new configuration.
        # 4. If login_path matches, check is .mylogin.cnf exists, if it doesn't
        #    and data configuration was found verify it  for missing values and
        #    continue if they are not any missing.
        # 5. If .mylogin.cnf exists and data configuration is found, overwrite
        #    any previews value from config_path if there is any.
        # 6. If login_path matches a secondary value but the configuration data
        #    could not be retrieved, do not continue and raise any error.
        # 7. In case errors have occurred trying to get data from config_path,
        #    and group did not matched, and in addition no secondary value,
        #    matched from login_path (port of socket) mention that config_path
        #    and login_path were not able to retrieve the connection data.

        # try login_path and overwrite the values.
        # Handle the format: config-path[[group]]
        config_path, group = _match(_CONN_CONFIGPATH, conn_format[0])
        port = None
        socket = None
        config_path_data = None
        login_path_data = None
        config_path_err_msg = None
        login_path = None
        if config_path:
            try:
                # If errors occurs, and group matched: raise any errors as the
                # group is exclusive of config_path.
                config_path_data = handle_config_path(config_path, group)
            except UtilError as err:
                if group:
                    raise
                else:
                    # Convert first letter to lowercase to include in error
                    # message with the correct case.
                    config_path_err_msg = \
                        err.errmsg[0].lower() + err.errmsg[1:]

        if group is None:
            # the conn_format can still be a login_path so continue
            # No '@' then handle has in the format: login-path[:port][:socket]
            login_path, port, socket = _match(_CONN_LOGINPATH, conn_format[0])

            # Check if the login configuration file (.mylogin.cnf) exists
            if login_path and not my_login_config_exists():
                if not config_path_data:
                    util_err_msg = (".mylogin.cnf was not found at is default "
                                    "location: {0}. Please configure your "
                                    "login-path data before using it (use the "
                                    "mysql_config_editor tool)."
                                    "".format(my_login_config_path()))
                    if config_path_err_msg and not (port or socket):
                        util_err_msg = ("{0} In addition, {1}"
                                        "").format(util_err_msg,
                                                   config_path_err_msg)
                    raise UtilError(util_err_msg)

            else:
                # If needed, create a MyDefaultsReader and search for
                # my_print_defaults tool.
                if not my_defaults_reader:
                    try:
                        my_defaults_reader = MyDefaultsReader(options)
                    except UtilError as err:
                        if config_path_err_msg and not (port or socket):
                            util_err_msg = ("{0} In addition, {1}"
                                            "").format(err.errmsg,
                                                       config_path_err_msg)
                            raise UtilError(util_err_msg)
                        else:
                            raise

                elif not my_defaults_reader.tool_path:
                    my_defaults_reader.search_my_print_defaults_tool()

                # Check if the my_print_default tool is able to read a
                # login-path from the mylogin configuration file
                if not my_defaults_reader.check_login_path_support():
                    util_err_msg = ("the used my_print_defaults tool does not "
                                    "support login-path options: {0}. "
                                    "Please confirm that the location to a "
                                    "tool with login-path support is included "
                                    "in the PATH (at the beginning)."
                                    "".format(my_defaults_reader.tool_path))
                    if config_path_err_msg and not (port or socket):
                        util_err_msg = ("{0} In addition, {1}"
                                        "").format(util_err_msg,
                                                   config_path_err_msg)
                    raise UtilError(util_err_msg)

                # Read and parse the login-path data (i.e., user, password and
                # host)
                login_path_data = my_defaults_reader.get_group_data(login_path)

        if config_path_data or login_path_data:
            if config_path_data:
                if not login_path_data:
                    login_path_data = config_path_data
                else:
                    # Overwrite values from login_path_data
                    config_path_data.update(login_path_data)
                    login_path_data = config_path_data

            user = login_path_data.get('user', None)
            passwd = login_path_data.get('password', None)
            host = login_path_data.get('host', None)
            if not port:
                port = login_path_data.get('port', None)
            if not socket:
                socket = login_path_data.get('socket', None)

            if os.name == "posix" and socket is not None:
                # if we are on unix systems and used a socket, hostname can be
                # safely assumed as being localhost so it is not required
                required_options = ('user', 'socket')
                host = 'localhost' if host is None else host
            else:
                required_options = ('user', 'host', 'port')

            missing_options = [opt for opt in required_options
                               if locals()[opt] is None]
            # If we are on unix and port is missing, user might have specified
            # a socket instead
            if os.name == "posix" and "port" in missing_options:
                i = missing_options.index("port")
                if socket:  # If we have a socket, port is not needed
                    missing_options.pop(i)
                else:
                    # if we don't have neither a port nor a socket, we need
                    # either a port or a socket
                    missing_options[i] = "port or socket"

            if missing_options:
                message = ",".join(missing_options)
                if len(missing_options) > 1:
                    comma_idx = message.rfind(",")
                    message = "{0} and {1}".format(message[:comma_idx],
                                                   message[comma_idx + 1:])
                pluralize = "s" if len(missing_options) > 1 else ""
                raise UtilError("Missing connection value{0} for "
                                "{1} option{0}".format(pluralize, message))

            # optional options, available only on config_path_data
            if config_path_data:
                ssl_ca = config_path_data.get('ssl-ca', None)
                ssl_cert = config_path_data.get('ssl-cert', None)
                ssl_key = config_path_data.get('ssl-key', None)
                ssl = config_path_data.get('ssl', None)

        else:
            if login_path and not config_path:
                raise UtilError("No login credentials found for login-path: "
                                "{0}. Please review the used connection string"
                                ": {1}".format(login_path, connection_values))
            elif not login_path and config_path:
                raise UtilError("No login credentials found for config-path: "
                                "{0}. Please review the used connection string"
                                ": {1}".format(login_path, connection_values))
            elif login_path and config_path:
                raise UtilError("No login credentials found for either "
                                "login-path: '{0}' nor config-path: '{1}'. "
                                "Please review the used connection string: {2}"
                                "".format(login_path, config_path,
                                          connection_values))

    elif len(conn_format) == 2:

        # Check to see if the user attempted to pass a list of connections.
        # This is true if there is at least one comma and multiple @ symbols.
        if ((connection_values.find(',') > 0) and
                (connection_values.find('@') > 1)):
            raise FormatError(_MULTIPLE_CONNECTIONS.format(connection_values))

        # Handle as in the format: user[:password]@host[:port][:socket]
        userpass, hostportsock = conn_format

        # Get user, password
        match = _CONN_USERPASS.match(userpass)
        if not match:
            raise FormatError(_BAD_CONN_FORMAT.format(connection_values))
        user = match.group('user')
        if user is None:
            # No password provided
            user = match.group('suser').rstrip(':')
        passwd = match.group('passwd')

        # Handle host, port and socket
        if len(hostportsock) <= 0:
            raise FormatError(_BAD_CONN_FORMAT.format(connection_values))

        if hostportsock[0] in ['"', "'"]:
            # need to strip the quotes
            host, port, socket = _match(_CONN_QUOTEDHOST, hostportsock)
            if host[0] == '"':
                host = host.strip('"')
            if host[0] == "'":
                host = host.strip("'")

        else:
            host, port, socket, _ = parse_server_address(hostportsock)

    else:
        # Unrecognized format
        raise FormatError(_BAD_CONN_FORMAT.format(connection_values))

    # Get character-set from options
    if isinstance(options, dict):
        charset = options.get("charset", None)
        # If one SSL option was found before, not mix with those in options.
        if not ssl_cert and not ssl_ca and not ssl_key and not ssl:
            ssl_cert = options.get("ssl_cert", None)
            ssl_ca = options.get("ssl_ca", None)
            ssl_key = options.get("ssl_key", None)
            ssl = options.get("ssl", None)

    else:
        # options is an instance of optparse.Values
        try:
            charset = options.charset  # pylint: disable=E1103
        except AttributeError:
            charset = None
        # If one SSL option was found before, not mix with those in options.
        if not ssl_cert and not ssl_ca and not ssl_key and not ssl:
            try:
                ssl_cert = options.ssl_cert  # pylint: disable=E1103
            except AttributeError:
                ssl_cert = None
            try:
                ssl_ca = options.ssl_ca  # pylint: disable=E1103
            except AttributeError:
                ssl_ca = None
            try:
                ssl_key = options.ssl_key  # pylint: disable=E1103
            except AttributeError:
                ssl_key = None
            try:
                ssl = options.ssl  # pylint: disable=E1103
            except AttributeError:
                ssl = None

    # Set parsed connection values
    connection = {
        "user": user,
        "host": host,
        "port": int(port) if port else 3306,
        "passwd": passwd if passwd else ''
    }

    if charset:
        connection["charset"] = charset
    if ssl_cert:
        connection["ssl_cert"] = ssl_cert
    if ssl_ca:
        connection["ssl_ca"] = ssl_ca
    if ssl_key:
        connection["ssl_key"] = ssl_key
    if ssl:
        connection["ssl"] = ssl
    # Handle optional parameters. They are only stored in the dict if
    # they were provided in the specifier.
    if socket is not None and os.name == "posix":
        connection['unix_socket'] = socket

    return connection


def parse_server_address(connection_str):
    """Parses host, port and socket from the given connection string.

    Returns a tuple of (host, port, socket, add_type) where add_type is
    the name of the parser that successfully parsed the hostname from
    the connection string.
    """
    # Default values to return.
    host = None
    port = None
    socket = None
    address_type = None
    unparsed = None
    # From the matchers look the one that match a host.
    # pylint: disable=R0101
    for IP_matcher in IP_matchers_list:
        try:
            group = _match(IP_matchers[IP_matcher], connection_str)
            if group:
                host = group[0]
                if IP_matcher == ipv6:
                    host = "[%s]" % host

                if group[1]:
                    part2_port_socket = _match(_CONN_port_ONLY, group[1],
                                               trow_error=False)
                    if not part2_port_socket:
                        unparsed = group[1]
                    else:
                        port = part2_port_socket[0]
                        if part2_port_socket[1]:
                            part4 = _match(_CONN_socket_ONLY,
                                           part2_port_socket[1],
                                           trow_error=False)
                            if not part4:
                                unparsed = part2_port_socket[1]
                            else:
                                socket = part4[0]
                                unparsed = part4[1]

            # If host is match we stop looking as is the most significant.
            if host:
                address_type = IP_matcher
                break
        # ignore the error trying to match.
        except FormatError:
            pass
    # we must alert, that the connection could not be parsed.
    if host is None:
        raise FormatError(_BAD_CONN_FORMAT.format(connection_str))
    _verify_parsing(connection_str, host, port, socket, address_type, unparsed)

    return host, port, socket, address_type


def _verify_parsing(connection_str, host, port, socket, address_type,
                    unparsed):
    """Verify that the connection string was totally parsed and not parts of
    it where not matched, otherwise raise an error.
    """
    exp_connection_str = connection_str
    log.debug("exp_connection_str {0}".format(exp_connection_str))
    parsed_connection_list = []
    if host:
        log.debug("host {0}".format(host))
        if address_type == ipv6 and "[" not in connection_str:
            host = host.replace("[", "")
            host = host.replace("]", "")
        parsed_connection_list.append(host)
    if port:
        log.debug("port {0}".format(port))
        parsed_connection_list.append(port)
    if socket:
        log.debug("socket {0}".format(socket))
        parsed_connection_list.append(socket)
    parsed_connection = ":".join(parsed_connection_list)
    log.debug('parsed_connection {0}'.format(parsed_connection))
    diff = None
    if not unparsed:
        log.debug('not unparsed found, creating diff')
        diff = connection_str.replace(host, "")
        if port:
            diff = diff.replace(port, "")
        if socket:
            diff = diff.replace(socket, "")
        log.debug("diff {0}".format(diff))
    log.debug("unparsed {0}".format(unparsed))
    if unparsed or (exp_connection_str != parsed_connection and
                    (diff and diff != ":")):
        log.debug("raising exception")
        parsed_args = "host:%s, port:%s, socket:%s" % (host, port, socket)
        log.debug(_UNPARSED_CONN_FORMAT.format(connection_str,
                                               parsed_args,
                                               unparsed))
        raise FormatError(_UNPARSED_CONN_FORMAT.format(connection_str,
                                                       parsed_args,
                                                       unparsed))


def _match(pattern, connection_str, trow_error=True):
    """Tries to match a pattern with the connection string and returns the
    groups.
    """
    grp = pattern.match(connection_str)
    if not grp:
        if trow_error:
            raise FormatError(_BAD_CONN_FORMAT.format(connection_str))
        return False
    return grp.groups()


def clean_IPv6(host_address):
    """Clean IPv6 host address
    """
    if host_address:
        host_address = host_address.replace("[", "")
        host_address = host_address.replace("]", "")
    return host_address


def format_IPv6(host_address):
    """Format IPv6 host address
    """
    if host_address:
        if "]" not in host_address:
            host_address = "[{0}]".format(host_address)
    return host_address


def parse_login_values_config_path(login_values, quietly=True):
    """Parse the login values to retrieve the user and password from a
    configuration file.

    login_values[in]    The login values to be parsed.
    quietly[in]         Do not raise exceptions (Default True).

    returns parsed (user, password) tuple or (login_values, None) tuple.
    """
    try:
        matches = _match(_CONN_CONFIGPATH, login_values, trow_error=False)
        if matches:
            path = matches[0]
            group = matches[1]
            data = handle_config_path(path, group, use_default=False)
            user = data.get('user', None)
            passwd = data.get('password', None)
            return user, passwd
    except (FormatError, UtilError):
        if not quietly:
            raise
    return login_values, None


def find_password(value):
    """Search for password in a string

    value[in]           String to search for password
    """
    if not isinstance(value, str):
        return False
    # has to have an @ sign
    if '@' not in value:
        return False
    match = _CONN_USERPASS.match(value)
    if not match:
        return False
    if match.group('passwd'):
        return True
    return False
#
# Copyright (c) 2011, 2012, 2013, Oracle and/or its affiliates. All rights
# reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains the methods for checking consistency among two databases.
"""


# The following are the queries needed to perform table locking.

LOCK_TYPES = ['READ', 'WRITE']

_SESSION_ISOLATION_LEVEL = \
    "SET SESSION TRANSACTION ISOLATION LEVEL REPEATABLE READ"

_START_TRANSACTION = "START TRANSACTION WITH CONSISTENT SNAPSHOT"

_LOCK_WARNING = "WARNING: Lock in progress. You must call unlock() " + \
                "to unlock your tables."

_FLUSH_TABLES_READ_LOCK = "FLUSH TABLES WITH READ LOCK"


class Lock(object):
    """Lock
    """
    def __init__(self, server, table_list, options=None):
        """Constructor

        Lock a list of tables based on locking type. Locking types and their
        behavior is as follows:

           - (default) use consistent read with a single transaction
           - lock all tables without consistent read and no transaction
           - no locks, no transaction, no consistent read
           - flush (replication only) - issue a FTWRL command

        server[in]         Server instance of server to run locks
        table_list[in]     list of tuples (table_name, lock_type)
        options[in]        dictionary of options
                           locking = [snapshot|lock-all|no-locks|flush],
                           verbosity int
                           silent bool
                           rpl_mode string
        """
        if options is None:
            options = {}
        self.locked = False
        self.silent = options.get('silent', False)
        # Determine locking type
        self.locking = options.get('locking', 'snapshot')
        self.verbosity = options.get('verbosity', 0)
        if self.verbosity is None:
            self.verbosity = 0
        else:
            self.verbosity = int(self.verbosity)

        self.server = server
        self.table_list = table_list

        self.query_opts = {'fetch': False, 'commit': False}

        # If no locking, we're done
        if self.locking == 'no-locks':
            return

        elif self.locking == 'lock-all':
            # Check lock requests for validity
            table_locks = []
            for tablename, locktype in table_list:
                if locktype.upper() not in LOCK_TYPES:
                    raise UtilDBError("Invalid lock type '%s' for table '%s'."
                                      % (locktype, tablename))
                # Build LOCK TABLE command
                table_locks.append("%s %s" % (tablename, locktype))
            lock_str = "LOCK TABLE "
            lock_str += ', '.join(table_locks)

            if self.verbosity >= 3 and not self.silent:
                print '# LOCK STRING:', lock_str

            # Execute the lock
            self.server.exec_query(lock_str, self.query_opts)

            self.locked = True

        elif self.locking == 'snapshot':
            self.server.exec_query(_SESSION_ISOLATION_LEVEL, self.query_opts)
            self.server.exec_query(_START_TRANSACTION, self.query_opts)

        # Execute a FLUSH TABLES WITH READ LOCK for replication uses only
        elif self.locking == 'flush' and options.get("rpl_mode", None):
            if self.verbosity >= 3 and not self.silent:
                print "# LOCK STRING: %s" % _FLUSH_TABLES_READ_LOCK
            self.server.exec_query(_FLUSH_TABLES_READ_LOCK, self.query_opts)
            self.locked = True
        else:
            raise UtilError("Invalid locking type: '%s'." % self.locking)

    def __del__(self):
        """Destructor

        Returns string - warning if the lock has not been disengaged.
        """
        if self.locked:
            return _LOCK_WARNING

        return None

    def unlock(self, abort=False):
        """Release the table lock.
        """
        if not self.locked:
            return

        if self.verbosity >= 3 and not self.silent and \
           self.locking != 'no-locks':
            print "# UNLOCK STRING:",
        # Call unlock:
        if self.locking in ['lock-all', 'flush']:
            if self.verbosity >= 3 and not self.silent:
                print "UNLOCK TABLES"
            self.server.exec_query("UNLOCK TABLES", self.query_opts)
            self.locked = False

        # Stop transaction if locking == 0
        elif self.locking == 'snapshot':
            if not abort:
                if self.verbosity >= 3 and not self.silent:
                    print "COMMIT"
                self.server.exec_query("COMMIT", self.query_opts)
            else:
                self.server.exec_queery("ROLLBACK", self.query_opts)
                if self.verbosity >= 3 and not self.silent:
                    print "ROLLBACK"
#
# Copyright (c) 2013, 2016 Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains output string messages used by MySQL Utilities.
"""

EXTERNAL_SCRIPT_DOES_NOT_EXIST = ("'{path}' script cannot be found. Please "
                                  "check the path and filename for accuracy "
                                  "and try again.")

ERROR_ANSI_QUOTES_MIX_SQL_MODE = ("One or more servers have SQL mode set to "
                                  "ANSI_QUOTES, the {utility} requires to all "
                                  "or none of the servers to be set with the "
                                  "SQL mode set to ANSI_QUOTES.")

ERROR_USER_WITHOUT_PRIVILEGES = ("User '{user}' on '{host}@{port}' does not "
                                 "have sufficient privileges to "
                                 "{operation} (required: {req_privileges}).")

PARSE_ERR_DB_PAIR = ("Cannot parse the specified database(s): '{db_pair}'. "
                     "Please verify that the database(s) are specified in "
                     "a valid format (i.e., {db1_label}[:{db2_label}]) and "
                     "that backtick quotes are properly used when required.")

PARSE_ERR_DB_PAIR_EXT = ("%s The use of backticks is required if non "
                         "alphanumeric characters are used for database "
                         "names. Parsing the specified database results "
                         "in {db1_label} = '{db1_value}' and "
                         "{db2_label} = '{db2_value}'." % PARSE_ERR_DB_PAIR)

PARSE_ERR_DB_OBJ_PAIR = ("Cannot parse the specified database objects: "
                         "'{db_obj_pair}'. Please verify that the objects "
                         "are specified in a valid format (i.e., {db1_label}"
                         "[.{obj1_label}]:{db2_label}[.{obj2_label}]) and "
                         "that backtick quotes are properly used if "
                         "required.")

PARSE_ERR_DB_OBJ_PAIR_EXT = ("%s The use of backticks is required if non "
                             "alphanumeric characters are used for identifier "
                             "names. Parsing the specified objects results "
                             "in: {db1_label} = '{db1_value}', "
                             "{obj1_label} = '{obj1_value}', "
                             "{db2_label} = '{db2_value}' and "
                             "{obj2_label} = '{obj2_value}'."
                             % PARSE_ERR_DB_OBJ_PAIR)

PARSE_ERR_DB_OBJ_MISSING_MSG = ("Incorrect object compare argument, one "
                                "specific object is missing. Please verify "
                                "that both object are correctly specified. "
                                "{detail} Format should be: "
                                "{db1_label}[.{obj1_label}]"
                                ":{db2_label}[.{obj2_label}].")

PARSE_ERR_DB_OBJ_MISSING = ("No object has been specified for "
                            "{db_no_obj_label} '{db_no_obj_value}', while "
                            "object '{only_obj_value}' was specified for "
                            "{db_obj_label} '{db_obj_value}'.")

PARSE_ERR_DB_MISSING_CMP = ("You must specify at least one database to "
                            "compare or use the --all option to compare all "
                            "databases.")

PARSE_ERR_OBJ_NAME_FORMAT = ("Cannot parse the specified qualified name "
                             "'{obj_name}' for {option}. Please verify that a "
                             "valid format is used (i.e., <db_name>"
                             "[.<tbl_name>]) and that backtick quotes are "
                             "properly used if required.")

PARSE_ERR_SPAN_KEY_SIZE_TOO_HIGH = (
    "The value {s_value} specified for option --span-key-size is too big. It "
    "must be smaller or equal than {max} (size of the key hash values for "
    "comparison).")

PARSE_ERR_SPAN_KEY_SIZE_TOO_LOW = (
    "The value {s_value} specified for option --span-key-size is too small "
    "and would cause inaccurate results, please retry with a bigger value "
    "or the default value of {default}.")

PARSE_ERR_OPT_INVALID_CMD = "Invalid {opt} option for '{cmd}'."

PARSE_ERR_OPT_INVALID_CMD_TIP = ("%s Use {opt_tip} instead."
                                 % PARSE_ERR_OPT_INVALID_CMD)

PARSE_ERR_OPT_INVALID_DATE = "Invalid {0} date format (yyyy-mm-dd): {1}"

PARSE_ERR_OPT_INVALID_DATE_TIME = ("Invalid {0} date/time format "
                                   "(yyyy-mm-ddThh:mm:ss): {1}")

PARSE_ERR_OPT_INVALID_NUM_DAYS = ("Invalid number of days (must be an integer "
                                  "greater than zero) for {0} date: {1}")

PARSE_ERR_OPT_INVALID_VALUE = ("The value for option {option} is not valid: "
                               "'{value}'.")

PARSE_ERR_OPT_REQ_NON_NEGATIVE_VALUE = ("Option '{opt}' requires a "
                                        "non-negative value.")

PARSE_ERR_OPT_REQ_GREATER_VALUE = ("Option '{opt}' requires a value greater "
                                   "than {val}.")

PARSE_ERR_OPT_REQ_VALUE = "Option '{opt}' requires a non-empty value."

PARSE_ERR_OPT_REQ_OPT = ("Option {opt} requires the following option(s): "
                         "{opts}.")

PARSE_ERR_OPTS_EXCLD = ("Options {opt1} and {opt2} cannot be used "
                        "together.")

PARSE_ERR_OPTS_REQ = "Option '{opt}' is required."

PARSE_ERR_OPTS_REQ_BY_CMD = ("'{cmd}' requires the following option(s): "
                             "{opts}.")

PARSE_ERR_SLAVE_DISCO_REQ = ("Option --discover-slaves-login or --slaves is "
                             "required.")

PARSE_ERR_OPTS_REQ_GREATER_OR_EQUAL = ("The {opt} option requires a value "
                                       "greater than or equal to {value}.")

WARN_OPT_NOT_REQUIRED = ("WARNING: The {opt} option is not required for "
                         "'{cmd}' (option ignored).")

WARN_OPT_NOT_REQUIRED_ONLY_FOR = ("%s Only used with the {only_cmd} command."
                                  % WARN_OPT_NOT_REQUIRED)

WARN_OPT_NOT_REQUIRED_FOR_TYPE = (
    "# WARNING: The {opt} option is not required for the {type} type "
    "(option ignored).")

WARN_OPT_ONLY_USED_WITH = ("# WARNING: The {opt} option is only used with "
                           "{used_with} (option ignored).")

WARN_OPT_USING_DEFAULT = ("WARNING: Using default value '{default}' for "
                          "option {opt}.")

ERROR_SAME_MASTER = ("The specified new master {n_master_host}:{n_master_port}"
                     " is the same as the "
                     "actual master {master_host}:{master_port}.")

SLAVES = "slaves"

CANDIDATES = "candidates"

ERROR_MASTER_IN_SLAVES = ("The master {master_host}:{master_port} "
                          "and one of the specified {slaves_candidates} "
                          "are the same {slave_host}:{slave_port}.")

SCRIPT_THRESHOLD_WARNING = ("WARNING: You have chosen to use external script "
                            "return code checking. Depending on which script "
                            "fails, this can leave the operation in an "
                            "undefined state. Please check your results "
                            "carefully if the operation aborts.")

HOST_IP_WARNING = ("You may be mixing host names and IP addresses. This may "
                   "result in negative status reporting if your DNS services "
                   "do not support reverse name lookup.")

ERROR_MIN_SERVER_VERSIONS = ("The {utility} requires server versions greater "
                             "or equal than {min_version}. Server version for "
                             "'{host}:{port}' is not supported.")

PARSE_ERR_SSL_REQ_SERVER = ("Options --ssl-ca, --ssl-cert and --ssl-key "
                            "requires use of --server.")

WARN_OPT_SKIP_INNODB = ("The use of InnoDB is mandatory since MySQL 5.7. The "
                        "former options like '--innodb=0/1/OFF/ON' or "
                        "'--skip-innodb' are ignored.")

FILE_DOES_NOT_EXIST = "The following path is invalid, '{path}'."

INSUFFICIENT_FILE_PERMISSIONS = ("You do not have permission to {permissions} "
                                 "file '{path}'.")

MSG_UTILITIES_VERSION = "MySQL Utilities {utility} version {version}."

MSG_MYSQL_VERSION = "Server '{server}' is using MySQL version {version}."

USER_PASSWORD_FORMAT = ("Format of {0} option is incorrect. Use userid:passwd "
                        "or userid.")
#
# Copyright (c) 2013, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module provides features to read MySQL configuration files, wrapping the
tool my_print_defaults.
"""

import optparse
import os.path
import re
import subprocess
import tempfile


_MY_PRINT_DEFAULTS_TOOL = "my_print_defaults"
MYLOGIN_FILE = ".mylogin.cnf"


def my_login_config_path():
    """Return the default path of the mylogin file (.mylogin.cnf).
    """
    if os.name == 'posix':
        # File located in $HOME for non-Windows systems
        return os.path.expanduser('~')
    else:
        # File located in %APPDATA%\MySQL for Windows systems
        return r'{0}\MySQL'.format(os.environ['APPDATA'])


def my_login_config_exists():
    """Check if the mylogin file (.mylogin.cnf) exists.
    """

    my_login_fullpath = os.path.normpath(os.path.join(my_login_config_path(),
                                                      MYLOGIN_FILE))
    return os.path.isfile(my_login_fullpath)


class MyDefaultsReader(object):
    """The MyDefaultsReader class is used to read the data stored from a MySQL
    configuration file. This class provide methods to read the options data
    stored in configurations files, using the my_print_defaults tool. To learn
    more about my_print_defaults see:
    http://dev.mysql.com/doc/en/my-print-defaults.html
    """

    def __init__(self, options=None, find_my_print_defaults_tool=True):
        """Constructor

        options[in]                 dictionary of options (e.g. basedir). Note,
                                    allows options values from optparse to be
                                    passed directly to this parameter.
        find_my_print_defaults[in]  boolean value indicating if the tool
                                    my_print_defaults should be located upon
                                    initialization of the object.
        """
        if options is None:
            options = {}
        # _config_data is a dictionary of option groups containing a dictionary
        # of the options data read from the configuration file.
        self._config_data = {}

        # Options values from optparse can be directly passed, check if it is
        # the case and handle them correctly.
        if isinstance(options, optparse.Values):
            try:
                self._basedir = options.basedir  # pylint: disable=E1103
            except AttributeError:
                # if the attribute is not found, then set it to None (default).
                self._basedir = None
            try:
                # if the attribute is not found, then set it to 0 (default).
                self._verbosity = options.verbosity  # pylint: disable=E1103
            except AttributeError:
                self._verbosity = 0
        else:
            self._basedir = options.get("basedir", None)
            self._verbosity = options.get("verbosity", 0)

        if find_my_print_defaults_tool:
            self.search_my_print_defaults_tool()
        else:
            self._tool_path = None

    @property
    def tool_path(self):
        """Sets tool_path property
        """
        return self._tool_path

    def search_my_print_defaults_tool(self, search_paths=None):
        """Search for the tool my_print_defaults.
        """
        if not search_paths:
            search_paths = []

        # Set the default search paths (i.e., default location of the
        # .mylogin.cnf file).
        default_paths = [my_login_config_path()]

        # Extend the list of path to search with the ones specified.
        if search_paths:
            default_paths.extend(search_paths)

        # Search for the tool my_print_defaults.
        try:
            self._tool_path = get_tool_path(self._basedir,
                                            _MY_PRINT_DEFAULTS_TOOL,
                                            defaults_paths=default_paths,
                                            search_PATH=True)
        except UtilError as err:
            raise UtilError("Unable to locate MySQL Client tools. "
                            "Please confirm that the path to the MySQL client "
                            "tools are included in the PATH. Error: %s"
                            % err.errmsg)

    def check_show_required(self):
        """Check if the '--show' password option is required/supported by this
        version of the my_print_defaults tool.

        At MySQL Server 5.6.25 and 5.7.8, my_print_defaults' functionality
        changed to mask passwords by default and added the '--show' password
        option to display passwords in cleartext (BUG#19953365, BUG#20903330).
        As this module requires the password to be displayed as cleartext to
        extract the password, the use of the '--show' password option is also
        required starting on these version of the server, however the
        my_print_defaults tool version did not increase with this change, so
        this method looks at the output of the help text of my_print_defaults
        tool to determine if the '--show' password option is supported by the
        my_print_defaults tool available at _tool_path.

        Returns True if this version of the tool supports the'--show' password
        option, otherwise False.
        """
        # The path to the tool must have been previously found.
        assert self._tool_path, ("First, the required MySQL tool must be "
                                 "found. E.g., use method "
                                 "search_my_print_defaults_tool.")

        # Create a temporary file to redirect stdout
        out_file = tempfile.TemporaryFile()
        if self._verbosity > 0:
            subprocess.call([self._tool_path, "--help"], stdout=out_file)
        else:
            # Redirect stderr to null
            null_file = open(os.devnull, "w+b")
            subprocess.call([self._tool_path, "--help"], stdout=out_file,
                            stderr=null_file)

        # Read my_print_defaults help output text
        out_file.seek(0)
        lines = out_file.readlines()
        out_file.close()

        # find the "--show" option used to show passwords in plain text.
        for line in lines:
            if "--show" in line:
                return True

        # The option was not found in the tool help output.
        return False

    def check_tool_version(self, major_version, minor_version):
        """Check the version of the my_print_defaults tool.

        Returns True if the version of the tool is equal or above the one that
        is specified, otherwise False.
        """
        # The path to the tool must have been previously found.
        assert self._tool_path, ("First, the required MySQL tool must be "
                                 "found. E.g., use method "
                                 "search_my_print_defaults_tool.")

        # Create a temporary file to redirect stdout
        out_file = tempfile.TemporaryFile()
        if self._verbosity > 0:
            subprocess.call([self._tool_path, "--version"], stdout=out_file)
        else:
            # Redirect stderr to null
            null_file = open(os.devnull, "w+b")
            subprocess.call([self._tool_path, "--version"], stdout=out_file,
                            stderr=null_file)
        # Read --version output
        out_file.seek(0)
        line = out_file.readline()
        out_file.close()

        # Parse the version value
        match = re.search(r'(?:Ver )(\d)\.(\d)', line)
        if match:
            major, minor = match.groups()
            return (
                (major_version < int(major)) or
                (major_version == int(major) and
                 minor_version <= int(minor))
            )
        else:
            raise UtilError("Unable to determine tool version - %s" %
                            self._tool_path)

    def check_login_path_support(self):
        """Checks if the used my_print_defaults tool supports login-paths.
        """
        # The path to the tool must have been previously found.
        assert self._tool_path, ("First, the required MySQL tool must be "
                                 "found. E.g., use method "
                                 "search_my_print_defaults_tool.")

        # Create a temporary file to redirect stdout
        out_file = tempfile.TemporaryFile()
        if self._verbosity > 0:
            subprocess.call([self._tool_path, "--help"], stdout=out_file)
        else:
            # Redirect stderr to null
            null_file = open(os.devnull, "w+b")
            subprocess.call([self._tool_path, "--help"], stdout=out_file,
                            stderr=null_file)
        # Read --help output
        out_file.seek(0)
        help_output = out_file.read()
        out_file.close()

        # Check the existence of a "login-path" option
        return ('login-path' in help_output)

    def _read_group_data(self, group):
        """Read group options data using my_print_defaults tool.
        """
        # The path to the tool must have been previously found.
        assert self._tool_path, ("First, the required MySQL tool must be "
                                 "found. E.g., use method "
                                 "search_my_print_defaults_tool.")

        mp_cmd = [self._tool_path, group]
        if self.check_show_required():
            mp_cmd.append("--show")

        # Group not found; use my_print_defaults to get group data.
        out_file = tempfile.TemporaryFile()
        if self._verbosity > 0:
            subprocess.call(mp_cmd, stdout=out_file)
        else:
            # Redirect stderr to null
            null_file = open(os.devnull, "w+b")
            subprocess.call(mp_cmd, stdout=out_file, stderr=null_file)

        # Read and parse group options values.
        out_file.seek(0)
        results = []
        for line in out_file:
            # Parse option value; ignore starting "--"
            key_value = line[2:].split("=", 1)
            if len(key_value) == 2:
                # Handle option format: --key=value and --key=
                results.append((key_value[0], key_value[1].strip()))
            elif len(key_value) == 1:
                # Handle option format: --key
                results.append((key_value[0], True))
            else:
                raise UtilError("Invalid option value format for "
                                "group %s: %s" % (group, line))
        out_file.close()

        if len(results):
            self._config_data[group] = dict(results)
        else:
            self._config_data[group] = None

        return self._config_data[group]

    def get_group_data(self, group):
        """Retrieve the data associated to the given group.
        """
        # Returns group's data locally stored, if available.
        try:
            return self._config_data[group]
        except KeyError:
            # Otherwise, get it using my_print_defaults.
            return self._read_group_data(group)

    def get_option_value(self, group, opt_name):
        """Retrieve the value associated to the given opt_name in the group.
        """
        # Get option value, if group's data is available.
        grp_options = self.get_group_data(group)
        if grp_options:
            return grp_options.get(opt_name, None)
        else:
            return None
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains the following methods design to support common option
parsing among the multiple utilities.

Methods:
  setup_common_options()     Setup standard options for utilities
"""

import copy
import optparse
from optparse import Option as CustomOption, OptionValueError
import os.path
import re

from datetime import datetime
from ip_parser import find_password, parse_login_values_config_path
from mysql.connector.conversion import MySQLConverter


_PERMITTED_FORMATS = ["grid", "tab", "csv", "vertical"]
_PERMITTED_DIFFS = ["unified", "context", "differ"]
_PERMITTED_RPL_DUMP = ["master", "slave"]


class UtilitiesParser(optparse.OptionParser):
    """Special subclass of parser that allows showing of version information
       when --help is used.
    """

    def print_help(self, output=None):
        """Show version information before help
        """
        print self.version
        optparse.OptionParser.print_help(self, output)

    def format_epilog(self, formatter):
        return self.epilog if self.epilog is not None else ''


def prefix_check_choice(option, opt, value):
    """Check option values using case insensitive prefix compare

    This method checks to see if the value specified is a prefix of one of the
    choices. It converts the string provided by the user (value) to lower case
    to permit case insensitive comparison of the user input. If multiple
    choices are found for a prefix, an error is thrown. If the value being
    compared does not match the list of choices, an error is thrown.

    option[in]             Option class instance
    opt[in]                option name
    value[in]              the value provided by the user

    Returns string - valid option chosen
    """
    # String of choices
    choices = ", ".join([repr(choice) for choice in option.choices])

    # Get matches for prefix given
    alts = [alt for alt in option.choices if alt.startswith(value.lower())]
    if len(alts) == 1:   # only 1 match
        return alts[0]
    elif len(alts) > 1:  # multiple matches
        raise OptionValueError(
            ("option %s: there are multiple prefixes "
             "matching: %r (choose from %s)") % (opt, value, choices))

    # Doesn't match. Show user possible choices.
    raise OptionValueError("option %s: invalid choice: %r (choose from %s)"
                           % (opt, value, choices))


def license_callback(self, opt, value, parser, *args, **kwargs):
    """Show license information and exit.
    """
    print(LICENSE_FRM.format(program=parser.prog))
    parser.exit()


def path_callback(option, opt, value, parser):
    """Verify that the given path is an existing file. If it is then add it
    to the parser values.

    option[in]        option instance
    opt[in]           option name
    value[in]         given user value
    parser[in]        parser instance
    """
    if not os.path.exists(value):
        parser.error("the given path '{0}' in option {1} does not"
                     " exist or can not be accessed".format(value, opt))

    if not os.path.isfile(value):
        parser.error("the given path '{0}' in option {1} does not"
                     " correspond to a file".format(value, opt))

    setattr(parser.values, option.dest, value)


def ssl_callback(option, opt, value, parser):
    """Verify that the given path is an existing file. If it is then add it
    to the parser values.

    option[in]        option instance
    opt[in]           option name
    value[in]         given user value
    parser[in]        parser instance
    """
    if not (value == 0 or value == 1 or value == ''):
        parser.error("the given value '{0}' in option {1} is not"
                     " valid, valid values are 0 or 1.".format(value, opt))

    setattr(parser.values, option.dest, value)


def add_config_path_option(parser):
    """Add the config_path option.

    parser[in]        the parser instance
    """
    # --config-path option: config_path
    parser.add_option("--config-path", action="callback",
                      callback=path_callback,
                      type="string", help="The path to a MySQL option file "
                                          "with the login options")


def add_ssl_options(parser):
    """Add the ssl options.

    parser[in]        the parser instance
    """
    # --ssl options: ssl_ca, ssl_cert, ssl_key
    parser.add_option("--ssl-ca", action="callback",
                      callback=path_callback,
                      type="string", help="path to a file that contains "
                      "a list of trusted SSL CAs.")

    parser.add_option("--ssl-cert", action="callback",
                      callback=path_callback,
                      type="string", help="name of the SSL certificate "
                      "file to use for establishing a secure connection.")

    parser.add_option("--ssl-key", action="callback",
                      callback=path_callback,
                      type="string", help="name of the SSL key file to "
                      "use for establishing a secure connection.")

    parser.add_option("--ssl", action="callback", callback=ssl_callback,
                      type="int", help="specifies if the server "
                      "connection requires use of SSL. If an encrypted "
                      "connection cannot be established, the connection "
                      "attempt fails. By default 0 (SSL not required).")


class CaseInsensitiveChoicesOption(CustomOption):
    """Case insensitive choices option class

    This is an extension of the Option class. It replaces the check_choice
    method with the prefix_check_choice() method above to provide
    shortcut aware choice selection. It also ensures the choice compare is
    done with a case insensitve test.
    """
    TYPE_CHECKER = copy.copy(CustomOption.TYPE_CHECKER)
    TYPE_CHECKER["choice"] = prefix_check_choice

    def __init__(self, *opts, **attrs):
        if 'choices' in attrs:
            attrs['choices'] = [attr.lower() for attr in attrs['choices']]
        CustomOption.__init__(self, *opts, **attrs)


def setup_common_options(program_name, desc_str, usage_str,
                         append=False, server=True,
                         server_default="root@localhost:3306",
                         extended_help=None,
                         add_ssl=False):
    """Setup option parser and options common to all MySQL Utilities.

    This method creates an option parser and adds options for user
    login and connection options to a MySQL database system including
    user, password, host, socket, and port.

    program_name[in]   The program name
    desc_str[in]       The description of the utility
    usage_str[in]      A brief usage example
    append[in]         If True, allow --server to be specified multiple times
                       (default = False)
    server[in]         If True, add the --server option
                       (default = True)
    server_default[in] Default value for option
                       (default = "root@localhost:3306")
    extended_help[in]  Extended help (by default: None).
    add_ssl[in]        adds the --ssl-options, however these are added
                       automatically if server is True, (default = False)

    Returns parser object
    """

    program_name = program_name.replace(".py", "")
    parser = UtilitiesParser(
        version=VERSION_FRM.format(program=program_name),
        description=desc_str,
        usage=usage_str,
        add_help_option=False,
        option_class=CaseInsensitiveChoicesOption,
        epilog=extended_help,
        prog=program_name)
    parser.add_option("--help", action="help", help="display a help message "
                      "and exit")
    parser.add_option("--license", action='callback',
                      callback=license_callback,
                      help="display program's license and exit")

    if server:
        # Connection information for the first server
        if append:
            parser.add_option("--server", action="append", dest="server",
                              help="connection information for the server in "
                              "the form: <user>[:<password>]@<host>[:<port>]"
                              "[:<socket>] or <login-path>[:<port>]"
                              "[:<socket>] or <config-path>[<[group]>].")

        else:
            parser.add_option("--server", action="store", dest="server",
                              type="string", default=server_default,
                              help="connection information for the server in "
                              "the form: <user>[:<password>]@<host>[:<port>]"
                              "[:<socket>] or <login-path>[:<port>]"
                              "[:<socket>] or <config-path>[<[group]>].")

    if server or add_ssl:
        add_ssl_options(parser)

    return parser


def add_character_set_option(parser):
    """Add the --character-set option.

    parser[in]        the parser instance
    """
    parser.add_option("--character-set", action="store", dest="charset",
                      type="string", default=None,
                      help="sets the client character set. The default is "
                      "retrieved from the server variable "
                      "'character_set_client'.")


_SKIP_VALUES = (
    "tables", "views", "triggers", "procedures",
    "functions", "events", "grants", "data",
    "create_db"
)


def add_skip_options(parser):
    """Add the common --skip options for database utilties.

    parser[in]        the parser instance
    """
    parser.add_option("--skip", action="store", dest="skip_objects",
                      default=None, help="specify objects to skip in the "
                      "operation in the form of a comma-separated list (no "
                      "spaces). Valid values = tables, views, triggers, proc"
                      "edures, functions, events, grants, data, create_db")


def check_skip_options(skip_list):
    """Check skip options for validity

    skip_list[in]     List of items from parser option.

    Returns new skip list with items converted to upper case.
    """
    new_skip_list = []
    if skip_list is not None:
        items = skip_list.split(",")
        for item in items:
            obj = item.lower()
            if obj in _SKIP_VALUES:
                new_skip_list.append(obj)
            else:
                raise UtilError("The value %s is not a valid value for "
                                "--skip." % item)
    return new_skip_list


def add_format_option(parser, help_text, default_val, sql=False,
                      extra_formats=None):
    """Add the format option.

    parser[in]        the parser instance
    help_text[in]     help text
    default_val[in]   default value
    sql[in]           if True, add 'sql' format
                      default=False
    extra_formats[in] list with extra formats

    Returns corrected format value
    """
    formats = _PERMITTED_FORMATS
    if sql:
        formats.append('sql')
    if extra_formats:
        formats.extend(extra_formats)
    parser.add_option("-f", "--format", action="store", dest="format",
                      default=default_val, help=help_text, type="choice",
                      choices=formats)


def add_format_option_with_extras(parser, help_text, default_val,
                                  extra_formats):
    """Add the format option.

    parser[in]        the parser instance
    help_text[in]     help text
    default_val[in]   default value
    extra_formats[in] list of additional formats to support

    Returns corrected format value
    """
    formats = _PERMITTED_FORMATS
    formats.extend(extra_formats)
    parser.add_option("-f", "--format", action="store", dest="format",
                      default=default_val, help=help_text, type="choice",
                      choices=formats)


def add_no_headers_option(parser, restricted_formats=None, help_msg=None):
    """Add the --no-headers option.

    parser[in]              The parser instance.
    restricted_formats[in]  List of formats supported by this option (only
                            applies to them).
    help_msg[in]            Alternative help message to use, otherwise a
                            default one is used.
    """
    # Create the help message according to any format restriction.
    if restricted_formats:
        plural = "s" if len(restricted_formats) > 1 else ""
        formats_msg = (" (only applies to format{0}: "
                       "{1})").format(plural, ", ".join(restricted_formats))
    else:
        formats_msg = ""
    if help_msg:
        help_msg = "{0}{1}.".format(help_msg, formats_msg)
    else:
        help_msg = "do not show column headers{0}.".format(formats_msg)
    # Add the option.
    parser.add_option("-h", "--no-headers", action="store_true",
                      dest="no_headers", default=False, help=help_msg)


def add_verbosity(parser, quiet=True):
    """Add the verbosity and quiet options.

    parser[in]        the parser instance
    quiet[in]         if True, include the --quiet option
                      (default is True)

    """
    parser.add_option("-v", "--verbose", action="count", dest="verbosity",
                      help="control how much information is displayed. "
                      "e.g., -v = verbose, -vv = more verbose, -vvv = debug")
    if quiet:
        parser.add_option("-q", "--quiet", action="store_true", dest="quiet",
                          help="turn off all messages for quiet execution.",
                          default=False)


def check_verbosity(options):
    """Check to see if both verbosity and quiet are being used.
    """
    # Warn if quiet and verbosity are both specified
    if options.quiet is not None and options.quiet and \
       options.verbosity is not None and options.verbosity > 0:
        print "WARNING: --verbosity is ignored when --quiet is specified."
        options.verbosity = None


def add_changes_for(parser, default="server1"):
    """Add the changes_for option.

    parser[in]        the parser instance
    """
    parser.add_option("--changes-for", action="store", dest="changes_for",
                      type="choice", default=default, help="specify the "
                      "server to show transformations to match the other "
                      "server. For example, to see the transformation for "
                      "transforming server1 to match server2, use "
                      "--changes-for=server1. Valid values are 'server1' or "
                      "'server2'. The default is 'server1'.",
                      choices=['server1', 'server2'])


def add_reverse(parser):
    """Add the show-reverse option.

    parser[in]        the parser instance
    """
    parser.add_option("--show-reverse", action="store_true", dest="reverse",
                      default=False, help="produce a transformation report "
                      "containing the SQL statements to transform the object "
                      "definitions specified in reverse. For example if "
                      "--changes-for is set to server1, also generate the "
                      "transformation for server2. Note: the reverse changes "
                      "are annotated and marked as comments.")


def add_difftype(parser, allow_sql=False, default="unified"):
    """Add the difftype option.

    parser[in]        the parser instance
    allow_sql[in]     if True, allow sql as a valid option
                      (default is False)
    default[in]       the default option
                      (default is unified)
    """
    choice_list = ['unified', 'context', 'differ']
    if allow_sql:
        choice_list.append('sql')
    parser.add_option("-d", "--difftype", action="store", dest="difftype",
                      type="choice", default="unified", choices=choice_list,
                      help="display differences in context format in one of "
                      "the following formats: [%s] (default: unified)." %
                      '|'.join(choice_list))


def add_engines(parser):
    """Add the engine and default-storage-engine options.

    parser[in]        the parser instance
    """
    # Add engine
    parser.add_option("--new-storage-engine", action="store",
                      dest="new_engine", default=None, help="change all "
                      "tables to use this storage engine if storage engine "
                      "exists on the destination.")
    # Add default storage engine
    parser.add_option("--default-storage-engine", action="store",
                      dest="def_engine", default=None, help="change all "
                      "tables to use this storage engine if the original "
                      "storage engine does not exist on the destination.")


def check_engine_options(server, new_engine, def_engine,
                         fail=False, quiet=False):
    """Check to see if storage engines specified in options exist.

    This method will check to see if the storage engine in new exists on the
    server. If new_engine is None, the check is skipped. If the storage engine
    does not exist and fail is True, an exception is thrown else if quiet is
    False, a warning message is printed.

    Similarly, def_engine will be checked and if not present and fail is True,
    an exception is thrown else if quiet is False a warning is printed.

    server[in]         server instance to be checked
    new_engine[in]     new storage engine
    def_engine[in]     default storage engine
    fail[in]           If True, issue exception on failure else print warning
                       default = False
    quiet[in]          If True, suppress warning messages (not exceptions)
                       default = False
    """
    def _find_engine(server, target, message, fail, default):
        """Find engine
        """
        if target is not None:
            found = server.has_storage_engine(target)
            if not found and fail:
                raise UtilError(message)
            elif not found and not quiet:
                print message

    server.get_storage_engines()
    message = "WARNING: %s storage engine %s is not supported on the server."

    _find_engine(server, new_engine,
                 message % ("New", new_engine),
                 fail, quiet)
    _find_engine(server, def_engine,
                 message % ("Default", def_engine),
                 fail, quiet)


def add_all(parser, objects):
    """Add the --all option.

    parser[in]        the parser instance
    objects[in]       name of the objects for which all includes
    """
    parser.add_option("-a", "--all", action="store_true", dest="all",
                      default=False, help="include all %s" % objects)


def check_all(parser, options, args, objects):
    """Check to see if both all and specific arguments are used.

    This method will throw an exception if there are arguments listed and
    the all option has been turned on.

    parser[in]        the parser instance
    options[in]       command options
    args[in]          arguments list
    objects[in]       name of the objects for which all includes
    """
    if options.all and len(args) > 0:
        parser.error("You cannot use the --all option with a list of "
                     "%s." % objects)


def add_locking(parser):
    """Add the --locking option.

    parser[in]        the parser instance
    """
    parser.add_option("--locking", action="store", dest="locking",
                      type="choice", default="snapshot",
                      choices=['no-locks', 'lock-all', 'snapshot'],
                      help="choose the lock type for the operation: no-locks "
                      "= do not use any table locks, lock-all = use table "
                      "locks but no transaction and no consistent read, "
                      "snaphot (default): consistent read using a single "
                      "transaction.")


def add_exclude(parser, object_type="objects",
                example1="db1.t1", example2="db1.t% or db%.%"):
    """Add the --exclude option.

    parser[in]        the parser instance
    example1[in]
    example2[in]
    """
    parser.add_option("-x", "--exclude", action="append", dest="exclude",
                      type="string", default=None, help="exclude one or more "
                      "{0} from the operation using either a specific "
                      "name (e.g. {1}), a LIKE pattern (e.g. {2}) or a REGEXP "
                      "search pattern. To use a REGEXP search pattern for all "
                      "exclusions, you must also specify the --regexp option. "
                      "Repeat the --exclude option for multiple exclusions."
                      "".format(object_type, example1, example2))


def check_exclude_pattern(exclude_list, use_regexp):
    """Check the --exclude pattern to determine if there are special symbols
    that may be regexp symbols and the --use-regexp option is not specified.
    Prints warning if this is true.

    parser[in]        the parser instance
    use_regexp[in]    the option to use regexp
    """
    # ignore null lists
    if not exclude_list:
        return True
    for row in exclude_list:
        # replace _ and % and see if still not alnum()
        test = row.replace('_', '').replace('%', '').replace('`', '')
        test = test.replace("'", "").replace('.', '').replace('"', '')
        if len(test) > 0 and not test.isalnum() and not use_regexp:
            print "# WARNING: One or more of your --exclude patterns " \
                  "contains symbols that could be regexp patterns. You may " \
                  "need to include --regexp to ensure your exclude pattern " \
                  "is evaluated as REGEXP and not a SQL LIKE expression."
            return False
    return True


def add_regexp(parser):
    """Add the --regexp option.

    parser[in]        the parser instance
    """
    parser.add_option("-G", "--basic-regexp", "--regexp", dest="use_regexp",
                      action="store_true", default=False, help="use 'REGEXP' "
                      "operator to match pattern. Default is to use 'LIKE'.")


def add_rpl_user(parser):
    """Add the --rpl-user option.

    parser[in]        the parser instance
    """
    parser.add_option("--rpl-user", action="store", dest="rpl_user",
                      type="string",
                      help="the user and password for the replication "
                           "user requirement, in the form: <user>[:<password>]"
                           " or <login-path>. E.g. rpl:passwd")


def add_rpl_mode(parser, do_both=True, add_file=True):
    """Add the --rpl and --rpl-file options.

    parser[in]        the parser instance
    do_both[in]       if True, include the "both" value for the --rpl option
                      Default = True
    add_file[in]      if True, add the --rpl-file option
                      Default = True
    """
    rpl_mode_both = ""
    rpl_mode_options = _PERMITTED_RPL_DUMP
    if do_both:
        rpl_mode_options.append("both")
        rpl_mode_both = (", and 'both' = include 'master' and 'slave' options "
                         "where applicable")
    parser.add_option("--rpl", "--replication", dest="rpl_mode",
                      action="store", help="include replication information. "
                      "Choices: 'master' = include the CHANGE MASTER command "
                      "using the source server as the master, "
                      "'slave' = include the CHANGE MASTER command for "
                      "the source server's master (only works if the source "
                      "server is a slave){0}.".format(rpl_mode_both),
                      choices=rpl_mode_options)
    if add_file:
        parser.add_option("--rpl-file", "--replication-file", dest="rpl_file",
                          action="store", help="path and file name to place "
                          "the replication information generated. Valid on if "
                          "the --rpl option is specified.")


def check_rpl_options(parser, options):
    """Check replication dump options for validity

    This method ensures the optional --rpl-* options are valid only when
    --rpl is specified.

    parser[in]        the parser instance
    options[in]       command options
    """
    if options.rpl_mode is None:
        errors = []
        if parser.has_option("--comment-rpl") and options.rpl_file is not None:
            errors.append("--rpl-file")

        if options.rpl_user is not None:
            errors.append("--rpl-user")

        # It's Ok if the options do not include --comment-rpl
        if parser.has_option("--comment-rpl") and options.comment_rpl:
            errors.append("--comment-rpl")

        if len(errors) > 1:
            num_opt_str = "s"
        else:
            num_opt_str = ""

        if len(errors) > 0:
            parser.error("The %s option%s must be used with the --rpl "
                         "option." % (", ".join(errors), num_opt_str))


def add_discover_slaves_option(parser):
    """Add the --discover-slaves-login option.

    This method adds the --discover-slaves-login option that is used to
    discover the list of slaves associated to the specified login (user and
    password).

    parser[in]      the parser instance.
    """
    parser.add_option("--discover-slaves-login", action="store",
                      dest="discover", default=None, type="string",
                      help="at startup, query master for all registered "
                      "slaves and use the user name and password specified to "
                      "connect. Supply the user and password in the form "
                      "<user>[:<password>] or <login-path>. For example, "
                      "--discover-slaves-login=joe:secret will use 'joe' as "
                      "the user and 'secret' as the password for each "
                      "discovered slave.")


def add_log_option(parser):
    """Add the --log option.

    This method adds the --log option that is used the specify the target file
    for logging messages from the utility.

    parser[in]      the parser instance.
    """
    parser.add_option("--log", action="store", dest="log_file", default=None,
                      type="string", help="specify a log file to use for "
                      "logging messages")


def add_master_option(parser):
    """Add the --master option.

    This method adds the --master option that is used to specify the connection
    string for the server with the master role.

    parser[in]      the parser instance.
    """
    parser.add_option("--master", action="store", dest="master", default=None,
                      type="string", help="connection information for master "
                      "server in the form: <user>[:<password>]@<host>[:<port>]"
                      "[:<socket>] or <login-path>[:<port>][:<socket>]"
                      " or <config-path>[<[group]>].")


def add_slaves_option(parser):
    """Add the --slaves option.

    This method adds the --slaves option that is used to specify a list of
    slaves, more precisely their connection strings (separated by comma).

    parser[in]      the parser instance.
    """
    parser.add_option("--slaves", action="store", dest="slaves",
                      type="string", default=None,
                      help="connection information for slave servers in "
                      "the form: <user>[:<password>]@<host>[:<port>]"
                      "[:<socket>] or <login-path>[:<port>][:<socket>]"
                      " or <config-path>[<[group]>]. "
                      "List multiple slaves in comma-separated list.")


def add_failover_options(parser):
    """Add the common failover options.

    This adds the following options:

      --candidates
      --discover-slaves-login
      --exec-after
      --exec-before
      --log
      --log-age
      --master
      --max-position
      --ping
      --seconds-behind
      --slaves
      --timeout
      --script-threshold

    parser[in]        the parser instance
    """
    parser.add_option("--candidates", action="store", dest="candidates",
                      type="string", default=None,
                      help="connection information for candidate slave servers"
                      " for failover in the form: <user>[:<password>]@<host>[:"
                      "<port>][:<socket>] or <login-path>[:<port>][:<socket>]"
                      " or <config-path>[<[group]>]"
                      " Valid only with failover command. List multiple slaves"
                      " in comma-separated list.")

    add_discover_slaves_option(parser)

    parser.add_option("--exec-after", action="store", dest="exec_after",
                      default=None, type="string", help="name of script to "
                      "execute after failover or switchover")

    parser.add_option("--exec-before", action="store", dest="exec_before",
                      default=None, type="string", help="name of script to "
                      "execute before failover or switchover")

    add_log_option(parser)

    parser.add_option("--log-age", action="store", dest="log_age", default=7,
                      type="int", help="specify maximum age of log entries in "
                      "days. Entries older than this will be purged on "
                      "startup. Default = 7 days.")

    add_master_option(parser)

    parser.add_option("--max-position", action="store", dest="max_position",
                      default=0, type="int", help="used to detect slave "
                      "delay. The maximum difference between the master's "
                      "log position and the slave's reported read position of "
                      "the master. A value greater than this means the slave "
                      "is too far behind the master. Default is 0.")

    parser.add_option("--ping", action="store", dest="ping", default=None,
                      help="Number of ping attempts for detecting downed "
                      "server.")

    parser.add_option("--seconds-behind", action="store", dest="max_delay",
                      default=0, type="int", help="used to detect slave "
                      "delay. The maximum number of seconds behind the master "
                      "permitted before slave is considered behind the "
                      "master. Default is 0.")

    add_slaves_option(parser)

    parser.add_option("--timeout", action="store", dest="timeout", default=300,
                      help="maximum timeout in seconds to wait for each "
                      "replication command to complete. For example, timeout "
                      "for slave waiting to catch up to master. "
                      "Default = 300.")

    parser.add_option("--script-threshold", action="store", default=None,
                      dest="script_threshold",
                      help="Value for external scripts to trigger aborting "
                      "the operation if result is greater than or equal to "
                      "the threshold. Default = None (no threshold "
                      "checking).")


def check_server_lists(parser, master, slaves):
    """Check to see if master is listed in slaves list

    Returns bool - True = master not in slaves, issue error if it appears
    """
    if slaves:
        for slave in slaves.split(',', 1):
            if master == slave:
                parser.error("You cannot list the master as a slave.")

    return True


def obj2sql(obj):
    """Convert a Python object to an SQL object.

    This function convert Python objects to SQL values using the
    conversion functions in the database connector package."""
    return MySQLConverter().quote(obj)


def parse_user_password(userpass_values, my_defaults_reader=None,
                        options=None):
    """ This function parses a string with the user/password credentials.

    This function parses the login string, determines the used format, i.e.
    user[:password], config-path or login-path. If the ':' (colon) is not in
    the login string, the it can refer to a config-path, login-path or to a
    username (without a password). In this case, first it is assumed that the
    specified value is a config-path and tries to retrive the user and password
    from the configuration file secondly assume it is a login-path and the
    function attempts to retrieve the associated username and password, in a
    quiet way (i.e., without raising exceptions). If it fails to retrieve the
    login-path data, then the value is assumed to be a username.

    userpass_values[in]     String indicating the user/password credentials. It
                            must be in the form: user[:password] or login-path.
    my_defaults_reader[in]  Instance of MyDefaultsReader to read the
                            information of the login-path from configuration
                            files. By default, the value is None.
    options[in]             Dictionary of options (e.g. basedir), from the used
                            utility. By default, it set with an empty
                            dictionary. Note: also supports options values
                            from optparse.

    Returns a tuple with the username and password.
    """
    if options is None:
        options = {}
    # Split on the first ':' to determine if a login-path is used.
    login_values = userpass_values.split(':', 1)
    if len(login_values) == 1:
        # Format is config-path, login-path or user (without a password):
        # First check if the value is a config-path
        # The following method call also initializes the user and passwd with
        # default values in case the login_values are not from a config-path
        user, passwd = parse_login_values_config_path(login_values[0],
                                                      quietly=True)

        # Second assume it's a login-path and quietly try to retrieve the user
        # and password, in case of success overwrite the values previously set
        # and in case of failure return these ones instead.

        # Check if the login configuration file (.mylogin.cnf) exists
        if login_values[0] and not my_login_config_exists():
            return user, passwd

        if not my_defaults_reader:
            # Attempt to create the MyDefaultsReader
            try:
                my_defaults_reader = MyDefaultsReader(options)
            except UtilError:
                # Raise an UtilError when my_print_defaults tool is not found.
                return user, passwd
        elif not my_defaults_reader.tool_path:
            # Try to find the my_print_defaults tool
            try:
                my_defaults_reader.search_my_print_defaults_tool()
            except UtilError:
                # Raise an UtilError when my_print_defaults tool is not found.
                return user, passwd

        # Check if the my_print_default tool is able to read a login-path from
        # the mylogin configuration file
        if not my_defaults_reader.check_login_path_support():
            return user, passwd

        # Read and parse the login-path data (i.e., user and password)
        try:
            loginpath_data = my_defaults_reader.get_group_data(login_values[0])
            if loginpath_data:
                user = loginpath_data.get('user', None)
                passwd = loginpath_data.get('password', None)
                return user, passwd
            else:
                return user, passwd
        except UtilError:
            # Raise an UtilError if unable to get the login-path group data
            return user, passwd

    elif len(login_values) == 2:
        # Format is user:password; return a tuple with the user and password
        return login_values[0], login_values[1]
    else:
        # Invalid user credentials format
        raise FormatError("Unable to parse the specified user credentials "
                          "(accepted formats: <user>[:<password> or "
                          "<login-path>): %s" % userpass_values)


def add_basedir_option(parser):
    """ Add the --basedir option.
    """
    parser.add_option("--basedir", action="store", dest="basedir",
                      default=None, type="string",
                      help="the base directory for the server")


def check_dir_option(parser, opt_value, opt_name, check_access=False,
                     read_only=False):
    """ Check if the specified directory option is valid.

    Check if the value specified for the option is a valid directory, and if
    the user has appropriate access privileges. An appropriate  parser error
    is issued if the specified directory is invalid.

    parser[in]          Instance of the option parser (optparse).
    opt_value[in]       Value specified for the option.
    opt_name[in]        Option name (e.g., --basedir).
    check_access[in]    Flag specifying if the access privileges need to be
                        checked. By default, False (no access check).
    read_only[in]       Flag indicating if the access required is only for
                        read or read/write. By default, False (read/write
                        access). Note: only used if check_access=True.

    Return the absolute path for the specified directory or None if an empty
    value is specified.
    """
    # Check existence of specified directory.
    if opt_value:
        full_path = get_absolute_path(opt_value)
        if not os.path.isdir(full_path):
            parser.error("The specified path for {0} option is not a "
                         "directory: {1}".format(opt_name, opt_value))
        if check_access:
            mode = os.R_OK if read_only else os.R_OK | os.W_OK
            if not os.access(full_path, mode):
                parser.error("You do not have enough privileges to access the "
                             "folder specified by {0}.".format(opt_name))
        return full_path
    return None


def check_script_option(parser, opt_value, check_executable=True):
    """ Check if the specified script option is valid.

    Check if the script specified for the option exists, and if
    the user has appropriate access privileges to it. An appropriate parser
    error is issued if the specified directory does not exist or is not
    executable.

    parser[in]            Instance of the option parser (optparse).
    opt_value[in]         Value specified for the option.
    check_executable[in]  Flag specifying if the executable privileges need to
                          be checked. By default, True(needs to be executable).

    Return the absolute path for the specified script or None if an empty
    value is specified.
    """
    if opt_value:
        abs_path = os.path.abspath(opt_value)
        if not os.path.isfile(abs_path):
            parser.error(EXTERNAL_SCRIPT_DOES_NOT_EXIST.format(
                path=opt_value))

        if check_executable and not os.access(abs_path, os.X_OK):
            parser.error(INSUFFICIENT_FILE_PERMISSIONS.format(
                path=opt_value, permissions='execute'))
        return opt_value
    else:
        return None


def get_absolute_path(path):
    """ Returns the absolute path.
    """
    return os.path.abspath(os.path.expanduser(os.path.normpath(path)))


def db_objects_list_to_dictionary(parser, obj_list, option_desc,
                                  db_over_tables=True, sql_mode=''):
    """Process database object list and convert to a dictionary.

    Check the qualified name format of the given database objects and convert
    the given list of object to a dictionary organized by database names and
    sets of specific objects.

    Note: It is assumed that the given object list is obtained from the
    arguments or an option returned by the parser.

    parser[in]            Instance of the used option/arguments parser
    obj_list[in]          List of objects to process.
    option_desc[in]       Short description of the option for the object list
                          (e.g., "the --exclude option", "the database/table
                          arguments") to refer appropriately in any parsing
                          error.
    db_over_tables[in]    If True specifying a db alone overrides all
                          occurrences of table objects from that db (e.g.
                          if True and we have both db and db.table1, db.table1
                          is ignored).

    returns a dictionary with the objects grouped by database (without
    duplicates). None value associated to a database entry means that all
    objects are to be considered.
    E.g. {'db_name1': set(['table1','table2']), 'db_name2': None}.
    """
    db_objs_dict = {}
    for obj_name in obj_list:
        m_objs = parse_object_name(obj_name, sql_mode)
        if m_objs[0] is None:
            parser.error(PARSE_ERR_OBJ_NAME_FORMAT.format(
                obj_name=obj_name, option=option_desc
            ))
        else:
            db_name, obj_name = m_objs
            # Remove backtick quotes.
            db_name = remove_backtick_quoting(db_name, sql_mode) \
                if is_quoted_with_backticks(db_name, sql_mode) else db_name
            obj_name = remove_backtick_quoting(obj_name, sql_mode) \
                if obj_name and is_quoted_with_backticks(obj_name, sql_mode) \
                else obj_name
            # Add database object to result dictionary.
            if not obj_name:
                # If only the database is specified and db_over_tables is True,
                # then add entry with db name and value None (to include all
                # objects) even if a previous specific object was already
                # added, else if db_over_tables is False, add None value to the
                #  list, so that we know db was specified without any
                # table/routine.
                if db_name in db_objs_dict:
                    if db_objs_dict[db_name] and not db_over_tables:
                        db_objs_dict[db_name].add(None)
                    else:
                        db_objs_dict[db_name] = None
                else:
                    if db_over_tables:
                        db_objs_dict[db_name] = None
                    else:
                        db_objs_dict[db_name] = set([None])
            else:
                # If a specific object object is given add it to the set
                # associated to the database, except if the database entry
                # is None (meaning that all objects are included).
                if db_name in db_objs_dict:
                    if db_objs_dict[db_name]:
                        db_objs_dict[db_name].add(obj_name)
                else:
                    db_objs_dict[db_name] = set([obj_name])
    return db_objs_dict


def get_ssl_dict(parser_options=None):
    """Returns a dictionary with the SSL certificates

    parser_options[in]   options instance from the used option/arguments parser

    Returns a dictionary with the SSL certificates, each certificate name as
    the key with underscore instead of dash. If no certificate has been given
    by the user in arguments, returns an empty dictionary.

    Note: parser_options is a Values instance, that does not have method get as
    a dictionary instance.
    """
    conn_options = {}
    if parser_options is not None:
        certs_paths = {}
        if 'ssl_ca' in dir(parser_options):
            certs_paths['ssl_ca'] = parser_options.ssl_ca
        if 'ssl_cert' in dir(parser_options):
            certs_paths['ssl_cert'] = parser_options.ssl_cert
        if 'ssl_key' in dir(parser_options):
            certs_paths['ssl_key'] = parser_options.ssl_key
        if 'ssl' in dir(parser_options):
            certs_paths['ssl'] = parser_options.ssl
        conn_options.update(certs_paths)
    return conn_options


def get_value_intervals_list(parser, option_value, option_name, value_name):
    """Get and check the list of values for the given option.

    Convert the string value for the given option to the corresponding
    list of integer values and tuple of integers (for intervals). For example,
    converts the option_value '3,5-8,11' to the list [3, (5,8), 11].

    A parser error is issued if the used values or format are invalid.

    parser[in]          Instance of the used option/arguments parser.
    option_value[in]    Value specified for the option (e.g., '3,5-8,11').
    option_name[in]     Name of the option (e.g., '--status').
    value_name[in]      Name describing each option value (e.g., 'status').

    Returns a list of integers and tuple of integers (for intervals)
    representing the given option value string.
    """
    # Filter empty values and convert all to integers (values and intervals).
    values = option_value.split(",")
    values = [value for value in values if value]
    if len(values) <= 0:
        parser.error(PARSE_ERR_OPT_INVALID_VALUE.format(option=option_name,
                                                        value=option_value))
    res_list = []
    for value in values:
        interval = value.split('-')
        if len(interval) == 2:
            # Convert lower and higher value of the interval.
            try:
                lv = int(interval[0])
            except ValueError:
                parser.error("Invalid {0} value '{1}' (must be a "
                             "non-negative integer) for interval "
                             "'{2}'.".format(value_name, interval[0], value))
            try:
                hv = int(interval[1])
            except ValueError:
                parser.error("Invalid {0} value '{1}' (must be a "
                             "non-negative integer) for interval "
                             "'{2}'.".format(value_name, interval[1], value))
            # Add interval (tuple) to the list.
            res_list.append((lv, hv))
        elif len(interval) == 1:
            # Add single value to the status list.
            try:
                res_list.append(int(value))
            except ValueError:
                parser.error("Invalid {0} value '{1}' (must be a "
                             "non-negative integer).".format(value_name,
                                                             value))
        else:
            # Invalid format.
            parser.error("Invalid format for {0} interval (a single "
                         "dash must be used): '{1}'.".format(value_name,
                                                             value))
    return res_list


def check_date_time(parser, date_value, date_type, allow_days=False):
    """Check the date/time value for the given option.

    Check if the date/time value for the option is valid. The supported
    formats are 'yyyy-mm-ddThh:mm:ss' and 'yyyy-mm-dd'. If the allow days
    flag is ON then an integer valuse representing the number of days is
    also accepted.

    A parser error is issued if the date/time value is invalid.

    parser[in]        Instance of the used option/arguments parser.
    date_value[in]    Date/time value specified for the option.
    date_type[in]     Name describing the type of date being checked
                      (e.g., start, end, modified).
    allow_days[in]    Flag indicating if the specified value can also be an
                      integer representing the number of of days (> 0).

    Returns the date in the format 'yyyy-mm-ddThh:mm:ss' or an integer
    representing the number of days.
    """
    if allow_days:
        # Check if it is a valid number of days.
        try:
            days = int(date_value)
        except ValueError:
            # Not a valid integer (i.e., number of days).
            days = None
        if days:
            if days < 1:
                parser.error(PARSE_ERR_OPT_INVALID_NUM_DAYS.format(
                    date_type, date_value))
            return days
    # Check if it is a valid date/time format.
    _, _, time = date_value.partition("T")
    if time:
        try:
            dt_date = datetime.strptime(date_value, '%Y-%m-%dT%H:%M:%S')
        except ValueError:
            parser.error(PARSE_ERR_OPT_INVALID_DATE_TIME.format(date_type,
                                                                date_value))
    else:
        try:
            dt_date = datetime.strptime(date_value, '%Y-%m-%d')
        except ValueError:
            parser.error(PARSE_ERR_OPT_INVALID_DATE.format(date_type,
                                                           date_value))
    return dt_date.strftime('%Y-%m-%dT%H:%M:%S')


def check_gtid_set_format(parser, gtid_set):
    """Check the format of the GTID set given for the option.

    Perform some basic checks to verify the syntax of the specified string
    for the GTID set value. A parse error is issued if the format is incorrect.

    parser[in]      Instance of the used option/arguments parser.
    gtid_set[in]    GTID set value specified for the option.
    """

    # UUID format: hhhhhhhh-hhhh-hhhh-hhhh-hhhhhhhhhhhh
    re_uuid = re.compile(
        r"(?:[a-f]|\d){8}(?:-(?:[a-f]|\d){4}){3}-(?:[a-f]|\d){12}",
        re.IGNORECASE)
    # interval format: n[-n]
    re_interval = re.compile(r"(?:\d+)(?:-\d+)?")
    uuid_sets = gtid_set.split(',')
    for uuid_set in uuid_sets:
        uuid_set_elements = uuid_set.split(':')
        if len(uuid_set_elements) < 2:
            parser.error("Invalid GTID set '{0}' for option --gtid-set, "
                         "missing UUID or interval. Valid format: "
                         "uuid:interval[:interval].".format(uuid_set))
        # Check server UUID format.
        if not re_uuid.match(uuid_set_elements[0]):
            parser.error("Invalid UUID '{0}' for option --gtid-set. Valid "
                         "format: hhhhhhhh-hhhh-hhhh-hhhh-hhhhhhhhhhhh."
                         "".format(uuid_set_elements[0]))
        # Check intervals.
        for interval in uuid_set_elements[1:]:
            if not re_interval.match(interval):
                parser.error("Invalid interval '{0}' for option --gtid-set. "
                             "Valid format: n[-n].".format(interval))
            try:
                start_val, end_val = interval.split('-')
                if int(start_val) >= int(end_val):
                    parser.error(
                        "Invalid interval '{0}' for option --gtid-set. Start "
                        "value must be lower than the end value."
                        "".format(interval))
            except ValueError:
                # Error raised for intervals with a single value.
                pass  # Ignore no need to compare start and end value.


def check_password_security(options, args, prefix=""):
    """Check command line for passwords and report a warning.

    This method checks all options for passwords in the form ':%@'. If
    this pattern is found, the method with issue a warning to stdout and
    return True, else it returns False.

    Note: this allows us to make it possible to abort if command-line
          passwords are found (not the default...yet).

    options[in]     list of options
    args[in]        list of arguments
    prefix[in]      (optional) allows preface statement with # or something
                    for making the message a comment in-stream

    Returns - bool : False = no passwords, True = password found and msg shown
    """
    result = False
    for value in options.__dict__.values():
        if isinstance(value, list):
            for item in value:
                if find_password(item):
                    result = True
        else:
            if find_password(value):
                result = True
    for arg in args:
        if find_password(arg):
            result = True
    if result:
        print("{0}WARNING: Using a password on the command line interface"
              " can be insecure.".format(prefix))

    return result
#
# Copyright (c) 2014, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains the MySQLOptionsParser used to read the MySQL
configuration files.

This module belongs to Connector python, and it should be removed once
C/py v2.0.0 is released and in the meanwhile will be used from here.

"""

import codecs
import io
import os
import re
from ConfigParser import SafeConfigParser, MissingSectionHeaderError

DEFAULT_OPTION_FILES = {
    'nt': 'C:\\my.ini',
    'posix': '/etc/mysql/my.cnf'
}

DEFAULT_EXTENSIONS = {
    'nt': ('ini', 'cnf'),
    'posix': 'cnf'
}


class MySQLOptionsParser(SafeConfigParser):
    """This class implements methods to parse MySQL option files"""

    def __init__(self, files=None, keep_dashes=True):
        """Initialize

        files[in]       The files to parse searching for configuration items.
        keep_dashes[in] If False, dashes in options are replaced with
                        underscores.

        Raises ValueError if defaults is set to True but defaults files
        cannot be found.
        """

        # Regular expression to allow options with no value(For Python v2.6)
        self.OPTCRE = re.compile(           # pylint: disable=C0103
            r'(?P<option>[^:=\s][^:=]*)'
            r'\s*(?:'
            r'(?P<vi>[:=])\s*'
            r'(?P<value>.*))?$'
        )

        self._options_dict = {}

        SafeConfigParser.__init__(self)
        self.default_extension = DEFAULT_EXTENSIONS[os.name]
        self.keep_dashes = keep_dashes

        if not files:
            raise ValueError('files argument should be given')
        if isinstance(files, str):
            self.files = [files]
        else:
            self.files = files

        self._parse_options(list(self.files))
        self._sections = self.get_groups_as_dict()

    def optionxform(self, optionstr):
        """Converts option strings

        optionstr[in] input to be converted.

        Converts option strings to lower case and replaces dashes(-) with
        underscores(_) if keep_dashes variable is set.

        """
        if not self.keep_dashes:
            optionstr = optionstr.replace('-', '_')
        return optionstr.lower()

    def _parse_options(self, files):
        """Parse options from files given as arguments.
         This method checks for !include or !includedir directives and if there
         is any, those files included by these directives are also parsed
         for options.

         files[in]       The files to parse searching for configuration items.

        Raises ValueError if any of the included or file given in arguments
        is not readable.
        """
        index = 0
        err_msg = "Option file '{0}' being included again in file '{1}'"

        for file_ in files:
            try:
                with open(file_, 'r') as op_file:
                    for line in op_file.readlines():
                        if line.startswith('!includedir'):
                            _, dir_path = line.split(None, 1)
                            for entry in os.listdir(dir_path):
                                entry = os.path.join(dir_path, entry)
                                if entry in files:
                                    raise ValueError(err_msg.format(
                                        entry, file_))
                                if (os.path.isfile(entry) and
                                        entry.endswith(
                                            self.default_extension)):
                                    files.insert(index + 1, entry)

                        elif line.startswith('!include'):
                            _, filename = line.split(None, 1)
                            if filename in files:
                                raise ValueError(err_msg.format(
                                    filename, file_))
                            files.insert(index + 1, filename)

                        index += 1

            except (IOError, OSError) as exc:
                raise ValueError("Failed reading file '{0}': {1}".format(
                    file_, str(exc)))

        read_files = self.read(files)
        not_read_files = set(files) - set(read_files)
        if not_read_files:
            raise ValueError("File(s) {0} could not be read.".format(
                ', '.join(not_read_files)))

    def read(self, filenames):
        """Read and parse a filename or a list of filenames.

        Overridden from ConfigParser and modified so as to allow options
        which are not inside any section header

        filenames[in]    The file names to read.

        Return list of successfully read files.
        """
        # Get python version since we must use str() to read strings from
        # the file for older, 2.6 versions of Python
        py26 = check_python_version((2, 6, 0), (2, 6, 99), False,
                                    None, False, False, False)
        if isinstance(filenames, str):
            filenames = [filenames]
        read_ok = []
        for priority, filename in enumerate(filenames):
            try:
                out_file = io.StringIO()
                for line in codecs.open(filename, encoding='utf-8'):
                    line = line.strip()
                    match_obj = self.OPTCRE.match(line)
                    if not self.SECTCRE.match(line) and match_obj:
                        optname, delimiter, optval = match_obj.group('option',
                                                                     'vi',
                                                                     'value')
                        if optname and not optval and not delimiter:
                            out_file.write(line + "=\n")
                        else:
                            out_file.write(line + '\n')
                    else:
                        out_file.write(line + '\n')
                out_file.seek(0)
                self._read(out_file, filename)
            except IOError:
                continue
            try:
                self._read(out_file, filename)
                for group in self._sections.keys():
                    try:
                        self._options_dict[group]
                    except KeyError:
                        self._options_dict[group] = {}
                    for option, value in self._sections[group].items():
                        if py26:
                            self._options_dict[group][option] = (str(value),
                                                                 priority)
                        else:
                            self._options_dict[group][option] = (value,
                                                                 priority)

                self._sections = self._dict()

            except MissingSectionHeaderError:
                self._read(out_file, filename)
            out_file.close()
            read_ok.append(filename)
        return read_ok

    def get_groups(self, *args):
        """Returns options as a dictionary.

        Returns options from all the groups specified as arguments, returns
        the options from all groups if no argument provided. Options are
        overridden when they are found in the next group.

        *args[in]    Each group to be returned can be requested by providing
                     its name as an argument.

        Returns a dictionary
        """
        if len(args) == 0:
            args = self._options_dict.keys()

        options = {}
        for group in args:
            try:
                for option, value in self._options_dict[group].items():
                    if option not in options or options[option][1] <= value[1]:
                        options[option] = value
            except KeyError:
                pass

        for key in options.keys():
            if key == '__name__' or key.startswith('!'):
                del options[key]
            else:
                options[key] = options[key][0]
        return options

    def get_groups_as_dict_with_priority(self, *args):  # pylint: disable=C0103
        """Returns options as dictionary of dictionaries.

        Returns options from all the groups specified as arguments. For each
        group the option are contained in a dictionary. The order in which
        the groups are specified is unimportant. Also options are not
        overridden in between the groups.

        The value is a tuple with two elements, first being the actual value
        and second is the priority of the value which is higher for a value
        read from a higher priority file.

        *args[in]    Each group to be returned can be requested by providing
                     its name as an argument.

        Returns an dictionary of dictionaries
        """
        if len(args) == 0:
            args = self._options_dict.keys()

        options = dict()
        for group in args:
            try:
                options[group] = dict(self._options_dict[group])
            except KeyError:
                pass

        for group in options.keys():
            for key in options[group].keys():
                if key == '__name__' or key.startswith('!'):
                    del options[group][key]
        return options

    def get_groups_as_dict(self, *args):
        """Returns options as dictionary of dictionaries.

        Returns options from all the groups specified as arguments. For each
        group the option are contained in a dictionary. The order in which
        the groups are specified is unimportant. Also options are not
        overridden in between the groups.

        *args[in]    Each group to be returned can be requested by providing
                     its name as an argument.

        Returns an dictionary of dictionaries
        """
        if len(args) == 0:
            args = self._options_dict.keys()

        options = dict()
        for group in args:
            try:
                options[group] = dict(self._options_dict[group])
            except KeyError:
                pass

        for group in options.keys():
            for key in options[group].keys():
                if key == '__name__' or key.startswith('!'):
                    del options[group][key]
                else:
                    options[group][key] = options[group][key][0]
        return options
#
# Copyright (c) 2011, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#
"""Module with parsers for General and Slow Query Log.
"""

import re
import decimal
import datetime



_DATE_PAT = r"\d{6}\s+\d{1,2}:\d{2}:\d{2}"

_HEADER_VERSION_CRE = re.compile(
    r"(.+), Version: (\d+)\.(\d+)\.(\d+)(?:-(\S+))?")
_HEADER_SERVER_CRE = re.compile(r"Tcp port:\s*(\d+)\s+Unix socket:\s+(.*)")

_SLOW_TIMESTAMP_CRE = re.compile(r"#\s+Time:\s+(" + _DATE_PAT + r")")
_SLOW_USERHOST_CRE = re.compile(r"#\s+User@Host:\s+"
                                r"(?:([\w\d]+))?\s*"
                                r"\[\s*([\w\d]+)\s*\]\s*"
                                r"@\s*"
                                r"([\w\d\.\-]*)\s*"
                                r"\[\s*([\d.]*)\s*\]\s*"
                                r"(?:Id\:\s*(\d+)?\s*)?")
_SLOW_STATS_CRE = re.compile(r"#\sQuery_time:\s(\d*\.\d{1,6})\s*"
                             r"Lock_time:\s(\d*\.\d{1,6})\s*"
                             r"Rows_sent:\s(\d*)\s*"
                             r"Rows_examined:\s(\d*)")

_GENERAL_ENTRY_CRE = re.compile(
    r'(?:(' + _DATE_PAT + r'))?\s*'
    r'(\d+)\s([\w ]+)\t*(?:(.+))?$')


class LogParserBase(object):
    """Base class for parsing MySQL log files

    LogParserBase should be inherited to create parsers for MySQL log files.
    This class has the following capabilities:

    - Take a stream and check whether it is a file type
    - Retrieve next line from stream
    - Parse header information from a log file (for General or Slow Query Log)
    - Implements the iterator protocol

    This class should not be used directly, but inhereted and extended to
    match the log file which needs to be parsed.
    """
    def __init__(self, stream):
        """Constructor

        stream[in]          A file type

        The stream argument must be a valid file type supporting for
        example the readline()-method. For example, the return of the buildin
        function open() can be used:
            LogParserBase(open("/path/to/mysql.log"))

        Raises LogParserError on errors.
        """
        self._stream = None
        self._version = None
        self._program = None
        self._port = None
        self._socket = None
        self._start_datetime = None
        self._last_seen_datetime = None

        # Check if we got a file type
        line = None
        try:
            self._stream = stream
            line = self._get_next_line()
        except AttributeError:
            raise LogParserError("Need a file type")

        # Not every log file starts with a header
        if line is not None and line.endswith('started with:'):
            self._parse_header(line)
        else:
            self._stream.seek(0)

    def _get_next_line(self):
        """Get next line from the log file

        This method reads the next line from the stream. Trailing
        newline (\n) and carraige return (\r) are removed.

        Returns next line as string or None
        """
        line = self._stream.readline()
        if not line:
            return None
        return line.rstrip('\r\n')

    def _parse_header(self, line):
        """Parse the header of a MySQL log file

        line[in]        A string, usually result of self._get_next_line()

        This method parses the header of a MySQL log file, that is the header
        found in the General and Slow Query log files. It sets attributes
        _version, _program, _port and _socket.
        Note that headers can repeat in a log file, for example, after a
        restart of the MySQL server.

        Example header:
        /usr/sbin/mysqld, Version: 5.5.17-log (Source distribution). started
        with:
        Tcp port: 0  Unix socket: /tmp/mysql.sock
        Time                 Id Command    Argument

        Raises LogParserError on errors.
        """
        if line is None:
            return
        # Header line containing executable and version, example:
        # /raid0/mysql/mysql/bin/mysqld,
        # Version: 5.5.17-log (Source distribution). started with:
        info = _HEADER_VERSION_CRE.match(line)
        if not info:
            raise LogParserError("Could not read executable and version from "
                                 "header")
        program, major, minor, patch, extra = info.groups()

        # Header line with server information, example:
        # Tcp port: 3306  Unix socket: /tmp/mysql.sock
        line = self._get_next_line()
        info = _HEADER_SERVER_CRE.match(line)
        if not info:
            raise LogParserError("Malformed server header line: %s" % line)
        tcp_port, unix_socket = info.groups()

        # Throw away column header line, example:
        # Time                 Id Command    Argument
        self._get_next_line()

        self._version = (int(major), int(minor), int(patch), extra)
        self._program = program
        self._port = int(tcp_port)
        self._socket = unix_socket

    @property
    def version(self):
        """Returns the MySQL server version

        This property returns a tuple descriving the version of the
        MySQL server producing the log file. The tuple looks like this:
            (major, minor, patch, extra)

        The extra part is optional and when not available will be None.
        Examples:
            (5,5,17,'log')
            (5,1,57,None)

        Note that the version can change in the same log file.

        Returns a tuple or None.
        """
        return self._version

    @property
    def program(self):
        """Returns the executable which wrote the log file

        This property returns the full path to the executable which
        produced the log file.

        Note that the executable can change in the same log file.

        Returns a string or None.
        """
        return self._program

    @property
    def port(self):
        """Returns the MySQL server TCP/IP port

        This property returns the TCP/IP port on which the MySQL server
        was listening.

        Note that the TCP/IP port can change in the same log file.

        Returns an integer or None.
        """
        return self._port

    @property
    def socket(self):
        """Returns the MySQL server UNIX socket

        This property returns full path to UNIX socket used the MySQL server
        to accept incoming connections on UNIX-like servers.

        Note that the UNIX socket location can change in the same log file.

        Returns a string or None.
        """
        return self._socket

    @property
    def start_datetime(self):
        """Returns timestamp of first read log entry

        This property returns the timestamp of the first read log entry.

        Returns datetime.datetime-object or None.
        """
        return self._start_datetime

    @property
    def last_seen_datetime(self):
        """Returns timestamp of last read log entry

        This property returns the timestamp of the last read log entry.

        Returns datetime.datetime-object or None
        """
        return self._last_seen_datetime

    def __iter__(self):
        """Class is iterable

        Returns a LogParserBase-object.
        """
        return self

    def next(self):
        """Returns the next log entry

        Raises StopIteration when no more entries are available.

        Returns a LogEntryBase-object.
        """
        entry = self._parse_entry()
        if entry is None:
            raise StopIteration
        return entry

    def _parse_entry(self):
        """Returns a parsed log entry
        """
        pass

    def __str__(self):
        """String representation of LogParserBase
        """
        return "<%(clsname)s, MySQL v%(version)s>" % dict(
            clsname=self.__class__.__name__,
            version='.'.join([str(v) for v in self._version[0:3]]) +
            (self._version[3] or '')
        )


class GeneralQueryLog(LogParserBase):
    """Class implementing a parser for the MySQL General Query Log

    The GeneralQueryLog-class implements a parse for the MySQL General Query
    Log and has the following capabilities:
    - Parse General Query Log entries
    - Possibility to handle special commands
    - Keep track of MySQL sessions and remove them
    - Process log headers found later in the log file
    """
    def __init__(self, stream):
        """Constructor

        stream[in]      file type

        Raises LogParserError on errors.
        """
        super(GeneralQueryLog, self).__init__(stream)
        self._sessions = {}
        self._cached_logentry = None

        self._commands = {
            # 'Sleep': None,
            'Quit': self._handle_quit,
            'Init DB': self._handle_init_db,
            'Query': self._handle_multi_line,
            # 'Field List': None,
            # 'Create DB': None,
            # 'Drop DB': None,
            # 'Refresh': None,
            # 'Shutdown': None,
            # 'Statistics': None,
            # 'Processlist': None,
            'Connect': self._handle_connect,
            # 'Kill': None,
            # 'Debug': None,
            # 'Ping': None,
            # 'Time': None,
            # 'Delayed insert': None,
            # 'Change user': None,
            # 'Binlog Dump': None,
            # 'Table Dump': None,
            # 'Connect Out': None,
            # 'Register Slave': None,
            'Prepare': self._handle_multi_line,
            'Execute': self._handle_multi_line,
            # 'Long Data': None,
            # 'Close stmt': None,
            # 'Reset stmt': None,
            # 'Set option': None,
            'Fetch': self._handle_multi_line,
            # 'Daemon': None,
            # 'Error': None,
        }

    def _new_session(self, session_id):
        """Create a new session using the given session ID

        session_id[in]      integer presenting a MySQL session

        Returns a dictionary.
        """
        self._sessions[session_id] = dict(
            database=None,
            user=None,
            host=None,
            time_last_action=None,
            to_delete=False
        )
        return self._sessions[session_id]

    @staticmethod
    def _handle_connect(entry, session, argument):
        """Handle a 'Connect'-command

        entry[in]       a GeneralQueryLogEntry-instance
        session[in]     a dictionary with current session information,
                        element of self._sessions
        argument[in]    a string, last part of a log entry

        This method reads user and database information from the argument of
        a 'Connect'-command. It sets the user, host and database for the
        current session and also sets the argument for the entry.

        """
        # Argument can be as follows:
        # root@localhost on test
        # root@localhost on
        try:
            connection, _, database = argument.split(' ')
        except ValueError:
            connection = argument.replace(' on', '')
            database = None
        session['user'], session['host'] = connection.split('@')
        session['database'] = database
        entry['argument'] = argument

    @staticmethod
    def _handle_init_db(entry, session, argument):
        """Handle an 'Init DB'-command

        entry[in]       a GeneralQueryLogEntry-instance
        session[in]     a dictionary with current session information,
                        element of self._sessions
        argument[in]    a string, last part of a log entry

        The argument parameter is always the database name.
        """
        # Example (of full line):
        #           3 Init DB   mysql
        session['database'] = argument
        entry['argument'] = argument

    def _handle_multi_line(self, entry, session, argument):
        """Handle a command which can span multiple lines

        entry[in]       a GeneralQueryLogEntry-instance
        session[in]     a dictionary with current session information,
                        element of self._sessions
        argument[in]    a string, last part of a log entry

        The argument parameter passed to this function is the last part of a
        General Query Log entry and usually is already the full query.

        This function's main purpose is to read log entries which span multiple
        lines, such as the Query and Prepare-commands.
        """
        # Examples:
        # 111205 10:01:14       6 Query SELECT Name FROM time_zone_name
        #                       WHERE Time_zone_id = 417
        # 111205 10:03:28       6 Query SELECT Name FROM time_zone_name
        # WHERE Time_zone_id = 417
        argument_parts = [argument, ]
        line = self._get_next_line()
        # Next line is None if the end of the file is reached.
        # Note: empty lines can appear and should be read (i.e., line == '').
        while line is not None:
            # Stop if it is a header.
            if line.endswith('started with:'):
                self._cached_logentry = line
                break
            # Stop if a new log entry is found.
            info = _GENERAL_ENTRY_CRE.match(line)
            if info is not None:
                self._cached_logentry = info.groups()
                break
            # Otherwise, append line and read next.
            argument_parts.append(line)
            line = self._get_next_line()

        entry['argument'] = '\n'.join(argument_parts)

    @staticmethod
    def _handle_quit(entry, session, argument):
        """Handle the 'Quit'-command

        entry[in]       a GeneralQueryLogEntry-instance
        session[in]     a dictionary with current session information,
                        element of self._sessions
        argument[in]    a string, last part of a log entry

        This function sets a flag that the session can be removed from the
        session list.
        """
        # Example (of full line):
        # 111205 10:06:53       6 Quit
        session['to_delete'] = True

    def _parse_command(self, logentry, entry):
        """Parse a log entry from the General Query Log

        logentry[in]    a string or tuple
        entry[in]       an instance of GeneralQueryLogEntry

        The logentry-parameter is either a line read from the log file or
        the result of a previous attempt to read a command.
        The entry argument should be an instance of GeneralQueryLogEntry.
        It returns the entry or None if nothing could be read.

        Raises LogParserError on errors.

        Returns the GeneralQueryLogEntry-instance or None
        """
        if logentry is None:
            return None
        if isinstance(logentry, tuple):
            dt, session_id, command, argument = logentry
        elif logentry.endswith('started with:'):
            while logentry.endswith('started with:'):
                # We got a header
                self._parse_header(logentry)
                logentry = self._get_next_line()
                if logentry is None:
                    return None
            return self._parse_command(logentry, entry)
        else:
            info = _GENERAL_ENTRY_CRE.match(logentry)
            if info is None:
                raise LogParserError("Failed parsing command line: %s"
                                     % logentry)
            dt, session_id, command, argument = info.groups()
        self._cached_logentry = None

        session_id = int(session_id)
        entry['session_id'] = session_id
        try:
            session = self._sessions[session_id]
        except KeyError:
            session = self._new_session(session_id)

        entry['command'] = command
        if dt is not None:
            entry['datetime'] = datetime.datetime.strptime(dt,
                                                           "%y%m%d %H:%M:%S")
            session['time_last_action'] = entry['datetime']
        else:
            entry['datetime'] = session['time_last_action']

        try:
            self._commands[command](entry, session, argument)
        except KeyError:
            # Generic command
            entry['argument'] = argument

        for key in entry.keys():
            if key in session:
                entry[key] = session[key]

        if session['to_delete'] is True:
            del self._sessions[session_id]
            del session

        return entry

    def _parse_entry(self):
        """Returns a parsed log entry

        The method _parse_entry() uses _parse_command() to parse
        a General Query Log entry. It is used by the iterator protocol methods.

        Returns a GeneralQueryLogEntry-instance or None.
        """
        entry = GeneralQueryLogEntry()
        if self._cached_logentry is not None:
            self._parse_command(self._cached_logentry, entry)
            return entry
        else:
            line = self._get_next_line()
        if line is None:
            return None

        self._parse_command(line, entry)
        return entry


class SlowQueryLog(LogParserBase):
    """Class implementing a parser for the MySQL Slow Query Log

    The SlowQueryLog-class implements a parser for the MySQL Slow Query Log and
    has the following capabilities:
    - Parse Slow Query Log entries
    - Process log headers found later in the log file
    - Parse connection and temporal information
    - Get statistics of the slow query
    """
    def __init__(self, stream):
        """Constructor

        stream[in]      A file type

        The stream argument must be a valid file type supporting for
        example the readline()-method. For example, the return of the build-in
        function open() can be used:
            SlowQueryLog(open("/path/to/mysql-slow.log"))

        Raises LogParserError on errors.
        """
        super(SlowQueryLog, self).__init__(stream)
        self._cached_line = None
        self._current_database = None

    @staticmethod
    def _parse_line(regex, line):
        """Parses a log line using given regular expression

        regex[in]   a SRE_Match-object
        line[in]    a string

        This function takes a log line and matches the regular expresion given
        with the regex argument. It returns the result of
        re.MatchObject.groups(), which is a tuple.

        Raises LogParserError on errors.

        Returns a tuple.
        """
        info = regex.match(line)
        if info is None:
            raise LogParserError('Failed parsing Slow Query line: %s' %
                                 line[:30])
        return info.groups()

    def _parse_connection_info(self, line, entry):
        """Parses connection info

        line[in]    a string
        entry[in]   a SlowQueryLog-instance

        The line paramater should be a string, a line read from the Slow Query
        Log. The entry argument should be an instance of SlowQueryLogEntry.

        Raises LogParserError on failure.
        """
        # Example:
        # # User@Host: root[root] @ localhost [127.0.0.1]
        (priv_user,
         unpriv_user,
         host,
         ip,
         sid) = self._parse_line(_SLOW_USERHOST_CRE, line)

        entry['user'] = priv_user if priv_user else unpriv_user
        entry['host'] = host if host else ip
        entry['session_id'] = sid

    def _parse_timestamp(self, line, entry):
        """Parses a timestamp

        line[in]    a string
        entry[in]   a SlowQueryLog-instance

        The line paramater should be a string, a line read from the Slow Query
        Log. The entry argument should be an instance of SlowQueryLogEntry.

        Raises LogParserError on failure.
        """
        # Example:
        # # Time: 111206 11:55:54
        info = self._parse_line(_SLOW_TIMESTAMP_CRE, line)

        entry['datetime'] = datetime.datetime.strptime(info[0],
                                                       "%y%m%d %H:%M:%S")
        if self._start_datetime is None:
            self._start_datetime = entry['datetime']
            self._last_seen_datetime = entry['datetime']

    def _parse_statistics(self, line, entry):
        """Parses statistics information

        line[in]    a string
        entry[in]   a SlowQueryLog-instance

        The line paramater should be a string, a line read from the Slow Query
        Log. The entry argument should be an instance of SlowQueryLogEntry.

        Raises LogParserError on errors.
        """
        # Example statistic line:
        # Query_time: 0.101194  Lock_time: 0.000331 Rows_sent: 24
        # Rows_examined: 11624
        result = self._parse_line(_SLOW_STATS_CRE, line)

        entry['query_time'] = decimal.Decimal(result[0])
        entry['lock_time'] = decimal.Decimal(result[1])
        entry['rows_sent'] = int(result[2])
        entry['rows_examined'] = int(result[3])

    def _parse_query(self, line, entry):
        """Parses the query

        line[in]    a string
        entry[in]   a SlowQueryLog-instance

        The line paramater should be a string, a line read from the Slow Query
        Log. The entry argument should be an instance of SlowQueryLogEntry.

        Query entries in the Slow Query Log could span several lines. They can
        optionally start with a USE-command and have session variables, such as
        'timestamp', set before the actual query.
        """
        # Example:
        # SET timestamp=1323169459;
        # SELECT SCHEMA_NAME FROM INFORMATION_SCHEMA.SCHEMATA
        #    WHERE SCHEMA_NAME = 'mysql';
        # # User@Host: root[root] @ localhost [127.0.0.1]
        query = []
        while True:
            if line is None:
                break
            if line.startswith('use'):
                entry['database'] = self._current_database = line.split(' ')[1]
            elif line.startswith('SET timestamp='):
                entry['datetime'] = datetime.datetime.fromtimestamp(
                    int(line[14:].strip(';')))
            elif (line.startswith('# Time:') or
                  line.startswith("# User@Host") or
                  line.endswith('started with:')):
                break
            query.append(line)
            line = self._get_next_line()

        if 'database' in entry:
            # This is not always correct: connections without current database
            # will get the database name of the previous query. However, it's
            # more likely current database is set. Fix would be that the server
            # includes a USE-statement for every entry.
            if (entry['database'] is None and
                    self._current_database is not None):
                entry['database'] = self._current_database
        entry['query'] = '\n'.join(query)
        self._cached_line = line

    def _parse_entry(self):
        """Parse and returns an entry of the Slow Query Log

        Each entry of the slow log consists of:
        1. An optional time line
        2. A connection information line with user, hostname and database
        3. A line containing statistics for the query
        4. An optional "use <database>" line
        5. A line setting the timestamp, insert_id, and last_insert_id
           session variables
        6. An optional administartor command line "# administator command"
        7. An optional SQL statement or the query

        Returns a SlowQueryLogEntry-instance or None
        """
        if self._cached_line is not None:
            line = self._cached_line
            self._cached_line = None
        else:
            line = self._get_next_line()
        if line is None:
            return None

        while line.endswith('started with:'):
            # We got a header
            self._parse_header(line)
            line = self._get_next_line()
            if line is None:
                return None

        entry = SlowQueryLogEntry()

        if line.startswith('# Time:'):
            self._parse_timestamp(line, entry)
            line = self._get_next_line()

        if line.startswith('# User@Host:'):
            self._parse_connection_info(line, entry)
            line = self._get_next_line()

        if line.startswith('# Query_time:'):
            self._parse_statistics(line, entry)
            line = self._get_next_line()

        self._parse_query(line, entry)

        return entry


class LogEntryBase(dict):
    """Class inherited by GeneralQueryEntryLog and SlowQueryEntryLog

    This class has the following capabilities:
    - Inherits from dict
    - Dictionary elements can be accessed using attributes. For example,
      logentry['database'] is accessible like logentry.database

    Should not be used directly.
    """
    def __init__(self):
        super(LogEntryBase, self).__init__()
        self['datetime'] = None
        self['database'] = None
        self['user'] = None
        self['host'] = None
        self['session_id'] = None

    def __getattr__(self, name):
        if name in self:
            return self[name]
        else:
            raise AttributeError("%s has no attribute '%s'" %
                                 (self.__class__.__name__, name))


class GeneralQueryLogEntry(LogEntryBase):
    """Class representing an entry of the General Query Log

    """
    def __init__(self):
        """Constructor

        GeneralQueryLogEntry inherits from LogEntryBase, which inherits from
        dict. Instances of GeneralQueryLogEntry can be used just like
        dictionaries.
        """
        super(GeneralQueryLogEntry, self).__init__()
        self['session_id'] = None
        self['command'] = None
        self['argument'] = None

    def __str__(self):
        """String representation of GeneralQueryLogEntry
        """
        param = self.copy()
        param['clsname'] = self.__class__.__name__
        try:
            if len(param['argument']) > 30:
                param['argument'] = param['argument'][:28] + '..'
        except TypeError:
            pass  # Nevermind when param['argument'] was not a string.
        try:
            param['datetime'] = param['datetime'].strftime("%Y-%m-%d %H:%M:%S")
        except AttributeError:
            param['datetime'] = ''
        return ("<%(clsname)s %(datetime)s [%(session_id)s]"
                " %(command)s: %(argument)s>" % param)


class SlowQueryLogEntry(LogEntryBase):
    """Class representing an entry of the Slow Query Log

    SlowQueryLogEntry inherits from LogEntryBase, which inherits from dict.
    Instances of SlowQueryLogEntry can be used just like dictionaries.
    """
    def __init__(self):
        """Constructor
        """
        super(SlowQueryLogEntry, self).__init__()
        self['query'] = None
        self['query_time'] = None
        self['lock_time'] = None
        self['rows_examined'] = None
        self['rows_sent'] = None

    def __str__(self):
        """String representation of SlowQueryLogEntry
        """
        param = self.copy()
        param['clsname'] = self.__class__.__name__
        try:
            param['datetime'] = param['datetime'].strftime("%Y-%m-%d %H:%M:%S")
        except AttributeError:
            param['datetime'] = ''
        return (
            "<%(clsname)s %(datetime)s [%(user)s@%(host)s] "
            "%(query_time)s/%(lock_time)s/%(rows_examined)s/%(rows_sent)s>"
        ) % param
#
# Copyright (c) 2012, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains auxiliary functions to handle pattern matching.
"""

import re


# Regular expression to match a database object identifier (support backticks)
REGEXP_OBJ_NAME = r'(`(?:[^`]|``)+`|\w+|\w+[\%\*]?|[\%\*])'

# Regular expression to match a database object identifier with ansi quotes
REGEXP_OBJ_NAME_AQ = r'("(?:[^"]|"")+"|\w+|\*)'

# Regular expression to match a qualified object identifier (with multiple
# parts). Example: db.obj, db or obj
REGEXP_QUALIFIED_OBJ_NAME = r'{0}(?:(?:\.){0})?'.format(REGEXP_OBJ_NAME)

# Same as the above but for use with ansi quotes
REGEXP_QUALIFIED_OBJ_NAME_AQ = r'{0}(?:(?:\.){0})?'.format(REGEXP_OBJ_NAME_AQ)


def convertSQL_LIKE2REGEXP(sql_like_pattern):
    """Convert a standard SQL LIKE pattern to a REGEXP pattern.

    Function that transforms a SQL LIKE pattern to a supported python
    regexp. Returns a python regular expression (i.e. regexp).

    sql_like_pattern[in] pattern in the SQL LIKE form to be converted.
    """
    # Replace '_' by equivalent regexp, except when precede by '\'
    # (escape character)
    regexp = re.sub(r'(?<!\\)_', '.', sql_like_pattern)
    # Replace '%' by equivalent regexp, except when precede by '\'
    # (escape character)
    regexp = re.sub(r'(?<!\\)%', '.*', regexp)
    # Set regexp to ignore cases; SQL patterns are case-insensitive by default.
    regexp = "(?i)^(" + regexp + ")$"
    return regexp


def parse_object_name(qualified_name, sql_mode='', wild=False):
    """Parses a qualified object name from the given string.

    qualified_name[in] MySQL object string (e.g. db.table)
    sql_mode[in]       The value of sql_mode from the server.
    wild[in]           Look for wildcards (stating at end of str)

    Returns tuple containing name split
    """
    if "ANSI_QUOTES" in sql_mode:
        regex_pattern = REGEXP_QUALIFIED_OBJ_NAME.replace("`", '"')
    else:
        regex_pattern = REGEXP_QUALIFIED_OBJ_NAME
    if wild:
        regex_pattern = regex_pattern + r'\Z'
    # Split the qualified name considering backtick quotes
    parts = re.match(regex_pattern, qualified_name)
    if parts:
        return parts.groups()
    else:
        return (None, None)
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains abstractions of MySQL replication functionality.
"""

import os
import time
import StringIO
import socket



_MASTER_INFO_COL = [
    'Master_Log_File', 'Read_Master_Log_Pos', 'Master_Host', 'Master_User',
    'Master_Password', 'Master_Port', 'Connect_Retry', 'Master_SSL_Allowed',
    'Master_SSL_CA_File', 'Master_SSL_CA_Path', 'Master_SSL_Cert',
    'Master_SSL_Cipher', 'Master_SSL_Key', 'Master_SSL_Verify_Server_Cert',
    'Heartbeat', 'Bind', 'Ignored_server_ids', 'Uuid', 'Retry_count',
    'SSL_CRL', 'SSL_CRL_Path', 'Enabled_auto_position', 'Channel_Name',
]

_SLAVE_IO_STATE, _SLAVE_MASTER_HOST, _SLAVE_MASTER_USER, _SLAVE_MASTER_PORT, \
    _SLAVE_MASTER_LOG_FILE, _SLAVE_MASTER_LOG_FILE_POS, _SLAVE_IO_RUNNING, \
    _SLAVE_SQL_RUNNING, _SLAVE_DO_DB, _SLAVE_IGNORE_DB, _SLAVE_DO_TABLE, \
    _SLAVE_IGNORE_TABLE, _SLAVE_WILD_DO_TABLE, _SLAVE_WILD_IGNORE_TABLE, \
    _SLAVE_DELAY, _SLAVE_REMAINING_DELAY, _SLAVE_IO_ERRORNO, _SLAVE_IO_ERROR, \
    _SLAVE_SQL_ERRORNO, _SLAVE_SQL_ERROR, _MASTER_UUID, _RETRIEVED_GTID_SET, \
    _EXECUTED_GTID_SET = \
    0, 1, 2, 3, 5, 6, 10, 11, 12, 13, 14, 15, 16, 17, 32, 33, 34, 35, 36, 37,\
    40, 51, 52

_PRINT_WIDTH = 75

_MASTER_DO_DB, _MASTER_IGNORE_DB = 2, 3

_RPL_USER_QUERY = """
    SELECT user, host, password = '' as has_password
    FROM mysql.user
    WHERE repl_slave_priv = 'Y'
"""
# Query for server versions >= 5.7.6.
_RPL_USER_QUERY_5_7_6 = """
    SELECT user, host, authentication_string = '' as has_password
    FROM mysql.user
    WHERE repl_slave_priv = 'Y'
"""

_WARNING = "# WARNING: %s"
_MASTER_BINLOG = "Server '%s' does not have binary logging turned on."
_NO_RPL_USER = "No --rpl-user specified and multiple users found with " + \
               "replication privileges."
_RPL_USER_PASS = "No --rpl-user specified and the user found with " + \
                 "replication privileges requires a password."

_GTID_EXECUTED = "SELECT @@GLOBAL.GTID_EXECUTED"
_GTID_WAIT = "SELECT WAIT_UNTIL_SQL_THREAD_AFTER_GTIDS('%s', %s)"


def _get_list(rows, cols):
    """Return a list of information in GRID format to stdout.

    rows[in]          rows of data
    cols[in]          column headings

    Returns list of strings
    """
    ostream = StringIO.StringIO()
    format_tabular_list(ostream, cols, rows)
    return ostream.getvalue().splitlines()


def negotiate_rpl_connection(server, is_master=True, strict=True,
                             options=None):
    """Determine replication connection

    This method attempts to determine if it is possible to build a CHANGE
    MASTER command based on the server passed. If it is possible, the method
    will return a CHANGE MASTER command. If there are errors and the strict
    option is turned on, it will throw errors if there is something missing.
    Otherwise, it will return the CHANGE MASTER command with warnings.

    If the server is a master, the following error checks will be performed.

      - if binary log is turned OFF, and strict = False, a warning message
        is added to the strings returned else an error is thrown

      - if the rpl_user option is missing, the method attempts to find a
        replication user. If more than one user is found or none are found, and
        strict = False, a warning message is added to the strings returned else
        an error is thrown

      - if a replication user is found but the user requires a password,
        the MASTER_USER and MASTER_PASSWORD options are commented out

    Note: the CHANGE MASTER command is formatted whereby each option is
          separated by a newline and indented two spaces

    Note: the make_change_master method does not support SSL connections

    server[in]        a Server class instance
    is_master[in]     if True, the server is acting as a master
                      Default = True
    strict[in]        if True, raise exception on errors
                      Default = True
    options[in]       replication options including rpl_user, quiet, multiline

    Returns list - strings containing the CHANGE MASTER command
    """
    if options is None:
        options = {}

    rpl_mode = options.get("rpl_mode", "master")
    rpl_user = options.get("rpl_user", None)
    quiet = options.get("quiet", False)

    # Copy options and add connected server
    new_opts = options.copy()
    new_opts["conn_info"] = server

    uname = None
    master_values = {}
    change_master = []

    # If server is a master, perform error checking
    # pylint: disable=R0101
    if is_master:
        master = Master(new_opts)
        master.connect()

        # Check master for binlog
        if not master.binlog_enabled():
            raise UtilError("Master must have binary logging turned on.")
        else:
            # Check rpl user
            if rpl_user is None:
                # Try to find the replication user
                res = master.get_rpl_users()
                if len(res) > 1:
                    uname = ""
                    passwd = ""
                    # Throw error if strict but not for rpl_mode = both
                    if strict and rpl_mode != 'both':
                        raise UtilRplError(_NO_RPL_USER)
                    else:
                        change_master.append(_WARNING % _NO_RPL_USER)
                else:
                    uname = res[0][0]
                    if res[0][2]:
                        # Throw error if strict but not for rpl_mode = both
                        if strict and rpl_mode != 'both':
                            raise UtilRplError(_RPL_USER_PASS)
                        else:
                            change_master.append(_WARNING % _RPL_USER_PASS)
                    passwd = res[0][1]
            else:
                # Parse username and password (supports login-paths)
                try:
                    uname, passwd = parse_user_password(rpl_user,
                                                        options=options)
                except FormatError:
                    raise UtilError(USER_PASSWORD_FORMAT.format("--rpl-user"))
                if not passwd:
                    passwd = ''

                # Check replication user privileges
                errors = master.check_rpl_user(uname, master.host)
                if errors != []:
                    raise UtilError(errors[0])

            res = master.get_status()
            if not res:
                raise UtilError("Cannot retrieve master status.")

            # Need to get the master values for the make_change_master command
            master_values = {
                'Master_Host': master.host,
                'Master_Port': master.port,
                'Master_User': uname,
                'Master_Password': passwd,
                'Master_Log_File': res[0][0],
                'Read_Master_Log_Pos': res[0][1],
            }

            if master.has_ssl:
                master_values['Master_SSL_Allowed'] = 1
                if master.ssl_ca:
                    master_values['Master_SSL_CA_File'] = master.ssl_ca
                if master.ssl_cert:
                    master_values['Master_SSL_Cert'] = master.ssl_cert
                if master.ssl_key:
                    master_values['Master_SSL_Key'] = master.ssl_key

    # Use slave class to get change master command
    slave = Slave(new_opts)
    slave.connect()
    cm_cmd = slave.make_change_master(False, master_values)

    if rpl_user is None and uname == "" and not quiet:
        cm_cmd = cm_cmd.replace("MASTER_PORT", "# MASTER_USER = '', "
                                "# MASTER_PASSWORD = '', MASTER_PORT")

    if options.get("multiline", False):
        cm_cmd = cm_cmd.replace(", ", ", \n  ") + ";"
        change_master.extend(cm_cmd.split("\n"))
    else:
        change_master.append(cm_cmd + ";")

    return change_master


class Replication(object):
    """
    The Replication class can be used to establish a replication connection
    between a master and a slave with the following utilities:

        - Create the replication user
        - Setup replication
        - Test prerequisites for replication
        - Conduct validation checks:
            - binlog
            - server ids
            - storage engine compatibility
            - innodb version compatibility
            - master binlog
            - lower case table name compatibility
            - slave connection to master
            - slave delay

    Replication prerequisite tests shall be constructed so that they return
    None if the check passes (no errors) or a list of strings containing the
    errors or warnings. They shall accept a dictionary of options set to
    options={}. This will allow for reduced code needed to call multiple tests.
    """

    def __init__(self, master, slave, options):
        """Constructor

        master[in]         Master Server object
        slave[in]          Slave Server object
        options[in]        Options for class
          verbose          print extra data during operations (optional)
                           default value = False
          master_log_file  master log file
                           default value = None
          master_log_pos   position in log file
                           default = -1 (no position specified)
          from_beginning   if True, start from beginning of logged events
                           default = False
        """
        self.verbosity = options.get("verbosity", 0)
        self.master_log_file = options.get("master_log_file", None)
        self.master_log_pos = options.get("master_log_pos", 0)
        self.from_beginning = options.get("from_beginning", False)
        self.ssl_ca = options.get("ssl_ca", None)
        self.ssl_cert = options.get("ssl_cert", None)
        self.ssl_key = options.get("ssl_key", None)
        self.ssl_opt = options.get("ssl", None)
        self.ssl = False
        if self.ssl_ca or self.ssl_cert or self.ssl_key or self.ssl_opt:
            self.ssl = True
        self.master = master
        self.slave = slave
        self.replicating = False
        self.query_options = {
            'fetch': False
        }

    def check_server_ids(self):
        """Check server ids on master and slave

        This method will check the server ids on the master and slave. It will
        raise exceptions for error conditions.

        Returns [] if compatible, list of errors if not compatible
        """
        master_server_id = self.master.get_server_id()
        slave_server_id = self.slave.get_server_id()
        if master_server_id == 0:
            raise UtilRplError("Master server_id is set to 0.")

        if slave_server_id == 0:
            raise UtilRplError("Slave server_id is set to 0.")

        # Check for server_id uniqueness
        if master_server_id == slave_server_id:
            raise UtilRplError("The slave's server_id is the same as the "
                               "master.")

        return []

    def check_server_uuids(self):
        """Check UUIDs on master and slave

        This method will check the UUIDs on the master and slave. It will
        raise exceptions for error conditions.

        Returns [] if compatible or no UUIDs used, list of errors if not
        """
        master_uuid = self.master.get_uuid()
        slave_uuid = self.slave.get_uuid()

        # Check for both not supporting UUIDs.
        if master_uuid is None and slave_uuid is None:
            return []

        # Check for unbalanced servers - one with UUID, one without
        if master_uuid is None or slave_uuid is None:
            raise UtilRplError("%s does not support UUIDs." %
                               "Master" if master_uuid is None else "Slave")

        # Check for uuid uniqueness
        if master_uuid == slave_uuid:
            raise UtilRplError("The slave's UUID is the same as the "
                               "master.")

        return []

    def check_innodb_compatibility(self, options):
        """Check InnoDB compatibility

        This method checks the master and slave to ensure they have compatible
        installations of InnoDB. It will print the InnoDB settings on the
        master and slave if quiet is not set. If pedantic is set, method
        will raise an error.

        options[in]   dictionary of options (verbose, pedantic)

        Returns [] if compatible, list of errors if not compatible
        """

        pedantic = options.get("pedantic", False)
        verbose = options.get("verbosity", 0) > 0

        errors = []

        master_innodb_stats = self.master.get_innodb_stats()
        slave_innodb_stats = self.slave.get_innodb_stats()

        if master_innodb_stats != slave_innodb_stats:
            if not pedantic:
                errors.append("WARNING: Innodb settings differ between master "
                              "and slave.")
            if verbose or pedantic:
                cols = ['type', 'plugin_version', 'plugin_type_version',
                        'have_innodb']
                rows = []
                rows.append(master_innodb_stats)
                errors.append("# Master's InnoDB Stats:")
                errors.extend(_get_list(rows, cols))
                rows = []
                rows.append(slave_innodb_stats)
                errors.append("# Slave's InnoDB Stats:")
                errors.extend(_get_list(rows, cols))
            if pedantic:
                for line in errors:
                    print line
                raise UtilRplError("Innodb settings differ between master "
                                   "and slave.")

        return errors

    def check_storage_engines(self, options):
        """Check compatibility of storage engines on master and slave

        This method checks that the master and slave have compatible storage
        engines. It will print the InnoDB settings on the master and slave if
        quiet is not set. If pedantic is set, method will raise an error.

        options[in]   dictionary of options (verbose, pedantic)

        Returns [] if compatible, list of errors if not compatible
        """

        pedantic = options.get("pedantic", False)
        verbose = options.get("verbosity", 0) > 0

        errors = []
        slave_engines = self.slave.get_storage_engines()
        results = self.master.check_storage_engines(slave_engines)
        if results[0] is not None or results[1] is not None:
            if not pedantic:
                errors.append("WARNING: The master and slave have differing "
                              "storage engine configurations!")
            if verbose or pedantic:
                cols = ['engine', 'support']
                if results[0] is not None:
                    errors.append("# Storage engine configuration on Master:")
                    errors.extend(_get_list(results[0], cols))
                if results[1] is not None:
                    errors.append("# Storage engine configuration on Slave:")
                    errors.extend(_get_list(results[1], cols))
            if pedantic:
                for line in errors:
                    print line
                raise UtilRplError("The master and slave have differing "
                                   "storage engine configurations!")

        return errors

    def check_master_binlog(self):
        """Check prerequisites for master for replication

        Returns [] if master ok, list of errors if binary logging turned off.
        """
        errors = []
        if not self.master.binlog_enabled():
            errors.append("Master must have binary logging turned on.")
        return errors

    def check_lctn(self):
        """Check lower_case_table_name setting

        Returns [] - no exceptions, list if exceptions found
        """
        errors = []
        slave_lctn = self.slave.get_lctn()
        master_lctn = self.master.get_lctn()
        if slave_lctn != master_lctn:
            return (master_lctn, slave_lctn)
        if slave_lctn == 1:
            msg = "WARNING: identifiers can have inconsistent case " + \
                  "when lower_case_table_names = 1 on the slave and " + \
                  "the master has a different value."
            errors.append(msg)

        return errors

    def get_binlog_exceptions(self):
        """Get any binary logging exceptions

        This method queries the master and slave status for the *-do-db and
        *-ignore-db settings. It returns the values of either of these for
        the master and slave.

        Returns [] - no exceptions, list if exceptions found
        """
        binlog_ex = []
        rows = []
        rows.extend(self.master.get_binlog_exceptions())
        rows.extend(self.slave.get_binlog_exceptions())
        if len(rows) > 0:
            cols = ['server', 'do_db', 'ignore_db']
            binlog_ex = _get_list(rows, cols)

        return binlog_ex

    def check_slave_connection(self):
        """Check to see if slave is connected to master

        This method will check the slave specified at instantiation to see if
        it is connected to the master specified. If the slave is connected
        to a different master, an error is returned. It will also raise an
        exception if the slave is stopped or if the server is not setup as a
        slave.

        Returns bool - True = slave connected to master
        """
        state = self.slave.get_io_running()
        if not state:
            raise UtilRplError("Slave is stopped.")
        if not self.slave.is_configured_for_master(self.master) or \
           state.upper() != "YES":
            return False
        return True

    def check_slave_delay(self):
        """Check to see if slave is behind master.

        This method checks slave_behind_master returning None if 0 or a
        message containing the value if non-zero. Also includes the slave's
        position as related to the master.

        Returns [] - no exceptions, list if exceptions found
        """
        m_log_file = None
        m_log_pos = 0
        errors = []
        res = self.master.get_status()
        if res != []:
            m_log_file = res[0][0]       # master's binlog file
            m_log_pos = res[0][1]        # master's binlog position
        else:
            raise UtilRplError("Cannot read master status.")
        delay_info = self.slave.get_delay()
        if delay_info is None:
            raise UtilRplError("The server specified as the slave is "
                               "not configured as a replication slave.")

        state, sec_behind, delay_remaining, \
            read_log_file, read_log_pos = delay_info

        if not state:
            raise UtilRplError("Slave is stopped.")
        if delay_remaining is None:  # if unknown, return the error
            errors.append("Cannot determine slave delay. Status: UNKNOWN.")
            return errors

        if sec_behind == 0:
            if m_log_file is not None and \
               (read_log_file != m_log_file or read_log_pos != m_log_pos):
                errors.append("Slave is behind master.")
                errors.append("Master binary log file = %s" % m_log_file)
                errors.append("Master binary log position = %s" % m_log_pos)
                errors.append("Slave is reading master binary log "
                              "file = %s" % read_log_file)
                errors.append("Slave is reading master binary log "
                              "position = %s" % read_log_pos)
            else:
                return errors
        else:
            errors.append("Slave is % seconds behind master." %
                          sec_behind)

        return errors

    def create_rpl_user(self, r_user, r_pass=None):
        """Create the replication user and grant privileges

        If the user exists, check privileges and add privileges as needed.
        Calls Master class method to execute.

        r_user[in]     user to create
        r_pass[in]     password for user to create (optional)

        Returns bool - True = success, False = errors
        """
        ssl = False
        if self.ssl:
            ssl = True
        return self.master.create_rpl_user(self.slave.host, self.slave.port,
                                           r_user, r_pass, self.verbosity, ssl)

    def setup(self, rpl_user, num_tries):
        """Setup replication among a slave and master.

        Note: Must have connected to a master and slave before calling this
        method.

        rpl_user[in]       Replication user in form user:passwd
        num_tries[in]      Number of attempts to wait for slave synch

        Returns True if success, False if error
        """
        if self.master is None or self.slave is None:
            print "ERROR: Must connect to master and slave before " \
                  "calling replicate()"
            return False

        result = True

        # Parse user and password (support login-paths)
        try:
            r_user, r_pass = parse_user_password(rpl_user)
        except FormatError:
            raise UtilError(USER_PASSWORD_FORMAT.format("--rpl-user"))

        # Check to see if rpl_user is present, else create her
        if not self.create_rpl_user(r_user, r_pass)[0]:
            return False

        # Read master log file information
        res = self.master.get_status()
        if not res:
            print "ERROR: Cannot retrieve master status."
            return False

        # If master log file, pos not specified, read master log file info
        read_master_info = False
        if self.master_log_file is None:
            res = self.master.get_status()
            if not res:
                print "ERROR: Cannot retrieve master status."
                return False

            read_master_info = True
            self.master_log_file = res[0][0]
            self.master_log_pos = res[0][1]
        else:
            # Check to make sure file is accessible and valid
            found = False
            res = self.master.get_binary_logs(self.query_options)
            for row in res:
                if row[0] == self.master_log_file:
                    found = True
                    break
            if not found:
                raise UtilError("Master binary log file not listed as a "
                                "valid binary log file on the master.")

        if self.master_log_file is None:
            raise UtilError("No master log file specified.")

        # Stop slave first
        res = self.slave.get_thread_status()
        if res is not None:
            if res[1] == "Yes" or res[2] == "Yes":
                res = self.slave.stop(self.query_options)

        # Connect slave to master
        if self.verbosity > 0:
            print "# Connecting slave to master..."
        master_values = {
            'Master_Host': self.master.host,
            'Master_Port': self.master.port,
            'Master_User': r_user,
            'Master_Password': r_pass,
            'Master_Log_File': self.master_log_file,
            'Read_Master_Log_Pos': self.master_log_pos,
        }

        # Use the options SSL certificates if defined,
        # else use the master SSL certificates if defined.
        if self.ssl:
            master_values['Master_SSL_Allowed'] = 1
            if self.ssl_ca:
                master_values['Master_SSL_CA_File'] = self.ssl_ca
            if self.ssl_cert:
                master_values['Master_SSL_Cert'] = self.ssl_cert
            if self.ssl_key:
                master_values['Master_SSL_Key'] = self.ssl_key

        elif self.master.has_ssl:
            master_values['Master_SSL_Allowed'] = 1
            master_values['Master_SSL_CA_File'] = self.master.ssl_ca
            master_values['Master_SSL_Cert'] = self.master.ssl_cert
            master_values['Master_SSL_Key'] = self.master.ssl_key

        change_master = self.slave.make_change_master(self.from_beginning,
                                                      master_values)
        res = self.slave.exec_query(change_master, self.query_options)
        if self.verbosity > 0:
            print "# %s" % change_master

        # Start slave
        if self.verbosity > 0:
            if not self.from_beginning:
                if read_master_info:
                    print "# Starting slave from master's last position..."
                else:
                    msg = "# Starting slave from master log file '%s'" % \
                          self.master_log_file
                    if self.master_log_pos >= 0:
                        msg += " using position %s" % self.master_log_pos
                    msg += "..."
                    print msg
            else:
                print "# Starting slave from the beginning..."
        res = self.slave.start(self.query_options)

        # Add commit because C/Py are auto_commit=0 by default
        self.slave.exec_query("COMMIT")

        # Check slave status
        i = 0
        while i < num_tries:
            time.sleep(1)
            res = self.slave.get_slaves_errors()
            status = res[0]
            sql_running = res[4]
            if self.verbosity > 0:
                io_errorno = res[1]
                io_error = res[2]
                io_running = res[3]
                sql_errorno = res[5]
                sql_error = res[6]
                print "# IO status: %s" % status
                print "# IO thread running: %s" % io_running
                # if io_errorno = 0 and error = '' -> no error
                if not io_errorno and not io_error:
                    print "# IO error: None"
                else:
                    print "# IO error: %s:%s" % (io_errorno, io_error)
                # if io_errorno = 0 and error = '' -> no error
                print "# SQL thread running: %s" % sql_running
                if not sql_errorno and not sql_error:
                    print "# SQL error: None"
                else:
                    print "# SQL error: %s:%s" % (io_errorno, io_error)
            if status == "Waiting for master to send event" and sql_running:
                break
            elif not sql_running:
                if self.verbosity > 0:
                    print "# Retry to start the slave SQL thread..."
                # SQL thread is not running, retry to start it
                res = self.slave.start_sql_thread(self.query_options)
            if self.verbosity > 0:
                print "# Waiting for slave to synchronize with master"
            i += 1
        if i == num_tries:
            print "ERROR: failed to sync slave with master."
            result = False

        if result is True:
            self.replicating = True

        return result

    def test(self, db, num_tries):
        """Test the replication setup.

        Requires a database name which is created on the master then
        verified it appears on the slave.

        db[in]             Name of a database to use in test
        num_tries[in]      Number of attempts to wait for slave synch
        """

        if not self.replicating:
            print "ERROR: Replication is not running among master and slave."
        print "# Testing replication setup..."
        if self.verbosity > 0:
            print "# Creating a test database on master named %s..." % db
        res = self.master.exec_query("CREATE DATABASE %s" % db,
                                     self.query_options)
        i = 0
        while i < num_tries:
            time.sleep(1)
            res = self.slave.exec_query("SHOW DATABASES")
            for row in res:
                if row[0] == db:
                    res = self.master.exec_query("DROP DATABASE %s" % db,
                                                 self.query_options)
                    print "# Success! Replication is running."
                    i = num_tries
                    break
            i += 1
            if i < num_tries and self.verbosity > 0:
                print "# Waiting for slave to synchronize with master"
        if i == num_tries:
            print "ERROR: Unable to complete testing."


class Master(Server):
    """The Slave class is a subclass of the Server class. It represents a
    MySQL server performing the role of a slave in a replication topology.
    The following utilities are provide in addition to the Server utilities:

        - check to see if replication user is defined and has privileges
        - get binary log exceptions
        - get master status
        - reset master

    """

    def __init__(self, options=None):
        """Constructor

        The method accepts one of the following types for options['conn_info']:

            - dictionary containing connection information including:
              (user, passwd, host, port, socket)
            - connection string in the form: user:pass@host:port:socket
            - an instance of the Server class

        options[in]        options for controlling behavior:
            conn_info      a dictionary containing connection information
                           (user, passwd, host, port, socket)
            role           Name or role of server (e.g., server, master)
            verbose        print extra data during operations (optional)
                           default value = False
            charset        Default character set for the connection.
                           (default latin1)
        """
        if options is None:
            options = {}

        assert options.get("conn_info") is not None

        self.options = options
        Server.__init__(self, options)

    def get_status(self):
        """Return the master status

        Returns result set
        """
        return self.exec_query("SHOW MASTER STATUS")

    def get_binlog_exceptions(self):
        """Get any binary logging exceptions

        This method queries the server status for the *-do-db and
        *-ignore-db settings.

        Returns [] - no exceptions, list if exceptions found
        """
        rows = []
        res = self.get_status()
        if res != []:
            do_db = res[0][_MASTER_DO_DB]
            ignore_db = res[0][_MASTER_IGNORE_DB]
            if len(do_db) > 0 or len(ignore_db) > 0:
                rows.append(('master', do_db, ignore_db))

        return rows

    def get_binlog_info(self):
        """Return the master's binary log information (file name and position).

        Returns a tuple with the binary log filename and position, or None if
        the server is not acting as a master.
        """
        res = self.get_status()
        if res:
            # Return binlog_file and binlog_pos.
            return res[0][0], res[0][1]
        else:
            # Status data is empty, server is not acting as a master.
            return None

    def get_rpl_users(self, options=None):
        """Attempts to find the users who have the REPLICATION SLAVE privilege

        options[in]    query options

        Returns tuple list - (string, string, bool) = (user, host,
                                                       has_password)
        """
        if options is None:
            options = {}
        # Use the correct query for server (changed for 5.7.6).
        if self.check_version_compat(5, 7, 6):
            query = _RPL_USER_QUERY_5_7_6
        else:
            query = _RPL_USER_QUERY
        return self.exec_query(query, options)

    def create_rpl_user(self, host, port, r_user, r_pass=None, verbosity=0,
                        ssl=False):
        """Create the replication user and grant privileges

        If the user exists, check privileges and add privileges as needed.

        host[in]       host of the slave
        port[in]       port of the slave
        r_user[in]     user to create
        r_pass[in]     password for user to create (optional)
        verbosity[in]  verbosity of output
                       Default = 0
        ssl[in]        If True the grant will include 'REQUIRE SSL'
                       (Default False).

        Returns tuple (bool, str) - (True, None) = success,
                                    (False, <error>) = error
        """

        grants_enabled = self.grant_tables_enabled()
        if not grants_enabled:
            return (True, None)

        if "]" in host:
            host = clean_IPv6(host)

        # Create user class instance
        user = User(self, "{0}:{1}@{2}:{3}".format(r_user, r_pass, host, port))
        if not user.exists():
            user.create()
            # Save current user for privilege checking
            user.current_user = "'{0}'@'{1}'".format(r_user, host)

        # Check privileges, but do not user the anonymous host
        if not user.has_privilege("*", "*", "REPLICATION SLAVE",
                                  globals_privs=False):
            if verbosity > 0:
                print "# Granting replication access to replication user..."
            query_str = ("GRANT REPLICATION SLAVE ON *.* TO "
                         "'{0}'@'{1}' ".format(r_user, host))
            if r_pass:
                query_str += "IDENTIFIED BY '{0}'".format(r_pass)

            if ssl:
                query_str = "{0} {1}".format(query_str, " REQUIRE SSL")
            try:
                self.exec_query(query_str)
            except UtilError:
                return (False, "ERROR: Cannot grant replication slave to "
                        "replication user.")

        return (True, None)

    def reset(self, options=None):
        """Reset the master

        options[in]    query options
        """
        if options is None:
            options = {}
        return self.exec_query("RESET MASTER", options)

    def check_rpl_health(self):
        """Check replication health of the master.

        This method checks to see if the master is setup correctly to
        operate in a replication environment. It returns a tuple with a
        bool to indicate if health is Ok (True), and a list to contain any
        errors encountered during the checks.

        Returns tuple (bool, []) - (True, []) = Ok,
                                   (False, error_list) = not setup correctly
        """
        errors = []
        rpl_ok = True

        if not self.is_alive():
            return (False, ["Cannot connect to server"])

        gtid_enabled = self.supports_gtid() == "ON"

        # Check for binlogging
        if not gtid_enabled and not self.binlog_enabled():
            errors.append("No binlog on master.")
            rpl_ok = False

        # See if there is at least one user with rpl privileges
        res = self.get_rpl_users()
        if len(res) == 0:
            errors.append("There are no users with replication privileges.")
            rpl_ok = False

        return (rpl_ok, errors)

    def _check_discovered_slave(self, conn_dict):
        """ Check discovered slave is configured to this master

        This method attempts to determine if the slave specified is
        configured to connect to this master.

        conn_dict[in]  dictionary of connection information

        Returns True if configured with this master otherwise an error is
        raised.
        """
        slave_conn = Slave(conn_dict)
        try:
            slave_conn.connect()
            # Skip discovered slaves that are not configured
            # to connect to the master
            return slave_conn.is_configured_for_master(self,
                                                       verify_state=False,
                                                       raise_error=True)
        finally:
            slave_conn.disconnect()

    def get_slaves(self, user, password):
        """Return the slaves registered for this master.

        This method returns a list of slaves (host, port) if this server is
        a master in a replication topology and has slaves registered.

        user[in]       user login
        password[in]   user password

        Returns list - [host:port, ...]
        """
        def _get_slave_info(host, port):
            """Return the slave info
            """
            if len(host) > 0:
                if ":" in host:
                    host = format_IPv6(host)
                slave_info = host
            else:
                slave_info = "unknown host"
            slave_info += ":%s" % port
            return slave_info

        slaves = []
        no_host_slaves = []
        connect_error_slaves = []
        res = self.exec_query("SHOW SLAVE HOSTS")
        verbose = self.options.get("verbose", False)
        if res != []:
            # Sort for conformity
            res.sort()  # pylint: disable=E1103

            for row in res:
                info = _get_slave_info(row[1], row[2])
                conn_dict = {
                    'conn_info': {'user': user, 'passwd': password,
                                  'host': row[1], 'port': row[2],
                                  'socket': None, 'ssl_ca': self.ssl_ca,
                                  'ssl_cert': self.ssl_cert,
                                  'ssl_key': self.ssl_key,
                                  'ssl': self.ssl},
                    'role': 'slave',
                    'verbose': verbose,
                }
                if not row[1]:
                    no_host_slaves.append(" - {0}".format(info))
                    break
                # Verify slave connection and configuration.
                try:
                    self._check_discovered_slave(conn_dict)
                    # Slave correctly configured.
                    slaves.append(info)
                except UtilError as err:
                    # Connection or configuration errors found.
                    connect_error_slaves.append(
                        " - {0}: {1}".format(info, err.errmsg)
                    )

        # Warn if slaves were found with configuration/connection issues.
        hint = ":" if verbose else " (--verbose for more details)."
        if no_host_slaves:
            print("WARNING: There are slaves that have not been registered"
                  " with --report-host or --report-port{0}".format(hint))
            if verbose:
                for row in no_host_slaves:
                    print(row)
        if connect_error_slaves:
            print("\nWARNING: Cannot connect to some slaves{0}".format(hint))
            if verbose:
                for row in connect_error_slaves:
                    print(row)

        return slaves

    def get_gtid_purged_statement(self):
        """General the SET @@GTID_PURGED statement for backup

        Returns string - statement for slave if GTID=ON, else None
        """
        if self.supports_gtid == "ON":
            gtid_executed = self.exec_query("SELECT @@GLOBAL.GTID_EXECUTED")[0]
            return "SET @@GLOBAL.GTID_PURGED = '{0}'".format(gtid_executed)
        else:
            return None


class MasterInfo(object):
    """The MasterInfo is an abstraction of the mechanism for storing the
    master information for slave servers. It is designed to return an
    implementation neutral representation of the information for use in
    newer servers that use a table to store the information as well as
    older servers that use a file to store the information.
    """

    def __init__(self, slave, options):
        """Constructor

        The method accepts one of the following types for options['conn_info']:

            - dictionary containing connection information including:
              (user, passwd, host, port, socket)
            - connection string in the form: user:pass@host:port:socket
            - an instance of the Server class

        options[in]        options for controlling behavior:
          filename         filename for master info file - valid only for
                           servers with master-info-repository=FILE or
                           versions prior to 5.6.5.
          verbosity        determines level of output. Default = 0.
          quiet            turns off all messages except errors.
                           Default is False.
        """

        assert slave is not None, "MasterInfo requires an instance of Slave."
        self.slave = slave
        self.filename = options.get("master_info", "master.info")
        self.quiet = options.get("quiet", False)
        self.verbosity = options.get("verbosity", 0)
        self.values = {}      # internal dictionary of the values
        self.repo = "FILE"
        if self.slave is not None:
            res = self.slave.show_server_variable("master_info_repository")
            if res is not None and res != [] and \
               res[0][1].upper() == "TABLE":
                self.repo = "TABLE"

    def read(self):
        """Read the master information

        This method reads the master information either from a file or a
        table depending on the availability of and setting for
        master-info-repository. If missing (server version < 5.6.5), it
        defaults to reading from a file.

        Returns bool - True = success
        """
        if self.verbosity > 2:
            print "# Reading master information from a %s." % self.repo.lower()
        if self.repo == "FILE":
            # Check host name of this host. If not the same, issue error.
            if self.slave.is_alias(socket.gethostname()):
                return self._read_master_info_file()
            else:
                raise UtilRplWarn("Cannot read master information file "
                                  "from a remote machine.")
        else:
            return self._read_master_info_table()

    def _check_read(self, refresh=False):
        """Check if master information has been read

        refresh[in]    if True, re-read the master information.
                       Default is False.

        If the master information has not been read, read it and populate
        the dictionary.
        """
        # Read the values if not already read or user says to refresh them.
        if self.values is None or self.values == {} or refresh:
            self.read()

    def _build_dictionary(self, rows):
        """Build the internal dictionary of values.

        rows[in]       Rows as read from the file or table
        """
        for i in range(0, len(rows)):
            self.values[_MASTER_INFO_COL[i]] = rows[i]

    def _read_master_info_file(self):
        """Read the contents of the master.info file.

        This method will raise an error if the file is missing or cannot be
        read by the user.

        Returns bool - success = True
        """
        contents = []
        res = self.slave.show_server_variable('datadir')
        if res is None or res == []:
            raise UtilRplError("Cannot get datadir.")
        datadir = res[0][1]
        if self.filename == 'master.info':
            self.filename = os.path.join(datadir, self.filename)

        if not os.path.exists(self.filename):
            raise UtilRplError("Cannot find master information file: "
                               "%s." % self.filename)
        try:
            mfile = open(self.filename, 'r')
            num = int(mfile.readline())
            # Protect overrun of array if master_info file length is
            # changed (more values added).
            if num > len(_MASTER_INFO_COL):
                num = len(_MASTER_INFO_COL)
        except:
            raise UtilRplError("Cannot read master information file: "
                               "%s.\nUser needs to have read access to "
                               "the file." % self.filename)
        # Build the dictionary
        i = 1
        while i < num:
            contents.append(mfile.readline().strip('\n'))
            i += 1
        self._build_dictionary(contents)
        mfile.close()

        return True

    def _read_master_info_table(self):
        """Read the contents of the slave_master_info table.

        This method will raise an error if the file is missing or cannot be
        read by the user.

        Returns bool - success = True
        """
        res = None
        try:
            res = self.slave.exec_query("SELECT * FROM "
                                        "mysql.slave_master_info")
        except UtilError, e:
            raise UtilRplError("Unable to read the slave_master_info table. "
                               "Error: %s" % e.errmsg)
        if res is None or res == []:
            return False

        # Protect overrun of array if the master_info table size has changed
        # (more rows than expected).
        num = len(res[0][1:])
        if num > len(_MASTER_INFO_COL):
            num = len(_MASTER_INFO_COL)
        # Build dictionary for the information with column information
        rows = []
        for i in range(0, num):
            rows.append(res[0][i + 1])
        self._build_dictionary(rows)

        return True

    def show_master_info(self, refresh=False):
        """Display the contents of the master information.

        refresh[in]    if True, re-read the master information.
                       Default is False.
        """
        # Check to see if we need to read the information
        self._check_read(refresh)
        stop = len(self.values)
        for i in range(0, stop):
            print "{0:>30} : {1}".format(_MASTER_INFO_COL[i],
                                         self.values[_MASTER_INFO_COL[i]])

    def check_master_info(self, refresh=False):
        """Check to see if master info file matches slave status

        This method will return a list of discrepancies if the master.info
        file does not match slave status. It will also raise errors if there
        are problem accessing the master.info file.

        refresh[in]    if True, re-read the master information.
                       Default is False.

        Returns [] - no exceptions, list if exceptions found
        """
        # Check to see if we need to read the information
        self._check_read(refresh)
        errors = []
        res = self.slave.get_status()
        if res != []:
            state = res[0][_SLAVE_IO_STATE]
            if not state:
                raise UtilRplError("Slave is stopped.")
            m_host = res[0][_SLAVE_MASTER_HOST]
            m_port = res[0][_SLAVE_MASTER_PORT]
            rpl_user = res[0][_SLAVE_MASTER_USER]
            if m_host != self.values['Master_Host'] or \
               int(m_port) != int(self.values['Master_Port']) or \
               rpl_user != self.values['Master_User']:
                errors.append("Slave is connected to master differently "
                              "than what is recorded in the master "
                              "information file. Master information file "
                              "= user=%s, host=%s, port=%s." %
                              (self.values['Master_User'],
                               self.values['Master_Host'],
                               self.values['Master_Port']))

        return errors

    def get_value(self, key, refresh=False):
        """Returns the value found for the key or None if key not found.

        refresh[in]    if True, re-read the master information.
                       Default is False.

        Returns value - Value found for the key or None if key missing
        """
        # Check to see if we need to read the information
        self._check_read(refresh)
        try:
            return self.values[key]
        except:
            return None

    def get_master_info(self, refresh=False):
        """Returns the master information dictionary.

        refresh[in]    if True, re-read the master information.
                       Default is False.

        Returns dict - master information
        """
        # Check to see if we need to read the information
        self._check_read(refresh)
        return self.values


class Slave(Server):
    """The Slave class is a subclass of the Server class. It represents a
    MySQL server performing the role of a slave in a replication topology.
    The following utilities are provide in addition to the Server utilities:

        - get methods to return status, binary log exceptions, slave delay,
          thread status, io error, and master information
        - form the change master command with either known master or user-
          supplied values
        - check to see if slave is connected to a master
        - display slave status
        - show master information
        - verify master information matches currently connected master
        - start, stop, and reset slave

    """

    def __init__(self, options=None):
        """Constructor

        The method accepts one of the following types for options['conn_info']:

            - dictionary containing connection information including:
              (user, passwd, host, port, socket)
            - connection string in the form: user:pass@host:port:socket
            - an instance of the Server class

        options[in]        options for controlling behavior:
            conn_info      a dictionary containing connection information
                           (user, passwd, host, port, socket)
            role           Name or role of server (e.g., server, master)
            verbose        print extra data during operations (optional)
                           default value = False
            charset        Default character set for the connection.
                           (default latin1)
        """
        if options is None:
            options = {}

        assert options.get("conn_info") is not None

        self.options = options
        Server.__init__(self, options)
        self.master_info = None

    def get_status(self, col_options=None):
        """Return the slave status

        col_options[in]    options for displaying columns (optional)

        Returns result set
        """
        if not col_options:
            col_options = {}
        return self.exec_query("SHOW SLAVE STATUS", col_options)

    def get_retrieved_gtid_set(self):
        """Get any events (gtids) read but not executed

        Returns a string with the list of gtids in Executed_Gtid_Set.

        Note: an empty string is returned if the server is not acting as a
              slave.
        """
        res = self.get_status()
        if res != []:
            return res[0][_RETRIEVED_GTID_SET]
        return ''

    def get_executed_gtid_set(self):
        """Get any events (gtids) executed

        Returns a string with the list of gtids in Executed_Gtid_Set.

        Note: an empty string is returned if the server is not acting as a
              slave.
        """
        res = self.get_status()
        if res:
            return res[0][_EXECUTED_GTID_SET]

        return ''

    def get_binlog_exceptions(self):
        """Get any binary logging exceptions

        This method queries the server status for the *-do-db and
        *-ignore-db settings.

        Returns [] - no exceptions, list if exceptions found
        """
        rows = []
        res = self.get_status()
        if res != []:
            do_db = res[0][_SLAVE_DO_DB]
            ignore_db = res[0][_SLAVE_IGNORE_DB]
            if len(do_db) > 0 or len(ignore_db) > 0:
                rows.append(('slave', do_db, ignore_db))

        return rows

    def get_master_host_port(self):
        """Get the slave's connected master host and port

        Returns tuple - (master host, master port) or
                        None if not acting as slave
        """
        res = self.get_status()
        if res == []:
            return None
        m_host = res[0][_SLAVE_MASTER_HOST]
        m_port = res[0][_SLAVE_MASTER_PORT]

        return (m_host, m_port)

    def is_connected(self):
        """Check to see if slave is connected to master

        This method will check the slave to see if it is connected to a master
        by checking if his I/O Thread is running.

        Returns bool - True = slave is connected
        """
        res = self.get_status()
        if res == []:
            return False
        return res[0][_SLAVE_IO_RUNNING].upper() == "YES"

    def get_rpl_master_user(self):
        """Get the rpl master user from the slave status

        Returns the slave_master_user as string or False if there is
        no slave status.
        """
        res = self.get_status()
        if not res:
            return False
        return res[0][_SLAVE_MASTER_USER]

    def get_master_uuid(self):
        """Get the master_uuid from the slave status.

        Return the master UUID or None if not an acting slave.
        """
        res = self.get_status()
        if not res:
            return None
        return res[0][_MASTER_UUID]

    def get_state(self):
        """Get the slave's connection state

        Returns state or None if not acting as slave
        """
        res = self.get_status()
        if res == []:
            return None
        state = res[0][_SLAVE_IO_STATE]

        return state

    def get_io_running(self):
        """Get the slave's IO thread status

        Returns IO_THREAD state or None if not acting as slave
        """
        res = self.get_status()
        if res == []:
            return None
        return res[0][_SLAVE_IO_RUNNING]

    def get_sql_running(self):
        """Get the slave's SQL thread status

        Returns SQL_THREAD state or None if not acting as slave
        """
        res = self.get_status()
        if res == []:
            return None
        return res[0][_SLAVE_SQL_RUNNING]

    def get_delay(self):
        """Return slave delay values

        This method retrieves the slave's delay parameters.

        Returns tuple - slave delay values or None if not connected
        """
        res = self.get_status()
        if res == []:
            return None

        # slave IO state
        state = res[0][_SLAVE_IO_STATE]
        # seconds behind master
        if res[0][_SLAVE_DELAY] is None:
            sec_behind = 0
        else:
            sec_behind = int(res[0][_SLAVE_DELAY])
        # remaining delay
        delay_remaining = res[0][_SLAVE_REMAINING_DELAY]
        # master's log file read
        read_log_file = res[0][_SLAVE_MASTER_LOG_FILE]
        # position in master's binlog
        read_log_pos = res[0][_SLAVE_MASTER_LOG_FILE_POS]

        return (state, sec_behind, delay_remaining,
                read_log_file, read_log_pos)

    def get_thread_status(self):
        """Return the slave threads status

        Returns tuple - (slave_io_state, slave_io_running, slave_sql_running)
                        or None if not connected
        """
        res = self.get_status()
        if res == []:
            return None

        # slave IO state
        state = res[0][_SLAVE_IO_STATE]
        # slave_io_running
        io_running = res[0][_SLAVE_IO_RUNNING]
        # slave_sql_running
        sql_running = res[0][_SLAVE_SQL_RUNNING]

        return (state, io_running, sql_running)

    def get_io_error(self):
        """Return the slave slave io error status

        Returns tuple - (slave_io_state, io_errorno, io_error)
                        or None if not connected
        """
        res = self.get_status()
        if res == []:
            return None

        state = res[0][_SLAVE_IO_STATE]
        io_errorno = int(res[0][_SLAVE_IO_ERRORNO])
        io_error = res[0][_SLAVE_IO_ERROR]

        return (state, io_errorno, io_error)

    def get_sql_error(self):
        """Return the slave slave sql error status

        Returns tuple - (sql_running, sql_errorno, sql_error)
                        or None if not connected
        """
        res = self.get_status()
        if not res:
            return None

        sql_running = res[0][_SLAVE_SQL_RUNNING]
        sql_errorno = int(res[0][_SLAVE_SQL_ERRORNO])
        sql_error = res[0][_SLAVE_SQL_ERROR]

        return (sql_running, sql_errorno, sql_error)

    def get_slaves_errors(self):
        """Return the slave slave io and sql error status

        Returns tuple - (slave_io_state, io_errorno, io_error, io_running,
                         sql_running, sql_errorno, sql_error)
                        or None if not connected
        """
        res = self.get_status()
        if not res:
            return None

        state = res[0][_SLAVE_IO_STATE]
        io_errorno = int(res[0][_SLAVE_IO_ERRORNO])
        io_error = res[0][_SLAVE_IO_ERROR]
        io_running = res[0][_SLAVE_IO_RUNNING]
        sql_running = res[0][_SLAVE_SQL_RUNNING]
        sql_errorno = int(res[0][_SLAVE_SQL_ERRORNO])
        sql_error = res[0][_SLAVE_SQL_ERROR]

        return (state, io_errorno, io_error, io_running, sql_running,
                sql_errorno, sql_error)

    def get_slave_rpl_filters(self):
        """Get the replication filter options for the slave.

        Get the replication filter information from the slave status.

        Returns a tuple with the replication filter options (Replicate_Do_DB,
        Replicate_Ignore_DB, Replicate_Do_Table, Replicate_Ignore_Table,
        Replicate_Wild_Do_Table, Replicate_Wild_Ignore_Table). An empty tuple
        () is returned if no filter is defined and None if the slave status is
        not available.
        """
        res = self.get_status()
        if not res:
            return None

        rpl_do_db = res[0][_SLAVE_DO_DB]
        rpl_ignore_db = res[0][_SLAVE_IGNORE_DB]
        rpl_do_table = res[0][_SLAVE_DO_TABLE]
        rpl_ignore_table = res[0][_SLAVE_IGNORE_TABLE]
        rpl_wild_do_table = res[0][_SLAVE_WILD_DO_TABLE]
        rpl_wild_ignore_table = res[0][_SLAVE_WILD_IGNORE_TABLE]

        if (rpl_do_db or rpl_ignore_db or rpl_do_table or rpl_ignore_table or
                rpl_wild_do_table or rpl_wild_ignore_table):
            return (rpl_do_db, rpl_ignore_db, rpl_do_table, rpl_ignore_table,
                    rpl_wild_do_table, rpl_wild_ignore_table)
        else:
            return ()

    def show_status(self):
        """Display the slave status from the slave server
        """
        col_options = {
            'columns': True
        }
        res = self.get_status(col_options)
        if res != [] and res[1] != []:
            stop = len(res[0])
            cols = res[0]
            rows = res[1]
            for i in range(0, stop):
                print "{0:>30} : {1}".format(cols[i], rows[0][i])
        else:
            raise UtilRplError("Cannot get slave status or slave is "
                               "not configured as a slave or not "
                               "started.")

    def get_rpl_user(self):
        """Return the master user from the master info record.

        Returns - tuple = (user, password) or (None, None) if errors
        """
        self.master_info = MasterInfo(self, self.options)
        m_host = self.master_info.get_value("Master_User")
        m_passwd = self.master_info.get_value("Master_Password")
        if m_host is not None:
            return (m_host, m_passwd)
        return (None, None)

    def start(self, options=None, autocommit_fix=True, until_gtid_set=None,
              sql_after_gtid=True, only_sql_thread=False):
        """Start the slave.

        Execute the START SLAVE statement (to start the IO and/or SQL threads),
        according to the used parameters.

        options[in]         query options
        autocommit_fix[in]  If True, turn off AUTOCOMMIT before start command.
                            True by default to always apply the fix.
        until_gtid_set[in]  GTID set to use to execute START SLAVE UNTIL. By
                            default None, until option is not applied.
        sql_after_gtid[in]  Indicates if the until option SQL_AFTER_GTIDS is
                            used or in alternative SQL_BEFORE_GTIDS. Only
                            applied if until_gtid_set is specified. By default
                            True, SQL_AFTER_GTIDS is used.
        only_sql_thread[in] If True only the SQL thread is started, otherwise
                            both (by default).
        """
        if options is None:
            options = {}

        # Temporary workaround for BUG#16533802 - remove when fixed (part 1/2).
        if autocommit_fix:
            autocommit_value = self.autocommit_set()
            # If disabled, turn it on.
            if not autocommit_value:
                self.toggle_autocommit(True)

        query = "START SLAVE"
        if only_sql_thread:
            query = "{0} SQL_THREAD".format(query)
        if until_gtid_set:
            # Use until option.
            until_type = (
                'SQL_AFTER_GTIDS' if sql_after_gtid else 'SQL_BEFORE_GTIDS'
            )
            query = "{0} UNTIL {1} = '{2}'".format(query, until_type,
                                                   until_gtid_set)
        res = self.exec_query(query, options)

        # Temporary workaround for BUG#16533802 - remove when fixed (part 2/2).
        if autocommit_fix:
            # If disabled originally, turn it off.
            if not autocommit_value:
                self.toggle_autocommit(False)

        return res

    def start_sql_thread(self, options=None):
        """Start the slave SQL thread

        options[in]    query options
        """
        if options is None:
            options = {}
        return self.exec_query("START SLAVE SQL_THREAD", options)

    def stop(self, options=None):
        """Stop the slave

        options[in]    query options
        """
        if options is None:
            options = {}
        return self.exec_query("STOP SLAVE", options)

    def stop_sql_thread(self, options=None):
        """Stop the slave SQL thread.

        options[in]    query options
        """
        if options is None:
            options = {}
        return self.exec_query("STOP SLAVE SQL_THREAD", options)

    def reset(self, options=None):
        """Reset the slave

        options[in]    query options
        """
        if options is None:
            options = {}
        return self.exec_query("RESET SLAVE", options)

    def reset_all(self, options=None):
        """Reset all information on this slave.

        options[in]    query options
        """
        if options is None:
            options = {}
        # Must be sure to do stop first
        self.stop()
        # RESET SLAVE ALL was implemented in version 5.5.16 and later
        if not self.check_version_compat(5, 5, 16):
            return self.reset()
        return self.exec_query("RESET SLAVE ALL", options)

    def wait_checksum_and_start(self, tbl_name, wait_timeout=30,
                                wait_interval=3, checksum_timeout=0,
                                options=None):
        """Checksum specified table and start slave.

        tbl_name[in]        Name of the table to perform the checksum.
        wait_timeout[in]    Timeout value to wait for the slave to stop SQL
                            thread (automatically stopped after catching up
                            with master). By default 30 seconds.
        wait_interval[in]   Wait interval to perform the next polling (check
                            if SQL thread is stopped) By default 3 seconds..
        options[in]     Query options.

        Returns the result of the table checksum,more precisely a tuple with
        the checksum and an error description. If the checksum is computed it
        returns (checksum, None), otherwise (None, <skip error description>)
        where <skip error description> is a brief description of the motive why
        the checksum was not computed.
        """
        # Wait for slave to stop (if timeout > 0).
        tick = 0
        checksum = None
        skip_checksum = True if wait_timeout > 0 else False
        while tick < wait_timeout:
            status = self.get_slaves_errors()
            io_running = status[3].upper() == 'YES'
            sql_running = status[4].upper() == 'YES'
            # Only check if SQl thread is running since START SLAVE UNTIL does
            # not stop the IO thread.
            if sql_running:
                time.sleep(wait_interval)
                tick += wait_interval
            else:
                skip_checksum = False
                # Report if replication was stopped due to an error.
                if not io_running and status[2]:
                    print("# IO thread ERROR found for {0}:{1}: {2} - "
                          "{3}".format(self.host, self.port, status[1],
                                       status[2]))
                if not sql_running and status[6]:
                    print("# SQL thread ERROR found for {0}:{1}: {2} - "
                          "{3}".format(self.host, self.port, status[5],
                                       status[6]))
                break

        if skip_checksum:
            # Checksum skipped.
            skip_error = "timeout catching up with master"
            self.stop_sql_thread(options)
        else:
            # Compute checksum.
            checksum, skip_error = self.checksum_table(
                tbl_name, exec_timeout=checksum_timeout
            )

        # Resume replication, start slave.
        self.start_sql_thread(options)

        return checksum, skip_error

    def num_gtid_behind(self, master_gtids):
        """Get the number of transactions the slave is behind the master.

        master_gtids[in]  the master's GTID_EXECUTED list

        Returns int - number of trans behind master
        """
        slave_gtids = self.exec_query(_GTID_EXECUTED)[0][0]
        gtids = self.exec_query("SELECT GTID_SUBTRACT('%s','%s')" %
                                (master_gtids[0][0], slave_gtids))[0]
        # Init gtid_behind count (if no GTIDs behind then 0 is returned)
        gtid_behind = 0
        # Check if there are GTIDs behind
        # (i.e. string with GTIDs set is not equal to '')
        if gtids[0]:
            gtids_list = gtids[0].split("\n")
            # Extract the interval for each GTID and compute its length
            for gtid_item in gtids_list:
                interval_list = gtid_item.rstrip(', ')
                interval_list = interval_list.split(':')[1:]
                for interval_str in interval_list:
                    interval = interval_str.split('-')
                    if len(interval) == 1:
                        # Interval has only one element
                        gtid_behind += 1
                    else:
                        # Compute interval size and sum to total GTIDs behind.
                        num_gtids = int(interval[1]) - int(interval[0]) + 1
                        gtid_behind += num_gtids
        return gtid_behind

    def wait_for_slave(self, binlog_file, binlog_pos, timeout=300):
        """Wait for the slave to read the master's binlog to specified position

        binlog_file[in]  master's binlog file
        binlog_pos[in]   master's binlog file position
        timeout[in]      maximum number of seconds to wait for event to occur

        Returns bool - True = slave has read to the file and pos,
                       False = slave is behind.
        """
        # Wait for slave to read the master log file
        _MASTER_POS_WAIT = "SELECT MASTER_POS_WAIT('%s', %s, %s)"
        res = self.exec_query(_MASTER_POS_WAIT % (binlog_file,
                                                  binlog_pos, timeout))
        if res is None or (res[0][0] is not None and int(res[0][0]) < 0):
            return False
        return True

    def wait_for_slave_gtid(self, master_gtid, timeout=300, verbose=False):
        """Wait for the slave to read the master's GTIDs.

        This method requires that the server supports GTIDs.

        master_gtid[in]  the list of gtids from the master
                         obtained via SELECT @@GLOBAL.GTID_EXECUTED on master
        timeout[in]      timeout for waiting for slave to catch up
                         Note: per GTID call. Default is 300 seconds (5 min.).
        verbose[in]      if True, print query used.
                         Default is False

        Returns bool - True = slave has read all GTIDs
                       False = slave is behind
        """
        master_gtids = master_gtid[0][0].split('\n')
        slave_wait_ok = True
        for gtid in master_gtids:
            try:
                if verbose:
                    print "# Slave %s:%s:" % (self.host, self.port)
                    print "# QUERY =", _GTID_WAIT % (gtid.strip(','), timeout)
                res = self.exec_query(_GTID_WAIT % (gtid.strip(','), timeout))
                if verbose:
                    print "# Return Code =", res[0][0]
                if res is None or res[0] is None or res[0][0] is None or \
                   int(res[0][0]) < 0:
                    slave_wait_ok = False
            except UtilRplError, e:
                raise UtilRplError("Error executing %s: %s" %
                                   ((_GTID_WAIT % (gtid.strip(','), timeout)),
                                    e.errmsg))
        return slave_wait_ok

    def make_change_master(self, from_beginning=False, master_values=None):
        """Make the CHANGE MASTER command.

        This method forms the CHANGE MASTER command based on the current
        settings of the slave. If the user supplies a dictionary of options,
        the method will use those values provided by the user if present
        otherwise it will use current settings.

        Note: the keys used in the dictionary are defined in the
              _MASTER_INFO_COL list defined above.

        from_beginning[in] if True, omit specification of master's binlog info
        master_values[in] if provided, use values in the dictionary

        Returns string - CHANGE MASTER command
        """
        if not master_values:
            master_values = {}
        if master_values == {} and not self.is_connected():
            raise UtilRplError("Cannot generate CHANGE MASTER command. The "
                               "slave is not connected to a master and no "
                               "master information was provided.")
        elif self.is_connected():
            m_info = MasterInfo(self, self.options)
            master_info = m_info.get_master_info()
            if master_info is None and master_values == {}:
                raise UtilRplError("Cannot create CHANGE MASTER command.")
        else:
            master_info = None

        # Form values for command.
        # If we cannot get the master info information, try the values passed
        if master_info is None:
            master_host = master_values['Master_Host']
            if "]" in master_host:
                master_host = clean_IPv6(master_host)
            master_port = master_values['Master_Port']
            master_user = master_values['Master_User']
            master_passwd = master_values['Master_Password']
            master_log_file = master_values['Master_Log_File']
            master_log_pos = master_values['Read_Master_Log_Pos']
            master_ssl = master_values.get('Master_SSL_Allowed', None)
            master_ssl_ca = master_values.get('Master_SSL_CA_File', None)
            master_ssl_cert = master_values.get('Master_SSL_Cert', None)
            master_ssl_key = master_values.get('Master_SSL_Key', None)
            if master_ssl and master_ssl_ca is None:
                master_ssl_ca = ''
        else:
            master_host = master_values.get('Master_Host',
                                            master_info['Master_Host'])
            master_port = master_values.get('Master_Port',
                                            master_info['Master_Port'])
            master_user = master_values.get('Master_User',
                                            master_info['Master_User'])
            master_passwd = master_values.get('Master_Password',
                                              master_info['Master_Password'])
            master_log_file = master_values.get('Master_Log_File',
                                                master_info['Master_Log_File'])
            master_log_pos = master_values.get(
                'Read_Master_Log_Pos',
                master_info['Read_Master_Log_Pos']
            )
            master_ssl = master_values.get(
                'Master_SSL_Allowed',
                master_info['Master_SSL_Allowed']
            )
            master_ssl_ca = master_values.get(
                'Master_SSL_CA_File',
                master_info['Master_SSL_CA_File']
            )
            master_ssl_cert = master_values.get(
                'Master_SSL_Cert',
                master_info['Master_SSL_Cert']
            )
            master_ssl_key = master_values.get(
                'Master_SSL_Key',
                master_info['Master_SSL_Key']
            )

        change_master = "CHANGE MASTER TO MASTER_HOST = '%s', " % master_host
        if master_user:
            change_master += "MASTER_USER = '%s', " % master_user
        # To rewrite a current password with blank password, not check against
        # empty string.
        if master_passwd is not None:
            change_master += "MASTER_PASSWORD = '%s', " % master_passwd
        change_master += "MASTER_PORT = %s" % master_port
        if master_ssl and master_ssl not in ('0', 'OFF'):
            change_master = "{0}, MASTER_SSL = {1}".format(change_master, 1)
        if master_ssl_ca is not None:
            change_master = (
                "{0}, MASTER_SSL_CA = '{1}'"
            ).format(change_master, master_ssl_ca)
        if master_ssl_cert:
            change_master = (
                "{0}, MASTER_SSL_CERT = '{1}'"
            ).format(change_master, master_ssl_cert)
        if master_ssl_key:
            change_master = (
                "{0}, MASTER_SSL_KEY = '{1}'"
            ).format(change_master, master_ssl_key)
        if self.supports_gtid() == "ON":
            change_master += ", MASTER_AUTO_POSITION=1"
        elif not from_beginning:
            change_master += ", MASTER_LOG_FILE = '%s'" % master_log_file
            if master_log_pos >= 0:
                change_master += ", MASTER_LOG_POS = %s" % master_log_pos

        return change_master

    def is_configured_for_master(self, master, verify_state=False,
                                 raise_error=False):
        """Check that slave is connected to the master at host, port.

        master[in]          Instance of the master.
        verify_state[in]    Flag to verify the state of the slave.
                            By default False, state verification ignored.
        raise_error[in]     Indicate if an Error is raised instead of
                            returning false (not configured for master).
                            By default False, return a boolean value.

        Returns bool - True = is connected
        """
        res = self.get_status()
        if res == [] or not res[0]:
            if raise_error:
                raise UtilRplError("Server '{0}:{1}' is not acting as a slave "
                                   "(slave status is empty)"
                                   ".".format(self.host, self.port))
            return False
        # We must not assume there is one and only one master for a slave.
        # Starting with 5.7.6, multi-master means a slave could have many
        # masters, each connected via a replication channel. Thus, we must
        # loop through the rows in the SHOW SLAVE STATUS and check every
        # master listed. If no matches to this master is found, we can
        # declare the slave not connected to the master otherwise, we can
        # stop the loop when the master is found.
        m_host = ""
        m_port = None
        master_found = False
        for row in res:
            # pylint: disable=W0633
            m_host = row[_SLAVE_MASTER_HOST]  # get master host
            m_port = row[_SLAVE_MASTER_PORT]  # get master port
            # Suppose the state is True for "Waiting for master to send event"
            # so we can ignore it if verify_state is not given as True.
            if verify_state:
                state = (row[_SLAVE_IO_STATE] ==
                         "Waiting for master to send event")
                if not state:
                    if raise_error:
                        raise UtilRplError("Slave '{0}:{1}' is not waiting"
                                           " for events from master."
                                           "".format(self.host, self.port))
                    return False
            # If we find a match, stop.
            if master.is_alias(m_host) and int(m_port) == int(master.port):
                master_found = True
                break
        # If no master found, report what we did find or in the case of
        # multi-master (more than one row in SHOW SLAVE STATUS), state this
        # master is not among the masters listed for the slave.
        if not master_found:
            if raise_error:
                if len(res) > 1:
                    raise UtilRplError("The list of masters for slave "
                                       "'{0}:{1}' does not include master"
                                       " '{2}:{3}'"
                                       ".".format(self.host, self.port,
                                                  master.host, master.port))
                else:
                    raise UtilRplError("Slave '{0}:{1}' is configured for "
                                       "master '{2}:{3}' and not '{4}:{5}'"
                                       ".".format(self.host, self.port,
                                                  m_host, m_port,
                                                  master.host, master.port))
            return False
        return True

    def check_rpl_health(self, master, master_log, master_log_pos,
                         max_delay, max_pos, verbosity):
        """Check replication health of the slave.

        This method checks to see if the slave is setup correctly to
        operate in a replication environment. It returns a tuple with a
        bool to indicate if health is Ok (True), and a list to contain any
        errors encountered during the checks.

        master[in]         Master class instance
        master_log[in]     master's log file
        master_log_pos[in] master's log file position
        max_delay[in]      if the slave delay (in seconds) is greater than this
                           value, the slave health is not Ok
        max_pos[in]        maximum position difference from master to slave to
                           determine if slave health is not Ok
        verbosity[in]      if > 1, return detailed errors else return only
                           short phrases

        Returns tuple (bool, []) - (True, []) = Ok,
                                   (False, error_list) = not setup correctly
        """
        errors = []
        rpl_ok = True

        if not self.is_alive():
            return (False, ["Cannot connect to server"])

        res = self.get_status()
        if res != [] and res[0] != []:
            res = res[0]
            self.get_master_host_port()
            m_log = res[_SLAVE_MASTER_LOG_FILE]
            m_log_pos = res[_SLAVE_MASTER_LOG_FILE_POS]
            io_running = res[_SLAVE_IO_RUNNING]
            sql_running = res[_SLAVE_SQL_RUNNING]
            s_delay = res[_SLAVE_DELAY]
            delay = s_delay if s_delay is not None else 0
            remaining_delay = res[_SLAVE_REMAINING_DELAY]
            io_error_num = res[_SLAVE_IO_ERRORNO]
            io_error_text = res[_SLAVE_IO_ERROR]

            # Check to see that slave is connected to the right master
            if not self.is_configured_for_master(master):
                return (False, ["Not connected to correct master."])

            # Check slave status for errors, threads activity
            if io_running.upper() != "YES":
                errors.append("IO thread is not running.")
                rpl_ok = False
            if sql_running.upper() != "YES":
                errors.append("SQL thread is not running.")
                rpl_ok = False
            if int(io_error_num) > 0:
                errors.append(io_error_text)
                rpl_ok = False

            # Check slave delay with threshhold of SBM, and master's log pos
            if int(delay) > int(max_delay):
                errors.append("Slave delay is %s seconds behind master." %
                              delay)
                if len(remaining_delay):
                    errors.append(remaining_delay)
                rpl_ok = False

            # Check master position
            if self.supports_gtid() != "ON":
                if m_log != master_log:
                    errors.append("Wrong master log file.")
                    rpl_ok = False
                elif (int(m_log_pos) + int(max_pos)) < int(master_log_pos):
                    errors.append("Slave's master position exceeds maximum.")
                    rpl_ok = False

            # Check GTID trans behind.
            elif self.supports_gtid() == "ON":
                master_gtids = master.exec_query(_GTID_EXECUTED)
                num_gtids_behind = self.num_gtid_behind(master_gtids)
                if num_gtids_behind > 0:
                    errors.append("Slave has %s transactions behind master." %
                                  num_gtids_behind)
                    rpl_ok = False

        else:
            errors.append("Not connected")
            rpl_ok = False

        if len(errors) > 1:
            errors = [", ".join(errors)]

        return (rpl_ok, errors)

    def get_rpl_details(self):
        """Return slave status variables for health reporting

        This method retrieves the slave's parameters for checking relationship
        with master.

        Returns tuple - slave values or None if not connected
        """
        res = self.get_status()
        if res == []:
            return None

        res = res[0]
        read_log_file = res[_SLAVE_MASTER_LOG_FILE]
        read_log_pos = res[_SLAVE_MASTER_LOG_FILE_POS]
        io_thread = res[_SLAVE_IO_RUNNING]
        sql_thread = res[_SLAVE_SQL_RUNNING]

        # seconds behind master
        if res[_SLAVE_DELAY] is None:
            sec_behind = 0
        else:
            sec_behind = int(res[_SLAVE_DELAY])
        delay_remaining = res[_SLAVE_REMAINING_DELAY]

        io_error_num = res[_SLAVE_IO_ERRORNO]
        io_error_text = res[_SLAVE_IO_ERROR]
        sql_error_num = res[_SLAVE_SQL_ERRORNO]
        sql_error_text = res[_SLAVE_SQL_ERROR]

        return (read_log_file, read_log_pos, io_thread, sql_thread, sec_behind,
                delay_remaining, io_error_num, io_error_text, sql_error_num,
                sql_error_text)

    def switch_master(self, master, user, passwd="", from_beginning=False,
                      master_log_file=None, master_log_pos=None,
                      show_command=False):
        """Switch slave to a new master

        This method stops the slave and issues a new change master command
        to the master specified then starts the slave. No prerequisites are
        checked and it does not wait to see if slave catches up to the master.

        master[in]           Master class instance
        user[in]             replication user
        passwd[in]           replication user password
        from_beginning[in]   if True, start from beginning of logged events
                             Default = False
        master_log_file[in]  master's log file (not needed for GTID)
        master_log_pos[in]   master's log file position (not needed for GTID)
        show_command[in]     if True, display the change master command
                             Default = False

        returns bool - True = success
        """
        hostport = "%s:%s" % (self.host, self.port)

        master_values = {
            'Master_Host': master.host,
            'Master_Port': master.port,
            'Master_User': user,
            'Master_Password': passwd,
            'Master_Log_File': master_log_file,
            'Read_Master_Log_Pos': master_log_pos,
        }
        if master.has_ssl:
            master_values['Master_SSL_Allowed'] = 1
            if master.ssl_ca:
                master_values['Master_SSL_CA_File'] = master.ssl_ca
            if master.ssl_cert:
                master_values['Master_SSL_Cert'] = master.ssl_cert
            if master.ssl_key:
                master_values['Master_SSL_Key'] = master.ssl_key
        change_master = self.make_change_master(from_beginning, master_values)
        if show_command:
            print "# Change master command for %s:%s" % (self.host, self.port)
            print "#", change_master
        try:
            self.exec_query(change_master)
        except UtilError as err:
            raise UtilRplError("Slave {0} change master failed. "
                               "{1}".format(hostport, err.errmsg))
        return True
#
# Copyright (c) 2014, 2016 Oracle and/or its affiliates. All rights
# reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains the multi-source replication utility. It is used to setup
replication among a slave and multiple masters.
"""

import os
import sys
import time
import logging


_MIN_SERVER_VERSION = (5, 6, 9)
_GTID_LISTS = ["Transactions executed on the servers:",
               "Transactions purged from the servers:",
               "Transactions owned by another server:"]
_GEN_UUID_COLS = ["host", "port", "role", "uuid"]
_GEN_GTID_COLS = ["host", "port", "role", "gtid"]


class ReplicationMultiSource(Daemon):
    """Setup replication among a slave and multiple masters.

    This class implements a multi-source replication using a round-robin
    scheduling for setup replication among all masters and slave.

    This class also implements a POSIX daemon.
    """
    def __init__(self, slave_vals, masters_vals, options):
        """Constructor.

        slave_vals[in]     Slave server connection dictionary.
        master_vals[in]    List of master server connection dictionaries.
        options[in]        Options dictionary.
        """
        pidfile = options.get("pidfile", None)
        if pidfile is None:
            pidfile = "./rplms_daemon.pid"
        super(ReplicationMultiSource, self).__init__(pidfile)

        self.slave_vals = slave_vals
        self.masters_vals = masters_vals
        self.options = options
        self.quiet = self.options.get("quiet", False)
        self.logging = self.options.get("logging", False)
        self.rpl_user = self.options.get("rpl_user", None)
        self.verbosity = options.get("verbosity", 0)
        self.interval = options.get("interval", 15)
        self.switchover_interval = options.get("switchover_interval", 60)
        self.format = self.options.get("format", False)
        self.topology = None
        self.report_values = [
            report.lower() for report in
            self.options["report_values"].split(",")
        ]

        # A sys.stdout copy, that can be used later to turn on/off stdout
        self.stdout_copy = sys.stdout
        self.stdout_devnull = open(os.devnull, "w")

        # Disable stdout when running --daemon with start, stop or restart
        self.daemon = options.get("daemon")
        if self.daemon:
            if self.daemon in ("start", "nodetach"):
                self._report("Starting multi-source replication daemon...",
                             logging.INFO, False)
            elif self.daemon == "stop":
                self._report("Stopping multi-source replication daemon...",
                             logging.INFO, False)
            else:
                self._report("Restarting multi-source replication daemon...",
                             logging.INFO, False)

            # Disable stdout
            sys.stdout = self.stdout_devnull
        else:
            self._report("# Starting multi-source replication...",
                         logging.INFO)
            print("# Press CTRL+C to quit.")

        # Check server versions
        try:
            self._check_server_versions()
        except UtilError as err:
            raise UtilRplError(err.errmsg)

        # Check user privileges
        try:
            self._check_privileges()
        except UtilError as err:
            msg = "Error checking user privileges: {0}".format(err.errmsg)
            self._report(msg, logging.CRITICAL, False)
            raise UtilRplError(err.errmsg)

    @staticmethod
    def _reconnect_server(server, pingtime=3):
        """Tries to reconnect to the server.

        This method tries to reconnect to the server and if connection fails
        after 3 attemps, returns False.

        server[in]      Server instance.
        pingtime[in]    Interval between connection attempts.
        """
        if server and server.is_alive():
            return True
        is_connected = False
        i = 0
        while i < 3:
            try:
                server.connect()
                is_connected = True
                break
            except UtilError:
                pass
            time.sleep(pingtime)
            i += 1
        return is_connected

    def _get_slave(self):
        """Get the slave server instance.

        Returns a Server instance of the slave from the replication topology.
        """
        return self.topology.slaves[0]["instance"]

    def _get_master(self):
        """Get the current master server instance.

        Returns a Server instance of the current master from the replication
        topology.
        """
        return self.topology.master

    def _check_server_versions(self):
        """Checks the server versions.
        """
        if self.verbosity > 0:
            print("# Checking server versions.\n#")

        # Connection dictionary
        conn_dict = {
            "conn_info": None,
            "quiet": True,
            "verbose": self.verbosity > 0,
        }

        # Check masters version
        for master_vals in self.masters_vals:
            conn_dict["conn_info"] = master_vals
            master = Master(conn_dict)
            master.connect()
            if not master.check_version_compat(*_MIN_SERVER_VERSION):
                raise UtilRplError(
                    ERROR_MIN_SERVER_VERSIONS.format(
                        utility="mysqlrplms",
                        min_version=".".join([str(val) for val in
                                              _MIN_SERVER_VERSION]),
                        host=master.host,
                        port=master.port
                    )
                )
            master.disconnect()

        # Check slave version
        conn_dict["conn_info"] = self.slave_vals
        slave = Slave(conn_dict)
        slave.connect()
        if not slave.check_version_compat(*_MIN_SERVER_VERSION):
            raise UtilRplError(
                ERROR_MIN_SERVER_VERSIONS.format(
                    utility="mysqlrplms",
                    min_version=".".join([str(val) for val in
                                          _MIN_SERVER_VERSION]),
                    host=slave.host,
                    port=slave.port
                )
            )
        slave.disconnect()

    def _check_privileges(self):
        """Check required privileges to perform the multi-source replication.

        This method check if the used users for the slave and masters have
        the required privileges to perform the multi-source replication.
        The following privileges are required:
            - on slave: SUPER, SELECT, INSERT, UPDATE, REPLICATION
                        SLAVE AND GRANT OPTION;
            - on the master: SUPER, SELECT, INSERT, UPDATE, REPLICATION SLAVE
                             AND GRANT OPTION.
        An exception is thrown if users doesn't have enough privileges.
        """
        if self.verbosity > 0:
            print("# Checking users privileges for replication.\n#")

        # Connection dictionary
        conn_dict = {
            "conn_info": None,
            "quiet": True,
            "verbose": self.verbosity > 0,
        }

        # Check privileges for master.
        master_priv = [('SUPER',), ('SELECT',), ('INSERT',), ('UPDATE',),
                       ('REPLICATION SLAVE',), ('GRANT OPTION',)]
        master_priv_str = ("SUPER, SELECT, INSERT, UPDATE, REPLICATION SLAVE "
                           "AND GRANT OPTION")

        for master_vals in self.masters_vals:
            conn_dict["conn_info"] = master_vals
            master = Master(conn_dict)
            master.connect()

            user_obj = User(master, "{0}@{1}".format(master.user, master.host))
            for any_priv_tuple in master_priv:
                has_privilege = any(
                    [user_obj.has_privilege('*', '*', priv)
                     for priv in any_priv_tuple]
                )
                if not has_privilege:
                    msg = ERROR_USER_WITHOUT_PRIVILEGES.format(
                        user=master.user, host=master.host, port=master.port,
                        operation='perform replication',
                        req_privileges=master_priv_str
                    )
                    self._report(msg, logging.CRITICAL, False)
                    raise UtilRplError(msg)
            master.disconnect()

        # Check privileges for slave
        slave_priv = [('SUPER',), ('SELECT',), ('INSERT',), ('UPDATE',),
                      ('REPLICATION SLAVE',), ('GRANT OPTION',)]
        slave_priv_str = ("SUPER, SELECT, INSERT, UPDATE, REPLICATION SLAVE "
                          "AND GRANT OPTION")

        conn_dict["conn_info"] = self.slave_vals
        slave = Slave(conn_dict)
        slave.connect()

        user_obj = User(slave, "{0}@{1}".format(slave.user, slave.host))
        for any_priv_tuple in slave_priv:
            has_privilege = any(
                [user_obj.has_privilege('*', '*', priv)
                 for priv in any_priv_tuple]
            )
            if not has_privilege:
                msg = ("User '{0}' on '{1}@{2}' does not have sufficient "
                       "privileges to perform replication (required: {3})."
                       "".format(slave.user, slave.host, slave.port,
                                 slave_priv_str))
                self._report(msg, logging.CRITICAL, False)
                raise UtilRplError(msg)
        slave.disconnect()

    def _check_host_references(self):
        """Check to see if using all host or all IP addresses.

        Returns bool - True = all references are consistent.
        """
        uses_ip = hostname_is_ip(self.topology.master.host)
        slave = self._get_slave()
        host_port = slave.get_master_host_port()
        host = None
        if host_port:
            host = host_port[0]
        if (not host or uses_ip != hostname_is_ip(slave.host) or
                uses_ip != hostname_is_ip(host)):
            return False
        return True

    def _setup_replication(self, master_vals, use_rpl_setup=True):
        """Setup replication among a master and a slave.

        master_vals[in]      Master server connection dictionary.
        use_rpl_setup[in]    Use Replication.setup() if True otherwise use
                             switch_master() on the slave. This is used to
                             control the first pass in the masters round-robin
                             scheduling.
        """
        conn_options = {
            "src_name": "master",
            "dest_name": "slave",
            "version": "5.0.0",
            "unique": True,
        }
        (master, slave,) = connect_servers(master_vals, self.slave_vals,
                                           conn_options)
        rpl_options = self.options.copy()
        rpl_options["verbosity"] = self.verbosity > 0

        # Start from beginning only on the first pass
        if rpl_options.get("from_beginning", False) and not use_rpl_setup:
            rpl_options["from_beginning"] = False

        # Create an instance of the replication object
        rpl = Replication(master, slave, rpl_options)

        if use_rpl_setup:
            # Check server ids
            errors = rpl.check_server_ids()
            for error in errors:
                self._report(error, logging.ERROR, True)

            # Check for server_id uniqueness
            errors = rpl.check_server_uuids()
            for error in errors:
                self._report(error, logging.ERROR, True)

            # Check InnoDB compatibility
            errors = rpl.check_innodb_compatibility(self.options)
            for error in errors:
                self._report(error, logging.ERROR, True)

            # Checking storage engines
            errors = rpl.check_storage_engines(self.options)
            for error in errors:
                self._report(error, logging.ERROR, True)

            # Check master for binary logging
            errors = rpl.check_master_binlog()
            if errors != []:
                raise UtilRplError(errors[0])

            # Setup replication
            if not rpl.setup(self.rpl_user, 10):
                msg = "Cannot setup replication."
                self._report(msg, logging.CRITICAL, False)
                raise UtilRplError(msg)
        else:
            # Parse user and password (support login-paths)
            try:
                (r_user, r_pass,) = parse_user_password(self.rpl_user)
            except FormatError:
                raise UtilError(USER_PASSWORD_FORMAT.format("--rpl-user"))

            # Switch master and start slave
            slave.switch_master(master, r_user, r_pass)
            slave.start({'fetch': False})

        # Disconnect from servers
        master.disconnect()
        slave.disconnect()

    def _switch_master(self, master_vals, use_rpl_setup=True):
        """Switches replication to a new master.

        This method stops replication with the old master if exists and
        starts the replication with a new one.

        master_vals[in]      Master server connection dictionary.
        use_rpl_setup[in]    Used to control the first pass in the masters
                             round-robin scheduling.
        """
        if self.topology:
            # Stop slave
            master = self._get_master()
            if master.is_alive():
                master.disconnect()
            slave = self._get_slave()
            if not slave.is_alive() and not self._reconnect_server(slave):
                msg = "Failed to connect to the slave."
                self._report(msg, logging.CRITICAL, False)
                raise UtilRplError(msg)
            slave.stop()
            slave.disconnect()

        self._report("# Switching to master '{0}:{1}'."
                     "".format(master_vals["host"],
                               master_vals["port"]), logging.INFO, True)

        try:
            # Setup replication on the new master
            self._setup_replication(master_vals, use_rpl_setup)

            # Create a Topology object
            self.topology = Topology(master_vals, [self.slave_vals],
                                     self.options)
        except UtilError as err:
            msg = "Error while switching master: {0}".format(err.errmsg)
            self._report(msg, logging.CRITICAL, False)
            raise UtilRplError(err.errmsg)

        # Only works for GTID_MODE=ON
        if not self.topology.gtid_enabled():
            msg = ("Topology must support global transaction ids and have "
                   "GTID_MODE=ON.")
            self._report(msg, logging.CRITICAL, False)
            raise UtilRplError(msg)

        # Check for mixing IP and hostnames
        if not self._check_host_references():
            print("# WARNING: {0}".format(HOST_IP_WARNING))
            self._report(HOST_IP_WARNING, logging.WARN, False)

    def _report(self, message, level=logging.INFO, print_msg=True):
        """Log message if logging is on.

        This method will log the message presented if the log is turned on.
        Specifically, if options['log_file'] is not None. It will also
        print the message to stdout.

        message[in]      Message to be printed.
        level[in]        Level of message to log. Default = INFO.
        print_msg[in]    If True, print the message to stdout. Default = True.
        """
        # First, print the message.
        if print_msg and not self.quiet:
            print(message)
        # Now log message if logging turned on
        if self.logging:
            logging.log(int(level), message.strip("#").strip(" "))

    def _format_health_data(self):
        """Return health data from topology.

        Returns tuple - (columns, rows).
        """
        if self.topology:
            try:
                health_data = self.topology.get_health()
                current_master = self._get_master()

                # Get data for the remaining masters
                for master_vals in self.masters_vals:
                    # Discard the current master
                    if master_vals["host"] == current_master.host and \
                       master_vals["port"] == current_master.port:
                        continue

                    # Connect to the master
                    conn_dict = {
                        "conn_info": master_vals,
                        "quiet": True,
                        "verbose": self.verbosity > 0,
                    }
                    master = Master(conn_dict)
                    master.connect()

                    # Get master health
                    rpl_health = master.check_rpl_health()

                    master_data = [
                        master.host,
                        master.port,
                        "MASTER",
                        get_server_state(master, master.host, 3,
                                         self.verbosity > 0),
                        master.supports_gtid(),
                        "OK" if rpl_health[0] else ", ".join(rpl_health[1]),
                    ]

                    # Get master status
                    master_status = master.get_status()
                    if len(master_status):
                        master_log, master_log_pos = master_status[0][0:2]
                    else:
                        master_log = None
                        master_log_pos = 0

                    # Show additional details if verbosity is turned on
                    if self.verbosity > 0:
                        master_data.extend([master.get_version(), master_log,
                                            master_log_pos, "", "", "", "", "",
                                            "", "", "", ""])
                    health_data[1].append(master_data)
                return health_data
            except UtilError as err:
                msg = "Cannot get health data: {0}".format(err)
                self._report(msg, logging.ERROR, False)
                raise UtilRplError(msg)
        return ([], [])

    def _format_uuid_data(self):
        """Return the server's uuids.

        Returns tuple - (columns, rows).
        """
        if self.topology:
            try:
                return (_GEN_UUID_COLS, self.topology.get_server_uuids())
            except UtilError as err:
                msg = "Cannot get UUID data: {0}".format(err)
                self._report(msg, logging.ERROR, False)
                raise UtilRplError(msg)
        return ([], [])

    def _format_gtid_data(self):
        """Return the GTID information from the topology.

        Returns tuple - (columns, rows).
        """
        if self.topology:
            try:
                return (_GEN_GTID_COLS, self.topology.get_gtid_data())
            except UtilError as err:
                msg = "Cannot get GTID data: {0}".format(err)
                self._report(msg, logging.ERROR, False)
                raise UtilRplError(msg)
        return ([], [])

    def _log_data(self, title, labels, data, print_format=True):
        """Helper method to log data.

        title[in]     Title to log.
        labels[in]    List of labels.
        data[in]      List of data rows.
        """
        self._report("# {0}".format(title), logging.INFO)
        for row in data:
            msg = ", ".join(
                ["{0}: {1}".format(*col) for col in zip(labels, row)]
            )
            self._report("# {0}".format(msg), logging.INFO, False)
        if print_format:
            print_list(sys.stdout, self.format, labels, data)

    def _log_master_status(self, master):
        """Logs the master information.

        master[in]    Master server instance.

        This method logs the master information from SHOW MASTER STATUS.
        """
        # If no master present, don't print anything.
        if master is None:
            return

        print("#")
        self._report("# {0}:".format("Current Master Information"),
                     logging.INFO)

        try:
            status = master.get_status()[0]
        except UtilError:
            msg = "Cannot get master status"
            self._report(msg, logging.ERROR, False)
            raise UtilRplError(msg)

        cols = ("Binary Log File", "Position", "Binlog_Do_DB",
                "Binlog_Ignore_DB")
        rows = (status[0] or "N/A", status[1] or "N/A", status[2] or "N/A",
                status[3] or "N/A")

        print_list(sys.stdout, self.format, cols, [rows])

        self._report("# {0}".format(
            ", ".join(["{0}: {1}".format(*item) for item in zip(cols, rows)]),
        ), logging.INFO, False)

        # Display gtid executed set
        master_gtids = []
        for gtid in status[4].split("\n"):
            if gtid:
                # Add each GTID to a tuple to match the required format to
                # print the full GRID list correctly.
                master_gtids.append((gtid.strip(","),))

        try:
            if len(master_gtids) > 1:
                gtid_executed = "{0}[...]".format(master_gtids[0][0])
            else:
                gtid_executed = master_gtids[0][0]
        except IndexError:
            gtid_executed = "None"

        self._report("# GTID Executed Set: {0}".format(gtid_executed),
                     logging.INFO)

    def stop_replication(self):
        """Stops multi-source replication.

        Stop the slave if topology is available.
        """
        if self.topology:
            # Get the slave instance
            slave = self._get_slave()
            # If slave is not connected, try to reconnect and stop replication
            if self._reconnect_server(slave):
                slave.stop()
                slave.disconnect()
        if self.daemon:
            self._report("Multi-source replication daemon stopped.",
                         logging.INFO, False)
        else:
            print("")
            self._report("# Multi-source replication stopped.",
                         logging.INFO, True)

    def stop(self):
        """Stops the daemon.

        Stop slave if topology is available and then stop the daemon.
        """
        self.stop_replication()
        super(ReplicationMultiSource, self).stop()

    def run(self):
        """Run the multi-source replication using the round-robin scheduling.

        This method implements the multi-source replication by using time
        slices for each master.
        """
        num_masters = len(self.masters_vals)
        use_rpl_setup = True

        # pylint: disable=R0101
        while True:
            # Round-robin scheduling on the masters
            for idx in range(num_masters):
                # Get the new master values and switch for the next one
                try:
                    master_vals = self.masters_vals[idx]
                    self._switch_master(master_vals, use_rpl_setup)
                except UtilError as err:
                    msg = ("Error while switching master: {0}"
                           "".format(err.errmsg))
                    self._report(msg, logging.CRITICAL, False)
                    raise UtilRplError(msg)

                # Get the new master and slave instances
                master = self._get_master()
                slave = self._get_slave()

                switchover_timeout = time.time() + self.switchover_interval

                while switchover_timeout > time.time():
                    # If servers not connected, try to reconnect
                    if not self._reconnect_server(master):
                        msg = ("Failed to connect to the master '{0}:{1}'."
                               "".format(master_vals["host"],
                                         master_vals["port"]))
                        self._report(msg, logging.CRITICAL, False)
                        raise UtilRplError(msg)

                    if not self._reconnect_server(slave):
                        msg = "Failed to connect to the slave."
                        self._report(msg, logging.CRITICAL, False)
                        raise UtilRplError(msg)

                    # Report
                    self._log_master_status(master)
                    if "health" in self.report_values:
                        (health_labels, health_data,) = \
                            self._format_health_data()
                        if health_data:
                            print("#")
                            self._log_data("Health Status:", health_labels,
                                           health_data)
                    if "gtid" in self.report_values:
                        (gtid_labels, gtid_data,) = self._format_gtid_data()
                        for i, row in enumerate(gtid_data):
                            if row:
                                print("#")
                                self._log_data("GTID Status - {0}"
                                               "".format(_GTID_LISTS[i]),
                                               gtid_labels, row)

                    if "uuid" in self.report_values:
                        (uuid_labels, uuid_data,) = self._format_uuid_data()
                        if uuid_data:
                            print("#")
                            self._log_data("UUID Status:", uuid_labels,
                                           uuid_data)

                    # Disconnect servers
                    master.disconnect()
                    slave.disconnect()

                    # Wait for reporting interval
                    time.sleep(self.interval)

            # Use Replication.setup() only for the first round
            use_rpl_setup = False
#
# Copyright (c) 2014, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains features to check the data consistency in a replication
topology (i.e., between the master and its slaves, or only slaves), providing
synchronization features to perform the check over the (supposed) same data of
a system with replication active (running).
"""
import re
import sys

from multiprocessing.pool import ThreadPool


# Regular expression to handle the server version format.
_RE_VERSION_FORMAT = r'^(\d+\.\d+(\.\d+)*).*$'


class RPLSynchronizer(object):
    """Class to manage the features of the replication synchronization checker.

    The RPLSynchronizer class is used to manage synchronization check between
    servers of a replication topology, namely between the master and its
    slaves or only between slaves. It provides functions to determine the
    slaves missing transactions (i.e., missing GTIDs) and check data
    consistency.
    """

    def __init__(self, master_cnx_dic, slaves_cnx_dic_lst, options):
        """Constructor.

        options[in]       dictionary of options (e.g., discover, timeouts,
                          verbosity).
        """
        self._verbosity = options.get('verbosity')
        self._rpl_timeout = options.get('rpl_timeout')
        self._checksum_timeout = options.get('checksum_timeout')
        self._interval = options.get('interval')

        self._rpl_topology = Topology(master_cnx_dic, slaves_cnx_dic_lst,
                                      options)
        self._slaves = self._rpl_topology.get_slaves_dict()

        # Verify all the servers in the topology has or does not sql_mode set
        # to 'ANSI_QUOTES'.
        match_group, unmatch_group = \
            self._rpl_topology.get_servers_with_different_sql_mode(
                'ANSI_QUOTES'
            )
        # List and Raise an error if just some of the server has sql_mode set
        # to 'ANSI_QUOTES' instead of all or none.
        if match_group and unmatch_group:
            sql_mode = match_group[0].select_variable("SQL_MODE")
            if sql_mode == '':
                sql_mode = '""'
            sql_mode = sql_mode.replace(',', ', ')
            print("# The SQL mode in the following servers is set to "
                  "ANSI_QUOTES: {0}".format(sql_mode))
            for server in match_group:
                sql_mode = server.select_variable("SQL_MODE")
                if sql_mode == '':
                    sql_mode = '""'
                sql_mode = sql_mode.replace(',', ', ')
                print("# {0}:{1} sql_mode={2}"
                      "".format(server.host, server.port, sql_mode))
            print("# The SQL mode in the following servers is not set to "
                  "ANSI_QUOTES:")
            for server in unmatch_group:
                sql_mode = server.select_variable("SQL_MODE")
                if sql_mode == '':
                    sql_mode = '""'
                print("# {0}:{1} sql_mode={2}"
                      "".format(server.host, server.port, sql_mode))

            raise UtilError(ERROR_ANSI_QUOTES_MIX_SQL_MODE.format(
                utility='mysqlrplsync'
            ))

        # Set base server used as reference for comparisons.
        self._base_server = None
        self._base_server_key = None
        self._set_base_server()

        # Check user permissions to perform the consistency check.
        self._check_privileges()

        # Check usage of replication filters.
        self._master_rpl_filters = {}
        self._slaves_rpl_filters = {}
        self._check_rpl_filters()

    def _set_base_server(self):
        """Set the base server used for comparison in the internal state.

        Set the master if used or the first slave from the topology as the
        base server. The base server is the one used as a reference for
        comparison with the others. This method sets two instance variables:
        _base_server with the Server instance, and _base_server_key with the
        string identifying the server (format: 'host@port').

        Note: base server might need to be changed (set again) if it is
        removed from the topology for some reason (e.g. GTID disabled).
        """
        master = self._get_master()
        self._base_server = master if master \
            else self._rpl_topology.slaves[0]['instance']
        self._base_server_key = "{0}@{1}".format(self._base_server.host,
                                                 self._base_server.port)

    def _get_slave(self, slave_key):
        """Get the slave server instance for the specified key 'host@port'.

        This function retrieves the Server instance of for a slave from the
        internal state by specifying the key that uniquely identifies it,
        i.e. 'host@port'.

        slave_key[in]   String with the format 'host@port' that uniquely
                        identifies a server.

        Returns a Server instance of the slave with the specified key value
        (i.e., 'host@port').
        """
        slave_dict = self._slaves[slave_key]
        return slave_dict['instance']

    def _get_master(self):
        """Get the master server instance.

        This function retrieves the Server instance of the master (in the
        replication topology).

        Returns a Server instance of the master.
        """
        return self._rpl_topology.master

    def _check_privileges(self):
        """Check required privileges to perform the synchronization check.

        This method check if the used users for the master and slaves possess
        the required privileges to perform the synchronization check. More
        specifically, the following privileges are required:
            - on the master: SUPER or REPLICATION CLIENT, LOCK TABLES and
                             SELECT;
            - on slaves: SUPER and SELECT.
        An exception is thrown if users doesn't have enough privileges.
        """
        if self._verbosity:
            print("# Checking users permission to perform consistency check.\n"
                  "#")

        # Check privileges for master.
        master_priv = [('SUPER', 'REPLICATION CLIENT'), ('LOCK TABLES',),
                       ('SELECT',)]
        master_priv_str = "SUPER or REPLICATION CLIENT, LOCK TABLES and SELECT"
        if self._get_master():
            server = self._get_master()
            user_obj = User(server, "{0}@{1}".format(server.user, server.host))
            for any_priv_tuple in master_priv:
                has_privilege = any(
                    [user_obj.has_privilege('*', '*', priv)
                     for priv in any_priv_tuple]
                )
                if not has_privilege:
                    raise UtilError(ERROR_USER_WITHOUT_PRIVILEGES.format(
                        user=server.user, host=server.host, port=server.port,
                        operation='perform the synchronization check',
                        req_privileges=master_priv_str
                    ))

        # Check privileges for slaves.
        slave_priv = [('SUPER',), ('SELECT',)]
        slave_priv_str = "SUPER and SELECT"
        for slave_key in self._slaves:
            server = self._get_slave(slave_key)
            user_obj = User(server, "{0}@{1}".format(server.user, server.host))
            for any_priv_tuple in slave_priv:
                has_privilege = any(
                    [user_obj.has_privilege('*', '*', priv)
                     for priv in any_priv_tuple]
                )
                if not has_privilege:
                    raise UtilError(
                        "User '{0}' on '{1}@{2}' does not have sufficient "
                        "privileges to perform the synchronization check "
                        "(required: {3}).".format(server.user, server.host,
                                                  server.port, slave_priv_str)
                    )

    def _check_rpl_filters(self):
        """Check usage of replication filters.

        Check the usage of replication filtering option on the master (if
        defined) and slaves, and set the internal state with the found options
        (to check later).
        """
        # Get binlog filtering option for the master.
        if self._get_master():
            m_filters = self._get_master().get_binlog_exceptions()
            if m_filters:
                # Set filtering option for master.
                self._master_rpl_filters['binlog_do_db'] = \
                    m_filters[0][1].split(',') if m_filters[0][1] else None
                self._master_rpl_filters['binlog_ignore_db'] = \
                    m_filters[0][2].split(',') if m_filters[0][2] else None

        # Get replication filtering options for each slave.
        for slave_key in self._slaves:
            slave = self._get_slave(slave_key)
            s_filters = slave.get_slave_rpl_filters()
            if s_filters:
                # Handle known server issues with some replication filters,
                # leading to inconsistent GTID sets. Sync not supported for
                # server with those issues.
                issues = [(0, 'replicate_do_db'), (1, 'replicate_ignore_db'),
                          (4, 'replicate_wild_do_table')]
                for index, rpl_opt in issues:
                    if s_filters[index]:
                        raise UtilError(
                            "Use of {0} option is not supported. There is a "
                            "known issue with the use this replication filter "
                            "and GTID for some server versions. Issue "
                            "detected for '{1}'.".format(rpl_opt, slave_key))
                # Set map (dictionary) with the slave filtering options.
                filters_map = {
                    'replicate_do_db':
                    s_filters[0].split(',') if s_filters[0] else None,
                    'replicate_ignore_db':
                    s_filters[1].split(',') if s_filters[1] else None,
                    'replicate_do_table':
                    s_filters[2].split(',') if s_filters[2] else None,
                    'replicate_ignore_table':
                    s_filters[3].split(',') if s_filters[3] else None,
                }
                # Handle wild-*-table filters differently to create
                # corresponding regexp.
                if s_filters[4]:
                    wild_list = s_filters[4].split(',')
                    filters_map['replicate_wild_do_table'] = wild_list
                    # Create auxiliary list with compiled regexp to match.
                    regexp_list = []
                    for wild in wild_list:
                        regexp = re.compile(convertSQL_LIKE2REGEXP(wild))
                        regexp_list.append(regexp)
                    filters_map['regexp_do_table'] = regexp_list
                else:
                    filters_map['replicate_wild_do_table'] = None
                    filters_map['regexp_do_table'] = None
                if s_filters[5]:
                    wild_list = s_filters[5].split(',')
                    filters_map['replicate_wild_ignore_table'] = wild_list
                    # Create auxiliary list with compiled regexp to match.
                    regexp_list = []
                    for wild in wild_list:
                        regexp = re.compile(convertSQL_LIKE2REGEXP(wild))
                        regexp_list.append(regexp)
                    filters_map['regexp_ignore_table'] = regexp_list
                else:
                    filters_map['replicate_wild_ignore_table'] = None
                    filters_map['regexp_ignore_table'] = None
                # Set filtering options for the slave.
                self._slaves_rpl_filters[slave_key] = filters_map

        # Print warning if filters are found.
        if self._master_rpl_filters or self._slaves_rpl_filters:
            print("# WARNING: Replication filters found on checked "
                  "servers. This can lead data consistency issues "
                  "depending on how statements are evaluated.\n"
                  "# More information: "
                  "http://dev.mysql.com/doc/en/replication-rules.html")
            if self._verbosity:
                # Print filter options in verbose mode.
                if self._master_rpl_filters:
                    print("# Master '{0}@{1}':".format(
                        self._get_master().host, self._get_master().port
                    ))
                    for rpl_filter in self._master_rpl_filters:
                        if self._master_rpl_filters[rpl_filter]:
                            print("#   - {0}: {1}".format(
                                rpl_filter,
                                ', '.join(
                                    self._master_rpl_filters[rpl_filter]
                                )
                            ))
                if self._slaves_rpl_filters:
                    for slave_key in self._slaves_rpl_filters:
                        print("# Slave '{0}':".format(slave_key))
                        filters_map = self._slaves_rpl_filters[slave_key]
                        for rpl_filter in filters_map:
                            if (rpl_filter.startswith('replicate') and
                                    filters_map[rpl_filter]):
                                print("#   - {0}: {1}".format(
                                    rpl_filter,
                                    ', '.join(filters_map[rpl_filter])
                                ))

    def _is_rpl_filtered(self, db_name, tbl_name=None, slave=None):
        """ Check if the given object is to be filtered by replication.

        This method checks if the given database or table name is
        supposed to be filtered by replication (i.e., not replicated),
        according to the defined replication filters for the master or
        the specified slave.

        db_name[in]     Name of the database to check (not backtick quoted) or
                        associated to the table to check..
        tbl_name[in]    Name of the table to check (not backtick quoted).
                        Table level filtering rules are only checked if this
                        value is not None. By default None, meaning that only
                        the database level rules are checked.
        slave[in]       Identification of the slave in the format 'host@port'
                        to check, determining which filtering rules will be
                        checked. If None only the master filtering rules are
                        checked, otherwise the rule of the specified slaves
                        are used. By default: None.

        Returns a boolean value indicating if the given database or table is
        supposed to be filtered by the replication  or not. More precisely,
        if True then updates associated to the object are (supposedly) not
        replicated, otherwise they are replicated.
        """
        def match_regexp(name, regex_list):
            """ Check if 'name' matches one of the regex in the given list.
            """
            for regex in regex_list:
                if regex.match(name):
                    return True
            return False

        # Determine object to check and set full qualified name.
        is_db = tbl_name is None
        obj_name = db_name if is_db else '{0}.{1}'.format(db_name, tbl_name)

        # Match replication filter for Master.
        if not slave and is_db and self._master_rpl_filters:
            if self._master_rpl_filters['binlog_do_db']:
                if obj_name in self._master_rpl_filters['binlog_do_db']:
                    return False
                else:
                    return True
            elif self._master_rpl_filters['binlog_ignore_db']:
                if obj_name in self._master_rpl_filters['binlog_ignore_db']:
                    return True

        # Match replication filters for the specified slave.
        if slave and slave in self._slaves_rpl_filters:
            rpl_filter = self._slaves_rpl_filters[slave]
            if is_db:
                if rpl_filter['replicate_do_db']:
                    if obj_name in rpl_filter['replicate_do_db']:
                        return False
                    else:
                        return True
                elif (rpl_filter['replicate_ignore_db'] and
                      obj_name in rpl_filter['replicate_ignore_db']):
                    return True
            else:
                if (rpl_filter['replicate_do_table'] and
                        obj_name in rpl_filter['replicate_do_table']):
                    return False
                if (rpl_filter['replicate_ignore_table'] and
                        obj_name in rpl_filter['replicate_ignore_table']):
                    return True
                if (rpl_filter['replicate_wild_do_table'] and
                        match_regexp(obj_name,
                                     rpl_filter['regexp_do_table'])):
                    return False
                if (rpl_filter['replicate_wild_ignore_table'] and
                        match_regexp(obj_name,
                                     rpl_filter['regexp_ignore_table'])):
                    return True
                if (rpl_filter['replicate_do_table'] or
                        rpl_filter['replicate_wild_do_table']):
                    return True

        # Do not filter replication for object (if no filter rule matched).
        return False

    def _apply_for_all_slaves(self, slaves, function, args=(), kwargs=None,
                              multithreading=False):
        """Apply specified function to all given slaves.

        This function allow the execution (concurrently or not) of the
        specified function with the given arguments on all the specified
        slaves.

        slaves[in]          List of slaves to apply the function. It is assumed
                            that the list is composed by strings with the
                            format 'host@port', identifying each slave.
        function[in]        Name of the function (string) to apply on all
                            slaves.
        args[in]            Tuple with all the function arguments (except
                            keyword arguments).
        kwargs[in]          Dictionary with all the function keyword arguments.
        multithreading[in]  Boolean value indicating if the function will be
                            applied concurrently on all slaves. By default
                            False, no concurrency.

        Return a list of tuples composed by two elements: a string identifying
        the slave ('host@port') and the result of the execution of the target
        function for the corresponding slave.
        """
        if kwargs is None:
            kwargs = {}
        if multithreading:
            # Create a pool of threads to execute the method for each slave.
            pool = ThreadPool(processes=len(slaves))
            thread_res_lst = []
            for slave_key in slaves:
                slave = self._get_slave(slave_key)
                thread_res = pool.apply_async(getattr(slave, function), args,
                                              kwargs)
                thread_res_lst.append((slave_key, thread_res))
            pool.close()
            # Wait for all threads to finish here to avoid RuntimeErrors when
            # waiting for the result of a thread that is already dead.
            pool.join()
            # Get the result from each slave and return the results.
            res = []
            for slave_key, thread_res in thread_res_lst:
                res.append((slave_key, thread_res.get()))
            return res
        else:
            res = []
            for slave_key in slaves:
                slave = self._get_slave(slave_key)
                slave_res = getattr(slave, function)(*args, **kwargs)
                res.append((slave_key, slave_res))
            return res

    def check_server_versions(self):
        """Check server versions.

        Check all server versions and report version differences.
        """
        srv_versions = {}
        # Get the server version of the master if used.
        master = self._get_master()
        if master:
            master_version = master.get_version()
            match = re.match(_RE_VERSION_FORMAT, master_version.strip())
            if match:
                # Add .0 as release version if not provided.
                if not match.group(2):
                    master_version = "{0}.0".format(match.group(1))
                else:
                    master_version = match.group(1)
            master_id = '{0}@{1}'.format(master.host, master.port)
            # Store the master version.
            srv_versions[master_version] = [master_id]

        # Get the server version for all slaves.
        for slave_key in self._slaves:
            slave = self._get_slave(slave_key)
            version = slave.get_version()
            match = re.match(_RE_VERSION_FORMAT, version.strip())
            if match:
                # Add .0 as release version if not provided.
                if not match.group(2):
                    version = "{0}.0".format(match.group(1))
                else:
                    version = match.group(1)
            # Store the slave version.
            if version in srv_versions:
                srv_versions[version].append(slave_key)
            else:
                srv_versions[version] = [slave_key]

        # Check the servers versions and issue a warning if different.
        if len(srv_versions) > 1:
            print("# WARNING: Servers using different versions:")
            for version in srv_versions:
                servers_str = ",".join(srv_versions[version])
                print("# - {0} for {1}.".format(version, servers_str))
            print("#")

    def check_gtid_sync(self):
        """Check GTIDs synchronization.

        Perform several GTID checks (enabled and errant transactions). If the
        master is available (was specified) then it also checks if GTIDs are
        in sync between master and its slaves and report the amount of
        transaction (i.e., GTIDs) behind the master for each slave.

        GTID differences might be an indicator of the existence of data
        consistency issues.

        Note: The master may not be specified, its use is not mandatory.
        """
        # Check if GTIDs are enabled on the topology.
        if self._get_master():  # Use of Master is not mandatory.
            # GTIDs must be enabled on the master.
            if self._get_master().supports_gtid().upper() != 'ON':
                raise UtilError(
                    "Master must support GTIDs and have GTID_MODE=ON."
                )
        # Skip slaves without GTID enabled and warn user.
        reset_base_srv = False
        for slave_key, slave_dict in self._slaves.items():
            slave = slave_dict['instance']
            support_gtid = slave.supports_gtid().upper()
            if support_gtid != 'ON':
                reason = "GTID_MODE=OFF" if support_gtid == 'OFF' \
                    else "not support GTIDs"
                print("# WARNING: Slave '{0}' will be skipped - "
                      "{1}.".format(slave_key, reason))
                print("#")
                del self._slaves[slave_key]
                self._rpl_topology.remove_slave(slave_dict)
                if slave_key == self._base_server_key:
                    reset_base_srv = True
        # At least on slave must have GTIDs enabled.
        if len(self._slaves) == 0:
            raise UtilError("No slaves found with GTID support and "
                            "GTID_MODE=ON.")
        # Reset base server if needed (it must have GTID_MODE=ON).
        if reset_base_srv:
            self._set_base_server()

        # Check the set of executed GTIDs and report differences, only if the
        # master is specified.
        if self._get_master():
            master_gtids = self._get_master().get_gtid_executed()
            slaves_gtids_data = \
                self._rpl_topology.slaves_gtid_subtract_executed(
                    master_gtids, multithreading=True
                )
            print("#\n# GTID differences between Master and Slaves:")
            for host, port, gtids_missing in slaves_gtids_data:
                slave_key = '{0}@{1}'.format(host, port)
                gtid_size = gtid_set_cardinality(gtids_missing)
                if gtid_size:
                    plural = 's' if gtid_size > 1 else ''
                    print("# - Slave '{0}' is {1} transaction{2} behind "
                          "Master.".format(slave_key, gtid_size, plural))
                    if self._verbosity:
                        print("#       Missing GTIDs: "
                              "{0}".format(gtids_missing))
                else:
                    print("# - Slave '{0}' is up-to-date.".format(slave_key))

        print("#")

    @staticmethod
    def _exist_in_obj_list(obj_name, obj_type, obj_list):
        """Check if object (name and type) exists in the given list.

        This function checks if the database object for the specified name and
        type exists in the specified list of database objects.

        obj_name[in]    Name of the object to check.
        obj_type[in]    Type of the object to check.
        obj_list[in]    List of objects to check. It is assumed that the list
                        has the format of the ones returned by the function
                        mysql.utilities.command.dbcompare.get_common_objects().
                        More precisely with the format:
                        [(obj_type1, (obj_name1,))..(obj_typeN, (obj_nameN,))]

        Returns a boolean value indicating if object with the specified name
        and type exists in the specified list of objects.
        """
        for obj_row in obj_list:
            if obj_row[0] == obj_type and obj_row[1][0] == obj_name:
                return True
        return False

    def _split_active_slaves(self, slaves):
        """Get the list of slaves with replication running and not.

        This method separates the list of given slaves into active (with the
        IO and SQL thread running) and non active slaves (with one of the
        threads stopped).

        slaves[in]      List of target slaves to separate.

        Returns a tuple with two elements, first with the list of active slaves
        and the second with the list of not active ones.
        """
        # Get slaves status.
        slaves_state = self._apply_for_all_slaves(slaves, 'get_slaves_errors',
                                                  multithreading=True)

        # Store IO and SQL thread status.
        active_slaves = []
        not_active_slaves = []
        for slave_key, state in slaves_state:
            # Locally store IO and SQL threads status.
            io_running = state[3].upper() == 'YES'
            self._slaves[slave_key]['IO_Running'] = io_running
            sql_running = state[4].upper() == 'YES'
            self._slaves[slave_key]['SQL_Running'] = sql_running
            if io_running and sql_running:
                active_slaves.append(slave_key)
            else:
                not_active_slaves.append(slave_key)
                print("#   WARNING: Slave not active '{0}' - "
                      "Sync skipped.".format(slave_key))
                if self._verbosity:
                    # Print warning if slave is stopped due to an error.
                    if not io_running and state[2]:
                        print("#    - IO thread stopped: ERROR {0} - "
                              "{1}".format(state[1], state[2]))
                    if not sql_running and state[6]:
                        print("#    - SQL thread stopped: ERROR {0} - "
                              "{1}".format(state[5], state[6]))

        # Return separated list of active and non active replication slaves.
        return active_slaves, not_active_slaves

    def _compute_sync_point(self, active_slaves=None, master_uuid=None):
        """Compute the GTID synchronization point.

        This method computes the GTID synchronization point based based on the
        GTID_EXECUTED set. If a master is available for synchronization the
        last GTID from the GTID_EXECUTED set is used as sync point  If no
        master is available the union of the GTID_EXECUTED sets among all
        active slaves is used as the sync point.

        active_slaves[in]   List of active slaves to consider. Only required
                            if the master is not available. It is assumed
                            that the list is composed by strings with the
                            format 'host@port', identifying each slave.
        master_uuid[in]     UUID of the master server used to compute its last
                            GTID (sync point). If not provided it is
                            determined, but can lead to issues for servers
                            >= 5.7.6 if specific tables are locked previously.

        Return a GTID set representing to synchronization point (to wait for
        slaves to catch up and stop).
        """
        if self._get_master():
            gtid_set = self._get_master().get_gtid_executed()
            master_uuid = master_uuid if master_uuid \
                else self._get_master().get_server_uuid()
            return get_last_server_gtid(gtid_set, master_uuid)
        else:
            # Get GTID_EXECUTED on all slaves.
            all_gtid_executed = self._apply_for_all_slaves(
                active_slaves, 'get_gtid_executed', multithreading=True
            )

            # Compute the union of all GTID sets for each UUID among slaves.
            gtid_sets_by_uuid = {}
            for _, gtid_executed in all_gtid_executed:
                gtids_list = gtid_executed.split("\n")
                for gtid in gtids_list:
                    gtid_set = gtid.rstrip(', ')
                    uuid = gtid_set.split(':')[0]
                    if uuid not in gtid_sets_by_uuid:
                        gtid_sets_by_uuid[uuid] = gtid_set
                    else:
                        union_set = gtid_set_union(gtid_sets_by_uuid[uuid],
                                                   gtid_set)
                        gtid_sets_by_uuid[uuid] = union_set

            # Return union of all know executed GTID.
            return ",".join(gtid_sets_by_uuid.itervalues())

    def _sync_slaves(self, slaves, gtid):
        """Set synchronization point (specified GTID set) for the given slaves.

        The method set the synchronization point for the given slaves by
        (concurrently) stopping and immediately executing START SLAVE UNTIL
        on all given slaves in order to stop upon reaching the given GTID set
        (i.e., committing all corresponding transactions for the given GTID
        sync point).

        slaves[in]      List of target slaves to synchronize (i.e., instruct
                        to stop upon reaching the synchronization point).
        gtid[in]        GTID set used as the synchronization point.
        """
        # Make running slaves stop until sync point (GTID) is reached.
        if self._verbosity:
            print("#   Setting data synchronization point for slaves.")
        # STOP slave (only SQL thread).
        self._apply_for_all_slaves(slaves, 'stop_sql_thread',
                                   multithreading=True)
        # START slave UNTIL sync point is reached.
        # Note: Only the SQL thread is stopped when the condition is reached.
        until_ops = {'until_gtid_set': gtid, 'sql_after_gtid': True,
                     'only_sql_thread': True}
        self._apply_for_all_slaves(slaves, 'start', (), until_ops,
                                   multithreading=True)

    def _checksum_and_resume_rpl(self, not_sync_slaves, sync_slave, table):
        """Checksum table and resume replication on slaves.

        This method computes (concurrently) the table checksum of the given
        slaves lists (those synced and not synced). For the list of not synced
        slaves the table checksum is immediately computed. For the list of
        synced slaves, first it waits for them to catch up and the sync point
        and only then compute the table checksum and resume replication.

        not_sync_slaves[in] List of not synced slaves.
        sync_slave[in]      List of (previously) synced slaves.
        table[in]           Target table to compute the checksum.

        Returns a list of tuples, each tuple containing the identification of
        the server and the corresponding checksum result.
        """
        if self._verbosity:
            print("#   Compute checksum on slaves (wait to catch up and resume"
                  " replication).")
            sys.stdout.flush()
        not_sync_checksum = []
        if not_sync_slaves:
            not_sync_checksum = self._apply_for_all_slaves(
                not_sync_slaves, 'checksum_table', (table,),
                {'exec_timeout': self._checksum_timeout},
                multithreading=True
            )
        sync_checksum = []
        if sync_slave:
            sync_checksum = self._apply_for_all_slaves(
                sync_slave, 'wait_checksum_and_start', (table,),
                {'wait_timeout': self._rpl_timeout,
                 'wait_interval': self._interval,
                 'checksum_timeout': self._checksum_timeout},
                multithreading=True
            )
        return not_sync_checksum + sync_checksum

    def _check_table_data_sync(self, table, slaves):
        """Check table data synchronization for specified slaves.

        This method check the data consistency for the specified table between
        the base server (master or slave) and the specified salves. This
        operation requires the definition of a "synchronization point" in order
        to ensure that the "supposed" same data is compared between servers.
        This coordination process is based on GTIDs (checking that all data
        until a given GTID has been processed on the slaves). A different
        algorithm is used to set the "synchronization point" depending if the
        master is used or not. The data consistency is checked relying on the
        CHECKSUM TABLE query.

        If an error occur during this process, any locked table must be
        unlocked and both master and slaves should resume their previous
        activity.

        Important note: this method assumes that the table exists on the base
        server and all specified slaves, therefore checking the existence of
        the table as well as other integrity checks (server versions, GTID
        definitions, etc.) need to be performed outside the scope of this
        method.

        table[in]       Qualified name of the table to check (quoted with
                        backticks).
        slaves[in]      List of slaves to check. Each element of the list must
                        be a string with the format 'host@port'.

        Returns the number of data consistency found.
        """
        success = False
        checksum_issues = 0
        # If no master used then add base server (slave) to slaves to sync.
        if not self._get_master():
            slaves = slaves + [self._base_server_key]

        # Separate active from non active slaves.
        active_slaves, not_active_slaves = self._split_active_slaves(slaves)

        if self._get_master():
            # Get uuid of the master server
            master_uuid = self._get_master().get_server_uuid()

            # Lock the table on the master to get GTID synchronization point
            # and perform the table checksum.
            try:
                self._get_master().exec_query(
                    "LOCK TABLES {0} READ".format(table)
                )

                last_exec_gtid = self._compute_sync_point(
                    master_uuid=master_uuid
                )
                if self._verbosity > 2:
                    print("#   Sync point GTID: {0}".format(last_exec_gtid))

                # Immediately instruct active slaves to stop on sync point.
                if active_slaves:
                    self._sync_slaves(active_slaves, last_exec_gtid)

                # Perform table checksum on master.
                base_server_checksum = self._get_master().checksum_table(
                    table, self._checksum_timeout
                )
                if base_server_checksum[0]:
                    success = True  # Successful checksum for base server.
                    if self._verbosity > 2:
                        print("#   Checksum on base server (Master): "
                              "{0}".format(base_server_checksum[0][1]))
                else:
                    print("#   [SKIP] {0} checksum on base server (Master) - "
                          "{1}".format(table, base_server_checksum[1]))
            finally:
                # Unlock table.
                self._get_master().exec_query("UNLOCK TABLES")
        elif active_slaves:
            # Perform sync without master, only based on active slave (if any).
            try:
                # Stop all active slaves to get the GTID synchronization point.
                self._apply_for_all_slaves(
                    active_slaves, 'stop_sql_thread', multithreading=True
                )

                sync_gtids = self._compute_sync_point(active_slaves)
                if self._verbosity > 2:
                    print("#   Sync point GTID: {0}".format(sync_gtids))

                # Instruct active slaves to stop on sync point.
                self._sync_slaves(active_slaves, sync_gtids)

            except UtilError:
                # Try to restart the slaves in case an error occurs.
                self._apply_for_all_slaves(
                    active_slaves, 'star_sql_thread', multithreading=True
                )

        # Compute checksum on all slaves and return to previous state.
        slaves_checksum = self._checksum_and_resume_rpl(not_active_slaves,
                                                        active_slaves, table)

        # Check if checksum for base server was successfully computed.
        if not self._get_master():
            for slave_key, checksum in slaves_checksum:
                if slave_key == self._base_server_key:
                    if checksum[0]:
                        success = True  # Successful checksum for base server.
                        base_server_checksum = checksum
                        slaves_checksum.remove((slave_key, checksum))
                        if self._verbosity > 2:
                            print("#   Checksum on base server: "
                                  "{0}".format(base_server_checksum[0][1]))
                    else:
                        print("#   [SKIP] {0} checksum on base server - "
                              "{1}".format(table, checksum[1]))
                    break

        # Compare checksum and report results.
        if success and slaves_checksum:
            for slave_key, checksum_res in slaves_checksum:
                if checksum_res[0] is None:
                    print("#   [SKIP] {0} checksum for Slave '{1}' - "
                          "{2}.".format(table, slave_key, checksum_res[1]))
                else:
                    if self._verbosity > 2:
                        checksum_val = ': {0}'.format(checksum_res[0][1])
                    else:
                        checksum_val = ''
                    if checksum_res[0] != base_server_checksum[0]:
                        print("#   [DIFF] {0} checksum for server '{1}'"
                              "{2}.".format(table, slave_key, checksum_val))
                        checksum_issues += 1
                    else:
                        print("#   [OK] {0} checksum for server '{1}'"
                              "{2}.".format(table, slave_key, checksum_val))

        return checksum_issues

    def check_data_sync(self, options, data_to_include, data_to_exclude):
        """Check data synchronization.

        Check if the data (in all tables) is in sync between the checked
        servers (master and its slaves, or only slaves). It reports structure
        difference database/tables missing or with a different definition and
        data differences between a base server and the others.

        Note: A different algorithm is applied to perform the synchronization,
        depending if the master is specified (available) or not.

        options[in]         Dictionary of options.
        data_to_include[in] Dictionary of data (set of tables) by database to
                            check.
        data_to_exclude[in] Dictionary of data (set of tables) by database to
                            exclude from check.

        Returns the number of consistency issues found (comparing database
        definitions and data).
        """
        issues_count = 0

        # Skip all database objects, except tables.
        options['skip_views'] = True
        options['skip_triggers'] = True
        options['skip_procs'] = True
        options['skip_funcs'] = True
        options['skip_events'] = True
        options['skip_grants'] = True

        diff_options = {}
        diff_options.update(options)
        diff_options['quiet'] = True  # Do not print messages.
        diff_options['suppress_sql'] = True  # Do not print SQL statements.
        diff_options['skip_table_opts'] = True  # Ignore AUTO_INCREMENT diffs.

        # Check the server version requirement to support sync features.
        # Slave servers of version >= 5.6.14 are required due to a known issue
        # for START SLAVE UNTIL with the SQL_AFTER_GTIDS option. More info:
        # https://dev.mysql.com/doc/refman/5.6/en/start-slave.html
        for slave_key in self._slaves:
            if not self._get_slave(slave_key).check_version_compat(5, 6, 14):
                raise UtilError(
                    "Server '{0}' version must be 5.6.14 or greater. Sync is "
                    "not supported for versions prior to 5.6.14 due to a "
                    "known issue with START SLAVE UNTIL and the "
                    "SQL_AFTER_GTIDS option.".format(slave_key))

        print("# Checking data consistency.\n#")
        base_srv_type = 'Master' if self._get_master() else 'Slave'
        print("# Using {0} '{1}' as base server for comparison."
              "".format(base_srv_type, self._base_server_key))

        # Get all databases from the base server.
        db_rows = self._base_server.get_all_databases()
        base_server_dbs = set([row[0] for row in db_rows])

        # Process databases to include/exclude from check.
        db_to_include = set()
        if data_to_include:
            db_to_include = set([db for db in data_to_include])
            base_server_dbs = base_server_dbs & db_to_include
            not_exist_db = db_to_include - base_server_dbs
            if not_exist_db:
                plurals = ('s', '') if len(not_exist_db) > 1 else ('', 'es')
                print('# WARNING: specified database{0} to check do{1} not '
                      'exist on base server and will be skipped: '
                      '{2}.'.format(plurals[0], plurals[1],
                                    ", ".join(not_exist_db)))
        db_to_exclude = set()
        if data_to_exclude:
            db_to_exclude = set(
                [db for db in data_to_exclude if not data_to_exclude[db]]
            )
            base_server_dbs = base_server_dbs - db_to_exclude

        # Check databases on slaves (except the base server).
        slaves_except_base = [key for key in self._slaves
                              if key != self._base_server_key]
        for slave_key in slaves_except_base:
            slave = self._get_slave(slave_key)
            db_rows = slave.get_all_databases()
            slave_dbs = set([row[0] for row in db_rows])
            # Process databases to include/exclude.
            if db_to_include:
                slave_dbs = slave_dbs & db_to_include
            if db_to_exclude:
                slave_dbs = slave_dbs - db_to_exclude
            # Add slave databases set to internal state.
            self._slaves[slave_key]['databases'] = slave_dbs
            # Report databases not on base server and filtered by replication.
            dbs_not_in_base_srv = slave_dbs - base_server_dbs
            filtered_dbs = set(
                [db for db in dbs_not_in_base_srv
                 if self._is_rpl_filtered(db, slave=self._base_server_key)]
            )
            dbs_not_in_base_srv -= filtered_dbs
            for db in filtered_dbs:
                print("# [SKIP] Database '{0}' - filtered by replication "
                      "rule on base server.".format(db))
            if dbs_not_in_base_srv:
                issues_count += len(dbs_not_in_base_srv)
                plural = 's' if len(dbs_not_in_base_srv) > 1 else ''
                print("# [DIFF] Database{0} NOT on base server but found on "
                      "'{1}': {2}".format(plural, slave_key,
                                          ",".join(dbs_not_in_base_srv)))

        # Determine server to check base replication filtering options.
        filter_srv = None if self._get_master() else self._base_server_key

        # Check data consistency for each table on the base server.
        # pylint: disable=R0101
        for db_name in base_server_dbs:
            # Skip database if filtered by defined replication rules.
            if self._is_rpl_filtered(db_name, slave=filter_srv):
                print("# [SKIP] Database '{0}' check - filtered by "
                      "replication rule.".format(db_name))
                continue
            print("# Checking '{0}' database...".format(db_name))
            slaves_to_check = {}
            # Check if database exists on slaves (except the base server).
            for slave_key in slaves_except_base:
                # Skip database if filtered by defined replication rules.
                if self._is_rpl_filtered(db_name, slave=slave_key):
                    print("# [SKIP] Database '{0}' check for '{1}' - filtered "
                          "by replication rule.".format(db_name, slave_key))
                    continue
                if db_name in self._slaves[slave_key]['databases']:
                    # Store slave database instance and common objects.
                    slave_db = Database(self._get_slave(slave_key), db_name,
                                        options)
                    slave_db.init()
                    slave_dic = {'db': slave_db}
                    in_both, in_basesrv, not_in_basesrv = get_common_objects(
                        self._base_server, self._get_slave(slave_key),
                        db_name, db_name, False, options)
                    # Process tables to include/exclude from check (on slaves).
                    if (data_to_include and db_name in data_to_include and
                            data_to_include[db_name]):
                        in_both = [
                            obj_row for obj_row in in_both
                            if obj_row[1][0] in data_to_include[db_name]
                        ]
                        in_basesrv = [
                            obj_row for obj_row in in_basesrv
                            if obj_row[1][0] in data_to_include[db_name]
                        ]
                        not_in_basesrv = [
                            obj_row for obj_row in not_in_basesrv
                            if obj_row[1][0] in data_to_include[db_name]
                        ]
                    if (data_to_exclude and db_name in data_to_exclude and
                            data_to_exclude[db_name]):
                        in_both = [
                            obj_row for obj_row in in_both
                            if obj_row[1][0] not in data_to_exclude[db_name]
                        ]
                        in_basesrv = [
                            obj_row for obj_row in in_basesrv
                            if obj_row[1][0] not in data_to_exclude[db_name]
                        ]
                        not_in_basesrv = [
                            obj_row for obj_row in not_in_basesrv
                            if obj_row[1][0] not in data_to_exclude[db_name]
                        ]
                    slave_dic['in_both'] = in_both
                    slave_dic['in_basesrv'] = in_basesrv
                    slaves_to_check[slave_key] = slave_dic
                    # Report tables not on base server and filtered by
                    # replication.
                    tbls_not_in = set(
                        [obj_row[1][0] for obj_row in not_in_basesrv
                         if obj_row[0] == 'TABLE']
                    )
                    filtered_tbls = set(
                        [tbl for tbl in tbls_not_in if self._is_rpl_filtered(
                            db_name, tbl_name=tbl, slave=self._base_server_key
                        )]
                    )
                    tbls_not_in -= filtered_tbls
                    for tbl in filtered_tbls:
                        print("# [SKIP] Table '{0}' - filtered by replication "
                              "rule on base server.".format(tbl))
                    if tbls_not_in:
                        plural = 's' if len(tbls_not_in) > 1 else ''
                        print("#   [DIFF] Table{0} NOT on base server but "
                              "found on '{1}': "
                              "{2}".format(plural, slave_key,
                                           ", ".join(tbls_not_in)))
                        issues_count += len(tbls_not_in)
                else:
                    print("#   [DIFF] Database '{0}' NOT on server "
                          "'{1}'.".format(db_name, slave_key))
                    issues_count += 1
            # Only check database if at least one slave has it.
            if slaves_to_check:
                db = Database(self._base_server, db_name, options)
                db.init()
                for db_obj in db.get_next_object():
                    obj_type = db_obj[0]
                    obj_name = db_obj[1][0]
                    # Process tables to include/exclude from check (on base
                    # server).
                    if (data_to_include and db_name in data_to_include and
                            data_to_include[db_name] and
                            obj_name not in data_to_include[db_name]):
                        # Skip to the next object if not in data to include.
                        continue
                    if (data_to_exclude and db_name in data_to_exclude and
                            data_to_exclude[db_name] and
                            obj_name in data_to_exclude[db_name]):
                        # Skip to the next object if in data to exclude.
                        continue
                    checksum_task = []
                    # Check object data on all valid slaves.
                    for slave_key in slaves_to_check:
                        # Skip table if filtered by defined replication rules.
                        if (obj_type == 'TABLE' and
                                self._is_rpl_filtered(db_name, obj_name,
                                                      slave=slave_key)):
                            print("# [SKIP] Table '{0}' check for '{1}' - "
                                  "filtered by replication rule."
                                  "".format(obj_name, slave_key))
                            continue
                        slave_dic = slaves_to_check[slave_key]
                        # Check if object does not exist on Slave.
                        if self._exist_in_obj_list(obj_name, obj_type,
                                                   slave_dic['in_basesrv']):
                            print("#   [DIFF] {0} '{1}.{2}' NOT on server "
                                  "'{3}'.".format(obj_type.capitalize(),
                                                  db_name, obj_name,
                                                  slave_key))
                            issues_count += 1
                            continue

                        # Quote object name with backticks.
                        q_obj = '{0}.{1}'.format(
                            quote_with_backticks(db_name, db.sql_mode),
                            quote_with_backticks(obj_name, db.sql_mode)
                        )

                        # Check object definition.
                        def_diff = diff_objects(
                            self._base_server, self._get_slave(slave_key),
                            q_obj, q_obj, diff_options, obj_type
                        )
                        if def_diff:
                            print("#   [DIFF] {0} {1} definition is "
                                  "different on '{2}'."
                                  "".format(obj_type.capitalize(), q_obj,
                                            slave_key))
                            issues_count += 1
                            if self._verbosity:
                                for diff in def_diff[3:]:
                                    print("#       {0}".format(diff))
                            continue

                        # Add slave to table checksum task.
                        checksum_task.append(slave_key)

                    # Perform table checksum on valid slaves.
                    if checksum_task and obj_type == 'TABLE':
                        print("# - Checking '{0}' table data..."
                              "".format(obj_name))
                        num_issues = self._check_table_data_sync(q_obj,
                                                                 checksum_task)
                        issues_count += num_issues

        print("#\n#...done.\n#")
        str_issues_count = 'No' if issues_count == 0 else str(issues_count)
        plural = 's' if issues_count > 1 else ''
        print("# SUMMARY: {0} data consistency issue{1} found.\n"
              "#".format(str_issues_count, plural))
        return issues_count
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains an abstraction of a MySQL server object used
by multiple utilities. It also contains helper methods for common
server operations used in multiple utilities.
"""

import os
import re
import socket
import string
import subprocess
import tempfile
import threading
import logging

import mysql.connector
from mysql.connector.constants import ClientFlag

from mysql.connector.errorcode import CR_SERVER_LOST


_FOREIGN_KEY_SET = "SET foreign_key_checks = {0}"
_AUTOCOMMIT_SET = "SET AUTOCOMMIT = {0}"
_GTID_ERROR = ("The server %s:%s does not comply to the latest GTID "
               "feature support. Errors:")


def tostr(value):
    """Cast value to str except when None

    value[in]          Value to be cast to str

    Returns value as str instance or None.
    """
    return None if value is None else str(value)


class MySQLUtilsCursorRaw(mysql.connector.cursor.MySQLCursorRaw):
    """
    Cursor for Connector/Python v2.0, returning str instead of bytearray
    """
    def fetchone(self):
        row = self._fetch_row()
        if row:
            return tuple([tostr(v) for v in row])
        return None

    def fetchall(self):
        rows = []
        all_rows = super(MySQLUtilsCursorRaw, self).fetchall()
        for row in all_rows:
            rows.append(tuple([tostr(v) for v in row]))
        return rows


class MySQLUtilsCursorBufferedRaw(
        mysql.connector.cursor.MySQLCursorBufferedRaw):
    """
    Cursor for Connector/Python v2.0, returning str instead of bytearray
    """
    def fetchone(self):
        row = self._fetch_row()
        if row:
            return tuple([tostr(v) for v in row])
        return None

    def fetchall(self):
        if self._rows is None:
            raise mysql.connector.InterfaceError(
                "No result set to fetch from."
            )

        rows = []
        all_rows = [r for r in self._rows[self._next_row:]]
        for row in all_rows:
            rows.append(tuple([tostr(v) for v in row]))
        return rows


def get_connection_dictionary(conn_info, ssl_dict=None):
    """Get the connection dictionary.

    The method accepts one of the following types for conn_info:

        - dictionary containing connection information including:
          (user, passwd, host, port, socket)
        - connection string in the form: user:pass@host:port:socket or
                                         login-path:port:socket
        - an instance of the Server class

    conn_info[in]          Connection information
    ssl_dict[in]           A dictionary with the ssl certificates
                           (ssl_ca, ssl_cert and ssl_key).

    Returns dict - dictionary for connection (user, passwd, host, port, socket)
    """
    if conn_info is None:
        return conn_info

    conn_val = {}
    if isinstance(conn_info, dict) and 'host' in conn_info:
        # Not update conn_info if already has any ssl certificate.
        if (ssl_dict is not None and
                not (conn_info.get("ssl_ca", None) or
                     conn_info.get("ssl_cert", None) or
                     conn_info.get("ssl_key", None) or
                     conn_info.get("ssl", None))):
            conn_info.update(ssl_dict)
        conn_val = conn_info
    elif isinstance(conn_info, Server):
        # get server's dictionary
        conn_val = conn_info.get_connection_values()
    elif isinstance(conn_info, basestring):
        # parse the string
        conn_val = parse_connection(conn_info, options=ssl_dict)
    else:
        raise ConnectionValuesError("Cannot determine connection information"
                                    " type.")

    return conn_val


def set_ssl_opts_in_connection_info(ssl_opts, connection_info):
    """Sets the ssl options in a connection information to be used with C/py.

    ssl_opts[in]           A dictionary with the ssl options (ssl_ca, ssl_cert
                           and ssl_key).
    connection_info[out]   A dictionary to set the ssl options after validate
                           them.

    The ssl options will be set on the connection_info if they are not None.
    In addition the SSL client flags are added if at least one ssl option is
    set.
    """
    # Add SSL parameters ONLY if they are not None
    add_ssl_flag = False
    if ssl_opts.get('ssl_ca') is not None:
        connection_info['ssl_ca'] = ssl_opts.get('ssl_ca')
        add_ssl_flag = True
    if ssl_opts.get('ssl_cert') is not None:
        connection_info['ssl_cert'] = ssl_opts.get('ssl_cert')
        add_ssl_flag = True
    if ssl_opts.get('ssl_key') is not None:
        connection_info['ssl_key'] = ssl_opts.get('ssl_key')
        add_ssl_flag = True
    if ssl_opts.get('ssl'):
        add_ssl_flag = True

    # When at least one of cert, key or ssl options are specified, the ca
    # option is not required for establishing the encrypted connection,
    # but C/py will not allow the None value for the ca option, so we use an
    # empty string i.e '' to avoid an error from C/py about ca option being
    # the None value.
    if ('ssl_cert' in connection_info.keys() or
            'ssl_key' in connection_info.keys() or
            ssl_opts.get('ssl')) and \
            'ssl_ca' not in connection_info.keys():
        connection_info['ssl_ca'] = ''

    # The ca certificate is verified only if the ssl option is also specified.
    if ssl_opts.get('ssl') and connection_info['ssl_ca']:
        connection_info['ssl_verify_cert'] = True

    if add_ssl_flag:
        cpy_flags = [ClientFlag.SSL,
                     ClientFlag.SSL_VERIFY_SERVER_CERT]
        connection_info['client_flags'] = cpy_flags


def _print_connection(prefix, conn_info):
    """Print connection information

    The method accepts one of the following types for conn_info:

        - dictionary containing connection information including:
          (user, passwd, host, port, socket)
        - connection string in the form: user:pass@host:port:socket or
                                         login-path:port:socket
        - an instance of the Server class

    conn_info[in]          Connection information
    """
    conn_val = get_connection_dictionary(conn_info)
    print "# %s on %s: ..." % (prefix, conn_val["host"]),


def get_local_servers(all_proc=False, start=3306, end=3333,
                      datadir_prefix=None):
    """Check to see if there are any servers running on the local host.

    This method attempts to locate all running servers. If provided, it will
    also limit the search to specific ports of datadirectory prefixes.

    This method uses ps for posix systems and netstat for Windows machines
    to determine the list of running servers.

    For posix, it matches on the datadir and if datadir is the path for the
    test directory, the server will be added to the list.

    For nt, it matches on the port in the range starting_port,
    starting_port + 10.

    all_proc[in]        If True, find all processes else only user processes
    start[in]           For Windows/NT systems: Starting port value to search.
                        Default = 3306
    end[in]             For Windows/NT systems: Ending port value to search.
                        Default = 3333
    datadir_prefix[in]  For posix systems, if not None, find only those servers
                        whose datadir starts with this prefix.

    Returns list - tuples of the form: (process_id, [datadir|port])
    """
    processes = []
    if os.name == "posix":
        tmp_file = tempfile.TemporaryFile()
        if all_proc:
            subprocess.call(["ps", "-A"], stdout=tmp_file)
        else:
            subprocess.call(["ps", "-f"], stdout=tmp_file)
        tmp_file.seek(0)
        for line in tmp_file.readlines():
            mysqld_safe = False
            mysqld = False
            datadir = False
            grep = False
            datadir_arg = ""
            proginfo = string.split(line)
            for arg in proginfo:
                if "datadir" in arg:
                    datadir = True
                    datadir_arg = arg
                if "mysqld" in arg:
                    mysqld = True
                if "mysqld_safe" in arg:
                    mysqld_safe = True
                if "grep" in arg:
                    grep = True
            # Check to see if this is a mysqld server and not mysqld_safe proc
            if ((mysqld and datadir) or (mysqld and not grep)) and \
               not mysqld_safe:
                # If provided, check datadir prefix
                if all_proc:
                    proc_id = proginfo[0]
                else:
                    proc_id = proginfo[1]
                if datadir_prefix is not None:
                    if datadir_prefix in datadir_arg:
                        processes.append((proc_id, datadir_arg[10:]))
                else:
                    processes.append((proc_id, datadir_arg[10:]))
    elif os.name == "nt":
        f_out = open("portlist", 'w+')
        execute_script("netstat -anop tcp", "portlist")
        f_out = open("portlist", 'r')
        for line in f_out.readlines():
            proginfo = string.split(line)
            if proginfo:
                # Look for port on either local or foreign address
                port = proginfo[1][proginfo[1].find(":") + 1:]
                if proginfo[1][0] == '0' and port.isdigit():
                    if int(port) >= int(start) and int(port) <= int(end):
                        processes.append((proginfo[4], port))
                        break
                if len(proginfo) > 2:
                    port = proginfo[2][proginfo[2].find(":") + 1:]
                    if port.isdigit() and \
                       int(port) >= int(start) and int(port) <= int(end):
                        processes.append((proginfo[4], port))
                        break
        f_out.close()
        os.unlink("portlist")
    return processes


def get_server(name, values, quiet, verbose=False):
    """Connect to a server and return Server instance

    If the name is 'master' or 'slave', the connection will be made via the
    Master or Slave class else a normal Server class shall be used.

    name[in]        Name of the server.
    values[in]      Dictionary of connection values.
    quiet[in]       If True, do not print messages.
    verbose[in]     Verbose value used by the returned server instances.
                    By default False.

    Returns Server class instance
    """
    from mysql.utilities.common.replication import Master, Slave

    server_conn = None

    # Try to connect to the MySQL database server.
    if not quiet:
        _print_connection(name, values)

    server_options = {
        'conn_info': values,
        'role': name,
        'verbose': verbose,
    }
    if name.lower() == 'master':
        server_conn = Master(server_options)
    elif name.lower() == 'slave':
        # pylint: disable=R0204
        server_conn = Slave(server_options)
    else:
        # pylint: disable=R0204
        server_conn = Server(server_options)
    try:
        server_conn.connect()
    except:
        if not quiet:
            print("")
        raise

    return server_conn


def _require_version(server, version):
    """Check version of server

    server[in]         Server instance
    version[in]        minimal version of the server required

    Returns boolean - True = version Ok, False = version < required
    """
    if version is not None and server is not None:
        major, minor, rel = version.split(".")
        if not server.check_version_compat(major, minor, rel):
            return False
    return True


def get_server_state(server, host, pingtime=3, verbose=False):
    """Return the state of the server.

    This method returns one of the following states based on the
    criteria shown.

      UP   - server is connected
      WARN - server is not connected but can be pinged
      DOWN - server cannot be pinged nor is connected

    server[in]     Server class instance
    host[in]       host name to ping if server is not connected
    pingtime[in]   timeout in seconds for ping operation
                   Default = 3 seconds
    verbose[in]    if True, show ping status messages
                   Default = False

    Returns string - state
    """
    if verbose:
        print "# Attempting to contact %s ..." % host,
    if server is not None and server.is_alive():
        if verbose:
            print "Success"
        return "UP"
    elif ping_host(host, pingtime):
        if verbose:
            print "Server is reachable"
        return "WARN"
    if verbose:
        print "FAIL"
    return "DOWN"


def connect_servers(src_val, dest_val, options=None):
    """Connect to a source and destination server.

    This method takes two groups of --server=user:password@host:port:socket
    values and attempts to connect one as a source connection and the other
    as the destination connection. If the source and destination are the
    same server and the unique parameter is False, destination is set to None.

    The method accepts one of the following types for the src_val and dest_val:

        - dictionary containing connection information including:
          (user, passwd, host, port, socket)
        - connection string in the form: user:pass@host:port:socket or
                                         login-path:port:socket or
                                         config-path[group]
        - an instance of the Server class

    src_val[in]        source connection information
    dest_val[in]       destination connection information
    options[in]        options to control behavior:
        quiet          do not print any information during the operation
                       (default is False)
        version        if specified (default is None), perform version
                       checking and fail if server version is < version
                       specified - an exception is raised
        src_name       name to use for source server
                       (default is "Source")
        dest_name      name to use for destination server
                       (default is "Destination")
        unique         if True, servers must be different when dest_val is
                       not None (default is False)
        verbose        Verbose value used by the returned server instances
                       (default is False).

    Returns tuple (source, destination) where
            source = connection to source server
            destination = connection to destination server (set to None)
                          if source and destination are same server
            if error, returns (None, None)
    """
    if options is None:
        options = {}
    quiet = options.get("quiet", False)
    src_name = options.get("src_name", "Source")
    dest_name = options.get("dest_name", "Destination")
    version = options.get("version", None)
    charset = options.get("charset", None)
    verbose = options.get('verbose', False)

    ssl_dict = {}
    if options.get("ssl_cert", None) is not None:
        ssl_dict['ssl_cert'] = options.get("ssl_cert")
    if options.get("ssl_ca", None) is not None:
        ssl_dict['ssl_ca'] = options.get("ssl_ca", None)
    if options.get("ssl_key", None) is not None:
        ssl_dict['ssl_key'] = options.get("ssl_key", None)
    if options.get("ssl", None) is not None:
        ssl_dict['ssl'] = options.get("ssl", None)

    source = None
    destination = None

    # Get connection dictionaries
    src_dict = get_connection_dictionary(src_val, ssl_dict)
    if "]" in src_dict['host']:
        src_dict['host'] = clean_IPv6(src_dict['host'])
    dest_dict = get_connection_dictionary(dest_val)
    if dest_dict and "]" in dest_dict['host']:
        dest_dict['host'] = clean_IPv6(dest_dict['host'])

    # Add character set
    if src_dict and charset:
        src_dict["charset"] = charset
    if dest_dict and charset:
        dest_dict["charset"] = charset

    # Check for uniqueness - dictionary
    if options.get("unique", False) and dest_dict is not None:
        dupes = False
        if "unix_socket" in src_dict and "unix_socket" in dest_dict:
            dupes = (src_dict["unix_socket"] == dest_dict["unix_socket"])
        else:
            dupes = (src_dict["port"] == dest_dict["port"]) and \
                    (src_dict["host"] == dest_dict["host"])
        if dupes:
            raise UtilError("You must specify two different servers "
                            "for the operation.")

    # If we're cloning so use same server for faster copy
    cloning = dest_dict is None or (src_dict == dest_dict)

    # Connect to the source server and check version
    if isinstance(src_val, Server):
        source = src_val
    else:
        source = get_server(src_name, src_dict, quiet, verbose=verbose)
        if not quiet:
            print "connected."
    if not _require_version(source, version):
        raise UtilError("The %s version is incompatible. Utility "
                        "requires version %s or higher." %
                        (src_name, version))

    # If not cloning, connect to the destination server and check version
    if not cloning:
        if isinstance(dest_val, Server):
            destination = dest_val
        else:
            destination = get_server(dest_name, dest_dict, quiet,
                                     verbose=verbose)
            if not quiet:
                print "connected."
        if not _require_version(destination, version):
            raise UtilError("The %s version is incompatible. Utility "
                            "requires version %s or higher." %
                            (dest_name, version))
    elif not quiet and dest_dict is not None and \
            not isinstance(dest_val, Server):
        try:
            _print_connection(dest_name, src_dict)
            print "connected."
        except:
            print("")
            raise
    return (source, destination)


def test_connect(conn_info, throw_errors=False, ssl_dict=None):
    """Test connection to a server.

    The method accepts one of the following types for conn_info:

        - dictionary containing connection information including:
          (user, passwd, host, port, socket)
        - connection string in the form: user:pass@host:port:socket or
                                         login-path:port:socket or
                                         config-path[group]
        - an instance of the Server class

    conn_info[in]          Connection information

    throw_errors           throw any errors found during the test,
                           false by default.
    ssl_dict[in]           A dictionary with the ssl certificates
                           (ssl_ca, ssl_cert and ssl_key).

    Returns True if connection success, False if error
    """
    # Parse source connection values
    try:
        src_val = get_connection_dictionary(conn_info, ssl_dict)
    except Exception as err:
        raise ConnectionValuesError("Server connection values invalid: {0}."
                                    "".format(err))
    try:
        conn_options = {
            'quiet': True,
            'src_name': "test",
            'dest_name': None,
        }
        s = connect_servers(src_val, None, conn_options)
        s[0].disconnect()
    except UtilError:
        if throw_errors:
            raise
        return False
    return True


def get_port(server1_vals):
    """Get the port for a connection using a socket.

    This method attempts to connect to a server to retrieve
    its port. It is used to try and update local connection
    values with a valid port number for servers connected
    via a socket.

    server1_vals[in]   connection dictionary for server1

    Returns string - port for server or None if cannot connect
                     or server is not connected via socket
    """
    socket = server1_vals.get('unix_socket', None)
    if socket:
        try:
            server1 = Server({'conn_info': server1_vals})
            server1.connect()
            port = server1.port
            server1.disconnect()
            return port
        except:
            pass
    return None


def check_hostname_alias(server1_vals, server2_vals):
    """Check to see if the servers are the same machine by host name.

    This method will attempt to compare two servers to see
    if they are the same host and port. However, if either is
    using a unix socket, it will connect to the server and attempt
    so that the port is updated.

    server1_vals[in]   connection dictionary for server1
    server2_vals[in]   connection dictionary for server2

    Returns bool - true = server1 and server2 are the same host
    """
    server1 = Server({'conn_info': server1_vals})
    server2 = Server({'conn_info': server2_vals})
    server1_socket = server1_vals.get('unix_socket', None)
    server2_socket = server1_vals.get('unix_socket', None)
    if server1_socket:
        server1.connect()
        server1.disconnect()
    if server2_socket:
        server2.connect()
        server2.disconnect()

    return (server1.is_alias(server2.host) and
            int(server1.port) == int(server2.port))


def stop_running_server(server, wait=10, drop=True):
    """Stop a running server.

    This method will stop a server using the mysqladmin utility to
    shutdown the server. It also destroys the datadir.

    server[in]          Server instance to clone
    wait[in]            Number of wait cycles for shutdown
                        default = 10
    drop[in]            If True, drop datadir

    Returns - True = server shutdown, False - unknown state or error
    """
    # Nothing to do if server is None
    if server is None:
        return True

    # Build the shutdown command
    res = server.show_server_variable("basedir")
    mysqladmin_client = "mysqladmin"
    if os.name != 'posix':
        mysqladmin_client = "mysqladmin.exe"
    mysqladmin_path = os.path.normpath(os.path.join(res[0][1], "bin",
                                                    mysqladmin_client))
    if not os.path.exists(mysqladmin_path):
        mysqladmin_path = os.path.normpath(os.path.join(res[0][1], "client",
                                                        mysqladmin_client))
    if not os.path.exists(mysqladmin_path) and os.name != 'posix':
        mysqladmin_path = os.path.normpath(os.path.join(res[0][1],
                                                        "client/debug",
                                                        mysqladmin_client))
    if not os.path.exists(mysqladmin_path) and os.name != 'posix':
        mysqladmin_path = os.path.normpath(os.path.join(res[0][1],
                                                        "client/release",
                                                        mysqladmin_client))
    if os.name == 'posix':
        cmd = "'{0}'".format(mysqladmin_path)
    else:
        cmd = '"{0}"'.format(mysqladmin_path)
    if server.socket is None and server.host == 'localhost':
        server.host = '127.0.0.1'
    cmd = "{0} shutdown --user={1} --host={2} ".format(cmd, server.user,
                                                       server.host)
    if server.passwd:
        cmd = "{0} --password={1} ".format(cmd, server.passwd)
    # Use of server socket only works with 'localhost' (not with 127.0.0.1).
    if server.socket and server.host == 'localhost':
        cmd = "{0} --socket={1} ".format(cmd, server.socket)
    else:
        cmd = "{0} --port={1} ".format(cmd, server.port)
    if server.has_ssl:
        if server.ssl_cert is not None:
            cmd = "{0} --ssl-cert={1} ".format(cmd, server.ssl_cert)
        if server.ssl_ca is not None:
            cmd = "{0} --ssl-ca={1} ".format(cmd, server.ssl_ca)
        if server.ssl_key is not None:
            cmd = "{0} --ssl-key={1} ".format(cmd, server.ssl_key)

    res = server.show_server_variable("datadir")
    datadir = os.path.normpath(res[0][1])
    # Kill all connections so shutdown will work correctly
    res = server.exec_query("SHOW PROCESSLIST")
    for row in res:
        if not row[7] or not row[7].upper().startswith("SHOW PROCESS"):
            try:
                server.exec_query("KILL CONNECTION %s" % row[0])
            except UtilDBError:  # Ok to ignore KILL failures
                pass

    # disconnect user
    server.disconnect()

    # Stop the server
    f_null = os.devnull
    f_out = open(f_null, 'w')
    proc = subprocess.Popen(cmd, shell=True,
                            stdout=f_out, stderr=f_out)
    ret_val = proc.wait()
    f_out.close()

    # if shutdown doesn't work, exit.
    if int(ret_val) != 0:
        return False

    # If datadir exists, delete it
    if drop:
        delete_directory(datadir)

    if os.path.exists("cmd.txt"):
        try:
            os.unlink("cmd.txt")
        except:
            pass

    return True


def log_server_version(server, level=logging.INFO):
    """Log server version message.

    This method will log the server version message.
    If no log file is provided it will also print the message to stdout.

    server[in]           Server instance.
    level[in]            Level of message to log. Default = INFO.
    print_version[in]    If True, print the message to stdout. Default = True.
    """
    host_port = "{host}:{port}".format(**get_connection_dictionary(server))
    version_msg = MSG_MYSQL_VERSION.format(server=host_port,
                                           version=server.get_version())
    logging.log(level, version_msg)


class Server(object):
    """The Server class can be used to connect to a running MySQL server.
    The following utilities are provided:

        - Connect to the server
        - Retrieve a server variable
        - Execute a query
        - Return list of all databases
        - Return a list of specific objects for a database
        - Return list of a specific objects for a database
        - Return list of all indexes for a table
        - Read SQL statements from a file and execute
    """

    def __init__(self, options=None):
        """Constructor

        The method accepts one of the following types for options['conn_info']:

            - dictionary containing connection information including:
              (user, passwd, host, port, socket)
            - connection string in the form: user:pass@host:port:socket or
                                             login-path:port:socket
            - an instance of the Server class

        options[in]        options for controlling behavior:
            conn_info      a dictionary containing connection information
                           (user, passwd, host, port, socket)
            role           Name or role of server (e.g., server, master)
            verbose        print extra data during operations (optional)
                           default value = False
            charset        Default character set for the connection.
                           (default None)
        """
        if options is None:
            options = {}

        assert options.get("conn_info") is not None

        self.verbose = options.get("verbose", False)
        self.db_conn = None
        self.host = None
        self.role = options.get("role", "Server")
        self.has_ssl = False
        conn_values = get_connection_dictionary(options.get("conn_info"))
        try:
            self.host = conn_values["host"]
            self.user = conn_values["user"]
            self.passwd = conn_values["passwd"] \
                if "passwd" in conn_values else None
            self.socket = conn_values["unix_socket"] \
                if "unix_socket" in conn_values else None
            self.port = 3306
            if conn_values["port"] is not None:
                self.port = int(conn_values["port"])
            self.charset = options.get("charset",
                                       conn_values.get("charset", None))
            # Optional values
            self.ssl_ca = conn_values.get('ssl_ca', None)
            self.ssl_cert = conn_values.get('ssl_cert', None)
            self.ssl_key = conn_values.get('ssl_key', None)
            self.ssl = conn_values.get('ssl', False)
            if self.ssl_cert or self.ssl_ca or self.ssl_key or self.ssl:
                self.has_ssl = True
        except KeyError:
            raise UtilError("Dictionary format not recognized.")
        self.connect_error = None
        # Set to TRUE when foreign key checks are ON. Check with
        # foreign_key_checks_enabled.
        self.fkeys = None
        self.autocommit = None
        self.read_only = False
        self.aliases = set()
        self.grants_enabled = None
        self._version = None

    @classmethod
    def fromServer(cls, server, conn_info=None):
        """ Create a new server instance from an existing one

        Factory method that will allow the creation of a new server instance
        from an existing server.

        server[in]       instance object that must be instance of the Server
                         class or a subclass.
        conn_info[in]    A dictionary with the connection information to
                         connect to the server

        Returns an instance of the calling class as a result.
        """

        if isinstance(server, Server):
            options = {"role": server.role,
                       "verbose": server.verbose,
                       "charset": server.charset}
            if conn_info is not None and isinstance(conn_info, dict):
                options["conn_info"] = conn_info
            else:
                options["conn_info"] = server.get_connection_values()

            return cls(options)
        else:
            raise TypeError("The server argument's type is neither Server nor "
                            "a subclass of Server")

    def is_alive(self):
        """Determine if connection to server is still alive.

        Returns bool - True = alive, False = error or cannot connect.
        """
        res = True
        try:
            if self.db_conn is None:
                res = False
            else:
                # ping and is_connected only work partially, try exec_query
                # to make sure connection is really alive
                retval = self.db_conn.is_connected()
                if retval:
                    self.exec_query("SHOW DATABASES")
                else:
                    res = False
        except:
            res = False
        return res

    def _update_alias(self, ip_or_hostname, suffix_list):
        """Update list of aliases for the given IP or hostname.

        Gets the list of aliases for host *ip_or_hostname*. If any
        of them matches one of the server's aliases, then update
        the list of aliases (self.aliases). It also receives a list (tuple)
        of suffixes that can be ignored when checking if two hostnames are
        the same.

        ip_or_hostname[in] IP or hostname to test.
        suffix_list[in]    Tuple with list of suffixes that can be ignored.

        Returns True if ip_or_hostname is a server alias, otherwise False.
        """
        host_or_ip_aliases = self._get_aliases(ip_or_hostname)
        host_or_ip_aliases.add(ip_or_hostname)

        # Check if any of aliases matches with one the servers's aliases
        common_alias = self.aliases.intersection(host_or_ip_aliases)
        if common_alias:  # There are common aliases, host is the same
            self.aliases.update(host_or_ip_aliases)
            return True
        else:  # Check with and without suffixes
            no_suffix_server_aliases = set()
            no_suffix_host_aliases = set()

            for suffix in suffix_list:
                # Add alias with and without suffix from self.aliases
                for alias in self.aliases:
                    if alias.endswith(suffix):
                        try:
                            host, _ = alias.rsplit('.', 1)
                            no_suffix_host_aliases.add(host)
                        except:
                            pass  # Ok if parts don't split correctly
                    no_suffix_server_aliases.add(alias)
                # Add alias with and without suffix from host_aliases
                for alias in host_or_ip_aliases:
                    if alias.endswith(suffix):
                        try:
                            host, _ = alias.rsplit('.', 1)
                            no_suffix_host_aliases.add(host)
                        except:
                            pass  # Ok if parts don't split correctly
                    no_suffix_host_aliases.add(alias)
            # Check if there is any alias in common
            common_alias = no_suffix_host_aliases.intersection(
                no_suffix_server_aliases)
            if common_alias:  # Same host, so update self.aliases
                self.aliases.update(
                    no_suffix_host_aliases.union(no_suffix_server_aliases)
                )
                return True

        return False

    def _get_aliases(self, host):
        """Gets the aliases for the given host
        """
        aliases = set([clean_IPv6(host)])
        if hostname_is_ip(clean_IPv6(host)):  # IP address
            try:
                my_host = socket.gethostbyaddr(clean_IPv6(host))
                aliases.add(my_host[0])
                # socket.gethostbyname_ex() does not work with ipv6
                if (my_host[0].count(":") >= 1 or
                        my_host[0] != "ip6-localhost"):
                    host_ip = socket.gethostbyname_ex(my_host[0])
                else:
                    addrinfo = socket.getaddrinfo(my_host[0], None)
                    host_ip = ([socket.gethostbyaddr(addrinfo[0][4][0])],
                               [fiveple[4][0] for fiveple in addrinfo],
                               [addrinfo[0][4][0]])
            except (socket.gaierror, socket.herror,
                    socket.error) as err:
                host_ip = ([], [], [])
                if self.verbose:
                    print("WARNING: IP lookup by address failed for {0},"
                          "reason: {1}".format(host, err.strerror))
        else:
            try:
                # server may not really exist.
                host_ip = socket.gethostbyname_ex(host)
            except (socket.gaierror, socket.herror,
                    socket.error) as err:
                if self.verbose:
                    print("WARNING: hostname: {0} may not be reachable, "
                          "reason: {1}".format(host, err.strerror))
                return aliases
            aliases.add(host_ip[0])
            addrinfo = socket.getaddrinfo(host, None)
            local_ip = None
            error = None
            for addr in addrinfo:
                try:
                    local_ip = socket.gethostbyaddr(addr[4][0])
                    break
                except (socket.gaierror, socket.herror,
                        socket.error) as err:
                    error = err

            if local_ip:
                host_ip = ([local_ip[0]],
                           [fiveple[4][0] for fiveple in addrinfo],
                           [addrinfo[0][4][0]])
            else:
                host_ip = ([], [], [])
                if self.verbose:
                    print("WARNING: IP lookup by name failed for {0},"
                          "reason: {1}".format(host, error.strerror))
        aliases.update(set(host_ip[1]))
        aliases.update(set(host_ip[2]))
        return aliases

    def is_alias(self, host_or_ip):
        """Determine if host_or_ip is an alias for this host

        host_or_ip[in] host or IP number to check

        Returns bool - True = host_or_ip is an alias
        """
        # List of possible suffixes
        suffixes = ('.local', '.lan', '.localdomain')

        host_or_ip = clean_IPv6(host_or_ip.lower())

        # for quickness, verify in the existing aliases, if they exist.
        if self.aliases:
            if host_or_ip.lower() in self.aliases:
                return True
            else:
                # get the alias for the given host_or_ip
                return self._update_alias(host_or_ip, suffixes)

        # no previous aliases information
        # First, get the local information
        hostname_ = socket.gethostname()
        try:
            local_info = socket.gethostbyname_ex(hostname_)
            local_aliases = set([local_info[0].lower()])
            # if dotted host name, take first part and use as an alias
            try:
                local_aliases.add(local_info[0].split('.')[0])
            except:
                pass
            local_aliases.update(['127.0.0.1', 'localhost', '::1', '[::1]'])
            local_aliases.update(local_info[1])
            local_aliases.update(local_info[2])
            local_aliases.update(self._get_aliases(hostname_))
        except (socket.herror, socket.gaierror, socket.error) as err:
            if self.verbose:
                print("WARNING: Unable to find aliases for hostname"
                      " '{0}' reason: {1}".format(hostname_, str(err)))
            # Try with the basic local aliases.
            local_aliases = set(['127.0.0.1', 'localhost', '::1', '[::1]'])

        # Get the aliases for this server host
        self.aliases = self._get_aliases(self.host)

        # Check if this server is local
        for host in self.aliases.copy():
            if host in local_aliases:
                # Is local then save the local aliases for future.
                self.aliases.update(local_aliases)
                break
            # Handle special suffixes in hostnames.
            for suffix in suffixes:
                if host.endswith(suffix):
                    # Remove special suffix and attempt to match with local
                    # aliases.
                    host, _ = host.rsplit('.', 1)
                    if host in local_aliases:
                        # Is local then save the local aliases for future.
                        self.aliases.update(local_aliases)
                        break

        # Check if the given host_or_ip is alias of the server host.
        if host_or_ip in self.aliases:
            return True

        # Check if any of the aliases of ip_or_host is also an alias of the
        # host server.
        return self._update_alias(host_or_ip, suffixes)

    def user_host_exists(self, user, host_or_ip):
        """Check to see if a user, host exists

        This method attempts to see if a user name matches the users on the
        server and that any user, host pair can match the host or IP address
        specified. This attempts to resolve wildcard matches.

        user[in]       user name
        host_or_ip[in] host or IP address

        Returns string - host from server that matches the host_or_ip or
                         None if no match.
        """
        res = self.exec_query("SELECT host FROM mysql.user WHERE user = '%s' "
                              "AND '%s' LIKE host " % (user, host_or_ip))
        if res:
            return res[0][0]
        return None

    def get_connection_values(self):
        """Return a dictionary of connection values for the server.

        Returns dictionary
        """
        conn_vals = {
            "user": self.user,
            "host": self.host
        }
        if self.passwd:
            conn_vals["passwd"] = self.passwd
        if self.socket:
            conn_vals["socket"] = self.socket
        if self.port:
            conn_vals["port"] = self.port
        if self.ssl_ca:
            conn_vals["ssl_ca"] = self.ssl_ca
        if self.ssl_cert:
            conn_vals["ssl_cert"] = self.ssl_cert
        if self.ssl_key:
            conn_vals["ssl_key"] = self.ssl_key
        if self.ssl:
            conn_vals["ssl"] = self.ssl

        return conn_vals

    def connect(self, log_version=False):
        """Connect to server

        Attempts to connect to the server as specified by the connection
        parameters.

        log_version[in]      If True, log server version. Default = False.

        Note: This method must be called before executing queries.

        Raises UtilError if error during connect
        """
        try:
            self.db_conn = self.get_connection()
            if log_version:
                log_server_version(self)
            # If no charset provided, get it from the "character_set_client"
            # server variable.
            if not self.charset:
                res = self.show_server_variable('character_set_client')
                self.db_conn.set_charset_collation(charset=res[0][1])
                self.charset = res[0][1]
            if self.ssl:
                res = self.exec_query("SHOW STATUS LIKE 'Ssl_cipher'")
                if res[0][1] == '':
                    raise UtilError("Can not encrypt server connection.")
            # if we connected via a socket, get the port
            if os.name == 'posix' and self.socket:
                res = self.show_server_variable('port')
                if res:
                    self.port = res[0][1]
        except UtilError:
            # Reset any previous value if the connection cannot be established,
            # before raising an exception. This prevents the use of a broken
            # database connection.
            self.db_conn = None
            raise
        self.connect_error = None
        # Valid values are ON and OFF, not boolean.
        self.read_only = self.show_server_variable("READ_ONLY")[0][1] == "ON"

    def get_connection(self):
        """Return a new connection to the server.

        Attempts to connect to the server as specified by the connection
        parameters and returns a connection object.

        Return the resulting MySQL connection object or raises an UtilError if
        an error occurred during the server connection process.
        """
        try:
            parameters = {
                'user': self.user,
                'host': self.host,
                'port': self.port,
            }
            if self.socket and os.name == "posix":
                parameters['unix_socket'] = self.socket
            if self.passwd and self.passwd != "":
                parameters['passwd'] = self.passwd
            if self.charset:
                parameters['charset'] = self.charset
            parameters['host'] = parameters['host'].replace("[", "")
            parameters['host'] = parameters['host'].replace("]", "")

            # Add SSL parameters ONLY if they are not None
            if self.ssl_ca is not None:
                parameters['ssl_ca'] = self.ssl_ca
            if self.ssl_cert is not None:
                parameters['ssl_cert'] = self.ssl_cert
            if self.ssl_key is not None:
                parameters['ssl_key'] = self.ssl_key

            # When at least one of cert, key or ssl options are specified,
            # the ca option is not required for establishing the encrypted
            # connection, but C/py will not allow the None value for the ca
            # option, so we use an empty string i.e '' to avoid an error from
            # C/py about ca option being the None value.
            if ('ssl_cert' in parameters.keys() or
                    'ssl_key' in parameters.keys() or
                    self.ssl) and \
                    'ssl_ca' not in parameters:
                parameters['ssl_ca'] = ''

            # The ca certificate is verified only if the ssl option is also
            # specified.
            if self.ssl and parameters['ssl_ca']:
                parameters['ssl_verify_cert'] = True

            if self.has_ssl:
                cpy_flags = [ClientFlag.SSL, ClientFlag.SSL_VERIFY_SERVER_CERT]
                parameters['client_flags'] = cpy_flags

            db_conn = mysql.connector.connect(**parameters)
            # Return MySQL connection object.
            return db_conn
        except mysql.connector.Error as err:
            raise UtilError(err.msg, err.errno)
        except AttributeError as err:
            raise UtilError(str(err))

    def disconnect(self):
        """Disconnect from the server.
        """
        try:
            self.db_conn.disconnect()
        except:
            pass

    def get_version(self):
        """Return version number of the server.

        Get the server version. The respective instance variable is set with
        the result after querying the server the first time. The version is
        immediately returned when already known, avoiding querying the server
        at each time.

        Returns string - version string or None if error
        """
        # Return the local version value if already known.
        if self._version:
            return self._version

        # Query the server for its version.
        try:
            res = self.show_server_variable("VERSION")
            if res:
                self._version = res[0][1]
        except UtilError:
            # Ignore errors and return _version, initialized with None.
            pass

        return self._version

    def check_version_compat(self, t_major, t_minor, t_rel):
        """Checks version of the server against requested version.

        This method can be used to check for version compatibility.

        t_major[in]        target server version (major)
        t_minor[in]        target server version (minor)
        t_rel[in]          target server version (release)

        Returns bool True if server version is GE (>=) version specified,
                     False if server version is LT (<) version specified
        """
        version_str = self.get_version()
        if version_str is not None:
            match = re.match(r'^(\d+\.\d+(\.\d+)*).*$', version_str.strip())
            if match:
                version = [int(x) for x in match.group(1).split('.')]
                version = (version + [0])[:3]  # Ensure a 3 elements list
                return version >= [int(t_major), int(t_minor), int(t_rel)]
            else:
                return False
        return True

    def exec_query(self, query_str, options=None, exec_timeout=0):
        """Execute a query and return result set

        This is the singular method to execute queries. It should be the only
        method used as it contains critical error code to catch the issue
        with mysql.connector throwing an error on an empty result set.

        Note: will handle exception and print error if query fails

        Note: if fetchall is False, the method returns the cursor instance

        query_str[in]      The query to execute
        options[in]        Options to control behavior:
            params         Parameters for query
            columns        Add column headings as first row
                           (default is False)
            fetch          Execute the fetch as part of the operation and
                           use a buffered cursor
                           (default is True)
            raw            If True, use a buffered raw cursor
                           (default is True)
            commit         Perform a commit (if needed) automatically at the
                           end (default: True).
        exec_timeout[in]   Timeout value in seconds to kill the query execution
                           if exceeded. Value must be greater than zero for
                           this feature to be enabled. By default 0, meaning
                           that the query will not be killed.

        Returns result set or cursor
        """
        if options is None:
            options = {}
        params = options.get('params', ())
        columns = options.get('columns', False)
        fetch = options.get('fetch', True)
        raw = options.get('raw', True)
        do_commit = options.get('commit', True)

        # Guard for connect() prerequisite
        assert self.db_conn, "You must call connect before executing a query."

        # If we are fetching all, we need to use a buffered
        if fetch:
            if raw:
                if mysql.connector.__version_info__ < (2, 0):
                    cur = self.db_conn.cursor(buffered=True, raw=True)
                else:
                    cur = self.db_conn.cursor(
                        cursor_class=MySQLUtilsCursorBufferedRaw)
            else:
                cur = self.db_conn.cursor(buffered=True)
        else:
            if mysql.connector.__version_info__ < (2, 0):
                cur = self.db_conn.cursor(raw=True)
            else:
                cur = self.db_conn.cursor(cursor_class=MySQLUtilsCursorRaw)

        # Execute query, handling parameters.
        q_killer = None
        try:
            if exec_timeout > 0:
                # Spawn thread to kill query if timeout is reached.
                # Note: set it as daemon to avoid waiting for it on exit.
                q_killer = QueryKillerThread(self, query_str, exec_timeout)
                q_killer.daemon = True
                q_killer.start()
            # Execute query.
            if params == ():
                cur.execute(query_str)
            else:
                cur.execute(query_str, params)
        except mysql.connector.Error as err:
            cur.close()
            if err.errno == CR_SERVER_LOST and exec_timeout > 0:
                # If the connection is killed (because the execution timeout is
                # reached), then it attempts to re-establish it (to execute
                # further queries) and raise a specific exception to track this
                # event.
                # CR_SERVER_LOST = Errno 2013 Lost connection to MySQL server
                # during query.
                self.db_conn.reconnect()
                raise UtilError("Timeout executing query", err.errno)
            else:
                raise UtilDBError("Query failed. {0}".format(err))
        except Exception:
            cur.close()
            raise UtilError("Unknown error. Command: {0}".format(query_str))
        finally:
            # Stop query killer thread if alive.
            if q_killer and q_killer.is_alive():
                q_killer.stop()

        # Fetch rows (only if available or fetch = True).
        # pylint: disable=R0101
        if cur.with_rows:
            if fetch or columns:
                try:
                    results = cur.fetchall()
                    if columns:
                        col_headings = cur.column_names
                        col_names = []
                        for col in col_headings:
                            col_names.append(col)
                        # pylint: disable=R0204
                        results = col_names, results
                except mysql.connector.Error as err:
                    raise UtilDBError("Error fetching all query data: "
                                      "{0}".format(err))
                finally:
                    cur.close()
                return results
            else:
                # Return cursor to fetch rows elsewhere (fetch = false).
                return cur
        else:
            # No results (not a SELECT)
            try:
                if do_commit:
                    self.db_conn.commit()
            except mysql.connector.Error as err:
                raise UtilDBError("Error performing commit: {0}".format(err))
            finally:
                cur.close()
            return cur

    def commit(self):
        """Perform a COMMIT.
        """
        # Guard for connect() prerequisite
        assert self.db_conn, "You must call connect before executing a query."

        self.db_conn.commit()

    def rollback(self):
        """Perform a ROLLBACK.
        """
        # Guard for connect() prerequisite
        assert self.db_conn, "You must call connect before executing a query."

        self.db_conn.rollback()

    def show_server_variable(self, variable):
        """Returns one or more rows from the SHOW VARIABLES command.

        variable[in]       The variable or wildcard string

        Returns result set
        """

        return self.exec_query("SHOW VARIABLES LIKE '%s'" % variable)

    def select_variable(self, var_name, var_type=None):
        """Get server system variable value using SELECT statement.

        This function displays the value of system variables using the SELECT
        statement. This can be used as a workaround for variables with very
        long values, as SHOW VARIABLES is subject to a version-dependent
        display-width limit.

        Note: Some variables may not be available using SELECT @@var_name, in
        such cases use SHOW VARIABLES LIKE 'var_name'.

        var_name[in]    Name of the variable to display.
        var_type[in]    Type of the variable ('session' or 'global'). By
                        default no type is used, meaning that the session
                        value is returned if it exists and the global value
                        otherwise.

        Return the value for the given server system variable.
        """
        if var_type is None:
            var_type = ''
        elif var_type.lower() in ('global', 'session', ''):
            var_type = '{0}.'.format(var_type)  # Add dot (.)
        else:
            raise UtilDBError("Invalid variable type: {0}. Supported types: "
                              "'global' and 'session'.".format(var_type))
        # Execute SELECT @@[var_type.]var_name.
        # Note: An error is issued if the given variable is not known.
        res = self.exec_query("SELECT @@{0}{1}".format(var_type, var_name))
        return res[0][0]

    def flush_logs(self, log_type=None):
        """Execute the FLUSH [log_type] LOGS statement.

        Reload internal logs cache and closes and reopens all log files, or
        only of the specified log_type.

        Note: The log_type option is available from MySQL 5.5.3.

        log_type[in]    Type of the log files to be flushed. Supported values:
                        BINARY, ENGINE, ERROR, GENERAL, RELAY, SLOW.
        """
        if log_type:
            self.exec_query("FLUSH {0} LOGS".format(log_type))
        else:
            self.exec_query("FLUSH LOGS")

    def get_uuid(self):
        """Return the uuid for this server if it is GTID aware.

        Returns uuid or None if server is not GTID aware.
        """
        if self.supports_gtid() != "NO":
            res = self.show_server_variable("server_uuid")
            return res[0][1]
        return None

    def supports_gtid(self):
        """Determine if server supports GTIDs

        Returns string - 'ON' = gtid supported and turned on,
                         'OFF' = supported but not enabled,
                         'NO' = not supported
        """
        # Check servers for GTID support
        version_ok = self.check_version_compat(5, 6, 5)
        if not version_ok:
            return "NO"
        try:
            res = self.exec_query("SELECT @@GLOBAL.GTID_MODE")
        except:
            return "NO"

        return res[0][0]

    def check_gtid_version(self):
        """Determine if server supports latest GTID changes

        This method checks the server to ensure it contains the latest
        changes to the GTID variables (from version 5.6.9).

        Raises UtilRplError when errors occur.
        """
        errors = []
        if self.supports_gtid() != "ON":
            errors.append("    GTID is not enabled.")
        if not self.check_version_compat(5, 6, 9):
            errors.append("    Server version must be 5.6.9 or greater.")
        if errors:
            error_str = "\n".join(errors)
            error_str = "\n".join([_GTID_ERROR % (self.host, self.port),
                                   error_str])
            raise UtilRplError(error_str)

    def check_gtid_executed(self, operation="copy"):
        """Check to see if the gtid_executed variable is clear

        If the value is not clear, raise an error with appropriate instructions
        for the user to correct the issue.

        operation[in]  Name of the operation (copy, import, etc.)
                       default = copy
        """
        res = self.exec_query("SHOW GLOBAL VARIABLES LIKE 'gtid_executed'")[0]
        if res[1].strip() == '':
            return
        err = ("The {0} operation contains GTID statements "
               "that require the global gtid_executed system variable on the "
               "target to be empty (no value). The gtid_executed value must "
               "be reset by issuing a RESET MASTER command on the target "
               "prior to attempting the {0} operation. "
               "Once the global gtid_executed value is cleared, you may "
               "retry the {0}.").format(operation)
        raise UtilRplError(err)

    def get_gtid_executed(self, skip_gtid_check=True):
        """Get the executed GTID set of the server.

        This function retrieves the (current) GTID_EXECUTED set of the server.

        skip_gtid_check[in]     Flag indicating if the check for GTID support
                                will be skipped or not. By default 'True'
                                (check is skipped).

        Returns a string with the GTID_EXECUTED set for this server.
        """
        if not skip_gtid_check:
            # Check server for GTID support.
            gtid_support = self.supports_gtid() == "NO"
            if gtid_support == 'NO':
                raise UtilRplError("Global Transaction IDs are not supported.")
            elif gtid_support == 'OFF':
                raise UtilError("Global Transaction IDs are not enabled.")
        # Get GTID_EXECUTED.
        try:
            return self.exec_query("SELECT @@GLOBAL.GTID_EXECUTED")[0][0]
        except UtilError:
            if skip_gtid_check:
                # Query likely failed because GTIDs are not supported,
                # therefore skip error in this case.
                return ""
            else:
                # If GTID check is not skipped re-raise exception.
                raise
        except IndexError:
            # If no rows are returned by query then return an empty string.
            return ''

    def gtid_subtract(self, gtid_set, gtid_subset):
        """Subtract given GTID sets.

        This function invokes GTID_SUBTRACT function on the server to retrieve
        the GTIDs from the given gtid_set that are not in the specified
        gtid_subset.

        gtid_set[in]        Base GTID set to subtract the subset from.
        gtid_subset[in]     GTID subset to be subtracted from the base set.

        Return a string with the GTID set resulting from the subtraction of the
        specified gtid_subset from the gtid_set.
        """
        try:
            return self.exec_query(
                "SELECT GTID_SUBTRACT('{0}', '{1}')".format(gtid_set,
                                                            gtid_subset)
            )[0][0]
        except IndexError:
            # If no rows are returned by query then return an empty string.
            return ''

    def gtid_subtract_executed(self, gtid_set):
        """Subtract GTID_EXECUTED to the given GTID set.

        This function invokes GTID_SUBTRACT function on the server to retrieve
        the GTIDs from the given gtid_set that are not in the GTID_EXECUTED
        set.

        gtid_set[in]        Base GTID set to subtract the GTID_EXECUTED.

        Return a string with the GTID set resulting from the subtraction of the
        GTID_EXECUTED set from the specified gtid_set.
        """
        from mysql.utilities.common.topology import _GTID_SUBTRACT_TO_EXECUTED
        try:
            result = self.exec_query(
                _GTID_SUBTRACT_TO_EXECUTED.format(gtid_set)
            )[0][0]
            # Remove newlines (\n and/or \r) from the GTID set string returned
            # by the server.
            return result.replace('\n', '').replace('\r', '')
        except IndexError:
            # If no rows are returned by query then return an empty string.
            return ''

    def inject_empty_trx(self, gtid, gtid_next_automatic=True):
        """ Inject an empty transaction.

        This method injects an empty transaction on the server for the given
        GTID.

        Note: SUPER privilege is required for this operation, more precisely
        to set the GTID_NEXT variable.

        gtid[in]                    GTID for the empty transaction to inject.
        gtid_next_automatic[in]     Indicate if the GTID_NEXT is set to
                                    AUTOMATIC after injecting the empty
                                    transaction. By default True.
        """
        self.exec_query("SET GTID_NEXT='{0}'".format(gtid))
        self.exec_query("BEGIN")
        self.commit()
        if gtid_next_automatic:
            self.exec_query("SET GTID_NEXT='AUTOMATIC'")

    def set_gtid_next_automatic(self):
        """ Set GTID_NEXT to AUTOMATIC.
        """
        self.exec_query("SET GTID_NEXT='AUTOMATIC'")

    def checksum_table(self, tbl_name, exec_timeout=0):
        """Compute checksum of specified table (CHECKSUM TABLE tbl_name).

        This function executes the CHECKSUM TABLE statement for the specified
        table and returns the result. The CHECKSUM is aborted (query killed)
        if a timeout value (greater than zero) is specified and the execution
        takes longer than the specified time.

        tbl_name[in]        Name of the table to perform the checksum.
        exec_timeout[in]    Maximum execution time (in seconds) of the query
                            after which it will be killed. By default 0, no
                            timeout.

        Returns a tuple with the checksum result for the target table. The
        first tuple element contains the result from the CHECKSUM TABLE query
        or None if an error occurred (e.g. execution timeout reached). The
        second element holds any error message or None if the operation was
        successful.
        """
        try:
            return self.exec_query(
                "CHECKSUM TABLE {0}".format(tbl_name),
                exec_timeout=exec_timeout
            )[0], None
        except IndexError:
            # If no rows are returned by query then return None.
            return None, "No data returned by CHECKSUM TABLE"
        except UtilError as err:
            # Return None if the query is killed (exec_timeout reached).
            return None, err.errmsg

    def get_gtid_status(self):
        """Get the GTID information for the server.

        This method attempts to retrieve the GTID lists. If the server
        does not have GTID turned on or does not support GTID, the method
        will throw and exception.

        Returns [list, list, list]
        """
        # Check servers for GTID support
        if self.supports_gtid() == "NO":
            raise UtilError("Global Transaction IDs are not supported.")

        res = self.exec_query("SELECT @@GLOBAL.GTID_MODE")
        if res[0][0].upper() == 'OFF':
            raise UtilError("Global Transaction IDs are not enabled.")

        gtid_data = [self.exec_query("SELECT @@GLOBAL.GTID_EXECUTED")[0],
                     self.exec_query("SELECT @@GLOBAL.GTID_PURGED")[0],
                     self.exec_query("SELECT @@GLOBAL.GTID_OWNED")[0]]

        return gtid_data

    def check_rpl_user(self, user, host):
        """Check replication user exists and has the correct privileges.

        user[in]      user name of rpl_user
        host[in]      host name of rpl_user

        Returns [] - no exceptions, list if exceptions found
        """
        errors = []
        ipv6 = False
        if "]" in host:
            ipv6 = True
            host = clean_IPv6(host)
        result = self.user_host_exists(user, host)
        if ipv6:
            result = format_IPv6(result)
        if result is None or result == []:
            errors.append("The replication user %s@%s was not found "
                          "on %s:%s." % (user, host, self.host, self.port))
        else:
            rpl_user = User(self, "%s@" % user + result)
            if not rpl_user.has_privilege('*', '*',
                                          'REPLICATION SLAVE'):
                errors.append("Replication user does not have the "
                              "correct privilege. She needs "
                              "'REPLICATION SLAVE' on all replicated "
                              "databases.")

        return errors

    def supports_plugin(self, plugin):
        """Check if the given plugin is supported.

        Check to see if the server supports a plugin. Return True if
        plugin installed and active.

        plugin[in]     Name of plugin to check

        Returns True if plugin is supported, and False otherwise.
        """
        _PLUGIN_QUERY = ("SELECT * FROM INFORMATION_SCHEMA.PLUGINS "
                         "WHERE PLUGIN_NAME ")
        res = self.exec_query("".join([_PLUGIN_QUERY, "LIKE ",
                                       "'%s" % plugin, "%'"]))
        if not res:
            return False
        # Now see if it is active.
        elif res[0][2] != 'ACTIVE':
            return False
        return True

    def get_all_databases(self, ignore_internal_dbs=True):
        """Return a result set containing all databases on the server
        except for internal databases (mysql, INFORMATION_SCHEMA,
        PERFORMANCE_SCHEMA).

        Note: New internal database 'sys' added by default for MySQL 5.7.7+.

        Returns result set
        """

        if ignore_internal_dbs:
            _GET_DATABASES = """
            SELECT SCHEMA_NAME
            FROM INFORMATION_SCHEMA.SCHEMATA
            WHERE SCHEMA_NAME != 'INFORMATION_SCHEMA'
            AND SCHEMA_NAME != 'PERFORMANCE_SCHEMA'
            AND SCHEMA_NAME != 'mysql'
            """
            # Starting from MySQL 5.7.7, sys schema is installed by default.
            if self.check_version_compat(5, 7, 7):
                _GET_DATABASES = "{0} AND SCHEMA_NAME != 'sys'".format(
                    _GET_DATABASES)
        else:
            _GET_DATABASES = """
            SELECT SCHEMA_NAME
            FROM INFORMATION_SCHEMA.SCHEMATA
            """

        return self.exec_query(_GET_DATABASES)

    def get_storage_engines(self):
        """Return list of storage engines on this server.

        Returns (list) (engine, support, comment)
        """

        _QUERY = """
            SELECT UPPER(engine), UPPER(support)
            FROM INFORMATION_SCHEMA.ENGINES
            ORDER BY engine
        """
        return self.exec_query(_QUERY)

    def check_storage_engines(self, other_list):
        """Compare storage engines from another server.

        This method compares the list of storage engines for the current
        server against a list supplied as **other_list**. It returns two
        lists - one for the storage engines on this server not on the other
        list, and another for the storage engines on the other list not on this
        server.

        Note: type case sensitive - make sure list is in uppercase

        other_list[in]     A list from another server in the form
                           (engine, support) - same output as
                           get_storage_engines()

        Returns (list, list)
        """
        # Guard for connect() prerequisite
        assert self.db_conn, "You must call connect before check engine lists."

        def _convert_set_to_list(set_items):
            """Convert a set to list
            """
            if len(set_items) > 0:
                item_list = []
                for item in set_items:
                    item_list.append(item)
            else:
                item_list = None
            return item_list

        # trivial, but guard against misuse
        this_list = self.get_storage_engines()
        if other_list is None:
            return (this_list, None)

        same = set(this_list) & set(other_list)
        master_extra = _convert_set_to_list(set(this_list) - same)
        slave_extra = _convert_set_to_list(set(other_list) - same)

        return (master_extra, slave_extra)

    def has_storage_engine(self, target):
        """Check to see if an engine exists and is supported.

        target[in]     name of engine to find

        Returns bool True - engine exists and is active, false = does not
                     exist or is not supported/not active/disabled
        """
        if len(target) == 0:
            return True  # This says we will use default engine on the server.
        if target is not None:
            engines = self.get_storage_engines()
            for engine in engines:
                if engine[0].upper() == target.upper() and \
                   engine[1].upper() in ['YES', 'DEFAULT']:
                    return True
        return False

    def substitute_engine(self, tbl_name, create_str,
                          new_engine, def_engine, quiet=False):
        """Replace storage engine in CREATE TABLE

        This method will replace the storage engine in the CREATE statement
        under the following conditions:
            - If new_engine is specified and it exists on destination, use it.
            - Else if existing engine does not exist and def_engine is specfied
              and it exists on destination, use it. Also, don't substitute if
              the existing engine will not be changed.

        tbl_name[in]       table name
        create_str[in]     CREATE statement
        new_engine[in]     name of storage engine to substitute (convert to)
        def_engine[in]     name of storage engine to use if existing engines
                           does not exist

        Returns string CREATE string with replacements if found, else return
                       original string
        """
        res = [create_str]
        exist_engine = ''
        is_create_like = False
        replace_msg = "# Replacing ENGINE=%s with ENGINE=%s for table %s."
        add_msg = "# Adding missing ENGINE=%s clause for table %s."
        if new_engine is not None or def_engine is not None:
            i = create_str.find("ENGINE=")
            if i > 0:
                j = create_str.find(" ", i)
                exist_engine = create_str[i + 7:j]
            else:
                # Check if it is a CREATE TABLE LIKE statement
                is_create_like = (create_str.find("CREATE TABLE {0} LIKE"
                                                  "".format(tbl_name)) == 0)

        # Set default engine
        #
        # If a default engine is specified and is not the same as the
        # engine specified in the table CREATE statement (existing engine) if
        # specified, and both engines exist on the server, replace the existing
        # engine with the default engine.
        #
        if def_engine is not None and \
                exist_engine.upper() != def_engine.upper() and \
                self.has_storage_engine(def_engine) and \
                self.has_storage_engine(exist_engine):

            # If no ENGINE= clause present, add it
            if len(exist_engine) == 0:
                if is_create_like:
                    alter_str = "ALTER TABLE {0} ENGINE={1}".format(tbl_name,
                                                                    def_engine)
                    res = [create_str, alter_str]
                else:
                    i = create_str.find(";")
                    i = len(create_str) if i == -1 else i
                    create_str = "{0} ENGINE={1};".format(create_str[0:i],
                                                          def_engine)
                    res = [create_str]
            # replace the existing storage engine
            else:
                create_str.replace("ENGINE=%s" % exist_engine,
                                   "ENGINE=%s" % def_engine)
            if not quiet:
                if len(exist_engine) > 0:
                    print replace_msg % (exist_engine, def_engine, tbl_name)
                else:
                    print add_msg % (def_engine, tbl_name)
            exist_engine = def_engine

        # Use new engine
        if (new_engine is not None and
                exist_engine.upper() != new_engine.upper() and
                self.has_storage_engine(new_engine)):
            if len(exist_engine) == 0:
                if is_create_like:
                    alter_str = "ALTER TABLE {0} ENGINE={1}".format(tbl_name,
                                                                    new_engine)
                    res = [create_str, alter_str]
                else:
                    i = create_str.find(";")
                    i = len(create_str) if i == -1 else i
                    create_str = "{0} ENGINE={1};".format(create_str[0:i],
                                                          new_engine)
                    res = [create_str]
            else:
                create_str = create_str.replace("ENGINE=%s" % exist_engine,
                                                "ENGINE=%s" % new_engine)
                res = [create_str]
            if not quiet:
                if len(exist_engine) > 0:
                    print replace_msg % (exist_engine, new_engine, tbl_name)
                else:
                    print add_msg % (new_engine, tbl_name)
        return res

    def get_innodb_stats(self):
        """Return type of InnoDB engine and its version information.

        This method returns a tuple containing the type of InnoDB storage
        engine (builtin or plugin) and the version number reported.

        Returns (tuple) (type = 'builtin' or 'plugin', version_number,
                         have_innodb = True or False)
        """
        # Guard for connect() prerequisite
        assert self.db_conn, "You must call connect before get innodb stats."

        _BUILTIN = """
            SELECT (support='YES' OR support='DEFAULT' OR support='ENABLED')
            AS `exists` FROM INFORMATION_SCHEMA.ENGINES
            WHERE engine = 'innodb';
        """
        _PLUGIN = """
            SELECT (plugin_library LIKE 'ha_innodb_plugin%') AS `exists`
            FROM INFORMATION_SCHEMA.PLUGINS
            WHERE LOWER(plugin_name) = 'innodb' AND
                  LOWER(plugin_status) = 'active';
        """
        _VERSION = """
            SELECT plugin_version, plugin_type_version
            FROM INFORMATION_SCHEMA.PLUGINS
            WHERE LOWER(plugin_name) = 'innodb';
        """

        inno_type = None
        results = self.exec_query(_BUILTIN)
        if results is not None and results != () and results[0][0] is not None:
            inno_type = "builtin"

        results = self.exec_query(_PLUGIN)
        if results is not None and results != () and \
           results != [] and results[0][0] is not None:
            inno_type = "plugin "

        results = self.exec_query(_VERSION)
        version = []
        if results is not None:
            version.append(results[0][0])
            version.append(results[0][1])
        else:
            version.append(None)
            version.append(None)

        results = self.show_server_variable("have_innodb")
        # pylint: disable=R0102
        if results is not None and results != [] and \
           results[0][1].lower() == "yes":
            have_innodb = True
        else:
            have_innodb = False

        return (inno_type, version[0], version[1], have_innodb)

    def read_and_exec_SQL(self, input_file, verbose=False):
        """Read an input file containing SQL statements and execute them.

        input_file[in]     The full path to the file
        verbose[in]        Print the command read
                           Default = False

        Returns True = success, False = error

        TODO : Make method read multi-line queries.
        """
        f_input = open(input_file)
        res = True
        while True:
            cmd = f_input.readline()
            if not cmd:
                break
            res = None
            if len(cmd) > 1:
                if cmd[0] != '#':
                    if verbose:
                        print cmd
                    query_options = {
                        'fetch': False
                    }
                    res = self.exec_query(cmd, query_options)
        f_input.close()
        return res

    def binlog_enabled(self):
        """Check binary logging status for the client.

        Returns bool - True - binary logging is ON, False = OFF
        """
        res = self.show_server_variable("log_bin")
        if not res:
            raise UtilRplError("Cannot retrieve status of log_bin variable.")
        if res[0][1] in ("OFF", "0"):
            return False
        return True

    def toggle_binlog(self, action="disable"):
        """Enable or disable binary logging for the client.

        Note: user must have SUPER privilege

        action[in]         if 'disable', turn off the binary log
                           elif 'enable' turn binary log on
                           do nothing if action != 'enable' or 'disable'
        """

        if action.lower() == 'disable':
            self.exec_query("SET SQL_LOG_BIN=0")
        elif action.lower() == 'enable':
            self.exec_query("SET SQL_LOG_BIN=1")

    def foreign_key_checks_enabled(self, force=False):
        """Check foreign key status for the connection.
        force[in]       if True, returns the value directly from the server
                        instead of returning the cached fkey value

        Returns bool - True - foreign keys are enabled
        """
        if self.fkeys is None or force:
            res = self.exec_query("SELECT @@GLOBAL.foreign_key_checks")
            self.fkeys = (res is not None) and (res[0][0] == "1")
        return self.fkeys

    def disable_foreign_key_checks(self, disable=True):
        """Enable or disable foreign key checks for the connection.

        disable[in]     if True, turn off foreign key checks
                        elif False turn foreign key checks on.
        """
        if self.fkeys is None:
            self.foreign_key_checks_enabled()

        # Only do something if foreign keys are OFF and shouldn't be disabled
        # or if they are ON and should be disabled
        if self.fkeys == disable:
            val = "OFF" if disable else "ON"
            self.exec_query(_FOREIGN_KEY_SET.format(val),
                            {'fetch': False, 'commit': False})
            self.fkeys = not self.fkeys

    def autocommit_set(self):
        """Check autocommit status for the connection.

        Returns bool - True if autocommit is enabled and False otherwise.
        """
        if self.autocommit is None:
            res = self.show_server_variable('autocommit')
            self.autocommit = (res and res[0][1] == '1')
        return self.autocommit

    def toggle_autocommit(self, enable=None):
        """Enable or disable autocommit for the connection.

        This method switch the autocommit value or enable/disable it according
        to the given parameter.

        enable[in]         if True, turn on autocommit (set to 1)
                           else if False turn autocommit off (set to 0).
        """
        if enable is None:
            # Switch autocommit value.
            if self.autocommit is None:
                # Get autocommit value if unknown
                self.autocommit_set()
            if self.autocommit:
                value = '0'
                self.autocommit = False
            else:
                value = '1'
                self.autocommit = True
        else:
            # Set AUTOCOMMIT according to provided value.
            if enable:
                value = '1'
                self.autocommit = True
            else:
                value = '0'
                self.autocommit = False
        # Change autocommit value.
        self.exec_query(_AUTOCOMMIT_SET.format(value), {'fetch': 'false'})

    def get_server_id(self):
        """Retrieve the server id.

        Returns int - server id.
        """
        try:
            res = self.show_server_variable("server_id")
        except:
            raise UtilRplError("Cannot retrieve server id from "
                               "%s." % self.role)

        return int(res[0][1])

    def get_server_uuid(self):
        """Retrieve the server uuid.

        Returns string - server uuid.
        """
        try:
            res = self.show_server_variable("server_uuid")
            if res is None or res == []:
                return None
        except:
            raise UtilRplError("Cannot retrieve server_uuid from "
                               "%s." % self.role)

        return res[0][1]

    def get_lctn(self):
        """Get lower_case_table_name setting.

        Returns lctn value or None if cannot get value
        """
        res = self.show_server_variable("lower_case_table_names")
        if res != []:
            return res[0][1]
        return None

    def get_binary_logs(self, options=None):
        """Return a list of the binary logs.

        options[in]        query options

        Returns list - binlogs or None if binary logging turned off
        """
        if options is None:
            options = {}
        if self.binlog_enabled():
            return self.exec_query("SHOW BINARY LOGS", options)

        return None

    def set_read_only(self, on=False):
        """Turn read only mode on/off

        on[in]         if True, turn read_only ON
                       Default is False
        """
        # Only turn on|off read only if it were off at connect()
        if on and not self.read_only:
            self.exec_query("SET @@GLOBAL.READ_ONLY = 'ON'")
            self.read_only = True
        elif not on and self.read_only:
            self.read_only = False
            self.exec_query("SET @@GLOBAL.READ_ONLY = 'OFF'")
        return None

    def grant_tables_enabled(self):
        """Check to see if grant tables are enabled

        Returns bool - True = grant tables are enabled, False = disabled
        """
        if self.grants_enabled is None:
            try:
                self.exec_query("SHOW GRANTS FOR 'snuffles'@'host'")
                self.grants_enabled = True
            except UtilError as error:
                if "--skip-grant-tables" in error.errmsg:
                    self.grants_enabled = False
                # Ignore other errors as they are not pertinent to the check
                else:
                    self.grants_enabled = True
        return self.grants_enabled

    def get_server_binlogs_list(self, include_size=False):
        """Find the binlog file names listed on a server.

        Obtains the binlog file names available on the server by using the
        'SHOW BINARY LOGS' query at the given server instance and returns these
        file names as a list.

        include_size[in]  Boolean value to indicate if the returning list shall
                          include the size of the file.

        Returns a list with the binary logs names available on master.
        """
        res = self.exec_query("SHOW BINARY LOGS")

        server_binlogs = []
        for row in res:
            if include_size:
                server_binlogs.append(row)
            else:
                server_binlogs.append(row[0])
        return server_binlogs

    def sql_mode(self, mode, enable):
        """Set the sql_mode

        This method sets the sql_mode passed. If enable is True,
        the method adds the mode, else, it removes the mode.

        mode[in]      The sql_mode you want to set
        enable[in]    If True, set the mode, else remove the mode.

        Returns string - new sql_mode setting or None=not enabled/disabled
        """
        SQL_MODE = 'SET @@GLOBAL.SQL_MODE = "{0}"'
        sql_mode = self.show_server_variable("sql_mode")
        if sql_mode[0]:
            modes = sql_mode[0][1].split(",")
            sql_mode_str = 'mt'
            if enable:
                if mode not in modes:
                    modes.append(mode)
                else:
                    sql_mode_str = None
            else:
                if mode in modes:
                    index = modes.index(mode)
                    modes.pop(index)
                else:
                    sql_mode_str = None
            if sql_mode_str:
                sql_mode_str = SQL_MODE.format(",".join(modes))
                self.exec_query(sql_mode_str)
                return sql_mode_str
        return None


class QueryKillerThread(threading.Thread):
    """Class to run a thread to kill an executing query.

    This class is used to spawn a thread than will kill the execution
    (connection) of a query upon reaching a given timeout.
    """

    def __init__(self, server, query, timeout):
        """Constructor.

        server[in]      Server instance where the target query is executed.
        query[in]       Target query to kill.
        timeout[in]     Timeout value in seconds used to kill the query when
                        reached.
        """
        threading.Thread.__init__(self)
        self._stop_event = threading.Event()
        self._query = query
        self._timeout = timeout
        self._server = server
        self._connection = server.get_connection()
        server.get_version()

    def run(self):
        """Main execution of the query killer thread.
        Stop the thread if instructed as such
        """
        connector_error = None
        # Kill the query connection upon reaching the given execution timeout.
        while not self._stop_event.is_set():
            # Wait during the defined time.
            self._stop_event.wait(self._timeout)
            # If the thread was asked to stop during wait, it does not try to
            # kill the query.
            if not self._stop_event.is_set():
                try:
                    if mysql.connector.__version_info__ < (2, 0):
                        cur = self._connection.cursor(raw=True)
                    else:
                        cur = self._connection.cursor(
                            cursor_class=MySQLUtilsCursorRaw)

                    # Get process information from threads table when available
                    # (for versions > 5.6.1), since it does not require a mutex
                    # and has minimal impact on server performance.
                    if self._server.check_version_compat(5, 6, 1):
                        cur.execute(
                            "SELECT processlist_id "
                            "FROM performance_schema.threads"
                            " WHERE processlist_command='Query'"
                            " AND processlist_info='{0}'".format(self._query))
                    else:
                        cur.execute(
                            "SELECT id FROM information_schema.processlist"
                            " WHERE command='Query'"
                            " AND info='{0}'".format(self._query))
                    result = cur.fetchall()

                    try:
                        process_id = result[0][0]
                    except IndexError:
                        # No rows are returned if the query ended in the
                        # meantime.
                        process_id = None

                    # Kill the connection associated to que process id.
                    # Note: killing the query will not work with
                    # connector-python,since it will hang waiting for the
                    #  query to return.
                    if process_id:
                        cur.execute("KILL {0}".format(process_id))
                except mysql.connector.Error as err:
                    # Hold error to raise at the end.
                    connector_error = err
                finally:
                    # Close cursor if available.
                    if cur:
                        cur.close()
                # Stop this thread.
                self.stop()

        # Close connection.
        try:
            self._connection.disconnect()
        except mysql.connector.Error:
            # Only raise error if no previous error has occurred.
            if not connector_error:
                raise
        finally:
            # Raise any previous error that already occurred.
            if connector_error is not None:
                # pylint: disable=E0702
                raise connector_error

    def stop(self):
        """Stop the thread.

        Set the event flag for the thread to stop as soon as possible.
        """
        self._stop_event.set()
#
# Copyright (c) 2011, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This file contains the methods for building SQL statements for definition
differences.
"""

import re

from mysql.connector.conversion import MySQLConverter


_IGNORE_COLUMN = -1  # Ignore column in comparisons and transformations
_FORCE_COLUMN = -2   # Force column to be included in build phase

# Define column control symbols
_DROP_COL, _ADD_COL, _CHANGE_COL_TYPE, _CHANGE_COL_ORDER = range(0, 4)

# List of database objects for enumeration
_DATABASE, _TABLE, _VIEW, _TRIG, _PROC, _FUNC, _EVENT, _GRANT = "DATABASE", \
    "TABLE", "VIEW", "TRIGGER", "PROCEDURE", "FUNCTION", "EVENT", "GRANT"

# Define database INFORMATION_SCHEMA column numbers
_DB_NAME, _DB_CHARSET, _DB_COLLATION, _DB_SQL_PATH = range(0, 4)

# Define table INFORMATION_SCHEMA column numbers and index values
_COLUMN_ORDINAL_POSITION, _COLUMN_NAME, _COLUMN_TYPE, _COLUMN_IS_NULLABLE, \
    _COLUMN_DEFAULT, _COLUMN_EXTRA, _COLUMN_COMMENT, _COLUMN_KEY = range(0, 8)

_TABLE_DEF, _COLUMN_DEF, _PART_DEF = range(0, 3)
_TABLE_DB, _TABLE_NAME, _TABLE_ENGINE, _TABLE_AUTO_INCREMENT, \
    _TABLE_AVG_ROW_LENGTH, _TABLE_CHECKSUM, _TABLE_COLLATION, _TABLE_COMMENT, \
    _TABLE_ROW_FORMAT, _TABLE_CREATE_OPTIONS = range(0, 10)

# Define view INFORMATION_SCHEMA column numbers
_VIEW_DB, _VIEW_NAME, _VIEW_BODY, _VIEW_CHECK, _VIEW_DEFINER, \
    _VIEW_SECURITY = range(0, 6)

# Define trigger INFORMATION_SCHEMA column numbers
_TRIGGER_DB, _TRIGGER_NAME, _TRIGGER_EVENT, _TRIGGER_TABLE, _TRIGGER_BODY, \
    _TRIGGER_TIME, _TRIGGER_DEFINER = range(0, 7)

# Define routine INFORMATION_SCHEMA column numbers
_ROUTINE_DB, _ROUTINE_NAME, _ROUTINE_BODY, _ROUTINE_SQL_DATA_ACCESS, \
    _ROUTINE_SECURITY_TYPE, _ROUTINE_COMMENT, _ROUTINE_DEFINER, \
    _ROUTINE_PARAMS, _ROUTINE_RETURNS, _ROUTINE_IS_DETERMINISTIC = range(0, 10)

# Define event INFORMATION_SCHEMA column numbers
_EVENT_DB, _EVENT_NAME, _EVENT_DEFINER, _EVENT_BODY, _EVENT_TYPE, \
    _EVENT_INTERVAL_FIELD, _EVENT_INTERVAL_VALUE, _EVENT_STATUS, \
    _EVENT_ON_COMPLETION, _EVENT_STARTS, _EVENT_ENDS = range(0, 11)

# Get the constraints but ignore primary keys
_CONSTRAINT_QUERY = """
  SELECT CONSTRAINT_NAME, CONSTRAINT_TYPE
  FROM INFORMATION_SCHEMA.TABLE_CONSTRAINTS
  WHERE TABLE_SCHEMA = '%(db)s' AND TABLE_NAME = '%(name)s'
        and CONSTRAINT_TYPE != 'PRIMARY KEY'
        and CONSTRAINT_TYPE != 'UNIQUE'
"""


def to_sql(obj):
    """Convert a value to a suitable SQL value placing quotes where needed.

    obj[in]           object (value) to convert

    Returns (string) converted value
    """
    to_sql.__dict__.setdefault('converter', MySQLConverter())
    obj = to_sql.converter.escape(obj)  # pylint: disable=E1101
    return str(to_sql.converter.quote(obj))  # pylint: disable=E1101


def quote_with_backticks(identifier, sql_mode=''):
    """Quote the given identifier with backticks, converting backticks (`) in
    the identifier name with the correct escape sequence (``) unless the
    identifier is quoted (") as in sql_mode set to ANSI_QUOTES.

    identifier[in] identifier to quote.

    Returns string with the identifier quoted with backticks.
    """
    if "ANSI_QUOTES" in sql_mode:
        return '"{0}"'.format(identifier.replace('"', '""'))
    else:
        return "`{0}`".format(identifier.replace("`", "``"))


def quote_with_backticks_definer(definer, sql_mode=''):
    """Quote the given definer clause with backticks.

    This functions quotes the given definer clause with backticks, converting
    backticks (`) in the string with the correct escape sequence (``).

    definer[in]     definer clause to quote.

    Returns string with the definer quoted with backticks.
    """
    if not definer:
        return definer
    parts = definer.split('@')
    if len(parts) != 2:
        return definer
    return '@'.join([quote_with_backticks(parts[0], sql_mode),
                     quote_with_backticks(parts[1], sql_mode)])


def remove_backtick_quoting(identifier, sql_mode=''):
    """Remove backtick quoting from the given identifier, reverting the
    escape sequence (``) to a backtick (`) in the identifier name.

    identifier[in] identifier to remove backtick quotes.

    Returns string with the identifier without backtick quotes.
    """
    double_quoting = identifier.startswith('"') and identifier.endswith('"')
    # remove quotes
    identifier = identifier[1:-1]
    if 'ANSI_QUOTES' in sql_mode and double_quoting:
        return identifier.replace('""', '"')
    else:
        # Revert backtick escape sequence
        return identifier.replace("``", "`")


def is_quoted_with_backticks(identifier, sql_mode=''):
    """Check if the given identifier is quoted with backticks.

    identifier[in] identifier to check.

    Returns True if the identifier has backtick quotes, and False otherwise.
    """
    if 'ANSI_QUOTES' in sql_mode:
        return (identifier[0] == "`" and identifier[-1] == "`") or \
            (identifier[0] == '"' and identifier[-1] == '"')
    else:
        return identifier[0] == "`" and identifier[-1] == "`"


def convert_special_characters(str_val):
    """Convert especial characters in the string to respective escape sequence.

    This method converts special characters in the input string to the
    corresponding MySQL escape sequence, according to:
    http://dev.mysql.com/doc/en/string-literals.html#character-escape-sequences

    str_val[in]  string value to be converted.

    Returns the input string with all special characters replaced by its
    respective escape sequence.
    """
    # Check if the input value is a string before performing replacement.
    if str_val and isinstance(str_val, basestring):
        # First replace backslash '\' character, to avoid replacing '\' in
        # further escape sequences. backslash_re matches '|' not followed by %
        # as \% and \_ do not need to be replaced, and when '|' appear at the
        # end of the string to be replaced correctly.
        backslash_re = r'\\(?=[^%_])|\\\Z'
        res = re.sub(backslash_re, r'\\\\', str_val)

        # Replace remaining especial characters
        res = res.replace('\x00', '\\0')  # \0
        res = res.replace("'", "\\'")  # \'
        res = res.replace('"', '\\"')  # \"
        res = res.replace('\b', '\\b')  # \b
        res = res.replace('\n', '\\n')  # \n
        res = res.replace('\r', '\\r')  # \r
        res = res.replace('\t', '\\t')  # \t
        res = res.replace(chr(26), '\\Z')  # \Z

        return res
    else:
        # Not a string, return the input value
        return str_val


def build_pkey_where_clause(table, row):
    """Build the WHERE clause based on the primary keys

    table[in]              instance of Table class for table
    row[in]                row of data

    Returns string - WHERE clause or "" if no keys
    """
    where_str = ""
    pkeys = table.get_primary_index()
    if len(pkeys) > 0:
        col_names = table.get_col_names()
        where_str += "WHERE "
        pkey_cond_lst = []
        for pkey in pkeys:
            key_col = pkey[0]                         # get the column name
            col_data = row[col_names.index(key_col)]  # get column value
            # quote key column with backticks
            q_key_col = quote_with_backticks(key_col, table.sql_mode)
            pkey_cond_lst.append("{0} = {1}".format(q_key_col,
                                                    to_sql(col_data)))
        where_str = "{0}{1}".format(where_str, ' AND '.join(pkey_cond_lst))

    return where_str


def build_set_clauses(table, table_cols, dest_row, src_row):
    """Build the SET clauses for an UPDATE statement

    table[in]              instance of Table class for table
    dest_row[in]           row of data for destination (to be changed)
    src_row[in]            row of data for source (to be changed to)

    Returns string - WHERE clause or "" if no keys
    """
    table.get_column_metadata()
    # do SETs
    set_str = ""
    do_comma = False
    for col_idx in range(0, len(table_cols)):
        if dest_row[col_idx] != src_row[col_idx]:
            # do comma
            if do_comma:
                set_str += ", "
            else:
                set_str = "SET "
                do_comma = True
            # Check for NULL for non-text fields that have no value in new row
            if src_row[col_idx] is None:
                set_str += "%s = %s" % (table_cols[col_idx], "NULL")
            else:
                set_str += "%s = %s" % (table_cols[col_idx],
                                        to_sql(src_row[col_idx]))

    return set_str


def transform_data(destination, source, operation, rows):
    """Transform data for tables.

    This method will generate INSERT, UPDATE, and DELETE statements for
    transforming data found to differ among tables.

    destination[in]    Table class instance of the destination
    source[in]         Table class instance of the source
    operation[in]      specify if INSERT, UPDATE, or DELETE
    rows[in]           rows for transformation as follows:
                       UPDATE - tuple (old, new)
                       DELETE - list to delete
                       INSERT - list to insert

    Returns list - SQL statement(s) for transforming the data or a warning
                   if the columns differ between the tables
    """
    statements = []

    # Get column names quoted with backticks
    dest_cols = destination.get_col_names(quote_backticks=True)
    src_cols = source.get_col_names(quote_backticks=True)

    # We cannot do the data changes if the columns are different in the
    # destination and source!
    if dest_cols != src_cols:
        return ["WARNING: Cannot generate SQL UPDATE commands for "
                "tables whose definitions are different. Check the "
                "table definitions for changes."]
    data_op = operation.upper()
    if data_op == "INSERT":
        for row in rows:
            formatted_row = []
            for col in row:
                formatted_row.append(to_sql(col))
            statements.append("INSERT INTO %s (%s) VALUES(%s);" %
                              (destination.q_table, ', '.join(dest_cols),
                               ', '.join(formatted_row)))
    elif data_op == "UPDATE":
        for i in range(0, len(rows[0])):
            row1 = rows[0][i]
            row2 = rows[1][i]
            sql_str = "UPDATE %s" % destination.q_table
            sql_str += " %s" % build_set_clauses(source, src_cols, row1, row2)
            sql_str += " %s" % build_pkey_where_clause(source, row2)
            statements.append("%s;" % sql_str)
    elif data_op == "DELETE":
        for row in rows:
            sql_str = "DELETE FROM %s " % destination.q_table
            sql_str += build_pkey_where_clause(source, row)
            statements.append("%s;" % sql_str)
    else:
        raise UtilError("Unknown data transformation option: %s." % data_op)

    return statements


class SQLTransformer(object):
    """
    The SQLTransformer class provides a mechanism for generating SQL statments
    for conforming an object to another for a specific database. For example,
    it will generate the ALTER statement(s) for transforming a table definition
    as well as the UPDATE statement(s) for transforming a row in the table.

    Note: This class is designed to work with the output of the Database class
          method get_db_objects with full INFORMATION_SCHEMA columns for the
          object definition.

    This class contains transformation methods for the objects supported.
    Each object's ALTER statement is generated using the following steps.
    Note: tables are a bit different due to their many parts but still follow
    the general layout.

    - a list of dictionaries structure is built to contain the parts of the
      statement where each dictionary has fields for format ('fmt') that
      contains the string format for building the value, column ('col') for
      containing the column number for the value, and value ('val') which
      is for holding the value.
    - any special formatting, conditionals, etc. concerning the fields is
      processed. In some cases this means filling the 'val' for the field.
    - the structure values are filled
    - the statement is build by concatenating those fields where 'val' is
      not empty.

    You can tell the fill values phase to ignore filling the value by using
    _IGNORE_COLUMN as the column number.

    You can tell the build phase to include the field (say after special
    processing has filled the value) by using _FORCE_COLUMN as the column
    number.
    """

    def __init__(self, destination_db, source_db, destination,
                 source, obj_type, verbosity, options=None):
        """Constructor

        destination_db[in] destination Database instance
        source_db[in]      source Database instance
        destination[in]    the original object definition or data
        source[in]         the source object definition or data
        obj_type[in]       type of object
        verbosity[in]      verbosity level
        options[in]        Options dictionary

        """
        self.destination_db = destination_db
        self.source_db = source_db
        self.destination = destination
        self.source = source
        self.obj_type = obj_type.upper()
        self.verbosity = verbosity
        self.dest_tbl = None
        self.src_tbl = None
        if options is None:
            options = {}
        self.skip_table_opts = options.get("skip_table_opts", False)

    def transform_definition(self):
        """Transform an object definition

        This method will transform an object definition to match the source
        configuration. It returns the appropriate SQL statement(s) to
        transform the object or None if no transformation is needed.

        Note: the method will throw an exception if the transformation cannot
              be completed or there is another error during processing

        Returns list - SQL statement(s) for transforming the object
        """
        trans_method = {
            _DATABASE: self._transform_database,
            _TABLE: self._transform_table,
            _VIEW: self._transform_view,
            _TRIG: self._transform_trigger,
            _PROC: self._transform_routine,
            _FUNC: self._transform_routine,
            _EVENT: self._transform_event,
        }
        try:
            return trans_method[self.obj_type]()
        except IndexError:
            raise UtilDBError("Unknown object type '%s' for transformation." %
                              self.obj_type)

    def _transform_database(self):
        """Transform a database definition

        This method will transform a database definition to match the source
        configuration. It returns the ALTER DATABASE SQL statement to
        transform the object or None if no transformation is needed.

        Returns list - ALTER DATABASE statement for transforming the database
        """
        statements = []

        # build a list of the parts
        statement_parts = [
            # preamble
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': "ALTER DATABASE"},
            # object name
            {'fmt': " %s", 'col': _IGNORE_COLUMN,
             'val': self.destination[_DB_NAME]},
            # charset
            {'fmt': " CHARACTER SET %s", 'col': _DB_CHARSET, 'val': ""},
            # collation
            {'fmt': " COLLATE = %s", 'col': _DB_COLLATION, 'val': ""},
        ]

        # if no changes, return None
        if not self._fill_values(statement_parts, False):
            return None

        sql_stmt = "%s;" % self._build_statement(statement_parts)
        statements.append(sql_stmt)

        return statements

    @staticmethod
    def _convert_option_values(option_values):
        """Convert a list of option=value to a list of names and name, value
        pairs.

        This method takes a list like the following where each element is a
        name=value string:

        (a=1, b=3, c=5, d=4)

        turning into a tuple containing a list of names and a list of
        name,value pairs as follows:

        ((a,b,c,d), ((a,1),(b,3),(c,5),(d,4)))

        Value pairs that do not have a value are ignored. For example,
        'a=3, b, c=2' will ignore 'b' but return a and c.

        option_values[in]  list of name=value strings

        Returns tuple - (list of names, list of (name, value))
        """
        names = []
        name_values = []
        for value_pair in option_values:
            name_value = value_pair.split('=')
            # Ignore any value pairs that do not have a value
            if len(name_value[0]) > 0:
                names.append(name_value[0].upper())
                name_values.append(name_value)
        return (names, name_values)

    @staticmethod
    def _find_value(name, name_values):
        """Find a value for a name in a list of tuple (name, value)

        name[in]           name of pair
        name_values[in]    list of tuples

        Returns string - value at index of match or None
        """
        name = name.upper()
        for item in name_values:
            if item[0].upper() == name:
                try:
                    return item[1]
                except IndexError:
                    return None

        return None

    def _parse_table_options(self, destination, source):
        """Parse the table options into a list and compare.

        This method returns a comma-separated list of table options that
        differ from the destination to the source.

        destination[in]    the original object definition or data
        source[in]         the source object definition or data

        Returns string - comma-separated values for table options that differ
                         or None if options are found in the destination that
                         are not in the source. These, we do not know how
                         to remove or turn off without extensive, specialized
                         code.
        """
        from mysql.utilities.common.dbcompare import get_common_lists

        # Here we have a comma-separated list of options in the form
        # name=value. To determine the inclusion/exclusion lists, we
        # must compare on names only so we make a list for each of only
        # the names.
        dest_opts_names = []
        dest_opts = [item.strip() for item in destination.split(',')]
        dest_opts_names, dest_opts_val = self._convert_option_values(dest_opts)
        dest_opts_names.sort()
        src_opts = [item.strip() for item in source.split(',')]
        src_opts_names, src_opts_val = self._convert_option_values(src_opts)
        src_opts_names.sort()
        in_both, in_dest_not_src, in_src_not_dest = \
            get_common_lists(dest_opts_names, src_opts_names)

        # Whoops! There are things set in the destination that aren't in the
        # source so we don't know if these are Ok or if we need to do
        # something special.
        if len(in_dest_not_src) > 0:
            return None

        changes = []
        # Now check for changes for both
        for name in in_both:
            dest_val = self._find_value(name, dest_opts_val)
            src_val = self._find_value(name, src_opts_val)
            if dest_val is not None and dest_val != src_val:
                changes.append("%s=%s" % (name.upper(), src_val))

        # Get values for those not in destination
        for item in in_src_not_dest:
            val = self._find_value(item, src_opts_val)
            if val is not None:
                changes.append("%s=%s" % (item.upper(), val))

        return ', '.join(changes)

    def _get_table_defns(self, destination, source):
        """Get the transform fpr the general options for a table

        This method creates an ALTER TABLE statement for table definitions
        that differ. The items covered include only those options described
        in the reference manual as table_options and include the following:

            engine, auto_increment, avg_row_count, checksum, collation,
            comment, and create options

        destination[in]    the original object definition or data
        source[in]         the source object definition or data

        Returns string - ALTER TABLE clause or None if no transform needed
        """
        changes = self._check_columns([_TABLE_COMMENT], destination, source)

        # build a list of the parts
        statement_parts = [
            # rename
            {'fmt': "RENAME TO %s.%s \n", 'col': _IGNORE_COLUMN, 'val': ""},
            # engine
            {'fmt': "ENGINE=%s", 'col': _TABLE_ENGINE, 'val': ""},
            # auto increment
            {'fmt': "AUTO_INCREMENT=%s", 'col': _TABLE_AUTO_INCREMENT,
             'val': ""},
            # collation
            {'fmt': "COLLATE=%s", 'col': _TABLE_COLLATION, 'val': ""},
            # comment - always include to ensure comments can be removed
            {'fmt': "COMMENT='%s'", 'col': _IGNORE_COLUMN,
             'val': source[_TABLE_COMMENT]},
            # create options - will be completed later
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': ""},
        ]

        dest_create = destination[_TABLE_CREATE_OPTIONS]
        src_create = source[_TABLE_CREATE_OPTIONS]
        if dest_create != src_create:
            create = statement_parts[5]
            opt_val = self._parse_table_options(dest_create, src_create)
            if opt_val is None:
                return ("# WARNING: the destination table contains options "
                        "that are not in the source.\n# Cannot generate ALTER "
                        "statement.")
            else:
                create['val'] = "%s" % opt_val
                changes = True

        # if no changes, return None
        if not changes and not self._fill_values(statement_parts, False,
                                                 destination, source):
            return None

        # We need to check the comment again and include it if source == ''
        if self._check_columns([_TABLE_COMMENT], destination, source) and \
           source[_TABLE_COMMENT] == '':
            statement_parts[4]['col'] = _FORCE_COLUMN

        # Check for rename
        if destination[_TABLE_NAME] != source[_TABLE_NAME]:
            statement_parts[0]['val'] = (source[_DB_NAME], source[_TABLE_NAME])

        # check and set commas
        do_comma = False
        for part in statement_parts:
            if do_comma:
                part['fmt'] = ', ' + part['fmt']
            elif part['col'] == _FORCE_COLUMN or part['val'] != '':
                do_comma = True

        return self._build_statement(statement_parts)

    @staticmethod
    def _get_column_format(col_data):
        """Build the column data type format string

        col_data[in]       the row containing the column definition

        Retuns string - column data type format
        """
        if col_data is None:
            return ""
        col_fmt = "%(type)s%(null)s%(default)s%(extra)s%(comment)s"
        values = {
            'type': col_data[_COLUMN_TYPE],
            'null': "",
            'default': "",
            'extra': "",
            'comment': "",
        }
        if col_data[_COLUMN_IS_NULLABLE].upper() == "NO":
            values['null'] = " NOT NULL"
        else:
            values['null'] = " NULL"
        if col_data[_COLUMN_DEFAULT] is not None:
            def_val = col_data[_COLUMN_DEFAULT]
            # add quotes if needed
            if def_val.upper() != "CURRENT_TIMESTAMP":
                def_val = to_sql(def_val)
            values['default'] = " DEFAULT %s" % def_val
        if len(col_data[_COLUMN_EXTRA]) > 0:
            if col_data[_COLUMN_EXTRA].upper() != "AUTO_INCREMENT":
                values['extra'] = " %s" % col_data[_COLUMN_EXTRA]
        if len(col_data[_COLUMN_COMMENT]) > 0:
            values['comment'] = " COMMENT '%s'" % col_data[_COLUMN_COMMENT]
        return col_fmt % values

    @staticmethod
    def _get_column_position(destination_def, source_def, destination, source,
                             drop_cols, add_cols):
        """Get the column position in the list

        destination_def[in] destination column definition
        source_def[in]      source column definition
        destination[in]     destination column definitions
        source[in]          source column definitions
        drop_cols[in]       list of columns to be dropped - used to
                            calculate position of existing columns by
                            eliminating those cols in destination that will be
                            dropped
        add_cols[in]        list of columns to be added - used to
                            calculate position of existing columns by
                            eliminating those cols in destination that will be
                            dropped

        Returns string - 'BEFORE' or 'AFTER' for column position or "" if
                         position cannot be determined (add or drop column)
        """

        # Converting ordinal position to index positions:
        #
        #    - ordinal positions start counting at 1
        #    - list indexes start at 0
        #
        # So if you want to find the column that is one less than the ordinal
        # position of the current column, you must subtract 1 then subtract 1
        # again to convert it to the list index.

        dest_loc_idx = None
        src_loc_idx = int(source_def[_COLUMN_ORDINAL_POSITION]) - 1
        if destination_def is not None:
            dest_loc_idx = int(destination_def[_COLUMN_ORDINAL_POSITION]) - 1

        # Check to see if previous column has been dropped. If it has,
        # don't include the BEFORE|AFTER - it will be ordered correctly.
        if dest_loc_idx is not None and dest_loc_idx - 1 >= 0 and \
           destination[dest_loc_idx - 1][_COLUMN_NAME] in drop_cols:
            return ""

        # Check to see if previous column has been added. If it has,
        # don't include the BEFORE|AFTER - it will be ordered correctly.
        if (src_loc_idx - 1 >= 0 and
                source[src_loc_idx - 1][_COLUMN_NAME] in add_cols):
            return ""

        # compare ordinal position - if not the same find where it goes
        if dest_loc_idx is None or dest_loc_idx != src_loc_idx:
            if src_loc_idx == 0:
                return " FIRST"
            for col in source:
                if src_loc_idx == int(col[_COLUMN_ORDINAL_POSITION]):
                    return " AFTER %s" % col[_COLUMN_NAME]
        return ""

    @staticmethod
    def _find_column(name, columns):
        """Find a column in a list by name

        name[in]           name of the column
        columns[in]        list of column definitions

        Returns - column definition or None if column not found
        """
        for col_def in columns:
            if name == col_def[_COLUMN_NAME]:
                return col_def
        return None

    def _get_column_change(self, column, destination, source,
                           drop_cols, add_cols):
        """Identify if column differs and return the changes

        column[in]         column name and operation type
        destination[in]    column definitions for destination
        source[in]         column definitions for source
        drop_cols[in]      list of columns to be dropped - used to
                           calculate position of existing columns
        add_cols[in]       list of columns to be added - used to
                           calculate position of existing columns

        Returns string - new changes for column or ""
        """
        operation = column[1]

        # Get column from the origins
        destination_def = self._find_column(column[0], destination)
        source_def = self._find_column(column[0], source)

        # Here we look for columns that are set for checking the order but
        # the extra data (null, etc.) is different. So we change it to
        # a type change instead. Exclude key column in compare.
        if operation == _CHANGE_COL_ORDER and \
           destination_def[:_COLUMN_KEY] != source_def[:_COLUMN_KEY]:
            operation = _CHANGE_COL_TYPE

        # Check for drop column
        if operation == _DROP_COL:
            colstr = "  DROP COLUMN %s" % destination_def[_COLUMN_NAME]
        else:
            # Determine position and get the type format string
            col_pos = self._get_column_position(destination_def, source_def,
                                                destination, source,
                                                drop_cols, add_cols)
            col_fmt = self._get_column_format(source_def)

            # Check for order changes
            if operation == _CHANGE_COL_ORDER:
                if len(col_pos) > 0:
                    colstr = "  CHANGE COLUMN %s %s %s%s" % \
                             (source_def[_COLUMN_NAME],
                              source_def[_COLUMN_NAME],
                              col_fmt, col_pos)
                else:
                    colstr = ""  # No change needed here
            # Add or change column
            elif operation == _ADD_COL:
                colstr = "  ADD COLUMN %s %s%s" % (source_def[_COLUMN_NAME],
                                                   col_fmt, col_pos)
            else:  # must be change
                colstr = "  CHANGE COLUMN %s %s " % \
                         (destination_def[_COLUMN_NAME],
                          destination_def[_COLUMN_NAME])
                colstr += "%s%s" % (col_fmt, col_pos)

        return colstr

    def _get_columns(self, destination, source):
        """Get the column definition changes

        This method loops through the columns and if different builds ALTER
        statments for transforming the columns of the destination table to the
        source table.

        destination[in]    the original object definition or data
        source[in]         the source object definition or data

        Returns string - ALTER statement or None if no column differences.
        """
        from mysql.utilities.common.dbcompare import get_common_lists

        drop_clauses = []
        add_clauses = []

        # Build lists with minimal matching data (column name and type) for
        # destination and source. Then do the compare. Result is as follows:
        #
        #   - those in both (name, type) will need to be checked for order
        #     of cols to generate CHANGE COLUMN x x <type> BEFORE|AFTER x
        #   - those in destination but not source will be dropped unless the
        #     name appears in source but not destination to generate
        #     DROP COULMN x
        #   - those in destination but not source where the name does appear in
        #     source is a change of type to generate CHANGE COLUMN x x <type>
        #   - those in source but not destination that don't match by name in
        #     destination but not source are new columns to generate
        #     ADD COLUMN x <type>
        #   - those columns that match on both name and type need to be
        #     checked for order changes to generate the
        #     CHANGE COLUMN x BEFORE|AFTER
        #   - we need to check those that the column order changes to see
        #     if they are actually extra col def changes

        dest_min = [item[1:3] for item in destination]  # name, type
        src_min = [item[1:3] for item in source]  # name, type

        # find matches by name + type
        # <both_min>, <dest_src_min>, <src_dest_min> = get_common_lists
        (both_min, _, _,) = get_common_lists(dest_min, src_min)
        dest_src_names = [item[0] for item in dest_min]  # only name
        src_dest_names = [item[0] for item in src_min]  # only name

        # find matches by name only
        both_names = [item[0] for item in both_min]   # only name
        both_check, dest_drop, src_new = get_common_lists(dest_src_names,
                                                          src_dest_names)

        # find matches by name but not type
        both_change_type = list(set(both_check) - set(both_names))

        # remove type changes and form list for checking order
        both_change_order = list(set(both_names) - set(both_change_type))

        column_drops = []
        column_changes = []  # a list of tuples in form (col_name, operation)

        # Form drops
        for col in dest_drop:
            column_drops.append((col, _DROP_COL))

        # Build the drop statements
        for col in column_drops:
            change_str = self._get_column_change(col, destination, source,
                                                 dest_drop, src_new)
            if len(change_str) > 0:
                # if first is specified, push to front of list
                if change_str.endswith(" FIRST"):
                    drop_clauses.insert(0, change_str)
                else:
                    drop_clauses.append(change_str)

        # Form change type
        for col in both_change_type:
            column_changes.append((col, _CHANGE_COL_TYPE))

        # Form add column
        for col in src_new:
            column_changes.append((col, _ADD_COL))

        # Form change order
        for col in both_change_order:
            column_changes.append((col, _CHANGE_COL_ORDER))

        # Build the add/change statements
        for col in column_changes:
            change_str = self._get_column_change(col, destination, source,
                                                 dest_drop, src_new)
            if len(change_str) > 0:
                # if first is specified, push to front of list
                if change_str.endswith(" FIRST"):
                    add_clauses.insert(0, change_str)
                else:
                    add_clauses.append(change_str)

        return (drop_clauses, add_clauses)

    def _get_foreign_keys(self, src_db, src_name, dest_db, dest_name):
        """Get the foreign key constraints

        This method returns the table foreign keys via ALTER TABLE clauses
        gathered from the Table class methods.

        src_db[in]         database name for source table
        src_name[in]       table name for source table
        dest_db[in]        database name for destination table
        dest_name[in]      table name for destination table

        Returns tuple - (drop, add/changes)
        """
        from mysql.utilities.common.table import Table
        from mysql.utilities.common.dbcompare import get_common_lists

        # Get the Table instances
        self.dest_tbl = Table(self.destination_db.source, "%s.%s" %
                              (dest_db, dest_name))
        self.src_tbl = Table(self.source_db.source, "%s.%s" %
                             (src_db, src_name))

        drop_constraints = []
        add_constraints = []

        # Now we do foreign keys
        dest_fkeys = self.dest_tbl.get_tbl_foreign_keys()
        src_fkeys = self.src_tbl.get_tbl_foreign_keys()

        # Now we determine the foreign keys we need to add and those to drop
        # <both_min>, <dest_src_min>, <src_dest_min> = get_common_lists
        _, drop_rows, add_rows = get_common_lists(dest_fkeys, src_fkeys)

        # Generate DROP foreign key clauses
        for fkey in drop_rows:
            drop_constraints.append("  DROP FOREIGN KEY %s" % fkey[0])
            # if fkey[0] not in drop_idx_recorded:
            #    constraints.append("  DROP INDEX %s" % fkey[0])

        # Generate Add foreign key clauses
        clause_fmt = "ADD CONSTRAINT %s FOREIGN KEY(%s) REFERENCES " + \
                     "`%s`.`%s`(%s)"
        for fkey in add_rows:
            add_constraints.append(clause_fmt % fkey)

        return (drop_constraints, add_constraints)

    @staticmethod
    def _get_index_sql_clauses(rows, sql_mode=''):
        """Return the ALTER TABLE index clauses for the table.

        This method returns the SQL index clauses for use in ALTER or CREATE
        TABLE commands for defining the indexes for the table.

        rows[in]           result set of index definitions

        Returns list - list of SQL index clause statements or
                       [] if no indexes
        """
        index_clauses = []

        if rows != []:
            pri_key_cols = []
            unique_indexes = []
            unique_key_cols = []
            unique_name = None
            unique_method = None
            unique_setting = None
            for key in rows:
                if key[2] == 'PRIMARY':
                    q_key = quote_with_backticks(key[4], sql_mode)
                    pri_key_cols.append(q_key)
                else:
                    if unique_name is None:
                        unique_name = key[2]
                        unique_method = key[10]
                        unique_setting = key[1]
                        unique_key_cols.append(key[4])
                    elif unique_name == key[2]:
                        unique_key_cols.append(key[4])
                    else:
                        unique_indexes.append((unique_name, unique_method,
                                               unique_setting,
                                               unique_key_cols))
                        unique_key_cols = []
                        unique_name = key[2]
                        unique_method = key[10]
                        unique_setting = key[1]
                        unique_key_cols.append(key[4])

            # add the last one
            if unique_name is not None:
                unique_indexes.append((unique_name, unique_method,
                                       unique_setting,
                                       unique_key_cols))

            # Build SQL statement clause
            if len(pri_key_cols) > 0:
                index_clauses.append("  ADD PRIMARY KEY(%s)" %
                                     ','.join(pri_key_cols))
            if len(unique_indexes) > 0:
                for idx in unique_indexes:
                    create_idx = "  ADD "
                    if int(idx[2]) != 1:
                        create_idx += "UNIQUE "
                    if idx[1] == "FULLTEXT":
                        create_idx += "FULLTEXT "
                    if (idx[1] == "RTREE"):
                        using = " USING %s" % (idx[1])
                    else:
                        using = ""
                    create_idx += "INDEX %s%s (%s)" % \
                                  (idx[0], using,
                                   ','.join(idx[3]))
                    index_clauses.append(create_idx)

        return index_clauses

    def _get_indexes(self, src_db, src_name, dest_db, dest_name):
        """Get the index constraints

        This method returns the table primary keys, and other indexes via
        ALTER TABLE clauses gathered from the Table class methods.

        src_db[in]         database name for source table
        src_name[in]       table name for source table
        dest_db[in]        database name for destination table
        dest_name[in]      table name for destination table

        Returns tuple - (drop, add/changes)
        """
        from mysql.utilities.common.table import Table
        from mysql.utilities.common.dbcompare import get_common_lists

        # Get the Table instances
        self.dest_tbl = Table(self.destination_db.source, "%s.%s" %
                              (dest_db, dest_name))
        self.src_tbl = Table(self.source_db.source, "%s.%s" %
                             (src_db, src_name))

        drop_indexes = []
        add_indexes = []

        # Get the list of indexes
        # Do not compare with the name of the tables
        dest_idx = [('',) + tuple(idx[1:])
                    for idx in self.dest_tbl.get_tbl_indexes()]
        src_idx = [('',) + tuple(idx[1:])
                   for idx in self.src_tbl.get_tbl_indexes()]

        # Now we determine the indexes we need to add and those to drop
        _, drop_idx, add_idx = get_common_lists(dest_idx, src_idx)
        if not drop_idx and not add_idx:
            return ([], [])

        # Generate DROP index clauses
        drop_idx_recorded = []  # used to avoid duplicate index drops
        for index in drop_idx:
            if index[2] == "PRIMARY":
                drop_indexes.append("  DROP PRIMARY KEY")
            elif index[2] not in drop_idx_recorded:
                drop_indexes.append("  DROP INDEX %s" % index[2])
                drop_idx_recorded.append(index[2])

        # Generate ADD index clauses
        if len(add_idx) > 0:
            add_indexes.extend(self._get_index_sql_clauses(
                add_idx,
                self.dest_tbl.sql_mode
            ))

        return (drop_indexes, add_indexes)

    @staticmethod
    def _check_for_partitions(destination_row, source_row):
        """Determine if there are transformations involving partitions

        This method returns TRUE if the destination and source differ in
        partitioning configurations

        destination_row[in] the original object definition or data
        source_row[in]      the source object definition or data

        Returns bool - True = differences found, False = no differences
        """
        #
        # TODO: Complete this operation with a new worklog.
        #       This release does not support transformation of partitions.

        part_changes_found = False
        if len(destination_row) != len(source_row):
            part_changes_found = True
        elif len(destination_row) == 0:
            return None
        elif len(destination_row) == 1:
            if not (destination_row[0][3] is None and
                    source_row[0][3] is None):
                part_changes_found = True
        else:
            part_stop = len(destination_row)
            row_stop = len(destination_row[0])
            for i in range(0, part_stop):
                for j in range(0, row_stop):
                    if destination_row[i][j] != source_row[i][j]:
                        part_changes_found = True
                        break
        return part_changes_found

    def _transform_table(self):
        """Transform a table definition

        This method will transform a table definition to match the source
        configuration. It returns the ALTER TABLE SQL statement to
        transform the object or None if no transformation is needed.

        Note: The incoming lists contain a tuple defined as:
              (table definitions, columns, partitions, constraints)
              for destination and source.

        Returns list - ALTER TABLE statements for transforming the table
        """
        statements = []

        # Collect a list of all of the ALTER clauses. Order is important in
        # building an ALTER TABLE statement. For safety (and correct execution)
        # we must order the clauses as follows:
        #
        #  - drop foreign key constraints
        #  - drop indexes
        #  - drop columns
        #  - add/change columns
        #  - add/change indexes
        #  - add/change foreign keys
        #  - general table changes
        #
        #  Note: partition changes not supported by this release

        src_db_name = self.source[_TABLE_DEF][_TABLE_DB]
        src_tbl_name = self.source[_TABLE_DEF][_TABLE_NAME]
        dest_db_name = self.destination[_TABLE_DEF][_TABLE_DB]
        dest_tbl_name = self.destination[_TABLE_DEF][_TABLE_NAME]

        # Quote identifiers with bacticks
        src_sql_mode = self.source_db.sql_mode
        q_src_db_name = quote_with_backticks(src_db_name, src_sql_mode)
        q_src_tbl_name = quote_with_backticks(src_tbl_name, src_sql_mode)
        dest_sql_mode = self.destination_db.sql_mode
        q_dest_db_name = quote_with_backticks(dest_db_name, dest_sql_mode)
        q_dest_tbl_name = quote_with_backticks(dest_tbl_name, dest_sql_mode)

        # build a list of the parts
        statement_parts = [
            # preamble
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': "ALTER TABLE"},
            # object name
            {'fmt': " %s.%s", 'col': _IGNORE_COLUMN,
             'val': (q_dest_db_name, q_dest_tbl_name)},
            # alter clauses - will be completed later
            {'fmt': " \n%s", 'col': _IGNORE_COLUMN, 'val': ""},
        ]

        # For foreign key changes, we need two collections: drop statements,
        # add and change statements. Method returns tuple of (drop, add).
        fkeys = self._get_foreign_keys(q_src_db_name, q_src_tbl_name,
                                       q_dest_db_name, q_dest_tbl_name)

        # For index changes, we need two collections: drop statements, add and
        # change statements. Method returns tuple of (drop, add).
        indexes = self._get_indexes(q_src_db_name, q_src_tbl_name,
                                    q_dest_db_name, q_dest_tbl_name)

        # For column changes, we need two collections: drop statements, add and
        # change statements. Method returns tuple of (drop, add/change).
        columns = self._get_columns(self.destination[_COLUMN_DEF],
                                    self.source[_COLUMN_DEF])

        # Now add drops then add/changes
        for i in range(0, 2):
            statements.extend(fkeys[i])
            statements.extend(indexes[i])
            statements.extend(columns[i])

        # General definition returns a single string of the option changes
        if not self.skip_table_opts:
            gen_defn = self._get_table_defns(self.destination[_TABLE_DEF],
                                             self.source[_TABLE_DEF])
        else:
            gen_defn = None

        if gen_defn is not None:
            statements.append(gen_defn)

        # Form the SQL command.
        statement_parts[2]['val'] = ', \n'.join(statements)

        sql_stmts = ["%s;" % self._build_statement(statement_parts)]

        # Currently, we check partitions last because this code will
        # generate a warning message. Later once this code it complete,
        # it can be moved where it belongs in the order of creation of
        # the ALTER TABLE statement
        if self._check_for_partitions(self.destination[_PART_DEF],
                                      self.source[_PART_DEF]):
            sql_stmts.append("# WARNING: Partition transformation is not "
                             "supported in this release.\n# Please check "
                             "the table definitions for partition changes.")

        return sql_stmts

    def _transform_view(self):
        """Transform a view definition

        This method will transform a view definition to match the source
        configuration. It returns the CREATE OR ALTER VIEW SQL statement to
        transform the object or None if no transformation is needed.

        Returns list - ALTER VIEW statement for transforming the view
        """
        statements = []

        # check for create
        do_create = self._check_columns([_VIEW_CHECK])

        # build a list of the parts
        statement_parts = [
            # preamble
            {'fmt': "%s", 'col': _IGNORE_COLUMN,
             'val': "CREATE" if do_create else "ALTER"},
            # definer
            {'fmt': " DEFINER=%s", 'col': _VIEW_DEFINER, 'val': ""},
            # security
            {'fmt': " SQL SECURITY %s", 'col': _VIEW_SECURITY, 'val': ""},
            # object type and name
            {'fmt': " VIEW %s.%s", 'col': _IGNORE_COLUMN,
             'val': (self.destination[_VIEW_DB],
                     self.destination[_VIEW_NAME])},
            # definition
            {'fmt': " AS \n  %s", 'col': _VIEW_BODY, 'val': ""},
            # check option (will be updated later)
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': ""}
        ]

        changes = False
        # view check option is special - we have to handle that separately
        if self.destination[_VIEW_CHECK] != self.source[_VIEW_CHECK]:
            if self.source[_VIEW_CHECK].upper() != 'NONE':
                check = statement_parts[5]
                check['val'] = " WITH %s CHECK OPTION" % \
                               self.source[_VIEW_CHECK]
            changes = True

        # if no changes, return None
        if not changes and not self._fill_values(statement_parts, do_create):
            return None

        # check to see if definer or security or check option have changed and
        # if so add definition (always needed if these change)
        if self._check_columns([_VIEW_DEFINER, _VIEW_SECURITY, _VIEW_CHECK]):
            statement_parts[4]['val'] = self.source[_VIEW_BODY]

        # form the drop if we do a create
        if do_create:
            statements.append("DROP VIEW IF EXISTS `%s`.`%s`;" %
                              (self.destination[_VIEW_DB],
                               self.destination[_VIEW_NAME]))

        sql_stmt = "%s;" % self._build_statement(statement_parts)
        statements.append(sql_stmt)

        return statements

    def _transform_trigger(self):
        """Transform a trigger definition

        This method will transform a trigger definition to match the source
        configuration. It returns the appropriate SQL statement(s) to
        transform the object or None if no transformation is needed.

        Returns list - SQL statement(s) for transforming the trigger
        """
        statements = []

        # build a list of the parts
        statement_parts = [
            # preamble
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': "CREATE"},
            # definer
            {'fmt': " DEFINER=%s", 'col': _TRIGGER_DEFINER, 'val': ""},
            # object name
            {'fmt': " TRIGGER %s.%s", 'col': _IGNORE_COLUMN,
             'val': (self.destination[_TRIGGER_DB],
                     self.destination[_TRIGGER_NAME])},
            # trigger timing
            {'fmt': " %s", 'col': _TRIGGER_TIME, 'val': ""},
            # trigger event
            {'fmt': " %s", 'col': _TRIGGER_EVENT, 'val': ""},
            # trigger table
            {'fmt': " ON %s." % self.destination[_TRIGGER_DB] +
                    "%s FOR EACH ROW",
             'col': _TRIGGER_TABLE, 'val': ""},
            # trigger body
            {'fmt': " %s;", 'col': _TRIGGER_BODY, 'val': ""},
        ]

        # Triggers don't have ALTER SQL so we just pass back a drop + create.
        # if no changes, return None
        if not self._fill_values(statement_parts, True):
            return None

        statements.append("DROP TRIGGER IF EXISTS `%s`.`%s`;" %
                          (self.destination[_TRIGGER_DB],
                           self.destination[_TRIGGER_NAME]))

        sql_stmt = self._build_statement(statement_parts)
        statements.append(sql_stmt)

        return statements

    def _transform_routine(self):
        """Transform a routine definition

        This method will transform a routine (FUNCTION or PROCEDURE) definition
        to match the source configuration. It returns the ALTER [FUNCTION |
        PROCEDURE] SQL statement to transform the object or None if no
        transformation is needed.

        Returns list - [CREATE|ALTER] [FUNCTION|PROCEDURE] statement for
                       transforming the routine
        """
        statements = []

        # check for create
        do_create = self._check_columns([_ROUTINE_BODY,
                                         _ROUTINE_DEFINER,
                                         _ROUTINE_PARAMS])

        # Quote destination db and routine names with backticks
        dest_sql_mode = self.destination_db.sql_mode
        q_dest_db = quote_with_backticks(self.destination[_ROUTINE_DB],
                                         dest_sql_mode)
        q_dest_routine = quote_with_backticks(self.destination[_ROUTINE_NAME],
                                              dest_sql_mode)

        # build a list of the parts
        statement_parts = [
            # delimiter
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': "DELIMITER //\n"},
            # preamble
            {'fmt': "%s", 'col': _IGNORE_COLUMN,
             'val': "CREATE" if do_create else "ALTER"},
            # definer
            {'fmt': " DEFINER=%s", 'col': _ROUTINE_DEFINER,
             'val': ""},
            # object type and name
            {'fmt': " %s %s.%s", 'col': _IGNORE_COLUMN,
             'val': (self.obj_type.upper(), q_dest_db, q_dest_routine)},
            # parameters
            {'fmt': " %s", 'col': _IGNORE_COLUMN, 'val': ""},
            # returns (Functions only)
            {'fmt': " RETURNS %s", 'col': _IGNORE_COLUMN, 'val': ""},
            # access method
            {'fmt': " %s", 'col': _ROUTINE_SQL_DATA_ACCESS, 'val': ""},
            # deterministic (Functions only)
            {'fmt': " %s", 'col': _IGNORE_COLUMN, 'val': ""},
            # security
            {'fmt': " SQL SECURITY %s", 'col': _ROUTINE_SECURITY_TYPE,
             'val': ""},
            # comment
            {'fmt': " COMMENT '%s'", 'col': _ROUTINE_COMMENT, 'val': ""},
            # body
            {'fmt': " %s", 'col': _ROUTINE_BODY, 'val': ""},
            # reset delimiter
            {'fmt': "%s", 'col': _IGNORE_COLUMN,
             'val': "//\nDELIMITER ;\n"},
        ]

        # if no changes, return None
        if not self._fill_values(statement_parts, do_create):
            return None

        # Add parameters and DEFINER if CREATE statement.
        if do_create:
            statement_parts[4]['val'] = \
                '({0})'.format(self.source[_ROUTINE_PARAMS])

            # Quote DEFINER with backticks
            statement_parts[2]['val'] = \
                quote_with_backticks_definer(self.source[_ROUTINE_DEFINER],
                                             dest_sql_mode)

        # Add the returns for functions
        # Only when doing create or modifications to the body
        if self.obj_type.upper() == "FUNCTION":
            if (do_create or
                    (self.destination[_ROUTINE_BODY] !=
                     self.source[_ROUTINE_BODY])):
                statement_parts[5]['val'] = self.source[_ROUTINE_RETURNS]
            # Add deterministic
            if do_create:
                if self.source[_ROUTINE_IS_DETERMINISTIC] == "YES":
                    statement_parts[7]['val'] = "DETERMINISTIC"
                else:
                    statement_parts[7]['val'] = "NOT DETERMINISTIC"

        # form the drop if we do a create
        if do_create:
            statements.append(
                "DROP {0} IF EXISTS {1}.{2};".format(
                    self.obj_type.upper(), q_dest_db, q_dest_routine
                )
            )

        statements.append(self._build_statement(statement_parts))

        return statements

    def _transform_event(self):
        """Transform a event definition

        This method will transform a event definition to match the source
        configuration. It returns the ALTER EVENT SQL statement to
        transform the object or None if no transformation is needed.

        Notes:

            The DEFINER does not compare properly for SHOW CREATE EVENT
            comparison.

            The RENAME cannot be processed because it requires a different
            name and mysqldiff compares on like names.

        Returns list - ALTER EVENT statement for transforming the event
        """
        statements = []

        # build a list of the parts
        statement_parts = [
            # preamble
            {'fmt': "%s", 'col': _IGNORE_COLUMN, 'val': "ALTER"},
            # definer
            {'fmt': " DEFINER=%s", 'col': _EVENT_DEFINER, 'val': ""},
            # type
            {'fmt': " %s", 'col': _IGNORE_COLUMN, 'val': "EVENT"},
            # object name
            {'fmt': " %s.%s", 'col': _IGNORE_COLUMN,
             'val': (self.destination[_EVENT_DB],
                     self.destination[_EVENT_NAME])},
            # schedule - will be filled in later
            {'fmt': " %s", 'col': _IGNORE_COLUMN, 'val': ""},
            # complete
            {'fmt': " ON COMPLETION %s", 'col': _EVENT_ON_COMPLETION,
             'val': ""},
            # rename
            {'fmt': " RENAME TO %s", 'col': _EVENT_NAME, 'val': ""},
            # status
            {'fmt': " %s", 'col': _EVENT_STATUS,
             'val': self.source[_EVENT_STATUS]},
            # event body
            {'fmt': " DO %s", 'col': _EVENT_BODY, 'val': ""},
        ]

        # We can only do the columns we know about and must ignore the others
        # like STARTS which may be Ok to differ.
        changes = self._check_columns([_EVENT_ON_COMPLETION, _EVENT_STATUS,
                                       _EVENT_BODY, _EVENT_NAME, _EVENT_ENDS,
                                       _EVENT_INTERVAL_FIELD, _EVENT_STARTS,
                                       _EVENT_INTERVAL_VALUE, _EVENT_TYPE])

        # We do the schedule separately because requires additional checks
        if changes:
            schedule = statement_parts[4]
            schedule['val'] = "ON SCHEDULE"
            if self.source[_EVENT_TYPE].upper() == "RECURRING":
                schedule['val'] += " EVERY %s" % \
                                   self.source[_EVENT_INTERVAL_VALUE]
            schedule['val'] += " %s" % \
                               self.source[_EVENT_INTERVAL_FIELD].upper()
            if self.source[_EVENT_STARTS] is not None:
                schedule['val'] += " STARTS '%s'" % self.source[_EVENT_STARTS]
            if self.source[_EVENT_ENDS] is not None:
                schedule['val'] += " ENDS '%s'" % self.source[_EVENT_ENDS]

        # if no changes, return None
        if not changes:
            return None

        self._fill_values(statement_parts, False)

        # We must fix the status value
        status = statement_parts[7]
        if status['val'].upper() == "DISABLED":
            status['val'] = "DISABLE"
        elif status['val'].upper() == "ENABLED":
            status['val'] = "ENABLE"
        elif status['val'].upper() == "SLAVESIDE_DISABLED":
            status['val'] = "DISABLE ON SLAVE"

        sql_stmt = "%s;" % self._build_statement(statement_parts)
        statements.append(sql_stmt)

        return statements

    def _check_columns(self, col_list, destination=None, source=None):
        """Check for special column changes to trigger a CREATE

        This method checks a specific list of columns to see if the values
        differ from the destination and source. If they do, the method returns
        True else it returns False.

        col_list[in]       a list of column numbers to check
        destination[in]    If not None, use this list for destination
                           (default = None)
        source[in]         If not None, use this list for source
                           (default = None)

        Returns bool - True = there are differences, False = no differences
        """
        if destination is None:
            destination = self.destination
        if source is None:
            source = self.source
        for column_num in col_list:
            if destination[column_num] != source[column_num]:
                return True
        return False

    def _fill_values(self, stmt_parts, create=False,
                     destination=None, source=None):
        """Fill the structure with values

        This method loops through all of the column dictionaries filling in
        the value for any that differ from the destination to the source. If
        create is True, it will also fill in the values from the source to
        permit the completion of a CREATE statement.

        stmt_parts[in]     a list of column dictionaries
        create[in]         if True, fill in all values
                           if False, fill in only those values that differ
                           (default = False)
        destination[in]         If not None, use this list for destination
                           (default = None)
        source[in]         If not None, use this list for source
                           (default = None)

        Returns bool - True if changes found
        """
        if destination is None:
            destination = self.destination
        if source is None:
            source = self.source
        changes_found = False
        for part in stmt_parts:
            col = part['col']
            if col != _IGNORE_COLUMN:
                if source[col] is not None and destination[col] != source[col]:
                    part['val'] = source[col]
                    changes_found = True
                elif create:
                    part['val'] = destination[col]

        return changes_found

    @staticmethod
    def _build_statement(stmt_parts):
        """Build the object definition statement

        This method will build a completed statement based on the list of parts
        provided.

        stmt_parts[in]     a list of column dictionaries
        create[in]         if True, fill in all values
                           if False, fill in only those values that differ
                           (default = False)

        Returns string - the object definition string
        """
        stmt_values = []
        for part in stmt_parts:
            if part['col'] == _FORCE_COLUMN or part['val'] != "":
                stmt_values.append(part['fmt'] % part['val'])

        return ''.join(stmt_values)
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains abstractions of a MySQL table and an index.
"""

import multiprocessing
import sys
from itertools import izip

from mysql.connector.conversion import MySQLConverter

# Constants
_MAXPACKET_SIZE = 1024 * 1024
_MAXBULK_VALUES = 25000
_MAXTHREADS_INSERT = 6
_MAXROWS_PER_THREAD = 100000
_MAXAVERAGE_CALC = 100

_FOREIGN_KEY_QUERY = """
  SELECT CONSTRAINT_NAME, COLUMN_NAME, REFERENCED_TABLE_SCHEMA,
         REFERENCED_TABLE_NAME, REFERENCED_COLUMN_NAME
  FROM INFORMATION_SCHEMA.KEY_COLUMN_USAGE
  WHERE TABLE_SCHEMA = '%s' AND TABLE_NAME = '%s' AND
        REFERENCED_TABLE_SCHEMA IS NOT NULL
"""


class Index(object):
    """
    The Index class encapsulates an index for a given table as defined by
    the output of SHOW INDEXES FROM. The class has the following
    capabilities:

        - Check for duplicates
        - Create DROP statement for index
        - Print index CREATE statement
    """

    def __init__(self, db, index_tuple, verbose=False, sql_mode=''):
        """Constructor

        db[in]             Name of database
        index_tuple[in]    A tuple from the get_tbl_indexes() result set
        verbose[in]        print extra data during operations (optional)
                           default value = False
        """

        # Initialize and save values
        self.db = db
        self.sql_mode = sql_mode
        self.q_db = quote_with_backticks(db, self.sql_mode)
        self.verbose = verbose
        self.columns = []
        self.table = index_tuple[0]
        self.q_table = quote_with_backticks(index_tuple[0], self.sql_mode)
        self.unique = not int(index_tuple[1])
        self.name = index_tuple[2]
        self.q_name = quote_with_backticks(index_tuple[2], self.sql_mode)
        col = (index_tuple[4], index_tuple[7])
        self.columns.append(col)
        self.accept_nulls = True if index_tuple[9] else False
        self.type = index_tuple[10]
        self.compared = False                    # mark as compared for speed
        self.duplicate_of = None                 # saves duplicate index
        # pylint: disable=R0102
        if index_tuple[7] > 0:
            self.column_subparts = True          # check subparts e.g. a(20)
        else:
            self.column_subparts = False

    @staticmethod
    def __cmp_columns(col_a, col_b):
        """Compare two columns on name and subpart lengths if present

        col_a[in]          First column to compare
        col_b[in]          Second column to compare

        Returns True if col_a has the same name as col_b and if the
        subparts are col_a.sub <= col_b.sub.
        """

        sz_this = col_a[1]
        sz_that = col_b[1]
        # if column has the same name
        if col_a[0] == col_b[0]:
            # if they both have sub_parts, compare them
            if sz_this and sz_that:
                return (sz_this <= sz_that)
            # if this index has a sub_part and the other does
            # not, it is potentially redundant
            elif sz_this and sz_that is None:
                return True
            # if neither have sub_parts, it is a match
            elif sz_this is None and sz_that is None:
                return True
        else:
            return False  # no longer a duplicate

    def __check_column_list(self, index):
        """Compare the column list of this index with another

        index[in]          Instance of Index to compare

        Returns True if column list is a subset of index.
        """

        num_cols_this = len(self.columns)
        num_cols_that = len(index.columns)
        same_size = num_cols_this == num_cols_that
        if self.type == "BTREE":
            indexes = izip(self.columns, index.columns)
            for idx_pair in indexes:
                if not self.__cmp_columns(*idx_pair):
                    return False
            # All index pairs are the same, so return index with smaller number
            # of columns.
            return num_cols_this <= num_cols_that
        else:  # HASH, RTREE, FULLTEXT
            if self.type != "FULLTEXT":
                # For RTREE or HASH type indexes, an index is redundant if
                # it has the exact same columns on the exact same order.
                indexes = izip(self.columns, index.columns)
                return (same_size and
                        all((self.__cmp_columns(*idx_pair)
                             for idx_pair in indexes)))
            else:  # FULLTEXT index
                # A FULLTEXT index A is redundant of FULLTEXT index B if
                # the columns of A are a subset of B's columns, the order
                # does not matter.
                return all(any(self.__cmp_columns(col, icol) for
                               icol in index.columns) for col in self.columns)

    def is_duplicate(self, index):
        """Compare this index with another

        index[in]          Instance of Index to compare

        Returns True if this index is a subset of the Index presented.
        """

        # Don't compare the same index - no two indexes can have the same name
        if self.name == index.name:
            return False
        else:
            return self.__check_column_list(index)

    def contains_columns(self, col_names):
        """Check if the current index contains the columns of the given index.

        Returns True if it contains all the columns of the given index,
        otherwise False.
        """
        if len(self.columns) < len(col_names):
            # If has less columns than given index it does not contain all.
            return False
        else:
            this_col_names = [col[0] for col in self.columns]
            # Check if all index column are included in current one..
            for col_name in col_names:
                if col_name not in this_col_names:
                    return False  # found one column not included.

        # Pass previous verification; contains all the columns of given index.
        return True

    def add_column(self, column, sub_part, accept_null):
        """Add a column to the list of columns for this index

        column[in]         Column to add
        sub_part[in]       Sub part of colunm e.g. a(20)
        accept_null[in]       True to indicate the column accepts nulls
        """

        col = (column, sub_part)
        if sub_part > 0:
            self.column_subparts = True
        if accept_null:
            self.accept_nulls = True
        self.columns.append(col)

    def get_drop_statement(self):
        """Get the drop statement for this index

        Note: Ignores PRIMARY key indexes.

        Returns the DROP statement for this index.
        """
        if self.name == "PRIMARY":
            return None
        query_str = "ALTER TABLE {db}.{table} DROP INDEX {name}".format(
            db=self.q_db, table=self.q_table, name=self.q_name
        )
        return query_str

    def get_remove_columns_statement(self, col_names):
        """Get the ALTER TABLE statement to remove columns for this index.

        col_names[in]   list of columns names to remove from the index.

        Returns the ALTER TABLE statement (DROP/ADD) to remove the given
        columns names from the index.
        """
        # Create the new columns list for the index.
        idx_cols = [col[0] for col in self.columns if col[0] not in col_names]
        if not idx_cols:
            # Return a DROP statement if no columns are left.
            query_str = "ALTER TABLE {db}.{table} DROP INDEX {name}".format(
                db=self.q_db, table=self.q_table, name=self.q_name
            )
        else:
            # Otherwise, return a DROP/ADD statement with remaining columns.
            idx_cols_str = ', '.join(idx_cols)
            query_str = ("ALTER TABLE {db}.{table} DROP INDEX {name}, "
                         "ADD INDEX {name} ({cols})".format(db=self.q_db,
                                                            table=self.q_table,
                                                            name=self.q_name,
                                                            cols=idx_cols_str))
        return query_str

    def __get_column_list(self, backtick_quoting=True):
        """Get the column list for an index

        This method is used to print the CREATE and DROP statements.

        backtick_quoting[in]    Indicates if the columns names are to be quoted
                                with backticks or not. By default: True.

        Returns a string representing the list of columns for a
        column list. e.g. 'a, b(10), c'
        """
        col_list = []
        for col in self.columns:
            name, sub_part = (col[0], col[1])
            if backtick_quoting:
                name = quote_with_backticks(name, self.sql_mode)
            if sub_part > 0:
                col_str = "{0}({1})".format(name, sub_part)
            else:
                col_str = name
            col_list.append(col_str)
        return ', '.join(col_list)

    def print_index_sql(self):
        """Print the CREATE INDEX for indexes and ALTER TABLE for a primary key
        """
        if self.name == "PRIMARY":
            print("ALTER TABLE {db}.{table} ADD PRIMARY KEY ({cols})"
                  "".format(db=self.q_db, table=self.q_table,
                            cols=self.__get_column_list()))
        else:
            create_str = ("CREATE {unique}{fulltext}INDEX {name} ON "
                          "{db}.{table} ({cols}) {using}")
            unique_str = 'UNIQUE ' if self.unique else ''
            fulltext_str = 'FULLTEXT ' if self.type == 'FULLTEXT' else ''
            if (self.type == "BTREE") or (self.type == "RTREE"):
                using_str = 'USING {0}'.format(self.type)
            else:
                using_str = ''
            print(create_str.format(unique=unique_str, fulltext=fulltext_str,
                                    name=self.q_name, db=self.q_db,
                                    table=self.q_table,
                                    cols=self.__get_column_list(),
                                    using=using_str))

    def get_row(self, verbosity=0):
        """Return index information as a list of columns for tabular output.
        """
        cols = self.__get_column_list(backtick_quoting=False)
        if verbosity > 0:
            return (self.db, self.table, self.name, self.type, self.unique,
                    self.accept_nulls, cols)
        return (self.db, self.table, self.name, self.type, cols)


class Table(object):
    """
    The Table class encapsulates a table for a given database. The class
    has the following capabilities:

        - Check to see if the table exists
        - Check indexes for duplicates and redundancies
        - Print list of indexes for the table
        - Extract table data
        - Import table data
        - Copy table data
    """

    def __init__(self, server1, name, options=None):
        """Constructor

        server[in]         A Server object
        name[in]           Name of table in the form (db.table)
        options[in]        options for class: verbose, quiet, get_cols,
            quiet     If True, do not print information messages
            verbose   print extra data during operations (optional)
                      (default is False)
            get_cols  If True, get the column metadata on construction
                      (default is False)
        """
        if options is None:
            options = {}
        self.verbose = options.get('verbose', False)
        self.quiet = options.get('quiet', False)
        self.server = server1

        # Get sql_mode set on server
        self.sql_mode = self.server.select_variable("SQL_MODE")

        # Keep table identifier considering backtick quotes
        if is_quoted_with_backticks(name, self.sql_mode):
            self.q_table = name
            self.q_db_name, self.q_tbl_name = parse_object_name(name,
                                                                self.sql_mode)
            self.db_name = remove_backtick_quoting(self.q_db_name,
                                                   self.sql_mode)
            self.tbl_name = remove_backtick_quoting(self.q_tbl_name,
                                                    self.sql_mode)
            self.table = ".".join([self.db_name, self.tbl_name])
        else:
            self.table = name
            self.db_name, self.tbl_name = parse_object_name(name,
                                                            self.sql_mode)
            self.q_db_name = quote_with_backticks(self.db_name, self.sql_mode)
            self.q_tbl_name = quote_with_backticks(self.tbl_name,
                                                   self.sql_mode)
            self.q_table = ".".join([self.q_db_name, self.q_tbl_name])
        self.obj_type = "TABLE"
        self.pri_idx = None

        # We store each type of index in a separate list to make it easier
        # to manipulate
        self.btree_indexes = []
        self.hash_indexes = []
        self.rtree_indexes = []
        self.fulltext_indexes = []
        self.unique_not_null_indexes = None
        self.text_columns = []
        self.blob_columns = []
        self.bit_columns = []
        self.column_format = None
        self.column_names = []
        self.column_name_type = []
        self.q_column_names = []
        self.indexes_q_names = []
        if options.get('get_cols', False):
            self.get_column_metadata()
        self.dest_vals = None
        self.storage_engine = None

        # Get max allowed packet
        res = self.server.exec_query("SELECT @@session.max_allowed_packet")
        if res:
            self.max_packet_size = res[0][0]
        else:
            self.max_packet_size = _MAXPACKET_SIZE
        # Watch for invalid values
        if self.max_packet_size > _MAXPACKET_SIZE:
            self.max_packet_size = _MAXPACKET_SIZE

        self._insert = "INSERT INTO %s.%s VALUES "
        self.query_options = {  # Used for skipping fetch of rows
            'fetch': False
        }

    def exists(self, tbl_name=None):
        """Check to see if the table exists

        tbl_name[in]       table name (db.table)
                           (optional) If omitted, operation is performed
                           on the class instance table name.

        return True = table exists, False = table does not exist
        """

        db, table = (None, None)
        if tbl_name:
            db, table = parse_object_name(tbl_name, self.sql_mode)
        else:
            db = self.db_name
            table = self.tbl_name
        res = self.server.exec_query("SELECT TABLE_NAME " +
                                     "FROM INFORMATION_SCHEMA.TABLES " +
                                     "WHERE TABLE_SCHEMA = '%s'" % db +
                                     " and TABLE_NAME = '%s'" % table)

        return (res is not None and len(res) >= 1)

    def get_column_metadata(self, columns=None):
        """Get information about the table for the bulk insert operation.

        This method builds lists that describe the metadata of the table. This
        includes lists for:

          column names
          column format for building VALUES clause
          blob fields - for use in generating INSERT/UPDATE for blobs
          text fields - for use in checking for single quotes

        columns[in]        if None, use EXPLAIN else use column list.
        """

        if columns is None:
            columns = self.server.exec_query("explain %s" % self.q_table)
        stop = len(columns)
        self.column_names = []
        self.q_column_names = []
        col_format_values = [''] * stop
        if columns is not None:
            for col in range(0, stop):
                if is_quoted_with_backticks(columns[col][0], self.sql_mode):
                    self.column_names.append(
                        remove_backtick_quoting(columns[col][0],
                                                self.sql_mode))
                    self.q_column_names.append(columns[col][0])
                else:
                    self.column_names.append(columns[col][0])
                    self.q_column_names.append(
                        quote_with_backticks(columns[col][0], self.sql_mode))
                col_type = columns[col][1].lower()
                if ('char' in col_type or 'enum' in col_type or
                        'set' in col_type or 'binary' in col_type):
                    self.text_columns.append(col)
                    col_format_values[col] = "'%s'"
                elif 'blob' in col_type or 'text'in col_type:
                    self.blob_columns.append(col)
                    col_format_values[col] = "%s"
                elif "date" in col_type or "time" in col_type:
                    col_format_values[col] = "'%s'"
                elif "bit" in col_type:
                    self.bit_columns.append(col)
                    col_format_values[col] = "%d"
                else:
                    col_format_values[col] = "%s"
        self.column_format = "%s%s%s" % \
                             (" (", ', '.join(col_format_values), ")")

    def get_col_names(self, quote_backticks=False):
        """Get column names for the export operation.

        quote_backticks[in]    If True the column names will be quoted with
                               backticks. Default is False.

        Return (list) column names
        """

        if self.column_format is None:
            self.column_names = []
            self.q_column_names = []
            rows = self.server.exec_query("explain {0}".format(self.q_table))
            for row in rows:
                self.column_names.append(row[0])
                self.q_column_names.append(quote_with_backticks(row[0],
                                                                self.sql_mode))

        return self.q_column_names if quote_backticks else self.column_names

    def get_col_names_types(self, quote_backticks=False):
        """Get a list of tuples of column name and type.

        quote_backticks[in]    If True the column name will be quoted with
                               backticks. Default is False.

        Return (list) of touple (column name, type)
        """

        self.column_name_type = []
        rows = self.server.exec_query("explain {0}".format(self.q_table))
        for row in rows:
            if quote_backticks:
                self.column_name_type.append(
                    [quote_with_backticks(row[0], self.sql_mode)] +
                    list(row[1:])
                )
            else:
                self.column_name_type.append(row)

        return self.column_name_type

    def has_index(self, index_q_name):
        """A method to determine if this table has a determinate index using
        his name.

        index_q_name[in]    the name of the index (must be quoted).

        returns True if this Table has an index with the given name, otherwise
        false.
        """
        if [idx_q_name for idx_q_name in self.indexes_q_names
                if idx_q_name == index_q_name]:
            return True
        return False

    def get_not_null_unique_indexes(self, refresh=False):
        """get all the unique indexes which columns does not accepts null
        values.
        refresh[in] Boolean value used to force the method to read index
                    information directly from the server, instead of using
                    cached values.

        Returns list of indexes.
        """
        # First check if the instance variable exists.
        if self.unique_not_null_indexes is None or refresh:
            # Get the indexes for the table.
            try:
                self.get_indexes()
            except UtilDBError:
                # Table may not exist yet. Happens on import operations.
                pass
            # Now for each of them, check if they are UNIQUE and NOT NULL.
            no_null_idxes = []
            no_null_idxes.extend(
                [idx for idx in self.btree_indexes if not idx.accept_nulls and
                 idx.unique]
            )
            no_null_idxes.extend(
                [idx for idx in self.hash_indexes if not idx.accept_nulls and
                 idx.unique]
            )
            no_null_idxes.extend(
                [idx for idx in self.rtree_indexes if not idx.accept_nulls and
                 idx.unique]
            )
            no_null_idxes.extend(
                [idx for idx in self.fulltext_indexes
                 if not idx.accept_nulls and idx.unique]
            )
            self.unique_not_null_indexes = no_null_idxes

        return self.unique_not_null_indexes

    def _build_update_blob(self, row, new_db, name):
        """Build an UPDATE statement to update blob fields.

        row[in]            a row to process
        new_db[in]         new database name
        name[in]           name of the table

        Returns UPDATE string
        """
        if self.column_format is None:
            self.get_column_metadata()

        blob_insert = "UPDATE %s.%s SET " % (new_db, name)
        where_values = []
        do_commas = False
        has_data = False
        stop = len(row)
        for col in range(0, stop):
            col_name = self.q_column_names[col]
            if col in self.blob_columns:
                if row[col] is not None and len(row[col]) > 0:
                    if do_commas:
                        blob_insert += ", "
                    blob_insert += "%s = " % col_name + "%s" % \
                                   MySQLConverter().quote(
                                       convert_special_characters(row[col]))
                    has_data = True
                    do_commas = True
            else:
                # Convert None values to NULL (not '' to NULL)
                if row[col] is None:
                    value = 'NULL'
                else:
                    value = "'{0}'".format(row[col])
                where_values.append("{0} = {1}".format(col_name, value))
        if has_data:
            return "{0} WHERE {1};".format(blob_insert,
                                           " AND ".join(where_values))
        return None

    def _build_insert_blob(self, row, new_db, tbl_name):
        """Build an INSERT statement for the given row.

        row[in]                a row to process
        new_db[in]             new database name
        tbl_name[in]           name of the table

        Returns INSERT string.
        """
        if self.column_format is None:
            self.get_column_metadata()

        converter = MySQLConverter()
        row_vals = []
        # Deal with blob, special characters and NULL values.
        for index, column in enumerate(row):
            # pylint: disable=W0212
            if index in self.blob_columns:
                row_vals.append(converter.quote(
                    convert_special_characters(column)))
            elif index in self.text_columns:
                if column is None:
                    row_vals.append("NULL")
                else:
                    row_vals.append(convert_special_characters(column))
            elif index in self.bit_columns:
                if column is None:
                    row_vals.append("NULL")
                else:
                    row_vals.append(converter._BIT_to_python(column))
            else:
                if column is None:
                    row_vals.append("NULL")
                else:
                    row_vals.append(column)

        # Create the insert statement.
        insert_stm = ("INSERT INTO {0}.{1} VALUES {2};"
                      "".format(new_db, tbl_name,
                                self.column_format % tuple(row_vals)))

        # Replace 'NULL' occurrences with NULL values.
        insert_stm = insert_stm.replace("'NULL'", "NULL")

        return insert_stm

    def get_column_string(self, row, new_db, skip_blobs=False):
        """Return a formatted list of column data.

        row[in]            a row to process
        new_db[in]         new database name
        skip_blobs[in]     boolean value, if True, blob columns are skipped

        Returns (string) column list
        """

        if self.column_format is None:
            self.get_column_metadata()

        blob_inserts = []
        values = list(row)
        is_blob_insert = False
        # find if we have some unique column indexes
        unique_indexes = len(self.get_not_null_unique_indexes())
        # If all columns are blobs or there aren't any UNIQUE NOT NULL indexes
        # then rows won't be correctly copied using the update statement,
        # so we must use insert statements instead.
        if not skip_blobs and \
                (len(self.blob_columns) == len(self.column_names) or
                 self.blob_columns and not unique_indexes):
            blob_inserts.append(self._build_insert_blob(row, new_db,
                                                        self.q_tbl_name))
            is_blob_insert = True
        else:
            # Find blobs
            if self.blob_columns:
                # Save blob updates for later...
                blob = self._build_update_blob(row, new_db, self.q_tbl_name)
                if blob is not None:
                    blob_inserts.append(blob)
                for col in self.blob_columns:
                    values[col] = "NULL"

        if not is_blob_insert:
            # Replace single quotes located in the value for a text field with
            # the correct special character escape sequence. This fixes SQL
            # errors related to using single quotes in a string value that is
            # single quoted. For example, 'this' is it' is changed to
            # 'this\' is it'.
            for col in self.text_columns:
                # Check if the value is not None before replacing quotes
                if values[col]:
                    # Apply escape sequences to special characters
                    values[col] = convert_special_characters(values[col])

            for col in self.bit_columns:
                if values[col] is not None:
                    # Convert BIT to INTEGER for dump.
                    # pylint: disable=W0212
                    values[col] = MySQLConverter()._BIT_to_python(values[col])

            # Build string (add quotes to "string" like types)
            val_str = self.column_format % tuple(values)

            # Change 'None' occurrences with "NULL"
            val_str = val_str.replace(", None", ", NULL")
            val_str = val_str.replace("(None", "(NULL")
            val_str = val_str.replace(", 'None'", ", NULL")
            val_str = val_str.replace("('None'", "(NULL")

        else:
            val_str = None

        return val_str, blob_inserts

    def make_bulk_insert(self, rows, new_db, columns_names=None,
                         skip_blobs=False):
        """Create bulk insert statements for the data

        Reads data from a table (rows) and builds group INSERT statements for
        bulk inserts.

        Note: This method does not print any information to stdout.

        rows[in]           a list of rows to process
        new_db[in]         new database name
        skip_blobs[in]     boolean value, if True, blob columns are skipped

        Returns (tuple) - (bulk insert statements, blob data inserts)
        """

        if self.column_format is None:
            self.get_column_metadata()

        data_inserts = []
        blob_inserts = []
        row_count = 0
        data_size = 0
        val_str = None

        for row in rows:
            if row_count == 0:
                if columns_names:
                    insert_str = "INSERT INTO {0}.{1} ({2}) VALUES ".format(
                        new_db, self.q_tbl_name, ", ".join(columns_names)
                    )
                else:
                    insert_str = self._insert % (new_db, self.q_tbl_name)
                if val_str:
                    row_count += 1
                    insert_str += val_str
                data_size = len(insert_str)

            col_data = self.get_column_string(row, new_db, skip_blobs)
            if len(col_data[1]) > 0:
                blob_inserts.extend(col_data[1])
            if col_data[0]:
                val_str = col_data[0]

                row_size = len(val_str)
                next_size = data_size + row_size + 3
                if ((row_count >= _MAXBULK_VALUES) or
                        (next_size > (int(self.max_packet_size) - 512))):
                    # add to buffer
                    data_inserts.append(insert_str)
                    row_count = 0
                else:
                    row_count += 1
                    if row_count > 1:
                        insert_str += ", "
                    insert_str += val_str
                    data_size += row_size + 3

        if row_count > 0:
            data_inserts.append(insert_str)

        return data_inserts, blob_inserts

    def get_storage_engine(self):
        """Get the storage engine (in UPPERCASE) for the table.

        Returns the name in UPPERCASE of the storage engine use for the table
        or None if the information is not found.
        """
        self.server.exec_query("USE {0}".format(self.q_db_name),
                               self.query_options)
        res = self.server.exec_query(
            "SHOW TABLE STATUS WHERE name = '{0}'".format(self.tbl_name)
        )
        try:
            # Return store engine converted to UPPER cases.
            return res[0][1].upper() if res[0][1] else None
        except IndexError:
            # Return None if table status information is not available.
            return None

    def get_segment_size(self, num_conn=1):
        """Get the segment size based on number of connections (threads).

        num_conn[in]       Number of threads(connections) to use
                           Default = 1 (one large segment)

        Returns (int) segment_size

                Note: if num_conn <= 1 - returns number of rows
        """

        # Get number of rows
        num_rows = 0
        try:
            res = self.server.exec_query("USE %s" % self.q_db_name,
                                         self.query_options)
        except:
            pass
        res = self.server.exec_query("SHOW TABLE STATUS LIKE '%s'" %
                                     self.tbl_name)
        if res:
            num_rows = int(res[0][4])

        if num_conn <= 1:
            return num_rows

        # Calculate number of threads and segment size to fetch
        thread_limit = num_conn
        if thread_limit > _MAXTHREADS_INSERT:
            thread_limit = _MAXTHREADS_INSERT
        if num_rows > (_MAXROWS_PER_THREAD * thread_limit):
            max_threads = thread_limit
        else:
            max_threads = int(num_rows / _MAXROWS_PER_THREAD)
        if max_threads == 0:
            max_threads = 1
        if max_threads > 1 and self.verbose:
            print "# Using multi-threaded insert option. Number of " \
                  "threads = %d." % max_threads
        return (num_rows / max_threads) + max_threads

    def _bulk_insert(self, rows, new_db, destination=None):
        """Import data using bulk insert

        Reads data from a table and builds group INSERT statements for writing
        to the destination server specified (new_db.name).

        This method is designed to be used in a thread for parallel inserts.
        As such, it requires its own connection to the destination server.

        Note: This method does not print any information to stdout.

        rows[in]           a list of rows to process
        new_db[in]         new database name
        destination[in]    the destination server
        """
        if self.dest_vals is None:
            self.dest_vals = self.get_dest_values(destination)

        # Spawn a new connection
        server_options = {
            'conn_info': self.dest_vals,
            'role': "thread",
        }
        dest = Server(server_options)
        dest.connect()

        # Test if SQL_MODE is 'NO_BACKSLASH_ESCAPES' in the destination server
        if dest.select_variable("SQL_MODE") == "NO_BACKSLASH_ESCAPES":
            # Change temporarily the SQL_MODE in the destination server
            dest.exec_query("SET @@SESSION.SQL_MODE=''")

        # Issue the write lock
        lock_list = [("%s.%s" % (new_db, self.q_tbl_name), 'WRITE')]
        my_lock = Lock(dest, lock_list, {'locking': 'lock-all', })

        # First, turn off foreign keys if turned on
        dest.disable_foreign_key_checks(True)

        if self.column_format is None:
            self.get_column_metadata()
        data_lists = self.make_bulk_insert(rows, new_db)
        insert_data = data_lists[0]
        blob_data = data_lists[1]

        # Insert the data first
        for data_insert in insert_data:
            try:
                dest.exec_query(data_insert, self.query_options)
            except UtilError, e:
                raise UtilError("Problem inserting data. "
                                "Error = %s" % e.errmsg)

        # Now insert the blob data if there is any
        for blob_insert in blob_data:
            try:
                dest.exec_query(blob_insert, self.query_options)
            except UtilError, e:
                raise UtilError("Problem updating blob field. "
                                "Error = %s" % e.errmsg)

        # Now, turn on foreign keys if they were on at the start
        dest.disable_foreign_key_checks(False)
        my_lock.unlock()
        del dest

    def insert_rows(self, rows, new_db, destination=None, spawn=False):
        """Insert rows in the table using bulk copy.

        This method opens a new connect to the destination server to insert
        the data with a bulk copy. If spawn is True, the method spawns a new
        process and returns it. This allows for using a multi-threaded insert
        which can be faster on some platforms. If spawn is False, the method
        will open a new connection to insert the data.

        num_conn[in]       Number of threads(connections) to use for insert
        rows[in]           List of rows to insert
        new_db[in]         Rename the db to this name
        destination[in]    Destination server
                           Default = None (copy to same server)
        spawn[in]          If True, spawn a new process for the insert
                           Default = False

        Returns If spawn == True, process
                If spawn == False, None
        """

        if self.column_format is None:
            self.get_column_metadata()

        if self.dest_vals is None:
            self.dest_vals = self.get_dest_values(destination)

        proc = None
        if spawn:
            proc = multiprocessing.Process(target=self._bulk_insert,
                                           args=(rows, new_db, destination))
        else:
            self._bulk_insert(rows, new_db, destination)

        return proc

    def _clone_data(self, new_db):
        """Clone table data.

        This method will copy all of the data for a table
        from the old database to the new database on the same server.

        new_db[in]         New database name for the table
        """
        query_str = "INSERT INTO %s.%s SELECT * FROM %s.%s" % \
                    (new_db, self.q_tbl_name, self.q_db_name, self.q_tbl_name)
        if self.verbose and not self.quiet:
            print query_str

        # Disable foreign key checks to allow data to be copied without running
        # into foreign key referential integrity issues
        self.server.disable_foreign_key_checks(True)
        self.server.exec_query(query_str)
        self.server.disable_foreign_key_checks(False)

    def copy_data(self, destination, cloning=False, new_db=None,
                  connections=1):
        """Retrieve data from a table and copy to another server and database.

        Reads data from a table and inserts the correct INSERT statements into
        the file provided.

        Note: if connections < 1 - retrieve the data one row at-a-time

        destination[in]    Destination server
        cloning[in]        If True, we are copying on the same server
        new_db[in]         Rename the db to this name
        connections[in]    Number of threads(connections) to use for insert
        """
        # Get sql_mode from destination
        dest_sql_mode = destination.select_variable("SQL_MODE")
        if new_db is None:
            new_db = self.q_db_name
        else:
            # If need quote new_db identifier with backticks
            if not is_quoted_with_backticks(new_db, dest_sql_mode):
                new_db = quote_with_backticks(new_db, dest_sql_mode)

        num_conn = int(connections)

        if cloning:
            self._clone_data(new_db)
        else:
            # Read and copy the data
            pthreads = []
            # Change the sql_mode if the mode is different on each server
            # and if "ANSI_QUOTES" is set in source, this is for
            # compatibility between the names.
            prev_sql_mode = ''
            if self.sql_mode != dest_sql_mode and \
               "ANSI_QUOTES" in self.sql_mode:
                prev_sql_mode = self.server.select_variable("SQL_MODE")
                self.server.exec_query("SET @@SESSION.SQL_MODE=''")
                self.sql_mode = ''

                self.q_tbl_name = quote_with_backticks(
                    self.tbl_name,
                    self.sql_mode
                )
                self.q_db_name = quote_with_backticks(
                    self.db_name,
                    self.sql_mode
                )
                self.q_table = ".".join([self.q_db_name, self.q_tbl_name])
                self.q_column_names = []
                for column in self.column_names:
                    self.q_column_names.append(
                        quote_with_backticks(column, self.sql_mode)
                    )
            for rows in self.retrieve_rows(num_conn):
                p = self.insert_rows(rows, new_db, destination, num_conn > 1)
                if p is not None:
                    p.start()
                    pthreads.append(p)

            if num_conn > 1:
                # Wait for all threads to finish
                for p in pthreads:
                    p.join()
            # restoring the previous sql_mode, changed if the sql_mode in both
            # servers is different and one is "ANSI_QUOTES"
            if prev_sql_mode:
                self.server.exec_query("SET @@SESSION.SQL_MODE={0}"
                                       "".format(prev_sql_mode))
                self.sql_mode = prev_sql_mode
                self.q_tbl_name = quote_with_backticks(
                    self.tbl_name,
                    self.sql_mode
                )
                self.q_db_name = quote_with_backticks(
                    self.db_name,
                    self.sql_mode
                )
                self.q_table = ".".join([self.q_db_name, self.q_tbl_name])
                for column in self.column_names:
                    self.q_column_names.append(
                        quote_with_backticks(column, self.sql_mode)
                    )

    def retrieve_rows(self, num_conn=1):
        """Retrieve the table data in rows.

        This method can be used to retrieve rows from a table as a generator
        specifying how many rows to retrieve at one time (segment_size is
        calculated based on number of rows / number of connections).

        Note: if num_conn < 1 - retrieve the data one row at-a-time

        num_conn[in]       Number of threads(connections) to use
                           Default = 1 (one large segment)

        Returns (yield) row data
        """

        if num_conn > 1:
            # Only get the segment size when needed.
            segment_size = self.get_segment_size(num_conn)

        # Execute query to get all of the data
        cur = self.server.exec_query("SELECT * FROM {0}".format(self.q_table),
                                     self.query_options)

        while True:
            rows = None
            if num_conn < 1:
                rows = []
                row = cur.fetchone()
                if row is None:
                    raise StopIteration()
                rows.append(row)
            elif num_conn == 1:
                rows = cur.fetchall()
                yield rows
                raise StopIteration()
            else:
                rows = cur.fetchmany(segment_size)
                if not rows:
                    raise StopIteration()
            if rows is None:
                raise StopIteration()
            yield rows

        cur.close()

    def get_dest_values(self, destination=None):
        """Get the destination connection values if not already set.

        destination[in]    Connection values for destination server

        Returns connection values for destination if set or self.server
        """
        # Get connection to database
        if destination is None:
            conn_val = {
                "host": self.server.host,
                "user": self.server.user,
                "passwd": self.server.passwd,
                "unix_socket": self.server.socket,
                "port": self.server.port
            }
        else:
            conn_val = {
                "host": destination.host,
                "user": destination.user,
                "passwd": destination.passwd,
                "unix_socket": destination.socket,
                "port": destination.port
            }
        return conn_val

    def get_tbl_indexes(self):
        """Return a result set containing all indexes for a given table

        Returns result set
        """
        res = self.server.exec_query("SHOW INDEXES FROM %s" % self.q_table)
        # Clear the cardinality column
        if res:
            new_res = []
            for row in res:
                new_row = []
                i = 0
                for item in row:
                    if not i == 6:
                        new_row.append(item)
                    else:
                        new_row.append("0")
                    i = i + 1
                new_res.append(tuple(new_row))
            res = new_res
        return res

    def get_tbl_foreign_keys(self):
        """Return a result set containing all foreign keys for the table

        Returns result set
        """
        res = self.server.exec_query(_FOREIGN_KEY_QUERY % (self.db_name,
                                                           self.tbl_name))
        return res

    @staticmethod
    def __append(indexes, index):
        """Encapsulated append() method to ensure the primary key index
        is placed at the front of the list.
        """

        # Put the primary key first so that it can be compared to all indexes
        if index.name == "PRIMARY":
            indexes.insert(0, index)
        else:
            indexes.append(index)

    @staticmethod
    def __check_index(index, indexes, master_list):
        """Check a single index for duplicate or redundancy against a list
        of other Indexes.

        index[in]          The Index to compare
        indexes[in]        A list of Index instances to compare
        master_list[in]    A list of know duplicate Index instances

        Returns a tuple of whether duplicates are found and if found the
        list of duplicate indexes for this table
        """

        duplicates_found = False
        duplicate_list = []
        if indexes and index:
            for idx in indexes:
                if index == idx:
                    continue
                # Don't compare b == a when a == b has already occurred
                if not index.compared and idx.is_duplicate(index):
                    # make sure we haven't already found this match
                    if not idx.column_subparts:
                        idx.compared = True
                    if idx not in master_list:
                        duplicates_found = True
                        # PRIMARY key can be identified as redundant of an
                        # unique index with more columns, in that case always
                        # mark the other as the duplicate.
                        if idx.name == "PRIMARY":
                            index.duplicate_of = idx
                            duplicate_list.append(index)
                        else:
                            idx.duplicate_of = index
                            duplicate_list.append(idx)
        return (duplicates_found, duplicate_list)

    def __check_index_list(self, indexes):
        """Check a list of Index instances for duplicates.

        indexes[in]        A list of Index instances to compare

        Returns a tuple of whether duplicates are found and if found the
        list of duplicate indexes for this table
        """

        duplicates_found = False
        duplicate_list = []
        # Caller must ensure there are at least 2 elements in the list.
        if len(indexes) < 2:
            return (False, None)
        for index in indexes:
            res = self.__check_index(index, indexes, duplicate_list)
            if res[0]:
                duplicates_found = True
                duplicate_list.extend(res[1])
        return (duplicates_found, duplicate_list)

    def __check_clustered_index_list(self, indexes):
        """ Check for indexes containing the clustered index from the list.

        indexes[in]     list of indexes instances to check.

        Returns the list of indexes that contain the clustered index or
        None (if none found).
        """
        redundant_indexes = []
        if not self.pri_idx:
            self.get_primary_index()
        pri_idx_cols = [col[0] for col in self.pri_idx]
        for index in indexes:
            if index.name == 'PRIMARY':
                # Skip primary key.
                continue
            elif index.contains_columns(pri_idx_cols):
                redundant_indexes.append(index)

        return redundant_indexes if redundant_indexes else []

    def _get_index_list(self):
        """Get the list of indexes for a table.
        Returns list containing indexes.
        """
        rows = self.get_tbl_indexes()
        return rows

    def get_primary_index(self):
        """Retrieve the primary index columns for this table.
        """
        pri_idx = []

        rows = self.server.exec_query("EXPLAIN {0}".format(self.q_table))

        # Return False if no indexes found.
        if not rows:
            return pri_idx

        for row in rows:
            if row[3] == 'PRI':
                pri_idx.append(row)

        self.pri_idx = pri_idx

        return pri_idx

    def get_column_explanation(self, column_name):
        """Retrieve the explain description for the given column.
        """
        column_exp = []

        rows = self.server.exec_query("EXPLAIN {0}".format(self.q_table))

        # Return False if no indexes found.
        if not rows:
            return column_exp

        for row in rows:
            if row[0] == column_name:
                column_exp.append(row)

        return column_exp

    def get_indexes(self):
        """Retrieve the indexes from the server and load them into lists
        based on type.

        Returns True - table has indexes, False - table has no indexes
        """

        self.btree_indexes = []
        self.hash_indexes = []
        self.rtree_indexes = []
        self.fulltext_indexes = []
        self.indexes_q_names = []

        if self.verbose:
            print "# Getting indexes for %s" % (self.table)
        rows = self._get_index_list()

        # Return False if no indexes found.
        if not rows:
            return False
        idx = None
        prev_name = ""
        for row in rows:
            if (row[2] != prev_name) or (prev_name == ""):
                prev_name = row[2]
                idx = Index(self.db_name, row, sql_mode=self.sql_mode)
                if idx.type == "BTREE":
                    self.__append(self.btree_indexes, idx)
                elif idx.type == "HASH":
                    self.__append(self.hash_indexes, idx)
                elif idx.type == "RTREE":
                    self.__append(self.rtree_indexes, idx)
                else:
                    self.__append(self.fulltext_indexes, idx)
            elif idx:
                idx.add_column(row[4], row[7], row[9])
            self.indexes_q_names.append(quote_with_backticks(row[2],
                                                             self.sql_mode))
        return True

    def check_indexes(self, show_drops=False):
        """Check for duplicate or redundant indexes and display all matches

        show_drops[in]     (optional) If True the DROP statements are printed

        Note: You must call get_indexes() prior to calling this method. If
        get_indexes() is not called, no duplicates will be found.
        """

        dupes = []
        res = self.__check_index_list(self.btree_indexes)
        # if there are duplicates, add them to the dupes list
        if res[0]:
            dupes.extend(res[1])
        res = self.__check_index_list(self.hash_indexes)
        # if there are duplicates, add them to the dupes list
        if res[0]:
            dupes.extend(res[1])
        res = self.__check_index_list(self.rtree_indexes)
        # if there are duplicates, add them to the dupes list
        if res[0]:
            dupes.extend(res[1])
        res = self.__check_index_list(self.fulltext_indexes)
        # if there are duplicates, add them to the dupes list
        if res[0]:
            dupes.extend(res[1])

        # Check if secondary keys contains the clustered index (i.e. Primary
        # key). In InnoDB, each record in a secondary index contains the
        # primary key columns. Therefore the use of keys that include the
        # primary key might be redundant.
        redundant_idxs = []
        if not self.storage_engine:
            self.storage_engine = self.get_storage_engine()
        if self.storage_engine == 'INNODB':
            all_indexes = self.btree_indexes
            all_indexes.extend(self.hash_indexes)
            all_indexes.extend(self.rtree_indexes)
            all_indexes.extend(self.fulltext_indexes)
            redundant_idxs = self.__check_clustered_index_list(all_indexes)

        # Print duplicate and redundant keys on composite indexes.
        if len(dupes) > 0:
            plural_1, verb_conj, plural_2 = (
                ('', 'is a', '') if len(dupes) == 1 else ('es', 'are', 's')
            )
            print("# The following index{0} {1} duplicate{2} or redundant "
                  "for table {3}:".format(plural_1, verb_conj, plural_2,
                                          self.table))
            for index in dupes:
                print("#")
                index.print_index_sql()
                print("#     may be redundant or duplicate of:")
                index.duplicate_of.print_index_sql()
            if show_drops:
                print("#\n# DROP statement{0}:\n#".format(plural_2))
                for index in dupes:
                    print("{0};".format(index.get_drop_statement()))
                print("#")

        # Print redundant indexes containing clustered key.
        if redundant_idxs:
            plural, verb_conj, plural_2 = (
                ('', 's', '') if len(redundant_idxs) == 1 else ('es', '', 's')
            )

            print("# The following index{0} for table {1} contain{2} the "
                  "clustered index and might be redundant:".format(plural,
                                                                   self.table,
                                                                   verb_conj))
            for index in redundant_idxs:
                print("#")
                index.print_index_sql()
            if show_drops:
                print("#\n# DROP/ADD statement{0}:\n#".format(plural_2))
                # Get columns from primary key to be removed.
                pri_idx_cols = [col[0] for col in self.pri_idx]
                for index in redundant_idxs:
                    print("{0};".format(
                        index.get_remove_columns_statement(pri_idx_cols)
                    ))
                print("#")

        if not self.quiet and not dupes and not redundant_idxs:
            print("# Table {0} has no duplicate nor redundant "
                  "indexes.".format(self.table))

    def show_special_indexes(self, fmt, limit, best=False):
        """Display a list of the best or worst queries for this table.

        This shows the best (first n) or worst (last n) performing queries
        for a given table.

        fmt[in]            format out output = sql, table, tab, csv
        limit[in]          number to limit the display
        best[in]           (optional) if True, print best performing indexes
                                      if False, print worst performing indexes
        """

        _QUERY = """
            SELECT
                t.TABLE_SCHEMA AS `db`, t.TABLE_NAME AS `table`,
                s.INDEX_NAME AS `index name`, s.COLUMN_NAME AS `field name`,
                s.SEQ_IN_INDEX `seq in index`, s2.max_columns AS `# cols`,
                s.CARDINALITY AS `card`, t.TABLE_ROWS AS `est rows`,
                ROUND(((s.CARDINALITY / IFNULL(
                IF(t.TABLE_ROWS < s.CARDINALITY, s.CARDINALITY, t.TABLE_ROWS),
                0.01)) * 100), 2) AS `sel_percent`
            FROM INFORMATION_SCHEMA.STATISTICS s
                INNER JOIN INFORMATION_SCHEMA.TABLES t
                ON s.TABLE_SCHEMA = t.TABLE_SCHEMA
                AND s.TABLE_NAME = t.TABLE_NAME
            INNER JOIN (
                SELECT TABLE_SCHEMA, TABLE_NAME, INDEX_NAME,
                    MAX(SEQ_IN_INDEX) AS max_columns
                FROM INFORMATION_SCHEMA.STATISTICS
                WHERE TABLE_SCHEMA = %s AND TABLE_NAME = %s
                      AND INDEX_NAME != 'PRIMARY'
                GROUP BY TABLE_SCHEMA, TABLE_NAME, INDEX_NAME
             ) AS s2
             ON s.TABLE_SCHEMA = s2.TABLE_SCHEMA
                AND s.TABLE_NAME = s2.TABLE_NAME
                AND s.INDEX_NAME = s2.INDEX_NAME
            WHERE t.TABLE_SCHEMA != 'mysql'
                AND t.TABLE_ROWS > 10 /* Only tables with some rows */
                AND s.CARDINALITY IS NOT NULL
                AND (s.CARDINALITY / IFNULL(
                IF(t.TABLE_ROWS < s.CARDINALITY, s.CARDINALITY, t.TABLE_ROWS),
                0.01)) <= 1.00
            ORDER BY `sel_percent`
        """
        query_options = {
            'params': (self.db_name, self.tbl_name,)
        }
        rows = []
        idx_type = "best"
        if not best:
            idx_type = "worst"
        if best:
            rows = self.server.exec_query(_QUERY + "DESC LIMIT %s" % limit,
                                          query_options)
        else:
            rows = self.server.exec_query(_QUERY + "LIMIT %s" % limit,
                                          query_options)
        if rows:
            print("#")
            if limit == 1:
                print("# Showing the {0} performing index from "
                      "{1}:".format(idx_type, self.table))
            else:
                print("# Showing the top {0} {1} performing indexes from "
                      "{2}:".format(limit, idx_type, self.table))
            print("#")
            cols = ("database", "table", "name", "column", "sequence",
                    "num columns", "cardinality", "est. rows", "percent")
            print_list(sys.stdout, fmt, cols, rows)
        else:
            print("# WARNING: Not enough data to calculate "
                  "best/worst indexes.")

    @staticmethod
    def __print_index_list(indexes, fmt, no_header=False, verbosity=0):
        """Print the list of indexes

        indexes[in]        list of indexes to print
        fmt[in]            format out output = sql, table, tab, csv
        no_header[in]      (optional) if True, do not print the header
        """
        if fmt == "sql":
            for index in indexes:
                index.print_index_sql()
        else:
            if verbosity > 0:
                cols = ("database", "table", "name", "type", "unique",
                        "accepts nulls", "columns")
            else:
                cols = ("database", "table", "name", "type", "columns")

            rows = []
            for index in indexes:
                rows.append(index.get_row(verbosity))
            print_list(sys.stdout, fmt, cols, rows, no_header)

    def print_indexes(self, fmt, verbosity):
        """Print all indexes for this table

        fmt[in]         format out output = sql, table, tab, csv
        """

        print "# Showing indexes from %s:\n#" % (self.table)
        if fmt == "sql":
            self.__print_index_list(self.btree_indexes, fmt,
                                    verbosity=verbosity)
            self.__print_index_list(self.hash_indexes, fmt, False,
                                    verbosity=verbosity)
            self.__print_index_list(self.rtree_indexes, fmt, False,
                                    verbosity=verbosity)
            self.__print_index_list(self.fulltext_indexes, fmt, False,
                                    verbosity=verbosity)
        else:
            master_indexes = []
            master_indexes.extend(self.btree_indexes)
            master_indexes.extend(self.hash_indexes)
            master_indexes.extend(self.rtree_indexes)
            master_indexes.extend(self.fulltext_indexes)
            self.__print_index_list(master_indexes, fmt,
                                    verbosity=verbosity)
        print "#"

    def has_primary_key(self):
        """Check to see if there is a primary key.
        Returns bool - True - a primary key was found,
                       False - no primary key.
        """
        primary_key = False
        rows = self._get_index_list()
        for row in rows:
            if row[2] == "PRIMARY":
                primary_key = True
        return primary_key

    def has_unique_key(self):
        """Check to see if there is a unique key.
        Returns bool - True - a unique key was found,
                       False - no unique key.
        """
        unique_key = False
        rows = self._get_index_list()
        for row in rows:
            if row[1] == '0':
                unique_key = True
        return unique_key
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains methods for working with mysql server tools.
"""

import inspect
import os
import re
import sys
import shlex
import shutil
import socket
import subprocess
import time

try:
    import ctypes
except ImportError:
    pass



def _add_basedir(search_paths, path_str):
    """Add a basedir and all known sub directories

    This method builds a list of possible paths for a basedir for locating
    special MySQL files like mysqld (mysqld.exe), etc.

    search_paths[inout] List of paths to append
    path_str[in]        The basedir path to append
    """
    search_paths.append(path_str)
    search_paths.append(os.path.join(path_str, "sql"))       # for source trees
    search_paths.append(os.path.join(path_str, "client"))    # for source trees
    search_paths.append(os.path.join(path_str, "share"))
    search_paths.append(os.path.join(path_str, "scripts"))
    search_paths.append(os.path.join(path_str, "bin"))
    search_paths.append(os.path.join(path_str, "libexec"))
    search_paths.append(os.path.join(path_str, "mysql"))


def get_tool_path(basedir, tool, fix_ext=True, required=True,
                  defaults_paths=None, search_PATH=False, quote=False):
    """Search for a MySQL tool and return the full path

    basedir[in]         The initial basedir to search (from mysql server)
    tool[in]            The name of the tool to find
    fix_ext[in]         If True (default is True), add .exe if running on
                        Windows.
    required[in]        If True (default is True), and error will be
                        generated and the utility aborted if the tool is
                        not found.
    defaults_paths[in]  Default list of paths to search for the tool.
                        By default an empty list is assumed, i.e. [].
    search_PATH[in]     Boolean value that indicates if the paths specified by
                        the PATH environment variable will be used to search
                        for the tool. By default the PATH will not be searched,
                        i.e. search_PATH=False.
    quote[in]           If True, the result path is surrounded with the OS
                        quotes.
    Returns (string) full path to tool
    """
    if not defaults_paths:
        defaults_paths = []
    search_paths = []
    if quote:
        if os.name == "posix":
            quote_char = "'"
        else:
            quote_char = '"'
    else:
        quote_char = ''
    if basedir:
        # Add specified basedir path to search paths
        _add_basedir(search_paths, basedir)
    if defaults_paths and len(defaults_paths):
        # Add specified default paths to search paths
        for path in defaults_paths:
            search_paths.append(path)
    else:
        # Add default basedir paths to search paths
        _add_basedir(search_paths, "/usr/local/mysql/")
        _add_basedir(search_paths, "/usr/sbin/")
        _add_basedir(search_paths, "/usr/share/")

    # Search in path from the PATH environment variable
    if search_PATH:
        for path in os.environ['PATH'].split(os.pathsep):
            search_paths.append(path)

    if os.name == "nt" and fix_ext:
        tool = tool + ".exe"
    # Search for the tool
    for path in search_paths:
        norm_path = os.path.normpath(path)
        if os.path.isdir(norm_path):
            toolpath = os.path.join(norm_path, tool)
            if os.path.isfile(toolpath):
                return r"%s%s%s" % (quote_char, toolpath, quote_char)
            else:
                if tool == "mysqld.exe":
                    toolpath = os.path.join(norm_path, "mysqld-nt.exe")
                    if os.path.isfile(toolpath):
                        return r"%s%s%s" % (quote_char, toolpath, quote_char)
    if required:
        raise UtilError("Cannot find location of %s." % tool)

    return None


def delete_directory(path):
    """Remove a directory (folder) and its contents.

    path[in]           target directory
    """
    if os.path.exists(path):
        # It can take up to 10 seconds for Windows to 'release' a directory
        # once a process has terminated. We wait...
        if os.name == "nt":
            stop = 10
            i = 1
            while i < stop and os.path.exists(path):
                shutil.rmtree(path, True)
                time.sleep(1)
                i += 1
        else:
            shutil.rmtree(path, True)


def estimate_free_space(path, unit_multiple=2):
    """Estimated free space for the given path.

    Calculates free space for the given path, returning the value
    on the size given by the unit_multiple.

    path[in]             the path to calculate the free space for.
    unit_multiple[in]    the unit size given as a multiple.
                         Accepts int values > to zero.
                         Size    unit_multiple
                          bytes        0
                          Kilobytes    1
                          Megabytes    2
                          Gigabytes    3
                         and so on...

    Returns folder/drive free space (in bytes)
    """
    unit_size = 1024 ** unit_multiple
    if os.name == 'nt':
        free_bytes = ctypes.c_ulonglong(0)
        ctypes.windll.kernel32.GetDiskFreeSpaceExW(ctypes.c_wchar_p(path),
                                                   None, None,
                                                   ctypes.pointer(free_bytes))
        return free_bytes.value / unit_size
    else:
        st = os.statvfs(path)  # pylint: disable=E1101
        return st.f_bavail * st.f_frsize / unit_size


def execute_script(run_cmd, filename=None, options=None, verbosity=False):
    """Execute a script.

    This method spawns a subprocess to execute a script. If a file is
    specified, it will direct output to that file else it will suppress
    all output from the script.

    run_cmd[in]        command/script to execute
    filename[in]       file path name to file, os.stdout, etc.
                       Default is None (do not log/write output)
    options[in]        arguments for script
                       Default is no arguments ([])
    verbosity[in]      show result of script
                       Default is False

    Returns int - result from process execution
    """
    if options is None:
        options = []
    if verbosity:
        f_out = sys.stdout
    else:
        if not filename:
            filename = os.devnull
        f_out = open(filename, 'w')

    is_posix = (os.name == "posix")
    command = shlex.split(run_cmd, posix=is_posix)

    if options:
        command.extend([str(opt) for opt in options])

    if verbosity:
        print("# SCRIPT EXECUTED: {0}".format(" ".join(command)))

    try:
        proc = subprocess.Popen(command, shell=False,
                                stdout=f_out, stderr=f_out)
    except:
        _, err, _ = sys.exc_info()
        raise UtilError(str(err))

    ret_val = proc.wait()
    if not verbosity:
        f_out.close()
    return ret_val


def ping_host(host, timeout):
    """Execute 'ping' against host to see if it is alive.

    host[in]           hostname or IP to ping
    timeout[in]        timeout in seconds to wait

    returns bool - True = host is reachable via ping
    """
    if sys.platform == "darwin":
        run_cmd = "ping -o -t %s %s" % (timeout, host)
    elif os.name == "posix":
        run_cmd = "ping -w %s %s" % (timeout, host)
    else:  # must be windows
        run_cmd = "ping -n %s %s" % (timeout, host)

    ret_val = execute_script(run_cmd)

    return (ret_val == 0)


def parse_mysqld_version(vers_str):
    """ Parse the MySQL version string.

    vers_str[in]     MySQL Version from client

    Returns string = version string
    """
    pattern = r"mysqld(?:\.exe)?\s+Ver\s+(\d+\.\d+\.\S+)\s"
    match = re.search(pattern, vers_str)
    if not match:
        return None
    version = match.group(1)
    try:
        # get the version digits. If more than 2, we get first 3 parts
        # pylint: disable=W0612
        maj_ver, min_ver, dev = version.split(".", 2)
        rel = dev.split("-", 1)
        return (maj_ver, min_ver, rel[0])
    except:
        return None


def get_mysqld_version(mysqld_path):
    """Return the version number for a mysqld executable.

    mysqld_path[in]    location of the mysqld executable

    Returns tuple - (major, minor, release), or None if error
    """
    out = open("version_check", 'w')
    proc = subprocess.Popen("%s --version" % mysqld_path,
                            stdout=out, stderr=out, shell=True)
    proc.wait()
    out.close()
    out = open("version_check", 'r')
    line = None
    for line in out.readlines():
        if "Ver" in line:
            break
    out.close()

    try:
        os.unlink('version_check')
    except:
        pass

    if line is None:
        return None
    # strip path for long, unusual paths that contain version number
    fixed_str = "{0} {1}".format("mysqld", line.strip(mysqld_path))
    return parse_mysqld_version(fixed_str)


def show_file_statistics(file_name, wild=False, out_format="GRID"):
    """Show file statistics for file name specified

    file_name[in]    target file name and path
    wild[in]         if True, get file statistics for all files with prefix of
                     file_name. Default is False
    out_format[in]   output format to print file statistics. Default is GRID.
    """

    def _get_file_stats(path, file_name):
        """Return file stats
        """
        stats = os.stat(os.path.join(path, file_name))
        return ((file_name, stats.st_size, time.ctime(stats.st_ctime),
                 time.ctime(stats.st_mtime)))

    columns = ["File", "Size", "Created", "Last Modified"]
    rows = []
    path, filename = os.path.split(file_name)
    if wild:
        for _, _, files in os.walk(path):
            for f in files:
                if f.startswith(filename):
                    rows.append(_get_file_stats(path, f))
    else:
        rows.append(_get_file_stats(path, filename))

    # Local import is needed because of Python compability issues
    from mysql.utilities.common.format import print_list
    print_list(sys.stdout, out_format, columns, rows)


def remote_copy(filepath, user, host, local_path, verbosity=0):
    """Copy a file from a remote machine to the localhost.

    filepath[in]       The full path and file name of the file on the remote
                       machine
    user[in]           Remote login
    local_path[in]     The path to where the file is to be copie

    Returns bool - True = succes, False = failure or exception
    """

    if os.name == "posix":  # use scp
        run_cmd = "scp %s@%s:%s %s" % (user, host, filepath, local_path)
        if verbosity > 1:
            print("# Command =%s" % run_cmd)
        print("# Copying file from %s:%s to %s:" %
              (host, filepath, local_path))
        proc = subprocess.Popen(run_cmd, shell=True)
        proc.wait()
    else:
        print("Remote copy not supported. Please use UNC paths and omit "
              "the --remote-login option to use a local copy operation.")
    return True


def check_python_version(min_version=PYTHON_MIN_VERSION,
                         max_version=PYTHON_MAX_VERSION,
                         raise_exception_on_fail=False,
                         name=None, print_on_fail=True,
                         exit_on_fail=True,
                         return_error_msg=False):
    """Check the Python version compatibility.

    By default this method uses constants to define the minimum and maximum
    Python versions required. It's possible to override this by passing new
    values on ``min_version`` and ``max_version`` parameters.
    It will run a ``sys.exit`` or raise a ``UtilError`` if the version of
    Python detected it not compatible.

    min_version[in]               Tuple with the minimum Python version
                                  required (inclusive).
    max_version[in]               Tuple with the maximum Python version
                                  required (exclusive).
    raise_exception_on_fail[in]   Boolean, it will raise a ``UtilError`` if
                                  True and Python detected is not compatible.
    name[in]                      String for a custom name, if not provided
                                  will get the module name from where this
                                  function was called.
    print_on_fail[in]             If True, print error else do not print
                                  error on failure.
    exit_on_fail[in]              If True, issue exit() else do not exit()
                                  on failure.
    return_error_msg[in]          If True, and is not compatible
                                  returns (result, error_msg) tuple.
    """

    # Only use the fields: major, minor and micro
    sys_version = sys.version_info[:3]

    # Test min version compatibility
    is_compat = min_version <= sys_version

    # Test max version compatibility if it's defined
    if is_compat and max_version:
        is_compat = sys_version < max_version

    if not is_compat:
        if not name:
            # Get the utility name by finding the module
            # name from where this function was called
            frm = inspect.stack()[1]
            mod = inspect.getmodule(frm[0])
            mod_name = os.path.splitext(
                os.path.basename(mod.__file__))[0]
            name = '%s utility' % mod_name

        # Build the error message
        if max_version:
            max_version_error_msg = 'or higher and lower than %s' % \
                '.'.join([str(el) for el in max_version])
        else:
            max_version_error_msg = 'or higher'

        error_msg = (
            'The %(name)s requires Python version %(min_version)s '
            '%(max_version_error_msg)s. The version of Python detected was '
            '%(sys_version)s. You may need to install or redirect the '
            'execution of this utility to an environment that includes a '
            'compatible Python version.'
        ) % {
            'name': name,
            'sys_version': '.'.join([str(el) for el in sys_version]),
            'min_version': '.'.join([str(el) for el in min_version]),
            'max_version_error_msg': max_version_error_msg
        }

        if raise_exception_on_fail:
            raise UtilError(error_msg)

        if print_on_fail:
            print('ERROR: %s' % error_msg)

        if exit_on_fail:
            sys.exit(1)

        if return_error_msg:
            return is_compat, error_msg

    return is_compat


def check_port_in_use(host, port):
    """Check to see if port is in use.

    host[in]            Hostname or IP to check
    port[in]            Port number to check

    Returns bool - True = port is available, False is not available
    """
    try:
        sock = socket.create_connection((host, port))
    except socket.error:
        return True
    sock.close()
    return False


def requires_encoding(orig_str):
    r"""Check to see if a string requires encoding

    This method will check to see if a string requires encoding to be used
    as a MySQL file name (r"[\w$]*").

    orig_str[in]        original string

    Returns bool - True = requires encoding, False = does not require encoding
    """
    ok_chars = re.compile(r"[\w$]*")
    parts = ok_chars.findall(orig_str)
    return len(parts) > 2 and parts[1].strip() == ''


def encode(orig_str):
    r"""Encode a string containing non-MySQL observed characters

    This method will take a string containing characters other than those
    recognized by MySQL (r"[\w$]*") and covert them to embedded ascii values.
    For example, "this.has.periods" becomes "this@002ehas@00e2periods"

    orig_str[in]        original string

    Returns string - encoded string or original string
    """
    # First, find the parts that match valid characters
    ok_chars = re.compile(r"[\w$]*")
    parts = ok_chars.findall(orig_str)

    # Now find each part that does not match the list of valid characters
    # Save the good parts
    i = 0
    encode_parts = []
    good_parts = []
    for part in parts:
        if not len(part):
            continue
        good_parts.append(part)
        if i == 0:
            i = len(part)
        else:
            j = orig_str[i:].find(part)
            encode_parts.append(orig_str[i:i + j])
            i += len(part) + j

    # Next, convert the non-valid parts to the form @NNNN (hex)
    encoded_parts = []
    for part in encode_parts:
        new_part = "".join(["@%04x" % ord(c) for c in part])
        encoded_parts.append(new_part)

    # Take the good parts and the encoded parts and reform the string
    i = 0
    new_parts = []
    for part in good_parts[:len(good_parts) - 1]:
        new_parts.append(part)
        new_parts.append(encoded_parts[i])
        i += 1
    new_parts.append(good_parts[len(good_parts) - 1])

    # Return the new string
    return "".join(new_parts)


def requires_decoding(orig_str):
    """Check to if a string required decoding

    This method will check to see if a string requires decoding to be used
    as a filename (has @NNNN entries)

    orig_str[in]        original string

    Returns bool - True = requires decoding, False = does not require decoding
    """
    return '@' in orig_str


def decode(orig_str):
    r"""Decode a string containing @NNNN entries

    This method will take a string containing characters other than those
    recognized by MySQL (r"[\w$]*") and covert them to character values.
    For example, "this@002ehas@00e2periods" becomes "this.has.periods".

    orig_str[in]        original string

    Returns string - decoded string or original string
    """
    parts = orig_str.split('@')
    if len(parts) == 1:
        return orig_str
    new_parts = [parts[0]]
    for part in parts[1:]:
        # take first four positions and convert to ascii
        new_parts.append(chr(int(part[0:4], 16)))
        new_parts.append(part[4:])
    return "".join(new_parts)


def check_connector_python(print_error=True,
                           min_version=CONNECTOR_MIN_VERSION):

    """Check to see if Connector Python is installed and accessible and
    meets minimum required version.

    By default this method uses constants to define the minimum
    C/Python version required. It's possible to override this by passing  a new
    value to ``min_version`` parameter.

    print_error[in]               If True, print error else do not print
                                  error on failure.
    min_version[in]               Tuple with the minimum C/Python version
                                  required (inclusive).

    """
    is_compatible = True
    try:
        import mysql.connector  # pylint: disable=W0612
    except ImportError:
        if print_error:
            print("ERROR: The MySQL Connector/Python module was not found. "
                  "MySQL Utilities requires the connector to be installed. "
                  "Please check your paths or download and install the "
                  "Connector/Python from http://dev.mysql.com.")
        return False
    else:
        try:
            sys_version = mysql.connector.version.VERSION[:3]
        except AttributeError:
            is_compatible = False

    if is_compatible and sys_version >= min_version:
        return True
    else:
        if print_error:
            print("ERROR: The MYSQL Connector/Python module was found "
                  "but it is either not properly installed or it is an "
                  "old version. MySQL Utilities requires Connector/Python "
                  "version > '{0}'. Download and install Connector/Python "
                  "from http://dev.mysql.com.".format(min_version))
        return False


def print_elapsed_time(start_time):
    """Print the elapsed time to stdout (screen)

    start_time[in]      The starting time of the test
    """
    stop_time = time.time()
    display_time = stop_time - start_time
    print("Time: {0:.2f} sec\n".format(display_time))


def join_and_build_str(list_of_strings, sep=', ', last_sep='and'):
    """Buils and returns a string from a list of elems.

    list_of_strings[in]    the list of strings that will be joined into a
                           single string.
    sep[in]                the separator that will be used to group all strings
                           except the last one.
    last_sep[in]           the separator that is used in last place
    """
    if list_of_strings:
        if len(list_of_strings) > 1:
            res_str = "{0} {1} {2}".format(
                sep.join(list_of_strings[:-1]), last_sep, list_of_strings[-1])
        else:  # list has a single elem
            res_str = list_of_strings[0]
    else:  # if list_of_str is empty, return empty string
        res_str = ""
    return res_str
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains abstractions of MySQL replication functionality.
"""

import sys
import logging
import time
import operator
import os

from multiprocessing.pool import ThreadPool



_HEALTH_COLS = ["host", "port", "role", "state", "gtid_mode", "health"]
_HEALTH_DETAIL_COLS = ["version", "master_log_file", "master_log_pos",
                       "IO_Thread", "SQL_Thread", "Secs_Behind",
                       "Remaining_Delay", "IO_Error_Num", "IO_Error",
                       "SQL_Error_Num", "SQL_Error", "Trans_Behind"]

_GTID_EXECUTED = "SELECT @@GLOBAL.GTID_EXECUTED"
_GTID_WAIT = "SELECT WAIT_UNTIL_SQL_THREAD_AFTER_GTIDS('%s', %s)"
_GTID_SUBTRACT_TO_EXECUTED = ("SELECT GTID_SUBTRACT('{0}', "
                              "@@GLOBAL.GTID_EXECUTED)")

_UPDATE_RPL_USER_QUERY = ("UPDATE mysql.user "
                          "SET password = PASSWORD('{passwd}')"
                          "where user ='{user}'")
# Query for server versions >= 5.7.6.
_UPDATE_RPL_USER_QUERY_5_7_6 = (
    "ALTER USER IF EXISTS '{user}'@'{host}' IDENTIFIED BY '{passwd}'")

_SELECT_RPL_USER_PASS_QUERY = ("SELECT user, host, grant_priv, password, "
                               "Repl_slave_priv FROM mysql.user "
                               "WHERE user ='{user}' AND host ='{host}'")
# Query for server versions >= 5.7.6.
_SELECT_RPL_USER_PASS_QUERY_5_7_6 = (
    "SELECT user, host, grant_priv, authentication_string, "
    "Repl_slave_priv FROM mysql.user WHERE user ='{user}' AND host ='{host}'")


def parse_topology_connections(options, parse_candidates=True):
    """Parse the --master, --slaves, and --candidates options

    This method returns a tuple with server connection dictionaries for
    the master, slaves, and candidates lists.

    If no master, will return (None, ...) for master element.
    If no slaves, will return (..., [], ...) for slaves element.
    If no canidates, will return (..., ..., []) for canidates element.

    Will raise error if cannot parse connection options.

    options[in]        options from parser

    Returns tuple - (master, slaves, candidates) dictionaries
    """
    try:
        timeout = options.conn_timeout
    except:
        timeout = None
    if timeout and options.verbosity > 2:
        print("Note: running with --connection-timeout={0}".format(timeout))

    # Create a basic configuration reader, without looking for the tool
    # my_print_defaults to avoid raising exceptions. This is used for
    # optimization purposes, to reuse data and avoid repeating the execution of
    # some methods in the parse_connection method (e.g. searching for
    # my_print_defaults).
    config_reader = MyDefaultsReader(options, False)

    if options.master:
        try:
            master_val = parse_connection(options.master, config_reader,
                                          options)
            # Add connection timeout if present in options
            if timeout:
                master_val['connection_timeout'] = timeout
        except FormatError as err:
            msg = ("Master connection values invalid or cannot be parsed: %s "
                   "(%s)." % (options.master, err))
            raise UtilRplError(msg)
        except UtilError as err:
            msg = ("Master connection values invalid or cannot be parsed: %s "
                   "(using login-path authentication: %s)" % (options.master,
                                                              err.errmsg))
            raise UtilRplError(msg)
    else:
        master_val = None

    slaves_val = []
    if options.slaves:
        slaves = options.slaves.split(",")
        for slave in slaves:
            try:
                s_values = parse_connection(slave, config_reader, options)
                # Add connection timeout if present in options
                if timeout:
                    s_values['connection_timeout'] = timeout
                slaves_val.append(s_values)
            except FormatError as err:
                msg = ("Slave connection values invalid or cannot be parsed: "
                       "%s (%s)" % (slave, err))
                raise UtilRplError(msg)
            except UtilError as err:
                msg = ("Slave connection values invalid or cannot be parsed: "
                       "%s (%s)" % (slave, err.errmsg))
                raise UtilRplError(msg)

    candidates_val = []
    if parse_candidates and options.candidates:
        candidates = options.candidates.split(",")
        for slave in candidates:
            try:
                s_values = parse_connection(slave, config_reader, options)
                # Add connection timeout if present in options
                if timeout:
                    s_values['connection_timeout'] = timeout
                candidates_val.append(s_values)
            except FormatError as err:
                msg = "Candidate connection values invalid or " + \
                      "cannot be parsed: %s (%s)" % (slave, err)
                raise UtilRplError(msg)
            except UtilError as err:
                msg = ("Candidate connection values invalid or cannot be "
                       "parsed: %s (%s)" % (slave, err.errmsg))
                raise UtilRplError(msg)

    return (master_val, slaves_val, candidates_val)


class Topology(Replication):
    """The Topology class supports administrative operations for an existing
    master-to-many slave topology. It has the following capabilities:

        - determine the health of the topology
        - discover slaves connected to the master provided they have
          --report-host and --report-port specified
        - switchover from master to a candidate slave
        - demote the master to a slave in the topology
        - perform best slave election
        - failover to a specific slave or best of slaves available

    Notes:

        - the switchover and demote methods work with versions prior to and
          after 5.6.5.
        - failover and best slave election require version 5.6.5 and later
          and GTID_MODE=ON.

    """

    def __init__(self, master_vals, slave_vals, options=None,
                 skip_conn_err=False):
        """Constructor

        The slaves parameter requires a dictionary in the form:

        master_vals[in]    master server connection dictionary
        slave_vals[in]     list of slave server connection dictionaries
        options[in]        options dictionary
          verbose          print extra data during operations (optional)
                           Default = False
          ping             maximum number of seconds to ping
                           Default = 3
          max_delay        maximum delay in seconds slave and be behind
                           master and still be 'Ok'. Default = 0
          max_position     maximum position slave can be behind master's
                           binlog and still be 'Ok'. Default = 0
        skip_conn_err[in]  if True, do not fail on connection failure
                           Default = True
        """
        super(Topology, self).__init__(master_vals, slave_vals, options or {})
        # Get options needed
        self.options = options or {}
        self.verbosity = options.get("verbosity", 0)
        self.verbose = self.verbosity > 0
        self.quiet = self.options.get("quiet", False)
        self.pingtime = self.options.get("ping", 3)
        self.max_delay = self.options.get("max_delay", 0)
        self.max_pos = self.options.get("max_position", 0)
        self.force = self.options.get("force", False)
        self.pedantic = self.options.get("pedantic", False)
        self.before_script = self.options.get("before", None)
        self.after_script = self.options.get("after", None)
        self.timeout = int(self.options.get("timeout", 300))
        self.logging = self.options.get("logging", False)
        self.rpl_user = self.options.get("rpl_user", None)
        self.script_threshold = self.options.get("script_threshold", None)
        self.master_vals = None

        # Attempt to connect to all servers
        self.master, self.slaves = self._connect_to_servers(master_vals,
                                                            slave_vals,
                                                            self.options,
                                                            skip_conn_err)
        self.discover_slaves(output_log=True)

    def _report(self, message, level=logging.INFO, print_msg=True):
        """Log message if logging is on

        This method will log the message presented if the log is turned on.
        Specifically, if options['log_file'] is not None. It will also
        print the message to stdout.

        message[in]    message to be printed
        level[in]      level of message to log. Default = INFO
        print_msg[in]  if True, print the message to stdout. Default = True
        """
        # First, print the message.
        if print_msg and not self.quiet:
            print message
        # Now log message if logging turned on
        if self.logging:
            logging.log(int(level), message.strip("#").strip(' '))

    def _connect_to_servers(self, master_vals, slave_vals, options,
                            skip_conn_err=True):
        """Connect to the master and one or more slaves

        This method will attempt to connect to the master and slaves provided.
        For slaves, if the --force option is specified, it will skip slaves
        that cannot be reached setting the slave dictionary to None
        in the list instead of a Slave class instance.

        The dictionary of the list of slaves returns is as follows.

        slave_dict = {
          'host'     : # host name for slave
          'port'     : # port for slave
          'instance' : Slave class instance or None if cannot connect
        }

        master_vals[in]    master server connection dictionary
        slave_vals[in]     list of slave server connection dictionaries
        options[in]        options dictionary
          verbose          print extra data during operations (optional)
                           Default = False
          ping             maximum number of seconds to ping
                           Default = 3
          max_delay        maximum delay in seconds slave and be behind
                           master and still be 'Ok'. Default = 0
          max_position     maximum position slave can be behind master's
                           binlog and still be 'Ok'. Default = 0
        skip_conn_err[in]  if True, do not fail on connection failure
                           Default = True

        Returns tuple - master instance, list of dictionary slave instances
        """
        master = None
        slaves = []

        # Set verbose value.
        verbose = self.options.get("verbosity", 0) > 0

        # attempt to connect to the master
        if master_vals:
            master = get_server('master', master_vals, True, verbose=verbose)
            if self.logging:
                log_server_version(master)

        for slave_val in slave_vals:
            host = slave_val['host']
            port = slave_val['port']
            try:
                slave = get_server('slave', slave_val, True, verbose=verbose)
                if self.logging:
                    log_server_version(slave)
            except:
                msg = "Cannot connect to slave %s:%s as user '%s'." % \
                      (host, port, slave_val['user'])
                if skip_conn_err:
                    if self.verbose:
                        self._report("# ERROR: %s" % msg, logging.ERROR)
                    slave = None
                else:
                    raise UtilRplError(msg)
            slave_dict = {
                'host': host,       # host name for slave
                'port': port,       # port for slave
                'instance': slave,  # Slave class instance or None
            }
            slaves.append(slave_dict)

        return (master, slaves)

    def _is_connected(self):
        """Check to see if all servers are connected.

        Method will skip any slaves that do not have an instance (offline)
        but requires the master be instantiated and connected.

        The method will also skip the checks altogether if self.force is
        specified.

        Returns bool - True if all connected or self.force is specified.
        """
        # Skip check if --force specified.
        if self.force:
            return True
        if self.master is None or not self.master.is_alive():
            return False
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            if slave is not None and not slave.is_alive():
                return False

        return True

    def remove_discovered_slaves(self):
        """Reset the slaves list to the original list at instantiation

        This method is used in conjunction with discover_slaves to remove
        any discovered slave from the slaves list. Once this is done,
        a call to discover slaves will rediscover the slaves. This is helpful
        for when failover occurs and a discovered slave is used for the new
        master.
        """
        new_list = []
        for slave_dict in self.slaves:
            if not slave_dict.get("discovered", False):
                new_list.append(slave_dict)
        self.slaves = new_list

    def check_master_info_type(self, repo="TABLE"):
        """Check all slaves for master_info_repository=repo

        repo[in]       value for master info = "TABLE" or "FILE"
                       Default is "TABLE"

        Returns bool - True if master_info_repository == repo
        """
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            if slave is not None:
                res = slave.show_server_variable("master_info_repository")
                if not res or res[0][1].upper() != repo.upper():
                    return False
        return True

    def discover_slaves(self, skip_conn_err=True, output_log=False):
        """Discover slaves connected to the master

        skip_conn_err[in]   Skip connection errors to the slaves (i.e. log the
                            errors but do not raise an exception),
                            by default True.
        output_log[in]      Output the logged information (i.e. print the
                            information of discovered slave to the output),
                            by default False.

        Returns bool - True if new slaves found
        """
        # See if the user wants us to discover slaves.
        discover = self.options.get("discover", None)
        if not discover or not self.master:
            return

        # Get user and password (support login-path)
        try:
            user, password = parse_user_password(discover,
                                                 options=self.options)
        except FormatError:
            raise UtilError(USER_PASSWORD_FORMAT.format("--discover-slaves"))

        # Find discovered slaves
        new_slaves_found = False
        self._report("# Discovering slaves for master at "
                     "{0}:{1}".format(self.master.host, self.master.port))
        discovered_slaves = self.master.get_slaves(user, password)
        # pylint: disable=R0101
        for slave in discovered_slaves:
            if "]" in slave:
                host, port = slave.split("]:")
                host = "{0}]".format(host)
            else:
                host, port = slave.split(":")
            msg = "Discovering slave at {0}:{1}".format(host, port)
            self._report(msg, logging.INFO, False)
            if output_log:
                print("# {0}".format(msg))
            # Skip hosts that are not registered properly
            if host == 'unknown host':
                continue
            # Check to see if the slave is already in the list
            else:
                found = False
                # Eliminate if already a slave
                for slave_dict in self.slaves:
                    if slave_dict['host'] == host and \
                       int(slave_dict['port']) == int(port):
                        found = True
                        break
                if not found:
                    # Now we must attempt to connect to the slave.
                    conn_dict = {
                        'conn_info': {'user': user, 'passwd': password,
                                      'host': host, 'port': port,
                                      'socket': None,
                                      'ssl_ca': self.master.ssl_ca,
                                      'ssl_cert': self.master.ssl_cert,
                                      'ssl_key': self.master.ssl_key,
                                      'ssl': self.master.ssl},
                        'role': slave,
                        'verbose': self.options.get("verbosity", 0) > 0,
                    }
                    slave_conn = Slave(conn_dict)
                    try:
                        slave_conn.connect()
                        # Skip discovered slaves that are not connected
                        # to the master (i.e. IO thread is not running)
                        if slave_conn.is_connected():
                            self.slaves.append({'host': host, 'port': port,
                                                'instance': slave_conn,
                                                'discovered': True})
                            msg = "Found slave: {0}:{1}".format(host, port)
                            self._report(msg, logging.INFO, False)
                            if output_log:
                                print("# {0}".format(msg))
                            if self.logging:
                                log_server_version(slave_conn)
                            new_slaves_found = True
                        else:
                            msg = ("Slave skipped (IO not running): "
                                   "{0}:{1}").format(host, port)
                            self._report(msg, logging.WARN, False)
                            if output_log:
                                print("# {0}".format(msg))
                    except UtilError, e:
                        msg = ("Cannot connect to slave {0}:{1} as user "
                               "'{2}'.").format(host, port, user)
                        if skip_conn_err:
                            msg = "{0} {1}".format(msg, e.errmsg)
                            self._report(msg, logging.WARN, False)
                            if output_log:
                                print("# {0}".format(msg))
                        else:
                            raise UtilRplError(msg)

        return new_slaves_found

    def _get_server_gtid_data(self, server, role):
        """Retrieve the GTID information from the server.

        This method builds a tuple of three lists corresponding to the three
        GTID lists (executed, purged, owned) retrievable via the global
        variables. It generates lists suitable for format and printing.

        role[in]           role of the server (used for report generation)

        Returns tuple - (executed list, purged list, owned list)
        """
        executed = []
        purged = []
        owned = []

        if server.supports_gtid() == "NO":
            return (executed, purged, owned)

        try:
            gtids = server.get_gtid_status()
        except UtilError, e:
            self._report("# ERROR retrieving GTID information: %s" % e.errmsg,
                         logging.ERROR)
            return None
        for gtid in gtids[0]:
            for row in gtid.split("\n"):
                if len(row):
                    executed.append((server.host, server.port, role,
                                     row.strip(",")))
        for gtid in gtids[1]:
            for row in gtid.split("\n"):
                if len(row):
                    purged.append((server.host, server.port, role,
                                   row.strip(",")))
        for gtid in gtids[2]:
            for row in gtid.split("\n"):
                if len(row):
                    owned.append((server.host, server.port, role,
                                  row.strip(",")))

        return (executed, purged, owned)

    def _check_switchover_prerequisites(self, candidate=None):
        """Check prerequisites for performing switchover

        This method checks the prerequisites for performing a switch from a
        master to a candidate slave.

        candidate[in]  if supplied, use this candidate instead of the
                       candidate supplied by the user. Must be instance of
                       Master class.

        Returns bool - True if success, raises error if not
        """
        if candidate is None:
            candidate = self.options.get("candidate", None)

        assert (candidate is not None), "A candidate server is required."
        assert (isinstance(candidate, Master)), \
            "A candidate server must be a Master class instance."

        # If master has GTID=ON, ensure all servers have GTID=ON
        gtid_enabled = self.master.supports_gtid() == "ON"
        if gtid_enabled:
            gtid_ok = True
            for slave_dict in self.slaves:
                slave = slave_dict['instance']
                # skip dead or zombie slaves
                if not slave or not slave.is_alive():
                    continue
                if slave.supports_gtid() != "ON":
                    gtid_ok = False
            if not gtid_ok:
                msg = "GTIDs are enabled on the master but not " + \
                      "on all of the slaves."
                self._report(msg, logging.CRITICAL)
                raise UtilRplError(msg)
            elif self.verbose:
                self._report("# GTID_MODE=ON is set for all servers.")

        # Need Slave class instance to check master and replication user
        slave = self._change_role(candidate)

        # Check eligibility
        candidate_ok = self._check_candidate_eligibility(slave.host,
                                                         slave.port,
                                                         slave)
        if not candidate_ok[0]:
            # Create replication user if --force is specified.
            if self.force and candidate_ok[1] == "RPL_USER":
                user, passwd = slave.get_rpl_user()
                res = candidate.create_rpl_user(slave.host, slave.port,
                                                user, passwd, self.ssl)
                if not res[0]:
                    print("# ERROR: {0}".format(res[1]))
                    self._report(res[1], logging.CRITICAL, False)
            else:
                msg = candidate_ok[2]
                self._report(msg, logging.CRITICAL)
                raise UtilRplError(msg)

        return True

    def _get_rpl_user(self, server):
        """Get the replication user

        This method returns the user and password for the replication user
        as read from the Slave class.

        Returns tuple - user, password
        """
        # Get replication user from server if rpl_user not specified
        if self.rpl_user is None:
            slave = self._change_role(server)
            user, passwd = slave.get_rpl_user()
            return (user, passwd)

        # Get user and password (support login-path)
        try:
            user, passwd = parse_user_password(self.rpl_user,
                                               options=self.options)
        except FormatError:
            raise UtilError(USER_PASSWORD_FORMAT.format("--rpl-user"))
        return (user, passwd)

    def run_script(self, script, quiet, options=None):
        """Run an external script

        This method executes an external script. Result is checked for
        success (res == 0). If the user specified a threshold and the
        threshold is exceeded, an error is raised.

        script[in]     script to execute
        quiet[in]      if True, do not print messages
        options[in]    options for script
                       Default is none (no options)

        Returns bool - True = success
        """
        if options is None:
            options = []
        if script is None:
            return
        self._report("# Spawning external script.")
        res = execute_script(script, None, options, self.verbose)
        if self.script_threshold and res >= int(self.script_threshold):
            raise UtilRplError("External script '{0}' failed. Result = {1}.\n"
                               "Specified threshold exceeded. Operation abort"
                               "ed.\nWARNING: The operation did not complete."
                               " Depending on when the external script was "
                               "called, you should check the topology "
                               "for inconsistencies.".format(script, res))
        if res == 0:
            self._report("# Script completed Ok.")
        elif not quiet:
            self._report("ERROR: %s Script failed. Result = %s" %
                         (script, res), logging.ERROR)

    def _check_filters(self, master, slave):
        """Check filters to ensure they are compatible with the master.

        This method compares the binlog_do_db with the replicate_do_db and
        the binlog_ignore_db with the replicate_ignore_db on the master and
        slave to ensure the candidate slave is not filtering out different
        databases than the master.

        master[in]     the Master class instance of the master
        slave[in]      the Slave class instance of the slave

        Returns bool - True = filters agree
        """
        m_filter = master.get_binlog_exceptions()
        s_filter = slave.get_binlog_exceptions()

        failed = False
        if len(m_filter) != len(s_filter):
            failed = True
        elif len(m_filter) == 0:
            return True
        elif m_filter[0][1] != s_filter[0][1] or \
                m_filter[0][2] != s_filter[0][2]:
            failed = True
        if failed:
            if self.verbose and not self.quiet:
                fmt = self.options.get("format", "GRID")
                rows = []
                if len(m_filter) == 0:
                    rows.append(('MASTER', '', ''))
                else:
                    rows.append(m_filter[0])
                if len(s_filter) == 0:
                    rows.append(('SLAVE', '', ''))
                else:
                    rows.append(s_filter[0])
                cols = ["role", "*_do_db", "*_ignore_db"]
                self._report("# Filter Check Failed.", logging.ERROR)
                print_list(sys.stdout, fmt, cols, rows)
            return False
        return True

    def _check_candidate_eligibility(self, host, port, slave,
                                     check_master=True, quiet=False):
        """Perform sanity checks for slave promotion

        This method checks the slave candidate to ensure it meets the
        requirements as follows.

        Check Name  Description
        ----------- --------------------------------------------------
        CONNECTED   slave is connected to the master
        GTID        slave has GTID_MODE = ON if master has GTID = ON
                    (GTID only)
        BEHIND      slave is not behind master
                    (non-GTID only)
        FILTER      slave's filters match the master
        RPL_USER    slave has rpl user defined
        BINLOG      slave must have binary logging enabled

        host[in]         host name for the slave (used for errors)
        port[in]         port for the slave (used for errors)
        slave[in]        Slave class instance of candidate
        check_master[in] if True, check that slave is connected to the master
        quiet[in]        if True, do not print messages even if verbosity > 0

        Returns tuple (bool, check_name, string) -
            (True, "", "") = candidate is viable,
            (False, check_name, error_message) = candidate is not viable
        """
        assert (slave is not None), "No Slave instance for eligibility check."

        gtid_enabled = slave.supports_gtid() == "ON"

        # Is slave connected to master?
        if self.verbose and not quiet:
            self._report("# Checking eligibility of slave %s:%s for "
                         "candidate." % (host, port))
        if check_master:
            msg = "#   Slave connected to master ... %s"
            if not slave.is_alive():
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "CONNECTED",
                        "Connection to slave server lost.")
            if not slave.is_configured_for_master(self.master):
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "CONNECTED",
                        "Candidate is not connected to the correct master.")
            if self.verbose and not quiet:
                self._report(msg % "Ok")

        # If GTID is active on master, ensure slave is on too.
        if gtid_enabled:
            msg = "#   GTID_MODE=ON ... %s"
            if slave.supports_gtid() != "ON":
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "GTID",
                        "Slave does not have GTID support enabled.")
            if self.verbose and not quiet:
                self._report(msg % "Ok")

        # Check for slave behind master
        if not gtid_enabled and check_master:
            msg = "#   Slave not behind master ... %s"
            rpl = Replication(self.master, slave, self.options)
            errors = rpl.check_slave_delay()
            if errors != []:
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "BEHIND", " ".join(errors))
            if self.verbose and not quiet:
                self._report(msg % "Ok")

        # Check filters unless force is on.
        if not self.force and check_master:
            msg = "#   Logging filters agree ... %s"
            if not self._check_filters(self.master, slave):
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "FILTERS",
                        "Master and slave filters differ.")
            elif self.verbose and not quiet:
                self._report(msg % "Ok")

        # If no GTIDs, we need binary logging enabled on candidate.
        if not gtid_enabled:
            msg = "#   Binary logging turned on ... %s"
            if not slave.binlog_enabled():
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "BINLOG",
                        "Binary logging is not enabled on the candidate.")
            if self.verbose and not quiet:
                self._report(msg % "Ok")

        # Check replication user - must exist with correct privileges
        try:
            user, _ = slave.get_rpl_user()
        except UtilError:
            if not self.rpl_user:
                raise

            # Get user and password (support login-path)
            try:
                user, _ = parse_user_password(self.rpl_user)
            except FormatError:
                raise UtilError(USER_PASSWORD_FORMAT.format("--rpl-user"))

            # Make new master forget was a slave using slave methods
            s_candidate = self._change_role(slave, slave=False)
            res = s_candidate.get_rpl_users()
            l = len(res)
            user, host, _ = res[l - 1]
            # raise

        msg = "#   Replication user exists ... %s"
        if user is None or slave.check_rpl_user(user, slave.host) != []:
            if not self.force:
                if self.verbose and not quiet:
                    self._report(msg % "FAIL", logging.WARN)
                return (False, "RPL_USER",
                        "Candidate slave is missing replication user.")
            else:
                self._report("Replication user not found but --force used.",
                             logging.WARN)
        elif self.verbose and not quiet:
            self._report(msg % "Ok")

        return (True, "", "")

    def read_all_retrieved_gtids(self, slave):
        """Ensure any GTIDS in relay log are read

        This method iterates over all slaves ensuring any events read from
        the master but not executed (read) from the relay log are read.

        This step is necessary for failover to ensure all transactions are
        applied to all slaves before the new master is selected.

        slave[in]       Server instance of the slave
        """
        # skip dead or zombie slaves
        if slave is None or not slave.is_alive():
            return
        gtids = slave.get_retrieved_gtid_set()
        if gtids:
            if self.verbose and not self.quiet:
                self._report("# Reading events in relay log for slave "
                             "%s:%s" % (slave.host, slave.port))
            try:
                slave.exec_query(_GTID_WAIT % (gtids.strip(','), self.timeout))
            except UtilRplError as err:
                raise UtilRplError("Error executing %s: %s" %
                                   ((_GTID_WAIT % (gtids.strip(','),
                                                   self.timeout)), err.errmsg))

    def _has_missing_transactions(self, candidate, slave):
        """Determine if there are transactions on the slave not on candidate

        This method uses the function gtid_subset() to determine if there are
        GTIDs (transactions) on the slave that are not on the candidate.

        Return code fopr query should be 0 when there are missing
        transactions, 1 if not, and -1 if there is a non-numeric result
        code generated.

        candidate[in]   Server instance of candidate (new master)
        slave[in]       Server instance of slave to check

        Returns boolean - True if there are transactions else False
        """
        slave_exec_gtids = slave.get_executed_gtid_set()
        slave_retrieved_gtids = slave.get_retrieved_gtid_set()
        cand_slave = self._change_role(candidate)
        candidate_exec_gtids = cand_slave.get_executed_gtid_set()
        slave_gtids = ",".join([slave_exec_gtids.strip(","),
                                slave_retrieved_gtids.strip(",")])
        res = slave.exec_query("SELECT gtid_subset('%s', '%s')" %
                               (slave_gtids, candidate_exec_gtids.strip(",")))
        if res and res[0][0].isdigit():
            result_code = int(res[0][0])
        else:
            result_code = -1

        if self.verbose and not self.quiet:
            if result_code != 1:
                self._report("# Missing transactions found on %s:%s. "
                             "SELECT gtid_subset() = %s" %
                             (slave.host, slave.port, result_code))
            else:
                self._report("# No missing transactions found on %s:%s. "
                             "Skipping connection of candidate as slave." %
                             (slave.host, slave.port))

        return result_code != 1

    def _prepare_candidate_for_failover(self, candidate, user, passwd=""):
        """Prepare candidate slave for slave promotion (in failover)

        This method uses the candidate slave specified and connects it to
        each slave in the topology performing a GTID_SUBSET query to wait
        for the candidate (acting as a slave) to catch up. This ensures
        the candidate is now the 'best' or 'most up-to-date' slave in the
        topology.

        Method works only for GTID-enabled candidate servers.

        candidate[in]  Slave class instance of candidate
        user[in]       replication user
        passwd[in]     replication user password

        Returns bool - True if successful,
                       raises exception if failure and forst is False
        """

        assert (candidate is not None), "Candidate must be a Slave instance."

        if candidate.supports_gtid() != "ON":
            msg = "Candidate does not have GTID turned on or " + \
                  "does not support GTIDs."
            self._report(msg, logging.CRITICAL)
            raise UtilRplError(msg)

        lock_options = {
            'locking': 'flush',
            'verbosity': 3 if self.verbose else self.verbosity,
            'silent': self.quiet,
            'rpl_mode': "master",
        }

        hostport = "%s:%s" % (candidate.host, candidate.port)
        for slave_dict in self.slaves:
            s_host = slave_dict['host']
            s_port = slave_dict['port']

            temp_master = slave_dict['instance']

            # skip dead or zombie slaves
            if temp_master is None or not temp_master.is_alive():
                continue

            # Gather retrieved_gtid_set to execute all events on slaves still
            # in the slave's relay log
            self.read_all_retrieved_gtids(temp_master)

            # Sanity check: ensure candidate and slave are not the same.
            if candidate.is_alias(s_host) and \
               int(s_port) == int(candidate.port):
                continue

            # Check for missing transactions. No need to connect to slave if
            # there are no transactions (GTIDs) to retrieve
            if not self._has_missing_transactions(candidate, temp_master):
                continue

            try:
                candidate.stop()
            except UtilError as err:
                if not self.quiet:
                    self._report("Candidate {0} failed to stop. "
                                 "{1}".format(hostport, err.errmsg))

            # Block writes to slave (temp_master)
            lock_ftwrl = Lock(temp_master, [], lock_options)
            temp_master.set_read_only(True)
            if self.verbose and not self.quiet:
                read_only = temp_master.show_server_variable("READ_ONLY")
                self._report("# Read only is {0} for {1}:{2}."
                             "".format(read_only[0][1], temp_master.host,
                                       temp_master.port))

            # Connect candidate to slave as its temp_master
            if self.verbose and not self.quiet:
                self._report("# Connecting candidate to %s:%s as a temporary "
                             "slave to retrieve unprocessed GTIDs." %
                             (s_host, s_port))

            if not candidate.switch_master(temp_master, user, passwd, False,
                                           None, None,
                                           self.verbose and not self.quiet):
                msg = "Cannot switch candidate to slave for " + \
                      "slave promotion process."
                self._report(msg, logging.CRITICAL)
                raise UtilRplError(msg)

            # Unblock writes to slave (temp_master).
            temp_master.set_read_only(False)
            if self.verbose and not self.quiet:
                read_only = temp_master.show_server_variable("READ_ONLY")
                self._report("# Read only is {0} for {1}:{2}."
                             "".format(read_only[0][1], temp_master.host,
                                       temp_master.port))
            lock_ftwrl.unlock()

            try:
                candidate.start()
                candidate.exec_query("COMMIT")
            except UtilError as err:
                if not self.quiet:
                    self._report("Candidate {0} failed to start. "
                                 "{1}".format(hostport, err.errmsg))

            if self.verbose and not self.quiet:
                self._report("# Waiting for candidate to catch up to slave "
                             "%s:%s." % (s_host, s_port))
            temp_master_gtid = temp_master.exec_query(_GTID_EXECUTED)
            candidate.wait_for_slave_gtid(temp_master_gtid, self.timeout,
                                          self.verbose and not self.quiet)

            # Disconnect candidate from slave (temp_master)
            candidate.stop()

        return True

    def _check_slaves_status(self, stop_on_error=False):
        """Check all slaves for error before performing failover.

        This method check the status of all slaves (before the new master catch
        up with them), using SHOW SLAVE STATUS, reporting any error found and
        warning the user if failover might result in an inconsistent
        replication topology. By default the process will not stop, but if
        the --pedantic option is used then failover will stop with an error.

        stop_on_error[in]  Define the default behavior of failover if errors
                           are found. By default: False (not stop on errors).
        """
        for slave_dict in self.slaves:
            s_host = slave_dict['host']
            s_port = slave_dict['port']
            slave = slave_dict['instance']

            # Verify if the slave is alive
            if not slave or not slave.is_alive():
                msg = "Slave '{host}@{port}' is not alive.".format(host=s_host,
                                                                   port=s_port)
                # Print warning or raise an error according to the default
                # failover behavior and defined options.
                if ((stop_on_error and not self.force) or
                        (not stop_on_error and self.pedantic)):
                    print("# ERROR: {0}".format(msg))
                    self._report(msg, logging.CRITICAL, False)
                    if stop_on_error and not self.force:
                        ignore_opt = "with the --force"
                    else:
                        ignore_opt = "without the --pedantic"
                    ignore_tip = ("Note: To ignore this issue use the "
                                  "utility {0} option.").format(ignore_opt)
                    raise UtilRplError("{err} {note}".format(err=msg,
                                                             note=ignore_tip))
                else:
                    print("# WARNING: {0}".format(msg))
                    self._report(msg, logging.WARN, False)
                    continue

            # Check SQL thread and errors (no need to check for IO errors)
            # Note: IO errors are excepted as the master is down
            res = slave.get_sql_error()

            # First, check if server is acting as a slave
            if not res:
                msg = ("Server '{host}@{port}' is not acting as a "
                       "slave.").format(host=s_host, port=s_port)
                # Print warning or raise an error according to the default
                # failover behavior and defined options.
                if ((stop_on_error and not self.force) or
                        (not stop_on_error and self.pedantic)):
                    print("# ERROR: {0}".format(msg))
                    self._report(msg, logging.CRITICAL, False)
                    if stop_on_error and not self.force:
                        ignore_opt = "with the --force"
                    else:
                        ignore_opt = "without the --pedantic"
                    ignore_tip = ("Note: To ignore this issue use the "
                                  "utility {0} option.").format(ignore_opt)
                    raise UtilRplError("{err} {note}".format(err=msg,
                                                             note=ignore_tip))
                else:
                    print("# WARNING: {0}".format(msg))
                    self._report(msg, logging.WARN, False)
                    continue

            # Now, check the SQL thread status
            sql_running = res[0]
            sql_errorno = res[1]
            sql_error = res[2]
            if sql_running == "No" or sql_errorno or sql_error:
                msg = ("Problem detected with SQL thread for slave "
                       "'{host}'@'{port}' that can result in an unstable "
                       "topology.").format(host=s_host, port=s_port)
                msg_thread = " - SQL thread running: {0}".format(sql_running)
                if not sql_errorno and not sql_error:
                    msg_error = " - SQL error: None"
                else:
                    msg_error = (" - SQL error: {errno} - "
                                 "{errmsg}").format(errno=sql_errorno,
                                                    errmsg=sql_error)
                msg_tip = ("Check the slave server log to identify "
                           "the problem and fix it. For more information, "
                           "see: http://dev.mysql.com/doc/refman/5.6/en/"
                           "replication-problems.html")
                # Print warning or raise an error according to the default
                # failover behavior and defined options.
                if ((stop_on_error and not self.force) or
                        (not stop_on_error and self.pedantic)):
                    print("# ERROR: {0}".format(msg))
                    self._report(msg, logging.CRITICAL, False)
                    print("# {0}".format(msg_thread))
                    self._report(msg_thread, logging.CRITICAL, False)
                    print("# {0}".format(msg_error))
                    self._report(msg_error, logging.CRITICAL, False)
                    print("#  Tip: {0}".format(msg_tip))
                    if stop_on_error and not self.force:
                        ignore_opt = "with the --force"
                    else:
                        ignore_opt = "without the --pedantic"
                    ignore_tip = ("Note: To ignore this issue use the "
                                  "utility {0} option.").format(ignore_opt)
                    raise UtilRplError("{err} {note}".format(err=msg,
                                                             note=ignore_tip))
                else:
                    print("# WARNING: {0}".format(msg))
                    self._report(msg, logging.WARN, False)
                    print("# {0}".format(msg_thread))
                    self._report(msg_thread, logging.WARN, False)
                    print("# {0}".format(msg_error))
                    self._report(msg_error, logging.WARN, False)
                    print("#  Tip: {0}".format(msg_tip))

    def find_errant_transactions(self):
        """Check all slaves for the existence of errant transactions.

        In particular, for all slaves it search for executed transactions that
        are not found on the other slaves (only on one slave) and not from the
        current master.

        Returns a list of tuples, each tuple containing the slave host, port
        and set of corresponding errant transactions, i.e.:
        [(host1, port1, set1), ..., (hostn, portn, setn)]. If no errant
        transactions are found an empty list is returned.
        """
        res = []

        # Get master UUID (if master is available otherwise get it from slaves)
        use_master_uuid_from_slave = True
        if self.master:
            master_uuid = self.master.get_uuid()
            use_master_uuid_from_slave = False

        # Check all slaves for executed transactions not in other slaves
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # Skip not defined or dead slaves
            if not slave or not slave.is_alive():
                continue
            tnx_set = slave.get_executed_gtid_set()

            # Get master UUID from slave if master is not available
            if use_master_uuid_from_slave:
                master_uuid = slave.get_master_uuid()

            slave_set = set()
            for others_slave_dic in self.slaves:
                if (slave_dict['host'] != others_slave_dic['host'] or
                        slave_dict['port'] != others_slave_dic['port']):
                    other_slave = others_slave_dic['instance']
                    # Skip not defined or dead slaves
                    if not other_slave or not other_slave.is_alive():
                        continue
                    errant_res = other_slave.exec_query(
                        _GTID_SUBTRACT_TO_EXECUTED.format(tnx_set))

                    # Only consider the transaction as errant if not from the
                    # current master.
                    # Note: server UUID can appear with mixed cases (e.g. for
                    # 5.6.9 servers the server_uuid is lower case and appears
                    # in upper cases in the GTID_EXECUTED set.
                    errant_set = set()
                    for tnx in errant_res:
                        if tnx[0] and not tnx[0].lower().startswith(
                                master_uuid.lower()):
                            errant_set.update(tnx[0].split(',\n'))

                    # Errant transactions exist on only one slave, therefore if
                    # the returned set is empty the loop can be break
                    # (no need to check the remaining slaves).
                    if not errant_set:
                        break

                    slave_set = slave_set.union(errant_set)
            # Store result
            if slave_set:
                res.append((slave_dict['host'], slave_dict['port'], slave_set))

        return res

    def _check_all_slaves(self, new_master):
        """Check all slaves for errors.

        Check each slave's status for errors during replication. If errors are
        found, they are printed as warning statements to stdout.

        new_master[in] the new master in Master class instance
        """
        slave_errors = []
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                continue
            rpl = Replication(new_master, slave, self.options)
            # Use pingtime to check slave status
            iteration = 0
            slave_ok = True
            while iteration < int(self.pingtime):
                res = rpl.check_slave_connection()
                if not res and iteration >= self.pingtime:
                    slave_error = None
                    if self.verbose:
                        res = slave.get_io_error()
                        slave_error = "%s:%s" % (res[1], res[2])
                    slave_errors.append((slave_dict['host'],
                                         slave_dict['port'],
                                         slave_error))
                    slave_ok = False
                    if self.verbose and not self.quiet:
                        self._report("# %s:%s status: FAIL " %
                                     (slave_dict['host'],
                                      slave_dict['port']), logging.WARN)
                elif res:
                    iteration = int(self.pingtime) + 1
                else:
                    time.sleep(1)
                    iteration += 1
            if slave_ok and self.verbose and not self.quiet:
                self._report("# %s:%s status: Ok " % (slave_dict['host'],
                                                      slave_dict['port']))

        if len(slave_errors) > 0:
            self._report("WARNING - The following slaves failed to connect to "
                         "the new master:", logging.WARN)
            for error in slave_errors:
                self._report("  - %s:%s" % (error[0], error[1]), logging.WARN)
                if self.verbose and error[2] is not None:
                    self._report(error[2], logging.WARN)
                else:
                    print
            return False

        return True

    def remove_slave(self, slave):
        """Remove a slave from the slaves dictionary list

        slave[in]      the dictionary for the slave to remove
        """
        for i, slave_dict in enumerate(self.slaves):
            if (slave_dict['instance'] and
                    slave_dict['instance'].is_alias(slave['host']) and
                    int(slave_dict['port']) == int(slave['port'])):
                # Disconnect to satisfy new server restrictions on termination
                self.slaves[i]['instance'].disconnect()
                self.slaves.pop(i)
                break

    def gtid_enabled(self):
        """Check if topology has GTID turned on.

        This method check if GTID mode is turned ON for all servers in the
        replication topology, skipping the check for not available servers.

        Returns bool - True = GTID_MODE=ON for all available servers (master
        and slaves) in the replication topology..
        """
        if self.master and self.master.supports_gtid() != "ON":
            return False  # GTID disabled or not supported.
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                continue
            if slave.supports_gtid() != "ON":
                return False  # GTID disabled or not supported.
        # GTID enabled for all topology (excluding not available servers).
        return True

    def get_servers_with_gtid_not_on(self):
        """Get the list of servers from the topology with GTID turned off.

        Note: not connected slaves will be ignored

        Returns a list of tuples identifying the slaves (host, port, gtid_mode)
                with GTID_MODE=OFF or GTID_MODE=NO (i.e., not available).
        """
        res = []
        # Check master GTID_MODE
        if self.master:
            gtid_mode = self.master.supports_gtid()
            if gtid_mode != "ON":
                res.append((self.master.host, self.master.port, gtid_mode))

        # Check slaves GTID_MODE
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # skip not available or not alive slaves
            if not slave or not slave.is_alive():
                continue
            gtid_mode = slave.supports_gtid()
            if gtid_mode != "ON":
                res.append((slave_dict['host'], slave_dict['port'], gtid_mode))

        return res

    def get_health(self):
        """Retrieve the replication health for the master and slaves.

        This method will retrieve the replication health of the topology. This
        includes the following for each server.

          - host       : host name
          - port       : connection port
          - role       : "MASTER" or "SLAVE"
          - state      : UP = connected, WARN = cannot connect but can ping,
                         DOWN = cannot connect nor ping
          - gtid       : ON = gtid supported and turned on, OFF = supported
                         but not enabled, NO = not supported
          - rpl_health : (master) binlog enabled,
                         (slave) IO tread is running, SQL thread is running,
                         no errors, slave delay < max_delay,
                         read log pos + max_position < master's log position
                         Note: Will show 'ERROR' if there are multiple
                         errors encountered otherwise will display the
                         health check that failed.

        If verbosity is set, it will show the following additional information.

          (master)
            - server version, binary log file, position

          (slaves)
            - server version, master's binary log file, master's log position,
              IO_Thread, SQL_Thread, Secs_Behind, Remaining_Delay,
              IO_Error_Num, IO_Error

        Note: The method will return health for the master and slaves or just
              the slaves if no master is specified. In which case, the master
              status shall display "no master specified" instead of a status
              for the connection.

        Returns tuple - (columns, rows)
        """
        rows = []
        columns = []
        columns.extend(_HEALTH_COLS)
        if self.verbosity > 0:
            columns.extend(_HEALTH_DETAIL_COLS)
        if self.master:
            # Get master health
            rpl_health = self.master.check_rpl_health()
            self._report("# Getting health for master: %s:%s." %
                         (self.master.host, self.master.port), logging.INFO,
                         False)
            have_gtid = self.master.supports_gtid()
            master_data = [
                self.master.host,
                self.master.port,
                "MASTER",
                get_server_state(self.master, self.master.host, self.pingtime,
                                 self.verbosity > 0),
                have_gtid,
                "OK" if rpl_health[0] else ", ".join(rpl_health[1]),
            ]

            m_status = self.master.get_status()
            if len(m_status):
                master_log, master_log_pos = m_status[0][0:2]
            else:
                master_log = None
                master_log_pos = 0

            # Show additional details if verbosity turned on
            if self.verbosity > 0:
                master_data.extend([self.master.get_version(), master_log,
                                    master_log_pos, "", "", "", "", "", "",
                                    "", "", ""])

            rows.append(master_data)
            if have_gtid == "ON":
                master_gtids = self.master.exec_query(_GTID_EXECUTED)
        else:
            # No master makes these impossible to determine.
            have_gtid = "OFF"
            master_log = ""
            master_log_pos = ""  # pylint: disable=R0204

        # Get the health of the slaves
        slave_rows = []
        for slave_dict in self.slaves:
            host = slave_dict['host']
            port = slave_dict['port']
            slave = slave_dict['instance']
            # Get correct port from slave
            if slave and port != slave.port:
                port = slave.port
            if slave is None:
                rpl_health = (False, ["Cannot connect to slave."])
            elif not slave.is_alive():
                # Attempt to reconnect to the database server.
                try:
                    slave.connect()
                    # Connection succeeded.
                    if not slave.is_configured_for_master(self.master):
                        rpl_health = (False,
                                      ["Slave is not connected to master."])
                        slave = None
                except UtilError:
                    # Connection failed.
                    rpl_health = (False, ["Slave is not alive."])
                    slave = None
            elif not self.master:
                rpl_health = (False, ["No master specified."])
            elif not slave.is_configured_for_master(self.master):
                rpl_health = (False, ["Slave is not connected to master."])
                slave = None

            if self.master and slave is not None:
                rpl_health = slave.check_rpl_health(self.master,
                                                    master_log, master_log_pos,
                                                    self.max_delay,
                                                    self.max_pos,
                                                    self.verbosity)

                # Now, see if filters are in compliance
                if not self._check_filters(self.master, slave):
                    if rpl_health[0]:
                        errors = rpl_health[1]
                        errors.append("Binary log and Relay log filters "
                                      "differ.")
                        rpl_health = (False, errors)

            slave_data = [
                host,
                port,
                "SLAVE",
                get_server_state(slave, host, self.pingtime,
                                 self.verbosity > 0),
                " " if slave is None else slave.supports_gtid(),
                "OK" if rpl_health[0] else ", ".join(rpl_health[1]),
            ]

            # Show additional details if verbosity turned on
            if self.verbosity > 0:
                if slave is None:
                    slave_data.extend([""] * 13)
                else:
                    slave_data.append(slave.get_version())
                    res = slave.get_rpl_details()
                    if res is not None:
                        slave_data.extend(res)
                        if have_gtid == "ON":
                            gtid_behind = slave.num_gtid_behind(master_gtids)
                            slave_data.extend([gtid_behind])
                        else:
                            slave_data.extend([""])
                    else:
                        slave_data.extend([""] * 13)

            slave_rows.append(slave_data)

        # order the slaves
        slave_rows.sort(key=operator.itemgetter(0, 1))
        rows.extend(slave_rows)

        return (columns, rows)

    def get_server_uuids(self):
        """Return a list of the server's uuids.

        Returns list of tuples = (host, port, role, uuid)
        """
        # Get the master's uuid
        uuids = []
        uuids.append((self.master.host, self.master.port, "MASTER",
                      self.master.get_uuid()))
        for slave_dict in self.slaves:
            uuids.append((slave_dict['host'], slave_dict['port'], "SLAVE",
                          slave_dict['instance'].get_uuid()))
        return uuids

    def get_gtid_data(self):
        """Get the GTID information from the topology

        This method retrieves the executed, purged, and owned GTID lists from
        the servers in the topology. It arranges them into three lists and
        includes the host name, port, and role of each server.

        Returns tuple - lists for GTID data
        """
        executed = []
        purged = []
        owned = []

        gtid_data = self._get_server_gtid_data(self.master, "MASTER")
        if gtid_data is not None:
            executed.extend(gtid_data[0])
            purged.extend(gtid_data[1])
            owned.extend(gtid_data[2])

        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            if slave is not None:
                gtid_data = self._get_server_gtid_data(slave, "SLAVE")
                if gtid_data is not None:
                    executed.extend(gtid_data[0])
                    purged.extend(gtid_data[1])
                    owned.extend(gtid_data[2])

        return (executed, purged, owned)

    def get_slaves_dict(self, skip_not_connected=True):
        """Get a dictionary representation of the slaves in the topology.

        This function converts the list of slaves in the topology to a
        dictionary with all elements in the list, using 'host@port' as the
        key for each element.

        skip_not_connected[in]  Boolean value indicating if not available or
                                not connected slaves should be skipped.
                                By default 'True' (not available slaves are
                                skipped).

        Return a dictionary representation of the slaves in the
        topology. Each element has a key with the format 'host@port' and
        a dictionary value with the corresponding slave's data.
        """
        res = {}
        for slave_dic in self.slaves:
            slave = slave_dic['instance']
            if skip_not_connected:
                if slave and slave.is_alive():
                    key = '{0}@{1}'.format(slave_dic['host'],
                                           slave_dic['port'])
                    res[key] = slave_dic
            else:
                key = '{0}@{1}'.format(slave_dic['host'], slave_dic['port'])
                res[key] = slave_dic
        return res

    def slaves_gtid_subtract_executed(self, gtid_set, multithreading=False):
        """Subtract GTID_EXECUTED from the given GTID set on all slaves.

        Compute the difference between the given GTID set and the GTID_EXECUTED
        set for each slave, providing the sets with the missing GTIDs from the
        GTID_EXECUTED set that belong to the input GTID set.

        gtid_set[in]        Input GTID set to find the missing element from
                            the GTID_EXECUTED for all slaves.
        multithreading[in]  Flag indicating if multithreading will be used,
                            meaning that the operation will be performed
                            concurrently on all slaves.
                            By default True (concurrent execution).

        Return a list of tuples with the result for each slave. Each tuple
        contains the identification of the server (host and port) and a string
        representing the set of GTIDs from the given set not in the
        GTID_EXECUTED set of the corresponding slave.
        """
        if multithreading:
            # Create a pool of threads to execute the method for each slave.
            pool = ThreadPool(processes=len(self.slaves))
            res_lst = []
            for slave_dict in self.slaves:
                slave = slave_dict['instance']
                if slave:  # Skip non existing (not connected) slaves.
                    thread_res = pool.apply_async(slave.gtid_subtract_executed,
                                                  (gtid_set, ))
                    res_lst.append((slave.host, slave.port, thread_res))
            pool.close()
            # Wait for all threads to finish here to avoid RuntimeErrors when
            # waiting for the result of a thread that is already dead.
            pool.join()
            # Get the result from each slave and return the results.
            res = []
            for host, port, thread_res in res_lst:
                res.append((host, port, thread_res.get()))
            return res
        else:
            res = []
            # Subtract gtid set on all slaves.
            for slave_dict in self.slaves:
                slave = slave_dict['instance']
                if slave:  # Skip non existing (not connected) slaves.
                    not_in_set = slave.gtid_subtract_executed(gtid_set)
                    res.append((slave.host, slave.port, not_in_set))
            return res

    def check_privileges(self, failover=False, skip_master=False):
        """Check privileges for the master and all known servers

        failover[in]        if True, check permissions for switchover and
                            failover commands. Default is False.
        skip_master[in]     Skip the check for the master.

        Returns list - [(user, host)] if not enough permissions,
                       [] if no errors
        """
        servers = []
        errors = []

        # Collect all users first.
        if skip_master:
            for slave_conn in self.slaves:
                slave = slave_conn['instance']
                # A slave instance is None if the connection failed during the
                # creation of the topology. In this case ignore the slave.
                if slave is not None:
                    servers.append(slave)
        else:
            if self.master is not None:
                servers.append(self.master)
                for slave_conn in self.slaves:
                    slave = slave_conn['instance']
                    # A slave instance is None if the connection failed during
                    # the creation of the topology. In this case ignore the
                    # slave.
                    if slave is not None:
                        servers.append(slave)

        # If candidates were specified, check those too.
        candidates = self.options.get("candidates", None)
        candidate_slaves = []
        if candidates:
            self._report("# Checking privileges on candidates.")
            for candidate in candidates:
                slave_dict = self.connect_candidate(candidate, False)
                slave = slave_dict['instance']
                if slave is not None:
                    servers.append(slave)
                    candidate_slaves.append(slave)

        for server in servers:
            user_inst = User(server, "{0}@{1}".format(server.user,
                                                      server.host))
            if not failover:
                if not user_inst.has_privilege("*", "*", "SUPER"):
                    errors.append((server.user, server.host, server.port,
                                   'SUPER'))
            else:
                if (not user_inst.has_privilege("*", "*", "SUPER") or
                        not user_inst.has_privilege("*", "*",
                                                    "GRANT OPTION") or
                        not user_inst.has_privilege("*", "*", "SELECT") or
                        not user_inst.has_privilege("*", "*", "RELOAD") or
                        not user_inst.has_privilege("*", "*", "DROP") or
                        not user_inst.has_privilege("*", "*", "CREATE") or
                        not user_inst.has_privilege("*", "*", "INSERT") or
                        not user_inst.has_privilege("*", "*",
                                                    "REPLICATION SLAVE")):
                    errors.append((server.user, server.host, server.port,
                                   'SUPER, GRANT OPTION, REPLICATION SLAVE, '
                                   'SELECT, RELOAD, DROP, CREATE, INSERT'))

        # Disconnect if we connected to any candidates
        for slave in candidate_slaves:
            slave.disconnect()

        return errors

    def run_cmd_on_slaves(self, command, quiet=False):
        """Run a command on a list of slaves.

        This method will run one of the following slave commands.

          start - START SLAVE;
          stop  - STOP SLAVE;
          reset - STOP SLAVE; RESET SLAVE;

        command[in]        command to execute
        quiet[in]          If True, do not print messages
                           Default is False
        :param command:
        :param quiet:
        """

        assert (self.slaves is not None), \
            "No slaves specified or connections failed."

        self._report("# Performing %s on all slaves." %
                     command.upper())

        for slave_dict in self.slaves:
            hostport = "%s:%s" % (slave_dict['host'], slave_dict['port'])
            msg = "#   Executing %s on slave %s " % (command, hostport)
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if not slave or not slave.is_alive():
                message = "{0}WARN - cannot connect to slave".format(msg)
                self._report(message, logging.WARN)
            elif command == 'reset':
                if (self.master and
                        not slave.is_configured_for_master(self.master) and
                        not quiet):
                    message = ("{0}WARN - slave is not configured with this "
                               "master").format(msg)
                    self._report(message, logging.WARN)
                try:
                    slave.reset()
                except UtilError:
                    if not quiet:
                        message = "{0}WARN - slave failed to reset".format(msg)
                        self._report(message, logging.WARN)
                else:
                    if not quiet:
                        self._report("{0}Ok".format(msg))
            elif command == 'start':
                if (self.master and
                        not slave.is_configured_for_master(self.master) and
                        not quiet):
                    message = ("{0}WARN - slave is not configured with this "
                               "master").format(msg)
                    self._report(message, logging.WARN)
                try:
                    slave.start()
                except UtilError:
                    if not quiet:
                        message = "{0}WARN - slave failed to start".format(msg)
                        self._report(message, logging.WARN)
                else:
                    if not quiet:
                        self._report("{0}Ok".format(msg))
            elif command == 'stop':
                if (self.master and
                        not slave.is_configured_for_master(self.master) and
                        not quiet):
                    message = ("{0}WARN - slave is not configured with this "
                               "master").format(msg)
                    self._report(message, logging.WARN)
                elif not slave.is_connected() and not quiet:
                    message = ("{0}WARN - slave is not connected to "
                               "master").format(msg)
                    self._report(message, logging.WARN)
                try:
                    slave.stop()
                except UtilError:
                    if not quiet:
                        message = "{0}WARN - slave failed to stop".format(msg)
                        self._report(message, logging.WARN)
                else:
                    if not quiet:
                        self._report("{0}Ok".format(msg))

    def connect_candidate(self, candidate, master=True):
        """Parse and connect to the candidate

        This method parses the candidate string and returns a slave dictionary
        if master=False else returns a Master class instance.

        candidate[in]  candidate connection string
        master[in]     if True, make Master class instance

        Returns slave_dict or Master class instance
        """
        # Need instance of Master class for operation
        conn_dict = {
            'conn_info': candidate,
            'quiet': True,
            'verbose': self.verbose,
        }
        if master:
            m_candidate = Master(conn_dict)
            m_candidate.connect()
            return m_candidate
        else:
            s_candidate = Slave(conn_dict)
            s_candidate.connect()
            slave_dict = {
                'host': s_candidate.host,
                'port': s_candidate.port,
                'instance': s_candidate,
            }
            return slave_dict

    def switchover(self, candidate):
        """Perform switchover from master to candidate slave.

        This method switches the role of master to a candidate slave. The
        candidate is checked for viability before the switch is made.

        If the user specified --demote-master, the method will make the old
        master a slave of the candidate.

        candidate[in]  the connection information for the --candidate option

        Return bool - True = success, raises exception on error
        """

        # Need instance of Master class for operation
        m_candidate = self.connect_candidate(candidate)

        # Switchover needs to succeed and prerequisites must be met else abort.
        self._report("# Checking candidate slave prerequisites.")
        try:
            self._check_switchover_prerequisites(m_candidate)
        except UtilError, e:
            self._report("ERROR: %s" % e.errmsg, logging.ERROR)
            if not self.force:
                return

        # Check if the slaves are configured for the specified master
        self._report("# Checking slaves configuration to master.")
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # Skip not defined or alive slaves (Warning displayed elsewhere)
            if not slave or not slave.is_alive():
                continue

            if not slave.is_configured_for_master(self.master):
                # Slave not configured for master (i.e. not in topology)
                msg = ("Slave {0}:{1} is not configured with master {2}:{3}"
                       ".").format(slave_dict['host'], slave_dict['port'],
                                   self.master.host, self.master.port)
                print("# ERROR: {0}".format(msg))
                self._report(msg, logging.ERROR, False)
                if not self.force:
                    raise UtilRplError("{0} Note: If you want to ignore this "
                                       "issue, please use the utility with "
                                       "the --force option.".format(msg))

        # Check rpl-user definitions
        if self.verbose and self.rpl_user:
            if self.check_master_info_type("TABLE"):
                msg = ("# When the master_info_repository variable is set to"
                       " TABLE, the --rpl-user option is ignored and the"
                       " existing replication user values are retained.")
                self._report(msg, logging.INFO)
                self.rpl_user = None
            else:
                msg = ("# When the master_info_repository variable is set to"
                       " FILE, the --rpl-user option may be used only if the"
                       " user specified matches what is shown in the SLAVE"
                       " STATUS output unless the --force option is used.")
                self._report(msg, logging.INFO)

        user, passwd = self._get_rpl_user(m_candidate)
        if not passwd:
            passwd = ''

        if not self.check_master_info_type("TABLE"):
            slave_candidate = self._change_role(m_candidate, slave=True)
            rpl_master_user = slave_candidate.get_rpl_master_user()

            if not self.force:
                if (user != rpl_master_user):
                    msg = ("The replication user specified with --rpl-user "
                           "does not match the existing replication user.\n"
                           "Use the --force option to use the "
                           "replication user specified with --rpl-user.")
                    self._report("ERROR: %s" % msg, logging.ERROR)
                    return

                # Can't get rpl pass from remote master_repo=file
                # but it can get the current used hashed to be compared.
                slave_qry = slave_candidate.exec_query
                # Use the correct query for server version (changed for 5.7.6)
                if slave_candidate.check_version_compat(5, 7, 6):
                    query = _SELECT_RPL_USER_PASS_QUERY_5_7_6
                else:
                    query = _SELECT_RPL_USER_PASS_QUERY
                passwd_hash = slave_qry(query.format(user=user,
                                                     host=m_candidate.host))
                # if user does not exist passwd_hash will be an empty query.
                if passwd_hash:
                    passwd_hash = passwd_hash[0][3]
                else:
                    passwd_hash = ""
                if passwd == '':
                    msg = ("The specified replication user is using a "
                           "password (but none was specified).\n"
                           "Use the --force option to force the use of "
                           "the user specified with  --rpl-user and no "
                           "password.")
                else:
                    msg = ("The specified replication user is using a "
                           "different password that the one specified.\n"
                           "Use the --force option to force the use of "
                           "the user specified with  --rpl-user and new "
                           "password.")
                # If 5.7.6+, check by trying to connect
                if self.master.check_version_compat(5, 7, 6):
                    config = {
                        'user': user,
                        'passwd': passwd,
                        'host': m_candidate.host,
                        'port': m_candidate.port,
                    }
                    s_conn = Server({'conn_info': config})
                    try:
                        s_conn.connect()
                    except:
                        self._report("ERROR: %s" % msg, logging.ERROR)
                        return
                    else:
                        s_conn.disconnect()
                # else compare the hash fom --rpl-user.
                else:
                    rpl_master_pass = slave_qry("SELECT PASSWORD('%s');" %
                                                passwd)
                    rpl_master_pass = rpl_master_pass[0][0]
                    if rpl_master_pass != passwd_hash:
                        self._report("ERROR: %s" % msg, logging.ERROR)
                        return
            # Use the correct query for server (changed for 5.7.6).
            self.master.toggle_binlog("DISABLE")
            if self.master.check_version_compat(5, 7, 6):
                query = _UPDATE_RPL_USER_QUERY_5_7_6
                self.master.exec_query(query.format(user=user,
                                                    host=m_candidate.host,
                                                    passwd=passwd))
            else:
                query = _UPDATE_RPL_USER_QUERY
                self.master.exec_query(query.format(user=user, passwd=passwd))
            self.master.toggle_binlog("ENABLE")

        if self.verbose:
            self._report("# Creating replication user if it does not exist.")
        self.master.toggle_binlog("DISABLE")
        res = m_candidate.create_rpl_user(m_candidate.host,
                                          m_candidate.port,
                                          user, passwd, ssl=self.ssl)
        self.master.toggle_binlog("ENABLE")
        if not res[0]:
            print("# ERROR: {0}".format(res[1]))
            self._report(res[1], logging.CRITICAL, False)

        # Call exec_before script - display output if verbose on
        try:
            self.run_script(self.before_script, False,
                            [self.master.host, self.master.port,
                             m_candidate.host, m_candidate.port])
        except Exception as err:  # pylint: disable=W0703
            self._report("# Before script failed! {0}".format(err),
                         level=logging.ERROR)

        if self.verbose:
            self._report("# Blocking writes on master.")
        lock_options = {
            'locking': 'flush',
            'verbosity': 3 if self.verbose else self.verbosity,
            'silent': self.quiet,
            'rpl_mode': "master",
        }
        lock_ftwrl = Lock(self.master, [], lock_options)
        self.master.set_read_only(True)
        if self.verbose and not self.quiet:
            read_only = self.master.show_server_variable("READ_ONLY")
            self._report("# Read only is {0} for {1}:{2}."
                         "".format(read_only[0][1], self.master.host,
                                   self.master.port))

        # Wait for all slaves to catch up.
        gtid_enabled = self.master.supports_gtid() == "ON"
        if gtid_enabled:
            master_gtid = self.master.exec_query(_GTID_EXECUTED)
        self._report("# Waiting for slaves to catch up to old master.")
        for slave_dict in self.slaves:
            master_info = self.master.get_status()[0]
            slave = slave_dict['instance']
            # skip dead or zombie slaves, and print warning
            if not slave or not slave.is_alive():
                if self.verbose:
                    msg = ("Slave {0}:{1} skipped (not "
                           "reachable)").format(slave_dict['host'],
                                                slave_dict['port'])
                    print("# WARNING: {0}".format(msg))
                    self._report(msg, logging.WARNING, False)
                continue
            if gtid_enabled:
                print_query = self.verbose and not self.quiet
                res = slave.wait_for_slave_gtid(master_gtid, self.timeout,
                                                print_query)
            else:
                res = slave.wait_for_slave(master_info[0], master_info[1],
                                           self.timeout)
            if not res:
                msg = "Slave %s:%s did not catch up to the master." % \
                      (slave_dict['host'], slave_dict['port'])
                if not self.force:
                    self._report(msg, logging.CRITICAL)
                    raise UtilRplError(msg)
                else:
                    self._report("# %s" % msg)

        # Stop all slaves
        self._report("# Stopping slaves.")
        self.run_cmd_on_slaves("stop", not self.verbose)

        # Unblock master
        self.master.set_read_only(False)
        if self.verbose and not self.quiet:
            read_only = self.master.show_server_variable("READ_ONLY")
            self._report("# Read only is {0} for {1}:{2}."
                         "".format(read_only[0][1], self.master.host,
                                   self.master.port))
        lock_ftwrl.unlock()

        # Make master a slave (if specified)
        if self.options.get("demote", False):
            self._report("# Demoting old master to be a slave to the "
                         "new master.")

            slave = self._change_role(self.master)
            slave.stop()

            slave_dict = {
                'host': self.master.host,  # host name for slave
                'port': self.master.port,  # port for slave
                'instance': slave,         # Slave class instance
            }
            self.slaves.append(slave_dict)

        # Move candidate slave to master position in lists
        self.master_vals = m_candidate.get_connection_values()
        self.master = m_candidate

        # Remove slave from list of slaves
        self.remove_slave({'host': m_candidate.host,
                           'port': m_candidate.port,
                           'instance': m_candidate})

        # Make new master forget was an slave using slave methods
        s_candidate = self._change_role(m_candidate)
        s_candidate.reset_all()

        # Switch all slaves to new master
        self._report("# Switching slaves to new master.")
        new_master_info = m_candidate.get_status()[0]
        master_values = {
            'Master_Host': m_candidate.host,
            'Master_Port': m_candidate.port,
            'Master_User': user,
            'Master_Password': passwd,
            'Master_Log_File': new_master_info[0],
            'Read_Master_Log_Pos': new_master_info[1],
        }

        # Use the options SSL certificates if defined,
        # else use the master SSL certificates if defined.
        if self.ssl:
            master_values['Master_SSL_Allowed'] = 1
            if self.ssl_ca:
                master_values['Master_SSL_CA_File'] = self.ssl_ca
            if self.ssl_cert:
                master_values['Master_SSL_Cert'] = self.ssl_cert
            if self.ssl_key:
                master_values['Master_SSL_Key'] = self.ssl_key

        elif m_candidate.has_ssl:
            master_values['Master_SSL_Allowed'] = 1
            master_values['Master_SSL_CA_File'] = m_candidate.ssl_ca
            master_values['Master_SSL_Cert'] = m_candidate.ssl_cert
            master_values['Master_SSL_Key'] = m_candidate.ssl_key

        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                if self.verbose:
                    self._report("# Skipping CHANGE MASTER for {0}:{1} (not "
                                 "connected).".format(slave_dict['host'],
                                                      slave_dict['port']))
                continue
            if self.verbose:
                self._report("# Executing CHANGE MASTER on {0}:{1}"
                             ".".format(slave_dict['host'],
                                        slave_dict['port']))
            change_master = slave.make_change_master(False, master_values)
            if self.verbose:
                self._report("# {0}".format(change_master))
            slave.exec_query(change_master)

        # Start all slaves
        self._report("# Starting all slaves.")
        self.run_cmd_on_slaves("start", not self.verbose)

        # Call exec_after script - display output if verbose on
        try:
            self.run_script(self.after_script, False,
                            [self.master.host, self.master.port])
        except Exception as err:  # pylint: disable=W0703
            self._report("# After script failed! {0}".format(err),
                         level=logging.ERROR)

        # Check all slaves for status, errors
        self._report("# Checking slaves for errors.")
        if not self._check_all_slaves(self.master):
            return False

        self._report("# Switchover complete.")

        return True

    def _change_role(self, server, slave=True):
        """Reverse role of Master and Slave classes

        This method can be used to get a Slave instance from a Master instance
        or a Master instance from a Slave instance.

        server[in]     Server class instance
        slave[in]      if True, create Slave class instance
                       Default is True

        Return Slave or Master instance
        """
        conn_dict = {
            'conn_info': get_connection_dictionary(server),
            'verbose': self.verbose,
        }
        if slave and not isinstance(server, Slave):
            slave_conn = Slave(conn_dict)
            slave_conn.connect()
            return slave_conn
        if not slave and not isinstance(server, Master):
            master_conn = Master(conn_dict)
            master_conn.connect()
            return master_conn
        return server

    def find_best_slave(self, candidates=None, check_master=True,
                        strict=False):
        """Find the best slave

        This method checks each slave in the topology to determine if
        it is a viable slave for promotion. It returns the first slave
        that is determined to be eligible for promotion.

        The method uses the order of the slaves in the topology as
        specified by the slaves list to search for a best slave. If a
        candidate slave is provided, it is checked first.

        candidates[in]   list of candidate connection dictionaries
        check_master[in] if True, check that slave is connected to the master
                         Default is True
        strict[in]       if True, use only the candidate list for slave
                         election and fail if no candidates are viable.
                         Default = False

        Returns dictionary = (host, port, instance) for 'best' slave,
                             None = no candidate slaves found
        """
        msg = "None of the candidates was the best slave."
        for candidate in candidates:
            slave_dict = self.connect_candidate(candidate, False)
            slave = slave_dict['instance']
            # Ignore dead or offline slaves
            if slave is None or not slave.is_alive():
                continue
            slave_ok = self._check_candidate_eligibility(slave.host,
                                                         slave.port,
                                                         slave,
                                                         check_master)
            if slave_ok is not None and slave_ok[0]:
                return slave_dict
            else:
                self._report("# Candidate %s:%s does not meet the "
                             "requirements." % (slave.host, slave.port),
                             logging.WARN)

        # If strict is on and we have found no viable candidates, return None
        if strict:
            self._report("ERROR: %s" % msg, logging.ERROR)
            return None

        if candidates is not None and len(candidates) > 0:
            self._report("WARNING: %s" % msg, logging.WARN)

        for slave_dict in self.slaves:
            s_host = slave_dict['host']
            s_port = slave_dict['port']
            slave = slave_dict['instance']
            # Fix port
            if slave:
                if os.name == "posix" and slave.socket:
                    slave_dict['port'] = slave.port
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                continue
            # Check eligibility
            try:
                slave_ok = self._check_candidate_eligibility(s_host, s_port,
                                                             slave,
                                                             check_master)
                if slave_ok is not None and slave_ok[0]:
                    return slave_dict
            except UtilError, e:
                self._report("# Slave eliminated due to error: %s" % e.errmsg,
                             logging.WARN)
                # Slave gone away, skip it.

        return None

    def failover(self, candidates, strict=False, stop_on_error=False):
        """Perform failover to best slave in a GTID-enabled topology.

        This method performs a failover to one of the candidates specified. If
        no candidates are specified, the method will use the list of slaves to
        choose a candidate. In either case, priority is given to the server
        listed first that meets the prerequisites - a sanity check to ensure if
        the candidate's GTID_MODE matches the other slaves.

        In the event the candidates list is exhausted, it will use the slaves
        list to find a candidate. If no servers are viable, the method aborts.

        If the strict parameter is True, the search is limited to the
        candidates list.

        Once a candidate is selected, the candidate is prepared to become the
        new master by collecting any missing GTIDs by being made a slave to
        each of the other slaves.

        Once prepared, the before script is run to trigger applications,
        then all slaves are connected to the new master. Once complete,
        all slaves are started, the after script is run to trigger
        applications, and the slaves are checked for errors.

        candidates[in]     list of slave connection dictionary of candidate
        strict[in]         if True, use only the candidate list for slave
                           election and fail if no candidates are viable.
                           Default = False
        stop_on_error[in]  Define the default behavior of failover if errors
                           are found. By default: False (not stop on errors).

        Returns bool - True if successful,
                       raises exception if failure and forst is False
        """
        # Get best slave from list of candidates
        new_master_dict = self.find_best_slave(candidates, False, strict)
        if new_master_dict is None:
            msg = "No candidate found for failover."
            self._report(msg, logging.CRITICAL)
            raise UtilRplError(msg)

        new_master = new_master_dict['instance']
        # All servers must have GTIDs match candidate
        gtid_mode = new_master.supports_gtid()
        if gtid_mode != "ON":
            msg = "Failover requires all servers support " + \
                "global transaction ids and have GTID_MODE=ON"
            self._report(msg, logging.CRITICAL)
            raise UtilRplError(msg)

        for slave_dict in self.slaves:
            # Ignore dead or offline slaves
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                continue
            if slave.supports_gtid() != gtid_mode:
                msg = "Cannot perform failover unless all " + \
                      "slaves support GTIDs and GTID_MODE=ON"
                self._report(msg, logging.CRITICAL)
                raise UtilRplError(msg)

        # We must also ensure the new master and all remaining slaves
        # have the latest GTID support.
        new_master.check_gtid_version()
        for slave_dict in self.slaves:
            # Ignore dead or offline slaves
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                continue
            slave.check_gtid_version()

        host = new_master_dict['host']
        port = new_master_dict['port']
        # Use try block in case master class has gone away.
        try:
            old_host = self.master.host
            old_port = self.master.port
        except:
            old_host = "UNKNOWN"
            old_port = "UNKNOWN"

        self._report("# Candidate slave %s:%s will become the new master." %
                     (host, port))

        user, passwd = self._get_rpl_user(self._change_role(new_master))

        # Check slaves for errors that might result in an unstable topology
        self._report("# Checking slaves status (before failover).")
        self._check_slaves_status(stop_on_error)

        # Prepare candidate
        self._report("# Preparing candidate for failover.")
        self._prepare_candidate_for_failover(new_master, user, passwd)

        # Create replication user on candidate.
        self._report("# Creating replication user if it does not exist.")

        # Need Master class instance to check master and replication user
        self.master = self._change_role(new_master, False)
        res = self.master.create_rpl_user(host, port, user, passwd,
                                          ssl=self.ssl)
        if not res[0]:
            print("# ERROR: {0}".format(res[1]))
            self._report(res[1], logging.CRITICAL, False)

        # Call exec_before script - display output if verbose on
        try:
            self.run_script(self.before_script, False,
                            [old_host, old_port, host, port])
        except Exception as err:  # pylint: disable=W0703
            self._report("# Before script failed! {0}".format(err),
                         level=logging.ERROR)

        # Stop all slaves
        self._report("# Stopping slaves.")
        self.run_cmd_on_slaves("stop", not self.verbose)

        # Take the new master out of the slaves list.
        self.remove_slave(new_master_dict)

        self._report("# Switching slaves to new master.")
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            # skip dead or zombie slaves
            if slave is None or not slave.is_alive():
                continue
            slave.switch_master(self.master, user, passwd, False, None, None,
                                self.verbose and not self.quiet)

        # Clean previous replication settings on the new master.
        self._report("# Disconnecting new master as slave.")
        # Make sure the new master is not acting as a slave (STOP SLAVE).
        self.master.exec_query("STOP SLAVE")
        # Execute RESET SLAVE ALL on the new master.
        if self.verbose and not self.quiet:
            self._report("# Execute on {0}:{1}: "
                         "RESET SLAVE ALL".format(self.master.host,
                                                  self.master.port))
        self.master.exec_query("RESET SLAVE ALL")

        # Starting all slaves
        self._report("# Starting slaves.")
        self.run_cmd_on_slaves("start", not self.verbose)

        # Call exec_after script - display output if verbose on
        try:
            self.run_script(self.after_script, False,
                            [old_host, old_port, host, port])
        except Exception as err:  # pylint: disable=W0703
            self._report("# After script failed! {0}".format(err),
                         level=logging.ERROR)

        # Check slaves for errors
        self._report("# Checking slaves for errors.")
        if not self._check_all_slaves(self.master):
            return False

        self._report("# Failover complete.")

        return True

    def get_servers_with_different_sql_mode(self, look_for):
        """Returns a tuple of two list with all the server instances in the
        Topology. The first list is the group of server that have the sql_mode
        given in look_for, the second list is the group of server that does not
        have this sql_mode.

        look_for[in]    The sql_mode to search for.

        Returns tuple of Lists - the group of servers instances that have the
            SQL mode given in look_for, and a group which sql_mode
            differs from the look_for or an empty list.
        """
        # Fill a dict with keys from the SQL modes names and as items the
        # servers with the same sql_mode.
        look_for_list = []
        inconsistent_list = []

        # Get Master sql_mode if given and clasify it.
        if self.master is not None:
            master_sql_mode = self.master.select_variable("SQL_MODE")
            if look_for in master_sql_mode:
                look_for_list.append(self.master)
            else:
                inconsistent_list.append(self.master)

        # Fill the lists with the slaves deppending of his sql_mode.
        for slave_dict in self.slaves:
            slave = slave_dict['instance']
            slave_sql_mode = slave.select_variable("SQL_MODE")
            if look_for in slave_sql_mode:
                look_for_list.append(slave)
            else:
                inconsistent_list.append(slave)

        return look_for_list, inconsistent_list
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains an abstraction of a topolgy map object used to discover
slaves and down-stream replicants for mapping topologies.
"""

import getpass
import os


_START_PORT = 3306


class TopologyMap(object):
    """The TopologyMap class can be used to connect to a running MySQL server
    and discover its slaves. Setting the option "recurse" permits the
    class to discover a replication topology by finding the slaves for each
    slave for the first master requested.

    To generate a topology map, the caller must call the
    generate_topology_map() method to build the topology. This is left as a
    separate state because it can be a lengthy process thereby too long for a
    constructor method.

    The class also includes methods for printing a graph of the topology
    as well as returning a list of master, slave tuples reporting the
    host name and port for each.
    """

    def __init__(self, seed_server, options=None):
        """Constructor

        seed_server[in]    Master (seed) server connection dictionary
        options[in]        options for controlling behavior:
          recurse          If True, check each slave found for add'l slaves
                           Default = False
          prompt_user      If True, prompt user if slave connection fails with
                           master connection parameters
                           Default = False
          quiet            if True, print only the data
                           Default = False
          width            width of report
                           Default = 75
          num_retries      Number of times to retry a failed connection attempt
                           Default = 0
        """
        if options is None:
            options = {}
        self.recurse = options.get("recurse", False)
        self.quiet = options.get("quiet", False)
        self.prompt_user = options.get("prompt", False)
        self.num_retries = options.get("num_retries", 0)
        self.socket_path = options.get("socket_path", None)
        self.verbose = options.get('verbosity', 0) > 0
        self.seed_server = seed_server
        self.topology = []
        self.options = options

    def _connect(self, conn):
        """Find the attached slaves for a list of server connections.

        This method connects to each server in the list and retrieves its
        slaves.
        It can be called recursively if the recurse parameter is True.

        conn[in]           Connection dictionary used to connect to server

        Returns tuple - master Server class instance, master:host string
        """
        conn_options = {
            'quiet': self.quiet,
            'src_name': "master",
            'dest_name': None,
            'version': "5.0.0",
            'unique': True,
            'verbose': self.verbose,
        }

        certs_paths = {}
        if 'ssl_ca' in dir(conn) and conn.ssl_ca is not None:
            certs_paths['ssl_ca'] = conn.ssl_ca
        if 'ssl_cert' in dir(conn) and conn.ssl_cert is not None:
            certs_paths['ssl_cert'] = conn.ssl_cert
        if 'ssl_key' in dir(conn) and conn.ssl_key is not None:
            certs_paths['ssl_key'] = conn.ssl_key

        conn_options.update(certs_paths)

        master_info = "{0}:{1}".format(conn['host'], conn['port'])
        master = None

        # Increment num_retries if not set when --prompt is used
        if self.prompt_user and self.num_retries == 0:
            self.num_retries += 1

        # Attempt to connect to the server given the retry limit
        for i in range(0, self.num_retries + 1):
            try:
                servers = connect_servers(conn, None, conn_options)
                master = servers[0]
                break
            except UtilError, e:
                print "FAILED.\n"
                if i < self.num_retries and self.prompt_user:
                    print "Connection to %s has failed.\n" % master_info + \
                        "Please enter the following information " + \
                        "to connect to this server."
                    conn['user'] = raw_input("User name: ")
                    conn['passwd'] = getpass.getpass("Password: ")
                else:
                    # retries expired - re-raise error if still failing
                    raise UtilError(e.errmsg)

        # Correct port for socket connections
        if os.name == 'posix' and master.socket:
            master_info = "{0}:{1}".format(conn['host'], master.port)

        return (master, master_info)

    @staticmethod
    def _check_permissions(server, priv):
        """Check to see if user has permissions to execute.

        server[in]     Server class instance
        priv[in]       privilege to check

        Returns True if permissions available, raises exception if not
        """
        # Check user permissions
        user_pass_host = server.user
        if server.passwd is not None and len(server.passwd) > 0:
            user_pass_host += ":" + server.passwd
        user_pass_host += "@" + server.host
        user = User(server, user_pass_host, False)
        if not user.has_privilege("*", "*", priv):
            raise UtilError("Not enough permissions. The user must have the "
                            "%s privilege." % priv)

    def _get_slaves(self, max_depth, seed_conn=None, masters_found=None):
        """Find the attached slaves for a list of server connections.

        This method connects to each server in the list and retrieves its
        slaves. It can be called recursively if the recurse option is True.

        max_depth[in]       Maximum depth of recursive search
        seed_conn[in]       Current master connection dictionary. Initially,
                            this is the seed server (original master defined
                            in constructor)
        masters_found[in]   a list of all servers in master roles - used to
                            detect a circular replication topology. Initially,
                            this is an empty list as the master detection must
                            occur as the topology is traversed.

        Returns list - list of slaves connected to each server in list
        """
        if not masters_found:
            masters_found = []
        topology = []
        if seed_conn is None:
            seed_conn = self.seed_server

        master, master_info = self._connect(seed_conn)
        if master is None:
            return []

        # Check user permissions
        self._check_permissions(master, "REPLICATION SLAVE")

        # Save the master for circular replication identification
        masters_found.append(master_info)

        if not self.quiet:
            print "# Finding slaves for master: %s" % master_info

        # See if the user wants us to discover slaves.
        discover = self.options.get("discover", None)
        if discover is None:
            return

        # Get user and password (supports login-paths)
        try:
            user, password = parse_user_password(discover,
                                                 options=self.options)
        except FormatError:
            raise UtilError(USER_PASSWORD_FORMAT.format("--discover-slaves"))

        # Get replication topology
        slaves = master.get_slaves(user, password)
        slave_list = []
        depth = 0
        if len(slaves) > 0:
            for slave in slaves:
                if slave.find(":") > 0:
                    host, port = slave.split(":", 1)
                else:
                    host = slave
                    port = _START_PORT  # Use the default
                slave_conn = self.seed_server.copy()
                slave_conn['host'] = host
                slave_conn['port'] = port

                io_sql_running = None
                # If verbose then get slave threads (IO and SQL) status
                if self.verbose:
                    # Create slave instance
                    conn_dict = {
                        'conn_info': {'user': user, 'passwd': password,
                                      'host': host, 'port': port,
                                      'socket': None},
                        'role': slave,
                        'verbose': self.verbose
                    }
                    slave_obj = Slave(conn_dict)
                    # Get IO and SQL status
                    try:
                        slave_obj.connect()
                        thread_status = slave_obj.get_thread_status()
                        if thread_status:
                            io_sql_running = (thread_status[1],
                                              thread_status[2])
                    except UtilError:
                        # Connection error
                        io_sql_running = ('ERROR', 'ERROR')

                # Now check for circular replication topology - do not recurse
                # if slave is also a master.
                if self.recurse and slave not in masters_found and \
                   ((max_depth is None) or (depth < max_depth)):
                    new_list = self._get_slaves(max_depth, slave_conn,
                                                masters_found)
                    if new_list == []:
                        slave_list.append((slave, [], io_sql_running))
                    else:
                        # Add IO and SQL state to slave from recursion
                        if io_sql_running:
                            new_list = [(new_list[0][0], new_list[0][1],
                                         io_sql_running)]
                        slave_list.append(new_list)
                    depth += 1
                else:
                    slave_list.append((slave, [], io_sql_running))
        topology.append((master_info, slave_list))

        return topology

    def generate_topology_map(self, max_depth):
        """Find the attached slaves for a list of server connections.

        This method generates the topology for the seed server specified at
        instantiation.

        max_depth[in]       Maximum depth of recursive search
        """
        self.topology = self._get_slaves(max_depth)

    def depth(self):
        """Return depth of the topology tree.

        Returns int - depth of topology tree.
        """
        return len(self.topology)

    def slaves_found(self):
        """Check to see if any slaves were found.

        Returns bool - True if slaves found, False if no slaves.
        """
        return not (len(self.topology) and self.topology[0][1] == [])

    def print_graph(self, topology_list=None, masters_found=None,
                    level=0, preamble=""):
        """Prints a graph of the topology map to standard output.

        This method traverses a list of the topology and prints a graph. The
        method is designed to be recursive traversing the list to print the
        slaves for each master in the topology. It will also detect a circular
        replication segment and indicate it on the graph.

        topology_list[in]   a list in the form (master, slave) of server
        masters_found[in]   a list of all servers in master roles - used to
                            detect a circular replication topology. Initially,
                            this is an empty list as the master detection must
                            occur as the topology is traversed.
        level[in]           the level of indentation - increases with each
                            set of slaves found in topology
        preamble[in]        prefix calculated during recursion to indent text
        """
        if not topology_list:
            topology_list = []
        if not masters_found:
            masters_found = []
        # if first iteration, use the topology list generated earlier
        if topology_list == []:
            if self.topology == []:
                # topology not generated yet
                raise UtilError("You must first generate the topology.")
            topology_list = self.topology

        # Detect if we are looking at a sublist or not. Get sublist.
        if len(topology_list) == 1:
            topology_list = topology_list[0]
        master = topology_list[0]

        # Save the master for circular replication identification
        masters_found.append(master)

        # For each slave, print the graph link
        slaves = topology_list[1]
        stop = len(slaves)
        if stop > 0:
            # Level 0 is always the first master in the topology.
            if level == 0:
                print("{0} (MASTER)".format(master))
            for i in range(0, stop):
                if len(slaves[i]) == 1:
                    slave = slaves[i][0]
                else:
                    slave = slaves[i]
                new_preamble = "{0}   ".format(preamble)
                print("{0}|".format(new_preamble))
                role = "(SLAVE"
                if slave[1] != [] or slave[0] in masters_found:
                    role = "{0} + MASTER".format(role)
                role = "{0})".format(role)

                # Print threads (IO and SQL) status if verbose
                t_status = ''
                if self.verbose:
                    try:
                        t_status = " [IO: {0}, SQL: {1}]".format(slave[2][0],
                                                                 slave[2][1])
                    except IndexError:
                        # This should never happened... (done to avoid crash)
                        t_status = " [IO: ??, SQL: ??]"

                print "{0}+--- {1}{2}".format(new_preamble, slave[0],
                                              t_status),

                if (slave[0] in masters_found):
                    print "<-->",
                else:
                    print "-",
                print role

                if slave[1] != []:
                    if i < stop - 1:
                        new_preamble = "{0}|".format(new_preamble)
                    else:
                        new_preamble = "{0} ".format(new_preamble)
                    self.print_graph(slave, masters_found,
                                     level + 1, new_preamble)

    def _get_row(self, topology_list):
        """Get a row (master, slave) for the topology map.

        topology_list[in]  The topology list

        Returns tuple - a row (master, slave)
        """
        new_row = []
        if len(topology_list) == 1:
            topology_list = topology_list[0]
        master = topology_list[0]
        slaves = topology_list[1]
        for slave in slaves:
            if len(slave) == 1:
                new_slave = slave[0]
            else:
                new_slave = slave
            new_row.append((master, new_slave[0]))
            new_row.extend(self._get_row(new_slave))
        return new_row

    def get_topology_map(self):
        """Get a list of the topology map suitable for export

        Returns list - a list of masters and their slaves in two columns
        """
        # Get a row for the list
        # make a list from the topology
        master_slaves = [self._get_row(row) for row in self.topology]
        return master_slaves[0]
#
# Copyright (c) 2010, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains and abstraction of a MySQL user object.
"""

import re

from collections import namedtuple, defaultdict


def change_user_privileges(server, user_name, user_passwd, host,
                           grant_list=None, revoke_list=None,
                           disable_binlog=False, create_user=False):
    """ Change the privileges of a new or existing user.

    This method GRANT or REVOKE privileges to a new user (creating it) or
    existing user.

    server[in]          MySQL server instances to apply changes
                        (from mysql.utilities.common.server.Server).
    user_name[in]       user name to apply changes.
    user_passwd[in]     user's password.
    host[in]            host name associated to the user account.
    grant_list[in]      List of privileges to GRANT.
    revoke_list[in]     List of privileges to REVOKE.
    disable_binlog[in]  Boolean value to determine if the binary logging
                        will be disabled to perform this operation (and
                        re-enabled at the end). By default: False (do not
                        disable binary logging).
    create_user[in]     Boolean value to determine if the user will be
                        created before changing its privileges. By default:
                        False (do no create user).
    """
    if disable_binlog:
        server.exec_query("SET SQL_LOG_BIN=0")
    if create_user:
        server.exec_query("CREATE USER '{0}'@'{1}' IDENTIFIED BY "
                          "'{2}'".format(user_name, host, user_passwd))
    if grant_list:
        grants_str = ", ".join(grant_list)
        server.exec_query("GRANT {0} ON *.* TO '{1}'@'{2}' IDENTIFIED BY "
                          "'{3}'".format(grants_str, user_name, host,
                                         user_passwd))
    if revoke_list:
        revoke_str = ", ".join(revoke_list)
        server.exec_query("REVOKE {0} ON *.* FROM '{1}'@'{2}'"
                          "".format(revoke_str, user_name, host))
    if disable_binlog:
        server.exec_query("SET SQL_LOG_BIN=1")


def parse_user_host(user_name):
    """Parse user, passwd, host, port from user:passwd@host

    user_name[in]      MySQL user string (user:passwd@host)

    returns - tuple - user, passwd, host
    """
    # Check for anonymous user. If not, continue.
    if user_name == "''@'%'":
        return ('', None, '%')
    no_ticks = user_name.replace("'", "")
    try:
        conn_values = parse_connection(no_ticks)
    except FormatError:
        raise UtilError("Cannot parse user:pass@host : %s." %
                        no_ticks)
    return (conn_values['user'], conn_values['passwd'], conn_values['host'])


def grant_proxy_ssl_privileges(server, user, passw, at='localhost',
                               privs="ALL PRIVILEGES", grant_opt=True,
                               ssl=True, grant_proxy=True, proxy_user='root',
                               proxy_host='localhost'):
    """Grant privileges to an user in a server with GRANT OPTION or/and
    REQUIRE SSL if required.

    server[in]         Server to execute the grant query at.
    user_name[in]      New user name.
    passw[in]          password of the new user.
    at[in]             Used in GRANT "TO '{0}'@'{1}'".format(user, at),
                       (default localhost)
    grant_opt[in]      if True, it will grant with GRANT OPTION (default True).
    ssl[in]            if True, it will set REQUIRE SSL (default True).
    grant_proxy[in]    if True, it will grant GRANT PROXY (default True).
    proxy_user[in]     username for the proxied account (default: root)
    proxy_host[in]     hostname for the proxied account (default: localhost)

    Note: Raises UtilError on any Error.
    """

    grant_parts = [
        "GRANT", privs,
        "ON *.*",
        "TO '{0}'@'{1}'".format(user, at),
        "IDENTIFIED BY '{0}'".format(passw) if passw else "",
        "REQUIRE SSL" if ssl else "",
        "WITH GRANT OPTION" if grant_opt else ""
    ]

    try:
        server.exec_query(" ".join(grant_parts))
    except UtilDBError as err:
        raise UtilError("Cannot create new user {0} at {1}:{2} reason:"
                        "{3}".format(user, server.host, server.port,
                                     err.errmsg))

    if grant_proxy:
        grant = ("GRANT PROXY ON '{0}'@'{1}' "
                 "TO '{2}'@'{3}' "
                 "WITH GRANT OPTION").format(proxy_user, proxy_host, user, at)
        try:
            server.exec_query(grant)
        except UtilDBError as err:
            raise UtilError("Cannot grant proxy to user {0} at {1}:{2} "
                            "reason:{3}".format(user, server.host,
                                                server.port, err.errmsg))


def check_privileges(server, operation, privileges, description,
                     verbosity=0, reporter=None):
    """Check required privileges.

    This method check if the used user possess the required privileges to
    execute a statement or operation.
    An exception is thrown if the user doesn't have enough privileges.

    server[in]        Server instance to check.
    operation[in]     The name of tha task that requires the privileges,
                      used in the error message if an exception is thrown.
    privileges[in]    List of the required privileges.
    description[in]   Description of the operation requiring the User's
                      privileges, used in the message if verbosity if given.
    verbosity[in]     Verbosity.
    reporter[in]      A method to invoke with messages and warnings
                      (by default print).
    """
    # print message with the given reporter.
    if reporter is None and verbosity > 0:
        print("# Checking user permission to {0}...\n"
              "#".format(description))
    elif reporter is not None and verbosity > 0:
        reporter("# Checking user permission to {0}...\n"
                 "#".format(description))

    # Check privileges
    user_obj = User(server, "{0}@{1}".format(server.user, server.host))
    need_privileges = []
    for privilege in privileges:
        if not user_obj.has_privilege('*', '*', privilege):
            need_privileges.append(privilege)

    if len(need_privileges) > 0:
        if len(need_privileges) > 1:
            privileges_needed = "{0} and {1}".format(
                ", ".join(need_privileges[:-1]),
                need_privileges[-1]
            )
        else:
            privileges_needed = need_privileges[0]
        raise UtilError(ERROR_USER_WITHOUT_PRIVILEGES.format(
            user=server.user, host=server.host, port=server.port,
            operation=operation, req_privileges=privileges_needed
        ))


class User(object):
    """
    The User class can be used to clone the user and its grants to another
    user with the following utilities:

        - Parsing user@host:passwd strings
        - Create, Drop user
        - Check to see if user exists
        - Retrieving and printing grants for user
    """

    def __init__(self, server1, user, verbosity=0):
        """Constructor

        server1[in]        Server class
        user[in]           MySQL user credentials string (user@host:passwd)
        verbose[in]        print extra data during operations (optional)
                           default value = False
        """

        self.server1 = server1
        if server1.db_conn:
            self.sql_mode = self.server1.select_variable("SQL_MODE")
        else:
            self.sql_mode = ""
        self.user, self.passwd, self.host = parse_user_host(user)
        self.verbosity = verbosity
        self.current_user = None
        self.grant_dict = None
        self.global_grant_dict = None
        self.grant_list = None
        self.global_grant_list = None
        self.query_options = {
            'fetch': False
        }

    def create(self, new_user=None, authentication=None):
        """Create the user

        Attempts to create the user. If the operation fails, an error is
        generated and printed.

        new_user[in]       MySQL user string (user@host:passwd)
                           (optional) If omitted, operation is performed
                           on the class instance user name.
        authentication[in] Special authentication clause for non-native
                           authentication plugins
        """
        auth_str = "SELECT * FROM INFORMATION_SCHEMA.PLUGINS WHERE " \
                   "PLUGIN_NAME = '{0}' AND PLUGIN_STATUS = 'ACTIVE';"
        query_str = "CREATE USER "
        user, passwd, host = None, None, None
        if new_user:
            user, passwd, host = parse_user_host(new_user)
            user_host_str = "'{0}'@'{1}' ".format(user, host)
        else:
            user_host_str = "'{0}'@'{1}' ".format(self.user, self.host)
            passwd = self.passwd
        query_str += user_host_str

        if passwd and authentication:
            print("WARNING: using a password and an authentication plugin is "
                  "not permited. The password will be used instead of the "
                  "authentication plugin.")
        if passwd:
            query_str += "IDENTIFIED BY '{0}'".format(passwd)
        elif authentication:
            # need to validate authentication plugin
            res = self.server1.exec_query(auth_str.format(authentication))
            if (res is None) or (res == []):
                raise UtilDBError("Plugin {0} not loaded or not active. "
                                  "Cannot create user.".format(authentication))
            query_str += "IDENTIFIED WITH '{0}'".format(authentication)
        if self.verbosity > 0:
            print query_str

        self.server1.exec_query(query_str, self.query_options)

    def drop(self, new_user=None):
        """Drop user from the server

        Attempts to drop the user. If the operation fails, an error is
        generated and printed.

        new_user[in]       MySQL user string (user@host:passwd)
                           (optional) If omitted, operation is performed
                           on the class instance user name.
        """
        query_str = "DROP USER "
        if new_user:
            user, _, host = parse_user_host(new_user)
            query_str += "'%s'@'%s' " % (user, host)
        else:
            query_str += "'%s'@'%s' " % (self.user, self.host)

        if self.verbosity > 0:
            print query_str

        try:
            self.server1.exec_query(query_str, self.query_options)
        except UtilError:
            return False
        return True

    def exists(self, user_name=None):
        """Check to see if the user exists

        user_name[in]      MySQL user string (user@host:passwd)
                           (optional) If omitted, operation is performed
                           on the class instance user name.

        return True = user exists, False = user does not exist
        """

        user, host, _ = self.user, self.host, self.passwd
        if user_name:
            user, _, host = parse_user_host(user_name)

        res = self.server1.exec_query("SELECT * FROM mysql.user "
                                      "WHERE user = %s and host = %s",
                                      {'params': (user, host)})

        return (res is not None and len(res) >= 1)

    @staticmethod
    def _get_grants_as_dict(grant_list, verbosity=0, sql_mode=''):
        """Transforms list of grant string statements into a dictionary.

        grant_list[in]    List of grant strings as returned from the server

        Returns a default_dict with the grant information
        """
        grant_dict = defaultdict(lambda: defaultdict(set))
        for grant in grant_list:
            grant_tpl = User._parse_grant_statement(grant[0], sql_mode)
            # Ignore PROXY privilege, it is not yet supported
            if verbosity > 0:
                if 'PROXY' in grant_tpl:
                    print("#WARNING: PROXY privilege will be ignored.")
            grant_tpl.privileges.discard('PROXY')
            if grant_tpl.privileges:
                grant_dict[grant_tpl.db][grant_tpl.object].update(
                    grant_tpl.privileges)
        return grant_dict

    def get_grants(self, globals_privs=False, as_dict=False, refresh=False):
        """Retrieve the grants for the current user

        globals_privs[in]     Include global privileges in clone (i.e. user@%)
        as_dict[in]           If True, instead of a list of plain grant
                              strings, return a dictionary with the grants.
        refresh[in]           If True, reads grant privileges directly from the
                              server and updates cached values, otherwise uses
                              the cached values.

        returns result set or None if no grants defined
        """

        # only read values from server if needed
        if refresh or not self.grant_list or not self.global_grant_list:
            # Get the users' connection user@host if not retrieved
            if self.current_user is None:
                res = self.server1.exec_query("SELECT CURRENT_USER()")
                parts = res[0][0].split('@')
                # If we're connected as some other user, use the user@host
                # defined at instantiation
                if parts[0] != self.user:
                    host = clean_IPv6(self.host)
                    self.current_user = "'%s'@'%s'" % (self.user, host)
                else:
                    self.current_user = "'%s'@'%s'" % (parts[0], parts[1])
            grants = []
            try:
                res = self.server1.exec_query("SHOW GRANTS FOR "
                                              "{0}".format(self.current_user))
                for grant in res:
                    grants.append(grant)
            except UtilDBError:
                pass  # Error here is ok - no grants found.

            # Cache user grants
            self.grant_list = grants[:]
            self.grant_dict = User._get_grants_as_dict(self.grant_list,
                                                       self.verbosity,
                                                       self.sql_mode)
            # If current user is already using global host wildcard '%', there
            # is no need to run the show grants again.
            if globals_privs:
                if self.host != '%':
                    try:
                        res = self.server1.exec_query(
                            "SHOW GRANTS FOR '{0}'{1}".format(self.user,
                                                              "@'%'"))
                        for grant in res:
                            grants.append(grant)
                        self.global_grant_list = grants[:]
                        self.global_grant_dict = User._get_grants_as_dict(
                            self.global_grant_list, self.verbosity)
                    except UtilDBError:
                        # User has no global privs, return the just the ones
                        # for current host
                        self.global_grant_list = self.grant_list
                        self.global_grant_dict = self.grant_dict
                else:
                    # if host is % then we already have the global privs
                    self.global_grant_list = self.grant_list
                    self.global_grant_dict = self.grant_dict

        if globals_privs:
            if as_dict:
                return self.global_grant_dict
            else:
                return self.global_grant_list
        else:
            if as_dict:
                return self.grant_dict
            else:
                return self.grant_list

    def get_grants_for_object(self, qualified_obj_name, obj_type_str,
                              global_privs=False):
        """ Retrieves the list of grants that the current user has that that
         have effect over a given object.

        qualified_obj_name[in]   String with the qualified name of the object.
        obj_type_str[in]         String with the type of the object that we are
                                 working with, must be one of 'ROUTINE',
                                 'TABLE' or 'DATABASE'.
        global_privs[in]         If True, the wildcard'%' host privileges are
                                 also taken into account


        This method takes the MySQL privilege hierarchy into account, e.g,
        if the qualified object is a table, it returns all the grant
        statements for this user regarding that table, as well as the grant
        statements for this user regarding the db where the table is at and
        finally any global grants that the user might have.

        Returns a list of strings with the grant statements.
        """

        grant_stm_lst = self.get_grants(global_privs)
        m_objs = parse_object_name(qualified_obj_name, self.sql_mode)
        grants = []
        if not m_objs:
            raise UtilError("Cannot parse the specified qualified name "
                            "'{0}'".format(qualified_obj_name))
        else:
            db_name, obj_name = m_objs
            # Quote database and object name if necessary
            if not is_quoted_with_backticks(db_name, self.sql_mode):
                db_name = quote_with_backticks(db_name, self.sql_mode)
            if obj_name and obj_name != '*':
                if not is_quoted_with_backticks(obj_name, self.sql_mode):
                    obj_name = quote_with_backticks(obj_name, self.sql_mode)

            # For each grant statement look for the ones that apply to this
            # user and object
            for grant_stm in grant_stm_lst:
                grant_tpl = self._parse_grant_statement(grant_stm[0],
                                                        self.sql_mode)
                if grant_tpl:
                    # Check if any of the privileges applies to this object
                    # and if it does then check if it inherited from this
                    # statement
                    if filter_grants(grant_tpl.privileges, obj_type_str):
                        # Add global grants
                        if grant_tpl.db == '*':
                            grants.append(grant_stm[0])
                            continue
                        # Add database level grants
                        if grant_tpl.db == db_name and grant_tpl.object == '*':
                            grants.append(grant_stm[0])
                            continue
                        # If it is an object, add existing object level grants
                        # as well.
                        if obj_name:
                            if (grant_tpl.db == db_name and
                                    grant_tpl.object == obj_name):
                                grants.append(grant_stm[0])

        return grants

    def has_privilege(self, db, obj, access, allow_skip_grant_tables=True,
                      globals_privs=True):
        """Check to see user has a specific access to a db.object.

        db[in]             Name of database
        obj[in]            Name of object
        access[in]         MySQL privilege to check (e.g. SELECT, SUPER, DROP)
        allow_skip_grant_tables[in]  If True, allow silent failure for
                           cases where the server is started with
                           --skip-grant-tables. Default=True
        globals_privs[in]  Include global privileges in clone (i.e. user@%)
                           Default is True

        Returns True if user has access, False if not
        """
        grants_enabled = self.server1.grant_tables_enabled()
        # If grants are disabled and it is Ok to allow skipped grant tables,
        # return True - privileges disabled so user can do anything.
        if allow_skip_grant_tables and not grants_enabled:
            return True
        # Convert privilege to upper cases.
        access = access.upper()

        # Get grant dictionary
        grant_dict = self.get_grants(globals_privs=globals_privs, as_dict=True)

        # If self has all privileges for all databases, no need to check,
        # simply return True
        if ("ALL PRIVILEGES" in grant_dict['*']['*'] and
                "GRANT OPTION" in grant_dict['*']['*']):
            return True

        # Quote db and obj with backticks if necessary
        if not is_quoted_with_backticks(db, self.sql_mode) and db != '*':
            db = quote_with_backticks(db, self.sql_mode)

        if not is_quoted_with_backticks(obj, self.sql_mode) and obj != '*':
            obj = quote_with_backticks(obj, self.sql_mode)

        # USAGE privilege is the same as no privileges,
        # so everyone has it.
        if access == "USAGE":
            return True
        # Even if we have ALL PRIVILEGES grant, we might not have WITH GRANT
        # OPTION privilege.
        # Check server wide grants.
        elif (access in grant_dict['*']['*'] or
              "ALL PRIVILEGES" in grant_dict['*']['*'] and
              access != "GRANT OPTION"):
            return True
        # Check database level grants.
        elif (access in grant_dict[db]['*'] or
              "ALL PRIVILEGES" in grant_dict[db]['*'] and
              access != "GRANT OPTION"):
            return True
        # Check object level grants.
        elif (access in grant_dict[db][obj] or
              "ALL PRIVILEGES" in grant_dict[db][obj] and
              access != "GRANT OPTION"):
            return True
        else:
            return False

    def contains_user_privileges(self, user, plus_grant_option=False):
        """Checks if privileges of given user are a subset of self's privileges

        user[in]               instance of the user class
        plus_grant_option[in]  if True, checks if besides the all the other
                               privileges, self has also the GRANT OPTION
                               in all of the bd, tables in which the user
                               passed as argument has privileges. Required for
                               instance if we will be using self to clone the
                               user.
        return_missing[in]     if True, return a set with the missing grants
                               instead of simply a boolean value.

        Returns True if the grants of the user passed as argument
        are a subset of the grants of self, otherwise returns False.
        """
        user_grants = user.get_grants(as_dict=True)

        # If we are cloning User1, using User2, then User2 needs
        # the GRANT OPTION privilege in each of the db,table where
        # User1 has privileges.
        if plus_grant_option:
            for db in user_grants:
                for table in user_grants[db]:
                    priv_set = user_grants[db][table]
                    # Ignore empty grant sets that might exist as a
                    # consequence of consulting the defaultdict.
                    if priv_set:
                        # Ignore USAGE grant as it means no privileges.
                        if (len(priv_set) == 1 and
                                "USAGE" in priv_set):
                            continue
                        else:
                            priv_set.add('GRANT OPTION')

        for db in user_grants:
            for table in user_grants[db]:
                priv_set = user_grants[db][table]
                for priv in priv_set:
                    if self.has_privilege(db, table, priv):
                        continue
                    else:
                        return False
        return True

    def missing_user_privileges(self, user, plus_grant_option=False):
        """Checks if privileges of given user are a subset of self's privileges

        user[in]               instance of the user class
        plus_grant_option[in]  if True, checks if besides the all the other
                               privileges, self has also the GRANT OPTION
                               in all of the bd, tables in which the user
                               passed as argument has privileges. Required for
                               instance if we will be using self to clone the
                               user.
        return_missing[in]     if True, return a set with the missing grants
                               instead of simply a boolean value.

        Returns empty set if the grants of the user passed as argument
        are a subset of the grants of self, otherwise a set with the missing
        privileges from self.
        """
        user_grants = user.get_grants(as_dict=True)
        missing_grants = set()

        # If we are cloning User1, using User2, then User2 needs
        # the GRANT OPTION privilege in each of the db,table where
        # User1 has privileges.
        if plus_grant_option:
            for db in user_grants:
                for table in user_grants[db]:
                    priv_set = user_grants[db][table]
                    # Ignore empty grant sets that might exist as a
                    # consequence of consulting the defaultdict.
                    if priv_set:
                        # Ignore USAGE grant as it means no privileges.
                        if (len(priv_set) == 1 and
                                "USAGE" in priv_set):
                            continue
                        else:
                            priv_set.add('GRANT OPTION')

        for db in user_grants:
            for table in user_grants[db]:
                priv_set = user_grants[db][table]
                for priv in priv_set:
                    if self.has_privilege(db, table, priv):
                        continue
                    else:
                        missing_grants.add((priv, db, table))

        return missing_grants

    def print_grants(self):
        """Display grants for the current user"""

        res = self.get_grants(True)
        for grant_tuple in res:
            print grant_tuple[0]

    def _get_authentication(self):
        """ Return authentication string """
        res = self.server1.exec_query("SELECT plugin FROM mysql.user "
                                      "WHERE user='{0}' and host='{1}'"
                                      "".format(self.user, self.host))
        if res == [] or res[0][0] == 'mysql_native_password':
            return None
        return res[0][0]

    def clone(self, new_user, destination=None, globals_privs=False):
        """Clone the current user to the new user

        Operation will create the new user account copying all of the
        grants for the current user to the new user. If operation fails,
        an error message is generated and the process halts.

        new_name[in]       MySQL user string (user@host:passwd)
        destination[in]    A connection to a new server to clone the user
                           (default is None)
        globals_privs[in]  Include global privileges in clone (i.e. user@%)

        Note: Caller must ensure the new user account does not exist.
        """

        res = self.get_grants(globals_privs)
        server = self.server1
        if destination is not None:
            server = destination
        for row in res:
            # Create an instance of the user class.
            user = User(server, new_user, self.verbosity)
            if not user.exists():
                # Get authentication plugin if different from native plugin
                auth = self._get_authentication()
                # Add authentication if available
                user.create(authentication=auth)

            if globals_privs and '%' in row[0]:
                base_user_ticks = "'" + self.user + "'@'" + '%' + "'"
            else:
                base_user_ticks = "'" + self.user + "'@'" + self.host + "'"
            user, _, host = parse_user_host(new_user)
            new_user_ticks = "'" + user + "'@'" + host + "'"
            grant = row[0].replace(base_user_ticks, new_user_ticks, 1)

            # Need to remove the IDENTIFIED BY clause for the base user.
            search_str = "IDENTIFIED BY PASSWORD"
            try:
                start = grant.index(search_str)
            except:
                start = 0

            if start > 0:
                end = grant.index("'", start + len(search_str) + 2) + 2
                grant = grant[0:start] + grant[end:]

            if self.verbosity > 0:
                print grant

            res = server.exec_query(grant, self.query_options)

    @staticmethod
    def _parse_grant_statement(statement, sql_mode=''):
        """ Returns a namedtuple with the parsed GRANT information.

        statement[in] Grant string in the sql format returned by the server.

        Returns named tuple with GRANT information or None.
        """

        grant_parse_re = re.compile(r"""
            GRANT\s(.+)?\sON\s # grant or list of grants
            (?:(?:PROCEDURE\s)|(?:FUNCTION\s))? # optional for routines only
            (?:(?:(\*|`?[^']+`?)\.(\*|`?[^']+`?)) # object where grant applies
            | ('[^']*'@'[^']*')) # For proxy grants user/host
            \sTO\s([^@]+@[\S]+) # grantee
            (?:\sIDENTIFIED\sBY\sPASSWORD
             (?:(?:\s<secret>)|(?:\s\'[^\']+\')?))? # optional pwd
            (?:\sREQUIRE\sSSL)? # optional SSL
            (\sWITH\sGRANT\sOPTION)? # optional grant option
            $ # End of grant statement
            """, re.VERBOSE)

        grant_tpl_factory = namedtuple("grant_info", "privileges proxy_user "
                                                     "db object user")
        match = re.match(grant_parse_re, statement)

        if match:
            # quote database name and object name with backticks
            if match.group(1).upper() != 'PROXY':
                db = match.group(2)
                if not is_quoted_with_backticks(db, sql_mode) and db != '*':
                    db = quote_with_backticks(db, sql_mode)
                obj = match.group(3)
                if not is_quoted_with_backticks(obj, sql_mode) and obj != '*':
                    obj = quote_with_backticks(obj, sql_mode)
            else:  # if it is not a proxy grant
                db = obj = None
            grants = grant_tpl_factory(
                # privileges
                set([priv.strip() for priv in match.group(1).split(",")]),
                match.group(4),  # proxied user
                db,  # database
                obj,  # object
                match.group(5),  # user
            )
            # If user has grant option, add it to the list of privileges
            if match.group(6) is not None:
                grants.privileges.add("GRANT OPTION")
        else:
            raise UtilError("Unable to parse grant statement "
                            "{0}".format(statement))

        return grants
#
# Copyright (c) 2011, 2016, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains classes and functions used to determine what MySQL
utilities are installed, their options, and usage. This module can be
used to allow a client to provide auto type and option completion.
"""

import glob
import os
import sys
import re
import subprocess


_MAX_WIDTH = 78

# These utilities should not be used with the console
_EXCLUDE_UTILS = ['mysqluc', ]

RE_USAGE = (
    r"(?P<Version>.*?)"
    r"(?P<Usage>Usage:\s.*?)\w+\s\-\s"  # This match first
    # section <Usage> matching all till find a " - "
    r"(?P<Description>.*?)"  # Description is the text next
    # to " - " and till next match.
    r"(?P<O>\w*):"  # This is beginning of Options section
    r"(?P<Options>.*(?=^Introduction.\-{12})|.*$)"
    # match Options till end or till find Introduction -.
    r"(?:^Introduction.\-{12}){0,1}"  # not catching group
    r"(?P<Introduction>.*(?=^Helpful\sHints.\-{13})|.*$)"
    # captures Introduction (optional)
    # it will match Introduction till end or till Hints -
    r"(?:^Helpful\sHints.\-{13}){0,1}"  # Not catching group
    r"(?P<Helpful_Hints>.*)"
    # captures Helpful Hints (optional)
)

RE_OPTIONS = (
    r"^(?P<Alias>\s\s\-.*?)\s{2,}"  # Option Alias
    # followed by 2 o more spaces is his description
    r"(?P<Desc>.*?)(?=^\s\s\-)"  # description is all
    # text till not found other alias in the form
    # <-|--Alias> at the begin of the line.
)

RE_OPTION = r"\s+\-\-(.*?)\s"  # match Alias of the form <--Alias>

RE_ALIAS = r"\s+\-(\w+)\s*"  # match Alias of the form <-Alias>

WARNING_FAIL_TO_READ_OPTIONS = ("WARNING: {0} failed to read options."
                                " This utility will not be shown in 'help "
                                "utilities' and cannot be accessed from the "
                                "console.")


def get_util_path(default_path=''):
    """Find the path to the MySQL utilities

    This method will attempt to

    default_path[in]   provides known location of utilities
                       if provided, method will search this location first
                       before searching PYTHONPATH

    Returns string - path to utilities or None if not found
    """
    def _search_paths(needles, paths):
        """Search and return normalized path
        """
        for path in paths:
            norm_path = os.path.normpath(path)
            hay_stack = [os.path.join(norm_path, n) for n in needles]
            for needle in hay_stack:
                if os.path.isfile(needle):
                    return norm_path

        return None

    needle_name = 'mysqlreplicate'
    needles = [needle_name + ".py"]
    if os.name == "nt":
        needles.append(needle_name + ".exe")
    else:
        needles.append(needle_name)

    # Try the default by itself
    path_found = _search_paths(needles, [default_path])
    if path_found:
        return path_found

    # Try the pythonpath environment variable
    pythonpath = os.getenv("PYTHONPATH")
    if pythonpath:
        # This is needed on windows without a python setup, cause needs to
        # find the executable scripts.
        path = _search_paths(needles, [os.path.join(n, "../")
                                       for n in pythonpath.split(";", 1)])
        if path:
            return path
        path = _search_paths(needles, pythonpath.split(";", 1))
        if path:
            return path

    # Try the system paths
    path_found = _search_paths(needles, sys.path)
    if path_found:
        return path_found

    return None


class Utilities(object):
    """The utilities class can be used to discover what utilities are installed
    on the system as well as the usage and options for each utility.

    The list of utilities are read at initialization.

    This class is designed to support the following operations:

        get_util_matches()    - find all utilities that match a prefix
        get_option_matches()  - find all options that match a prefix for a
                                given utility
        get_usage()           - return the usage statement for a given utility
        show_utilities()      - display a 2-column list of utilities and their
                                descriptions
        show_options()        - display a 2-column list of the options for a
                                given utility including the name and
                                description of each option
    """

    def __init__(self, options=None):
        """Constructor
        """
        if options is None:
            options = {}
        self.util_list = []
        self.width = options.get('width', _MAX_WIDTH)
        self.util_path = get_util_path(options.get('utildir', ''))
        self.extra_utilities = options.get('add_util', {})
        self.hide_utils = options.get('hide_util', False)

        self.program_usage = re.compile(RE_USAGE, re.S | re.M)
        self.program_options = re.compile(RE_OPTIONS, re.S | re.M)
        self.program_option = re.compile(RE_OPTION)
        self.program_name = re.compile(RE_ALIAS)

        self.util_cmd_dict = {}
        self.posible_utilities = {}
        self.posible_utilities.update(AVAILABLE_UTILITIES)
        if self.extra_utilities and self.hide_utils:
            self.posible_utilities = self.extra_utilities
        else:
            self.posible_utilities.update(self.extra_utilities)
        self.available_utilities = self.posible_utilities
        for util_name, ver_compatibility in self.posible_utilities.iteritems():
            name_utility = "{0} utility".format(util_name)
            if ver_compatibility:
                min_v, max_v = ver_compatibility
                res = check_python_version(min_version=min_v,
                                           max_version=max_v,
                                           name=name_utility,
                                           print_on_fail=False,
                                           exit_on_fail=False,
                                           return_error_msg=True)
            else:
                res = check_python_version(name=name_utility,
                                           print_on_fail=False,
                                           exit_on_fail=False,
                                           return_error_msg=True)
            if isinstance(res, tuple):
                is_compat, error_msg = res
                if not is_compat:
                    self.available_utilities.remove(util_name)
                    print(WARNING_FAIL_TO_READ_OPTIONS.format(util_name))
                    print("ERROR: {0}\n".format(error_msg))
                    continue
            self._find_utility_cmd(util_name)

    @staticmethod
    def find_executable(util_name):
        """Search the system path for an executable matching the utility

        util_name[in]  Name of utility

        Returns string - name of executable (util_name or util_name.exe) or
                         original name if not found on the system path
        """
        paths = os.getenv("PATH").split(os.pathsep)
        for path in paths:
            new_path = os.path.join(path, util_name + "*")
            if os.name == "nt":
                new_path = '"{0}"'.format(new_path)
            found_path = glob.glob(new_path)
            if found_path:
                return os.path.split(found_path[0])[1]
        return util_name

    def _find_utility_cmd(self, utility_name):
        """ Locate the utility scripts

        util_name[in]   utility to find

        This method builds a dict of commands for invoke the utilities.
        """
        util_path = self.find_executable(os.path.join(self.util_path,
                                                      utility_name))
        util_path_parts = os.path.split(util_path)
        parts = os.path.splitext(util_path_parts[len(util_path_parts) - 1])
        # filter extensions
        exts = ['.py', '.exe', '', 'pyc']
        if (parts[0] not in _EXCLUDE_UTILS and
                (len(parts) == 1 or (len(parts) == 2 and parts[1] in exts))):
            util_name = str(parts[0])
            file_ext = parts[1]
            command = "{0}{1}".format(util_name, file_ext)

            util_path = self.util_path
            utility_path = command
            if not os.path.exists(command):
                utility_path = os.path.join(util_path, utility_name)

            # Now try the extensions
            if not os.path.exists(utility_path):
                if file_ext:
                    utility_path = "{0}{1}".format(utility_path, file_ext)
                else:
                    for ext in exts:
                        try_path = "{0}{1}".format(utility_path, ext)
                        if os.path.exists(try_path):
                            utility_path = try_path

            if not os.path.exists(utility_path):
                print("WARNING: Unable to locate utility {0}."
                      "".format(utility_name))
                print(WARNING_FAIL_TO_READ_OPTIONS.format(util_name))
                return

            # Check for running against .exe
            if utility_path.endswith(".exe"):
                cmd = []
            # Not using .exe
            else:
                cmd = [sys.executable]

            cmd.extend([utility_path])
            self.util_cmd_dict[utility_name] = tuple(cmd)

    def find_utilities(self, this_utils=None):
        """ Locate the utility scripts
        this_utils[in]   list of utilities to find, default None to find all.

        This method builds a list of utilities.
        """

        if not this_utils:
            # Not utilities name to find was passed, find help for all those
            # utilities not previously found in a previos call.
            utils = self.available_utilities
            working_utils = [util['name'] for util in self.util_list]
            if len(working_utils) >= len(self.util_list):
                utils = [name for name in utils if name not in working_utils]
            if len(utils) < 1:
                return
        else:
            # utilities name given to find for, find help for all these which
            # was not previously found in a previos call.
            working_utils = [util['name'] for util in self.util_list]
            utils = [util for util in this_utils if util not in working_utils]
            if len(utils) < 1:
                return

        # Execute the utility command using get_util_info()
        # that returns --help partially parsed.
        for util_name in utils:
            if util_name in self.util_cmd_dict:
                cmd = self.util_cmd_dict.pop(util_name)
                util_info = self.get_util_info(list(cmd), util_name)
                if util_info and util_info["usage"]:
                    util_info["cmd"] = tuple(cmd)
                    self.util_list.append(util_info)
                    working_utils.append(util_name)

        self.util_list.sort(key=lambda util_list: util_list['name'])

    def get_util_info(self, cmd, util_name):
        """Get information about utility

        cmd[in]        a list with the elements that conform the command
                       to invoke the utility
        util_name[in]  name of utility to get information

        Returns dictionary - name, description, usage, options
        """
        cmd.extend(["--help"])
        # rmv print('executing ==> {0}'.format(cmd))
        try:
            proc = subprocess.Popen(cmd, shell=False,
                                    stdout=subprocess.PIPE,
                                    stderr=subprocess.PIPE)
            stdout_temp, stderr_temp = proc.communicate()
            returncode = proc.returncode
        except OSError:
            # always OS error if not found.
            # No such file or directory
            stdout_temp = ""
            returncode = 0

        # Parse the help output and save the information found
        usage = None
        description = None

        if stderr_temp or returncode:
            print(WARNING_FAIL_TO_READ_OPTIONS.format(util_name))
            if stderr_temp:
                print("The execution of the command returned: {0}"
                      "".format(stderr_temp))
            else:
                print("UNKNOWN. To diagnose, exit mysqluc and attempt the "
                      "command: {0} --help".format(util_name))
            return None

        res = self.program_usage.match(stdout_temp.replace("\r", ""))
        if not res:
            print(WARNING_FAIL_TO_READ_OPTIONS.format(util_name))
            print("An error occurred while trying to parse the options "
                  "from the utility")
            return None
        else:
            usage = res.group("Usage").replace("\n", "")
            desc_clean = res.group("Description").replace("\n", " ").split()
            description = (" ".join(desc_clean)) + " "
            # standardize string.
            Options = res.group("Options") + "\n  -"

        # Create dictionary for the information
        utility_data = {
            'name': util_name,
            'description': description,
            'usage': usage,
            'options': Options
        }
        return utility_data

    def parse_all_options(self, utility):
        """ Parses all options for the given utility.

        utility[inout]   that contains the options info to parse
        """
        options_info = utility['options']
        if isinstance(options_info, list):
            # nothing to do if it is a list.
            return

        options = []
        res = self.program_options.findall(options_info)

        for opt in res:
            option = {}
            name = self.program_option.search(opt[0] + " ")
            if name:
                option['name'] = str(name.group(1))
            alias = self.program_name.search(opt[0] + " ")
            if alias:
                option['alias'] = str(alias.group(1))
            else:
                option['alias'] = None

            desc_clean = opt[1].replace("\n", " ").split()
            option['description'] = " ".join(desc_clean)
            option['long_name'] = option['name']
            parts = option['name'].split('=')
            option['req_value'] = len(parts) == 2
            if option['req_value']:
                option['name'] = parts[0]
            if option:
                options.append(option)

        utility['options'] = options

    def get_util_matches(self, util_prefix):
        """Get list of utilities that match a prefix

        util_prefix[in] prefix for name of utility

        Returns dictionary entry for utility based on matching first n chars
        """
        matches = []
        if not util_prefix.lower().startswith('mysql'):
            util_prefix = 'mysql' + util_prefix
        for util in self.available_utilities:
            if util[0:len(util_prefix)].lower() == util_prefix:
                matches.append(util)
        # make sure the utilities description has been found for the matches.
        self.find_utilities(matches)
        matches = [util for util in self.util_list if util['name'] in matches]
        return matches

    def get_option_matches(self, util_info, option_prefix, find_alias=False):
        """Get list of option dictionary entries for options that match
        the prefix.

        util_info[in]     utility information
        option_prefix[in] prefix for option name
        find_alias[in]    if True, match alias (default = False)

        Returns list of dictionary items that match prefix
        """
        # Check type of util_info
        if util_info is None or util_info == {} or \
                not isinstance(util_info, dict):
            raise UtilError("Empty or invalide utility dictionary.")

        matches = []

        stop = len(option_prefix)
        if isinstance(util_info['options'], str):
            self.parse_all_options(util_info)
        for option in util_info['options']:
            if option is None:
                continue
            name = option.get('name', None)
            if name is None:
                continue
            if find_alias:
                if option.get('alias', '') == option_prefix:
                    matches.append(option)
            else:
                if name[0:stop] == option_prefix:
                    matches.append(option)

        return matches

    def show_utilities(self, print_list=None):
        """Show list of utilities as a 2-column list.

        print_list[in]    list of utilities to print - default is None
                          which means print all utilities
        """

        if print_list is None:
            if len(self.util_list) != len(self.available_utilities):
                self.find_utilities()
            list_of_utilities = self.util_list
        else:
            list_of_utilities = print_list
        print
        if len(list_of_utilities) > 0:
            print_dictionary_list(['Utility', 'Description'],
                                  ['name', 'description'],
                                  list_of_utilities, self.width)
        else:
            print
            print "No utilities match the search term."
        print

    def get_options_dictionary(self, utility_options):
        """Retrieve the options dictionary.

        This method builds a new dictionary that contains the options for the
        utilities read.

        utility_options[in]   list of options for utilities or the utility.

        Return dictionary - list of options for all utilities.
        """
        dictionary_list = []

        if isinstance(utility_options, dict):
            if isinstance(utility_options['options'], str):
                # options had not been parsed yet
                self.parse_all_options(utility_options)
            options = utility_options['options']
        else:
            options = utility_options

        for option in options:
            name = option.get('long_name', '')
            if len(name) == 0:
                continue
            name = '--' + name
            alias = option.get('alias', None)
            if alias is not None:
                name = '-' + alias + ", " + name
            item = {
                'long_name': name,
                'description': option.get('description', '')
            }
            dictionary_list.append(item)

        return dictionary_list

    def show_options(self, options):
        """Show list of options for a utility by name.

        options[in]    structure containing the options

        This method displays a list of the options and their descriptions
        for the given utility.
        """
        if len(options) > 0:
            dictionary_list = self.get_options_dictionary(options)
            print
            print
            print_dictionary_list(['Option', 'Description'],
                                  ['long_name', 'description'],
                                  dictionary_list, self.width)
            print

    @staticmethod
    def get_usage(util_info):
        """Get the usage statement for the utility

        util_info[in]  dictionary entry for utility information

        Returns string usage statement
        """
        # Check type of util_info
        if util_info is None or util_info == {} or \
                not isinstance(util_info, dict):
            return False

        return util_info['usage']


def kill_process(pid, force=False, silent=False):
    """This function tries to kill the given subprocess.

    pid [in]    Process id of the subprocess to kill.
    force [in]  Boolean value, if False try to kill process with SIGTERM
                (Posix only) else kill it forcefully.
    silent[in]  If true, do no print message

    Returns True if operation was successful and False otherwise.
    """
    res = True
    if os.name == "posix":
        if force:
            os.kill(pid, subprocess.signal.SIGABRT)
        else:
            os.kill(pid, subprocess.signal.SIGTERM)
    else:
        with open(os.devnull, 'w') as f_out:
            ret_code = subprocess.call("taskkill /F /T /PID {0}".format(pid),
                                       shell=True, stdout=f_out, stdin=f_out)
            if ret_code not in (0, 128):
                res = False
                if not silent:
                    print("Unable to successfully kill process with PID "
                          "{0}".format(pid))
    return res
#
# Copyright (c) 2011, 2013, Oracle and/or its affiliates. All rights reserved.
#
# This program is free software; you can redistribute it and/or modify
# it under the terms of the GNU General Public License as published by
# the Free Software Foundation; version 2 of the License.
#
# This program is distributed in the hope that it will be useful,
# but WITHOUT ANY WARRANTY; without even the implied warranty of
# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
# GNU General Public License for more details.
#
# You should have received a copy of the GNU General Public License
# along with this program; if not, write to the Free Software
# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA 02110-1301 USA
#

"""
This module contains classes and functions used to manage a user-defined
variables.
"""

import re



class Variables(dict):
    """
    The Variables class contains user-defined variables for replacement
    in custom commands.
    """

    def __init__(self, options=None, data=None):
        """Constructor

        options[in]        Width
        data[in]           Data to initialize class
        """
        self.options = options or {}
        self.width = options.get('width', 80)
        super(Variables, self).__init__(data or {})

    def find_variable(self, name):
        """Find a variable

        This method searches for a variable in the list and returns it
        if found.

        name[in]           Name of variable

        Returns dict - variable if found, None if not found.
        """
        if name in self:
            return {name: self[name]}
        return None

    def add_variable(self, name, value):
        """Add variable to the list

        name[in]           Name of variable
        value[in]          Value to store
        """
        self[name] = value

    def get_matches(self, prefix):
        """Get a list of variables that match a prefix

        This method returns a list of the variables that match the first N
        characters specified by var_prefix.

        var_prefix[in]     Prefix for search

        Returns list - matches or [] for no matches
        """
        result = []
        for key, value in self.iteritems():
            if key.startswith(prefix):
                result.append({key: value})
        return result

    def show_variables(self, variables=None):
        """Display variables

        This method displays the variables included in the list passed or all
        variables is list passed is empty.

        variables[in]      List of variables
        """
        if self.options.get("quiet", False):
            return

        var_list = [{'name': key, 'value': value}
                    for key, value in self.iteritems()]

        print "\n"
        if not self:
            print "There are no variables defined.\n"
            return

        print_dictionary_list(['Variable', 'Value'], ['name', 'value'],
                              var_list, self.width)
        print

    def replace_variables(self, cmd_string):
        """Replace all instances of variables with their values.

        This method will search a string for all variables designated by the
        '$' prefix and replace it with values from the list.

        cmd_string[in]     String to search

        Returns string - string with variables replaced
        """
        new_cmd = cmd_string
        finds = re.findall(r'\$(\w+)', cmd_string)
        for variable in finds:
            try:
                new_cmd = new_cmd.replace('$' + variable, str(self[variable]))
            except KeyError:
                # something useful when variable was not found?
                pass
        return new_cmd

    def search_by_key(self, pattern):
        """Find value by key pattern

        pattern[in]    regex pattern

        Returns tuple - key, value
        """
        regex = re.compile(pattern)

        for key, value in self.iteritems():
            if regex.match(key):
                yield key, value
